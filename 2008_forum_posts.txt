<p><a class="mention" href="/u/dnmason">@dnmason</a><br>
Hi I finally managed to try it and I am still getting this error:</p>
<pre><code class="lang-auto">Error:		Image not found while running JACoP from a macro
1-Use "open(path)" in your macro to open images
2-Make sure you have called the right image ! in line 25:
	
		run ( "JACoP " , "imga=[" + list1 [ i ] + "] imgb=[" + list1 [ j ] + "] pearson" &lt;)&gt; ; 

</code></pre>
<p>It’s 2 days that I am trying to deal with this problem, basically it saves the JACoP output for 3 combination and then stops to function.</p>
<p>Thank you again for the previous reply.</p>
<p>Cheers,<br>
George</p> ;;;; <p>It looks good.   What is the intended use?  What modality is the PSF image from?</p>
<p>Would it make sense to calculate the background as the value of the 5th percentile intensity instead of the median?  For the z case the background initial guess was too high.</p>
<p>Brian</p> ;;;; <p>Sure! <a href="https://gunet-my.sharepoint.com/:f:/g/personal/ermir_zulfaj_gu_se/EqMmZ4bLJSNHrXMWaLeUHycBjMBgs2RU3uh2rOx7HQ-cHA?e=eMdj1k" rel="noopener nofollow ugc">Here</a> are two different type of images (to show some variation).</p>
<p>Let me know if you need more, or if the link isn´t working.</p>
<p>Understand. It might be ok even if the second radius extends beyond the tissue. The classifier will exclude this area, and if one reports ratios per segment I guess it is alright:)</p> ;;;; <p>Based on my current coding literacy, it will take a while before I can fully digest your notebook. Thanks for sharing Tim.</p> ;;;; <p>Chatgpt did a good job. Thanks Brian.<br>
I don’t know why but after many attempts I did improve the fitting by using a smaller range of the x_line of [x0-50,x0+50].</p> ;;;; <p>I just went through PSF fitting myself and wrote everything up in <a href="https://github.com/fmi-faim/napari-psf-analysis/blob/3c56c6aa8d22d586a045ac0698d759f2bfbba5dc/scratchpad/PSF%20Fitting.ipynb">this notebook</a>. I would be very happy to get some feedback and further insights <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>I have series of CT and PET DICOMs that are spatially co-registered. I only use OsiriX as a viewer. These series are linked somehow via the metadata, so that when I open them in the OsiriX 2D or 3D viewer the PET is overlaid onto the CT.</p>
<p>When I upload these types of DICOM series to OMERO, they can not be viewed as a spatially co-registered overlay. How can I view them as a spatially co-registered overlay using OMERO? Thanks.</p> ;;;; <p>Hi</p>
<p>It didn’t work for me either, and I figured the solution might be improved by passing an initial guess to curve_fit.  I asked chatgpt how to do that and it output the following code…</p>
<pre><code class="lang-auto">def func(x, a, x0, sigma):
    return a*np.exp(-(x-x0)**2/(2*sigma**2))

from scipy.optimize import curve_fit

# Initial guess for parameters
a_guess = np.max(x_line)
x0_guess = x[np.argmax(x_line)]
sigma_guess = np.std(x)

# Fit the data
popt, pcov = curve_fit(func, x, x_line, p0=[a_guess, x0_guess, sigma_guess])
print(popt)

plt.figure()
plt.plot(x,x_line)
plt.plot(x, func(x, *popt))
</code></pre>
<p>That seemed to improve the solution</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/c/ec1651bba13fbae81b42298de306c1ffa8e3723e.png" alt="image" data-base62-sha1="xGwv74tKx5qODijD4zy4ZTMvoiq" width="600" height="474"></p> ;;;; <p>Good to hear it works as-is. I totally forgot that the pattern logic <em>does</em> allow including “other” groups besides the ones that are actually meaningful to Ashlar! <img src="https://emoji.discourse-cdn.com/twitter/flushed.png?v=12" title=":flushed:" class="emoji" alt=":flushed:" loading="lazy" width="20" height="20"></p>
<p>I see you have a reference to <code>WellA01</code> in the filename. Ashlar can handle plate-based data too – add a <code>{well}</code> group to your pattern and pass the <code>--plates</code> option and you will get one ome-tiff per well. I see A01 twice in the filename so you’ll need to use the same “dummy group” approach to hide the second one. Something like this:</p>
<pre><code class="lang-bash">ashlar 'fileseries...pattern=Well{well}_Point{point}_{series:4}...' --plates
</code></pre> ;;;; <aside class="quote no-group" data-username="weichen" data-post="2" data-topic="78944">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/weichen/40/47733_2.png" class="avatar"> Wei-Chen CHU:</div>
<blockquote>
<p>it format. Therefore, it is necessary to use a 32-bit format to store the final pixel intensity.</p>
</blockquote>
</aside>
<p>Thank you very much for the answer</p> ;;;; <p>Could you provide an original image to test? I suspect this will take a script or at least some scripting, or an involved manual process if you want to run through a few steps on each image.</p>
<p>Depending on consistency, you may also need to decide what to do if your second radius extends out beyond the tissue to contain background.</p> ;;;; <p>Still not sure I completely understand, maybe these are sequential slices or strip stains, but either way, you may want to look into some of the existing QuPath alignment options like</p><aside class="quote quote-modified" data-post="1" data-topic="61803">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/nicokiaru/40/3076_2.png" class="avatar">
    <a href="https://forum.image.sc/t/warpy-registration-of-whole-slide-images-at-cellular-resolution-with-fiji-and-qupath/61803">Warpy - Registration of whole slide images at cellular resolution with Fiji and QuPath</a> <a class="badge-wrapper  bullet" href="/c/announcements/10"><span class="badge-category-bg" style="background-color: #AB9364;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for announcements, such as new software releases and upcoming community events.">Announcements</span></a>
  </div>
  <blockquote>
    Dear community, 
We are happy to share <a href="https://imagej.net/plugins/bdv/warpy/warpy" rel="noopener nofollow ugc">Warpy, a whole slide image registration workflow</a> that can reach cellular resolution over a large area (an area that typically requires more than an affine transformation). It is bridging <a href="https://qupath.github.io/" rel="noopener nofollow ugc">QuPath</a>, <a href="https://imagej.net/software/fiji/downloads" rel="noopener nofollow ugc">Fiji</a> and <a href="https://github.com/SuperElastix/elastix" rel="noopener nofollow ugc">elastix</a>. 
Briefly, a Project is defined in QuPath, and can be opened in Fiji to be registered either automatically, manually with <a href="https://imagej.net/plugins/bigwarp" rel="noopener nofollow ugc">BigWarp</a>, or semi-automatically (BigWarp is used to correct the result of the automated registration). The resulting transformat…
  </blockquote>
</aside>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/qupath/qupath-extension-align">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/qupath/qupath-extension-align" target="_blank" rel="noopener">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/0/50033b9846eaafa1e9d79ed32ba68ff16bae9152_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/0/50033b9846eaafa1e9d79ed32ba68ff16bae9152_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/0/50033b9846eaafa1e9d79ed32ba68ff16bae9152_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50033b9846eaafa1e9d79ed32ba68ff16bae9152.png 2x" data-dominant-color="F1F0EF"></div>

<h3><a href="https://github.com/qupath/qupath-extension-align" target="_blank" rel="noopener">GitHub - qupath/qupath-extension-align: QuPath extension to interactively...</a></h3>

  <p>QuPath extension to interactively align images. Contribute to qupath/qupath-extension-align development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<aside class="quote quote-modified" data-post="1" data-topic="35521">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mike_nelson/40/26532_2.png" class="avatar">
    <a href="https://forum.image.sc/t/qupath-multiple-image-alignment-and-object-transfer/35521">QuPath- Multiple Image Alignment and Object Transfer</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    <details><summary>Hi all!  Here are a set of scripts focused on image alignment (as of 0.2.0M9).</summary>First off, I would like to thank Dirk Homann and Verena van der Heide at Mt. Sinai for having a fun project to work on! Also &lt;a class="mention" href="/u/petebankhead"&gt;@petebankhead&lt;/a&gt; &lt;a class="mention" href="/u/smcardle"&gt;@smcardle&lt;/a&gt; and &lt;a class="mention" href="/u/zbigniew_mikulski"&gt;@Zbigniew_Mikulski&lt;/a&gt; for their conti…</details>
  </blockquote>
</aside>
 ;;;; <p>Thank you!<br>
Truly appreciate for taking your time Mike!<img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Absolutely. Don´t know what is possible. But ideal would be:</p>
<ol>
<li>the latter; “follow the rather bumpy pattern of the innermost section of the heart ROI”</li>
<li>Very similar. Correct, they all have figure 8s, with the larger gap being intended “middle” of the rings. My initial thought was to have an ROI to only include the left heart chamber (subdivided) and a separate for the right chamber (not subdivided);<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/b/ebce4f3911e6ad0103fc669c74dc9d9f5aa5561c.jpeg" alt="image" data-base62-sha1="xE2dJCkwKDUkdVMuuP87D1MU4Qk" width="423" height="373">
</li>
<li>Yes sorry. Equal radii would be more suitable for my objective.</li>
</ol>
<p>I will go through the link!</p>
<p>A million thanks Mike!</p> ;;;; <p>I have the raw .mrxs images that have a shift after each scan so they are not aligned. Also I thought the preprocessing to be 1. Aligning 2. Color deconvolution 3. Merging 4. Invert</p> ;;;; <p>In addition to the kryo 5.4.0 work mentioned above, the <a class="mention-group notify" href="/groups/ome">@ome</a> formats team has identified a number of other potential dependency version conflicts that we will need to evaluate. A summary is here:</p>
<aside class="onebox githubissue" data-onebox-src="https://github.com/ome/bioformats/issues/3970">
  <header class="source">

      <a href="https://github.com/ome/bioformats/issues/3970" target="_blank" rel="noopener">github.com/ome/bioformats</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/ome/bioformats/issues/3970" target="_blank" rel="noopener">6.13.0 dependency updates</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2023-03-27" data-time="16:22:30" data-timezone="UTC">04:22PM - 27 Mar 23 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/melissalinkert" target="_blank" rel="noopener">
          <img alt="melissalinkert" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be29c9d469302e3e81426de707da188f61e2147f.png" class="onebox-avatar-inline" width="20" height="20">
          melissalinkert
        </a>
      </div>
    </div>

    <div class="labels">
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">See https://forum.image.sc/t/plugin-maintainers-can-you-test-fiji-2-11-0/78852
<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">
Comparing the list of updated dependencies in Fiji 2.11.0 to the current state of Bio-Formats, there are several things we'll need to evaluate:

- kryo 5.4.0 (#3967)
- guava 31.1-jre
- version conflicts with minio dependencies
- conflicting commons-io version (dependency of cisd:jhdf5)
- version conflicts with edu.car:cdm-core dependencies
- ij 1.54c (for compiling/testing bio-formats-plugins component)
- jackson 2.14.2
- jaxb-runtime 2.3.5
- joda-time 2.12.2
- json 20230227 (#3966)
- logback 1.2.11 / slf4j 1.7.36 (#3956)
- snakeyaml 1.33 (#3965)
- version conflicts with file-leak-detector (test-suite component only)</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>We will prioritize that work for the next Bio-Formats release (6.13.0), but it will take us some time to work through the complete list.</p> ;;;; <p>Hello everyone,<br>
I have 2 CSV files that contain x-y points coordinates. I also have a calibrated image that has a reference system. I want to use these CSV files to draw 2 curves on the reference image. So I can analyze how fit these curves are with respect to the experimental curve in the image.<br>
A developer previously provided me with a macro, which can draw a curve via points in the CSV file. I want to develop this code to be able to add multiple csv files and draw multiple curves. It also would be great if it is possible to change the color and thickness of the curves. For example, one curve is a yellow dash line and the other is a thick black line. Here you can find the initial macro code:</p>
<p>xpoints = Table.getColumn(“x_points”);<br>
ypoints = Table.getColumn(“y_points”);<br>
toUnscaled(xpoints, ypoints);<br>
makeSelection(“freeline”, xpoints, ypoints);</p>
<p>I also attached the reference image, the same image after using the above code and drawing one curve and two csv files that I want to use. Thank you very much.<br>
<a class="attachment" href="/uploads/short-url/AajR9p6SnqkPJAy5VwfBHF5qyR0.tif">final1.tif</a> (8.7 MB)<br>
<a class="attachment" href="/uploads/short-url/mmKSkCXRBbpSbtXu7A9BE9qRLso.tif">62.tif</a> (8.7 MB)<br>
<a class="attachment" href="/uploads/short-url/riduqkZi6cCEUQTb0V1gPrGvnZd.csv">fitted_points62.csv</a> (32.4 KB)<br>
<a class="attachment" href="/uploads/short-url/zzz7FPzHTvdip79kmX577EHW9yl.csv">fitted_points19.csv</a> (22.3 KB)</p> ;;;; <p>Hi <a class="mention" href="/u/ermir_zulfaj">@Ermir_Zulfaj</a> and welcome to the forum!</p>
<p>In order to help you with your question, it would be nice to specify a few more parameters -</p>
<ol>
<li>Do you intend for the areas to be smooth or follow the rather bumpy pattern of the innermost section of the heart ROI?</li>
<li>How similar are the tissue sections, are they all figure 8s with the largest gap being the intended “middle” of the rings?</li>
<li>The image you have showing the rings is somewhat different from your request, I assume because it was a quick figure - do you want equal areas or equal radii? Concentric areas of equal “width” will always have greater area in the outer rings.</li>
</ol>
<p>Thoughts about the procedure: possibly fill the holes in the annotation, find the largest hole object to establish the center, then expand that “hole object” a certain distance, twice. The distances may not be the same if equal area is desired. That is somewhat similar to what <a href="https://forum.image.sc/t/reduce-annotations-invasion-assay-tumor-margins/24305/25">we worked out here related to tumor margin expansion</a>.</p>
<p>Cheers,<br>
Mike</p> ;;;; <p>Thanks!! I just updated ashlar and ran it with the same command line and it works! I do not think that there is a need for a fix, I just put an arbitrary name for the the second group. That being said, if you need the definition of multiple groups (channels for example), you can use the <a href="https://pypi.python.org/pypi/regex/" rel="noopener nofollow ugc">regex</a> module instead of re.<br>
Thanks again for all your help!</p> ;;;; <aside class="quote no-group" data-username="mporter" data-post="8" data-topic="79096">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/7c8e57/40.png" class="avatar"> Michael Porter:</div>
<blockquote>
<p>One of them is over web sockets, but it works in OMERO-ij, so I wouldn’t imagine that being the problem anyway.</p>
</blockquote>
</aside>
<p>I must say I haven’t tested connections over web sockets. But that would only be 1 server.<br>
That’s weird…</p> ;;;; <p>Yeah, the latest commit on the master branch at <a href="https://github.com/labsyspharm/ashlar" class="inline-onebox" rel="noopener nofollow ugc">GitHub - labsyspharm/ashlar: ASHLAR: Alignment by Simultaneous Harmonization of Layer/Adjacency Registration</a> has the feature. I don’t think the fileseries will work with the tile ID in two places… I have an idea how to fix it though.</p> ;;;; <p>Wow! that was a fast reply!! I appreciate it! Have you already commited those changes? I probably can install ashlar from the git. the ID number is basically a repeat of the tile number.<br>
Thanks again</p> ;;;; <p>Hi,</p>
<p>I installed my OMERO on my desktop. However, I don’t have username and password? I am following the instructions on the link <a href="https://omero-guides.readthedocs.io/en/latest/upload/docs/import-desktop-client.html" rel="noopener nofollow ugc">https://omero-guides.readthedocs.io/en/latest/upload/docs/import-desktop-client.html</a></p>
<p>Thank you for your attention to this matter.<br>
Robert</p> ;;;; <p>Only .tif files are currently supported for the <code>fileseries</code> reader, but I just finished adding support there for all Bioformats-supported formats. This includes .nd2 ! Can you explain more about what the <code>jjjj</code> ID number is and how it varies? Is it different for every image file?</p> ;;;; <p>Hi Fijiyama users !<br>
The block matching rigid registration has been performing terribly compared to previous versions any way to revert back to previous releases?</p> ;;;; <p>Hi <a class="mention" href="/u/jmuhlich">@jmuhlich</a>, I’ve been trying to stitch a set of images (single cycle) that are in nd2 format with the file pattern <code>WellA01_PointA01_{iiii}_Channel405,488,pheno_A594,pheno_Cy5_Seq{jjjj}.nd2</code>, where i is tiles and j is just an ID number. I have been trying the following command:</p>
<pre><code class="lang-auto">ashlar "fileseries|./A01_data/|pattern=WellA01_PointA01_{series:4}_Channel405,488,pheno_A594,pheno_Cy5_Seq{seq:4}.nd2|overlap=0.05|width=7|height=7|layout=snake|pixel_size=0.65" -o new_ashlar.ome.tiff
</code></pre>
<p>however, I get the following error:</p>
<pre><code class="lang-auto">Stitching and registering input images
Cycle 0:
    reading fileseries|./A01_data/|pattern=WellA01_PointA01_{series:4}_Channel405,488,pheno_A594,pheno_Cy5_Seq{seq:4}.nd2|overlap=0.05|width=7|height=7|layout=snake|pixel_size=0.65
Traceback (most recent call last):
  File "/Users/hleaploj/Playground/scry/scry_env/bin/ashlar", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/ashlar/scripts/ashlar.py", line 212, in main
    return process_single(
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/ashlar/scripts/ashlar.py", line 237, in process_single
    reader = build_reader(filepaths[0], plate_well=plate_well)
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/ashlar/scripts/ashlar.py", line 358, in build_reader
    reader = reader_class(path, **kwargs)
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/ashlar/fileseries.py", line 180, in __init__
    self.metadata = FileSeriesMetadata(
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/ashlar/fileseries.py", line 54, in __init__
    self._enumerate_tiles()
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/ashlar/fileseries.py", line 85, in _enumerate_tiles
    img = skimage.io.imread(str(path))
  File "/Users/hleaploj/Library/Python/3.9/lib/python/site-packages/skimage/io/_io.py", line 53, in imread
    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)
  File "/Users/hleaploj/Library/Python/3.9/lib/python/site-packages/skimage/io/manage_plugins.py", line 207, in call_plugin
    return func(*args, **kwargs)
  File "/Users/hleaploj/Library/Python/3.9/lib/python/site-packages/skimage/io/_plugins/imageio_plugin.py", line 15, in imread
    return np.asarray(imageio_imread(*args, **kwargs))
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/imageio/core/functions.py", line 158, in imread
    with imopen()(uri, "ri", format=format) as file:
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/imageio/core/imopen.py", line 126, in __call__
    raise e
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/imageio/core/imopen.py", line 119, in __call__
    plugin_instance = LegacyPlugin(request, **kwargs)
  File "/Users/hleaploj/Playground/scry/scry_env/lib/python3.9/site-packages/imageio/core/imopen.py", line 224, in __init__
    raise ValueError(
ValueError: Could not find a format to read the specified file in ImageMode.single_image mode
</code></pre>
<p>Can you point me to the documentation or a way to resolve this error? I’ve been going through your code to no avail.</p>
<p>Thanks in advance</p> ;;;; <p>Hi <a class="mention" href="/u/wayne">@Wayne</a>,</p>
<p>Is there a trick to run the Inverse FFT on the “FFT Stack”, as this is not recognised as a frequency domain image?</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/5/9525f53941c857029f7f346e740cb29bc8febaf0.png" alt="image" data-base62-sha1="lhqsZceLrgQbHiXAeBQtFLfe3v2" width="261" height="126"></p>
<p>Thanks a lot</p> ;;;; <p>Hi</p>
<p>I’ve tried it with two different servers, both using name rather than IP address. One of them is over web sockets, but it works in OMERO-ij, so I wouldn’t imagine that being the problem anyway.</p> ;;;; <p>Thank you,<br>
I will try it and see if will work.<br>
I am new in more complex macro for Fiji, and probably I am making a small error in the script</p> ;;;; <p>Hi Romain,</p>
<p>I forgot that DiAna requires two images for input. I got the distances using the 3D ImageJ Suite. First, I set the measurements I wanted in 3D Manager Options. Then I thresholded your image, opened 3D Manager and clicked on the Distance button. Is this the kind of table you are looking for?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50b56cc9e86ec9f4f6d2e55f384679a99dc31962.png" data-download-href="/uploads/short-url/bvYTJ1So6vIz6n3eUNe5pmPx4K6.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50b56cc9e86ec9f4f6d2e55f384679a99dc31962.png" alt="image" data-base62-sha1="bvYTJ1So6vIz6n3eUNe5pmPx4K6" width="690" height="77" data-dominant-color="F1F1F2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1540×172 11.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Hope this helps.</p> ;;;; <p>Hi <a class="mention" href="/u/romainguiet">@romainGuiet</a> ,</p>
<p>what a nice challenge! If you want to merge labels depending on their edge-to-edge distance, you could dilate the labels by half that distance, merge those which are touching, and afterwards mask the new label image with the old label image. That should be about 5 lines of Python code. The procedure is demonstrated in <a href="https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/20h_segmentation_post_processing/merging_labels_based_on_edge_to_edge_distance.html">this notebook</a>. All shown operations are also available in CLIJ. Thus, this procedure should be doable in Fiji as well. Note: Isotropic voxels are needed for applying this strategy.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6dc7f34051c5897a93807feeeb99991f650e0a3e.png" data-download-href="/uploads/short-url/fFar0tngAR46XPEQWg47XybwLo2.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6dc7f34051c5897a93807feeeb99991f650e0a3e_2_539x500.png" alt="image" data-base62-sha1="fFar0tngAR46XPEQWg47XybwLo2" width="539" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6dc7f34051c5897a93807feeeb99991f650e0a3e_2_539x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6dc7f34051c5897a93807feeeb99991f650e0a3e_2_808x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6dc7f34051c5897a93807feeeb99991f650e0a3e.png 2x" data-dominant-color="717273"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1040×963 86.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>If you’re interested in merging labels according to their centroid-distances, it is also possible using <a href="https://github.com/clEsperanto/pyclesperanto_prototype">clesperanto</a>. I just wrote a quick <a href="https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/20h_segmentation_post_processing/merging_labels_based_on_centroid_distance.html">notebook</a> demonstrating how to do it. Not all functions I’m using there are available in CLIJ though.</p>
<p>Looking forward to other solutions that may come up in this thread. It’s great challenge <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Best,<br>
Robert</p> ;;;; <p>That’s unfortunate.</p>
<p>Are you using the IP address directly, or a server name?<br>
Does it work with a different server?</p> ;;;; <p>Here are the instructions for reporting bugs in ImageJ. <a href="https://imagej.net/discuss/bugs" class="inline-onebox" rel="noopener nofollow ugc">Reporting Issues</a></p> ;;;; <p>Hi <a class="mention" href="/u/pierre.pouchin">@pierre.pouchin</a></p>
<p>Unfortunately, no. I notice that there is no longer a comma in the port input, but I get the same error as before.</p>
<p>Thanks, and I’m happy to test other things out too.</p>
<p>Michael.</p> ;;;; <p>Hello!</p>
<p>I have trained a pixel classifier using QuPath on transverse histological sections of hearts. It is performing well. For the next step, I would like compare different regions within the heart (innermost vs outer) in a standardised manner.</p>
<p>Example images. Inner/blue vs outer/yellow.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/2/8274c89eddb88c31a70658dccf3dc1fb42820177.jpeg" alt="image" data-base62-sha1="iC4hizKRR4QsBnitWJGAwa9UQBx" width="407" height="434"><br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/9800e98aa1d7aa9d0b4ea6a9cfc116c6e2e7514b.jpeg" alt="image" data-base62-sha1="lGGwRtElDAJBQYXXUHJDVxU1kkb" width="400" height="416"></p>
<p>I was thinking that one could subdivide the ROI of the heart into 2 regions (same area, inner vs outer), use the classifier, and then compare the regions. These links seem to be appropriate for subdividing a selected ROI (<a href="https://gist.github.com/petebankhead/e177b07784460a76d70f89347edd988d" class="inline-onebox" rel="noopener nofollow ugc">Split a rectangle into equal parts along its longest dimension in QuPath v0.2. · GitHub</a> and <a href="https://petebankhead.github.io/qupath/scripts/2018/08/08/three-regions.html" class="inline-onebox" rel="noopener nofollow ugc">Creating annotations around the tumor | Pete’s blog</a>).</p>
<p>Is it possible to adjust the script for an annulus shaped object? Or you have any other suggestions for achieving this?</p>
<p>kindest,</p> ;;;; <p>My mistake. this file mistakenly had the SUBIFD set but no subres IFD anymore (code commented out).<br>
The file worked ok after commenting out the SUBIFD tag as well.<br>
Thanks to you both for troubleshooting that!<br>
Great work!</p> ;;;; <p>Hi <a class="mention" href="/u/gopherconfocal">@GopherConfocal</a> , thank you for your swift answer!</p>
<p>I guess you refer to the <code>DiAna_Analyse</code>?<br>
When I tested it, I needed to specify the max number of neighbor (I set to 100 but ideally I won’t have to because I don’t know how many I can have) and it outputs a table that I would need to parse (like “3D Suite” but a bit different see below)</p>
<p>DiAna Table<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/172f517514e4b10ad3890f0681894c24e232c34d.png" data-download-href="/uploads/short-url/3j6mm4uW6a2GyaPHtwtCTytcSex.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/172f517514e4b10ad3890f0681894c24e232c34d_2_456x500.png" alt="image" data-base62-sha1="3j6mm4uW6a2GyaPHtwtCTytcSex" width="456" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/172f517514e4b10ad3890f0681894c24e232c34d_2_456x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/172f517514e4b10ad3890f0681894c24e232c34d_2_684x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/172f517514e4b10ad3890f0681894c24e232c34d_2_912x1000.png 2x" data-dominant-color="F6F6F6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">954×1046 81.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>3D Suite table<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2c5287b42c1ca6533d541eeb096ba8880853197.png" data-download-href="/uploads/short-url/ndVL5KnWq6Jp2ZiOJj9QKtZqWAn.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c5287b42c1ca6533d541eeb096ba8880853197_2_656x500.png" alt="image" data-base62-sha1="ndVL5KnWq6Jp2ZiOJj9QKtZqWAn" width="656" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c5287b42c1ca6533d541eeb096ba8880853197_2_656x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c5287b42c1ca6533d541eeb096ba8880853197_2_984x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c5287b42c1ca6533d541eeb096ba8880853197_2_1312x1000.png 2x" data-dominant-color="EEEEEE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1534×1169 246 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Is there an option to output a matrix ?</p>
<p>BEst</p>
<p>Romain</p> ;;;; <aside class="quote no-group" data-username="Delpierre" data-post="7" data-topic="78907">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/d/f05b48/40.png" class="avatar"> Delpierre:</div>
<blockquote>
<p><a class="attachment" href="/uploads/short-url/cjiF4QhotiKVny9KxZkzkwi1GIl.tif">A1_01_1_1_Transmitted_001.tif</a> (14.3 MB)</p>
</blockquote>
</aside>
<p>That file is corrupted. It has a SubIFDs tag with a value of 0, which crashes OME-TIFF readers.</p> ;;;; <p>The <a href="https://imagej.net/plugins/distance-analysis" rel="noopener nofollow ugc">Distance Analysis (DiAna)</a> plugin should do what you’re looking for. It works with 3D data. You also need to have the 3D ImageJ suite of plugins installed as well.</p> ;;;; <p>Dear Community,</p>
<h3>
<a name="sample-image-andor-code-1" class="anchor" href="#sample-image-andor-code-1"></a>Sample image and/or code</h3>
<p>I have a label image of objects in 3D (see below)</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/e/7e75ed1cb97642b637eb9717629105ead22ac3d9.png" data-download-href="/uploads/short-url/i2IOEGJgSs0zIf2M3FxFLxhWMMp.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e75ed1cb97642b637eb9717629105ead22ac3d9_2_387x375.png" alt="image" data-base62-sha1="i2IOEGJgSs0zIf2M3FxFLxhWMMp" width="387" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e75ed1cb97642b637eb9717629105ead22ac3d9_2_387x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e75ed1cb97642b637eb9717629105ead22ac3d9_2_580x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/e/7e75ed1cb97642b637eb9717629105ead22ac3d9.png 2x" data-dominant-color="160A10"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">644×623 108 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><a class="attachment" href="/uploads/short-url/hLvS4Zc72RpN6m0mFhDrV7D2TUC.tif">label.tif</a> (14.4 MB)</p>
<h3>
<a name="analysis-goals-2" class="anchor" href="#analysis-goals-2"></a>Analysis goals</h3>
<p>I would like to “cluster” these objects based on their distance from one to the other.<br>
Let’s say that if objects are distant of less than 1 micron, then they all belong to the same cluster.<br>
The <em>final</em> output would be a new label image, with label equals to the cluster number.</p>
<p>I looked into a couple of existing tools :</p>
<ul>
<li>
<code>3D Suite</code> and <code>DiAna</code>, <a class="mention" href="/u/thomasboudier">@ThomasBoudier</a>
</li>
<li>
<code>clij, clij2, clijx</code>, <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a>
</li>
<li>
<code>MorpholibJ</code>, <a class="mention" href="/u/dlegland">@dlegland</a>
</li>
</ul>
<p>I think that “none” of them is doing what I’m looking for …<br>
BUT I have the strange memory of a plugin that can output a matrix of the distances between objects<br>
BUT I can’t find it !!! <img src="https://emoji.discourse-cdn.com/twitter/sob.png?v=12" title=":sob:" class="emoji" alt=":sob:" loading="lazy" width="20" height="20"><br>
Was it just a dream ? <img src="https://emoji.discourse-cdn.com/twitter/thinking.png?v=12" title=":thinking:" class="emoji" alt=":thinking:" loading="lazy" width="20" height="20"></p>
<p>The closest function I found is in <code>3D Suite</code> but it outputs a table that I would have to “parse”</p>
<p>Thank you for your inputs <img src="https://emoji.discourse-cdn.com/twitter/pray.png?v=12" title=":pray:" class="emoji" alt=":pray:" loading="lazy" width="20" height="20"></p>
<p>Cheers,</p>
<p>Romain</p> ;;;; <p>I am having a similar issue if anyone is able to give advice. The error I’m getting is included below. What is odd is that this is a communal computer where everyone has their own login credentials. Other people are able to use LabKit just fine, so I assume that the software is up-to-date. I would guess it has to be something to do with my specific settings?</p>
<p>Any help would be appreciated. Thank you.</p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.54c; Java 1.8.0_172 [64-bit]; Windows 10 10.0; 265MB of 98115MB (&lt;1%)</p>
<p>java.lang.reflect.InvocationTargetException<br>
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>
at java.lang.reflect.Method.invoke(Method.java:498)<br>
at ij.macro.Functions.call(Functions.java:4621)<br>
at ij.macro.Functions.getStringFunction(Functions.java:277)<br>
at ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)<br>
at ij.macro.Interpreter.getString(Interpreter.java:1498)<br>
at ij.macro.Interpreter.doStatement(Interpreter.java:336)<br>
at ij.macro.Interpreter.doStatements(Interpreter.java:267)<br>
at ij.macro.Interpreter.run(Interpreter.java:163)<br>
at ij.macro.Interpreter.run(Interpreter.java:93)<br>
at ij.macro.Interpreter.run(Interpreter.java:107)<br>
at ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)<br>
at Imaris_Bridge$cEval.RunMacro(Imaris_Bridge.java:80)<br>
at Imaris_Bridge$cEval.lambda$Execute$1(Imaris_Bridge.java:62)<br>
at java.lang.Thread.run(Thread.java:748)<br>
Caused by: java.lang.NoSuchMethodError: bdv.ui.BdvDefaultCards.setup(Lbdv/ui/CardPanel;Lbdv/viewer/ViewerPanel;Lbdv/viewer/ConverterSetups;)V<br>
at bdv.util.BdvHandlePanel.(BdvHandlePanel.java:115)<br>
at sc.fiji.labkit.ui.BasicLabelingComponent.initBdv(BasicLabelingComponent.java:89)<br>
at sc.fiji.labkit.ui.BasicLabelingComponent.(BasicLabelingComponent.java:77)<br>
at sc.fiji.labkit.ui.plugin.imaris.ImarisSegmentationComponent.(ImarisSegmentationComponent.java:86)<br>
at sc.fiji.labkit.ui.plugin.imaris.LabkitImarisPlugin.show(LabkitImarisPlugin.java:166)<br>
at sc.fiji.labkit.ui.plugin.imaris.LabkitImarisPlugin.run(LabkitImarisPlugin.java:110)<br>
at sc.fiji.labkit.ui.plugin.imaris.LabkitImarisPlugin.imageFromImaris(LabkitImarisPlugin.java:247)<br>
… 17 more</p> ;;;; <aside class="quote no-group" data-username="pj_saez" data-post="3" data-topic="79115">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/pj_saez/40/67317_2.png" class="avatar"> Pablo J. Sáez:</div>
<blockquote>
<p>Even if big-stitcher works, would it not be faster anyways using the GPU with clij?</p>
</blockquote>
</aside>
<p>Bigstitcher can be blazingly fast. Give it a try! We haven’t developed stitching in clij yet because Bigstitcher is so amazing.</p> ;;;; <p>Hello <a class="mention" href="/u/marina_herrero">@Marina_Herrero</a></p>
<p>I do not know too much about the error we get. In particular the</p>
<p><code>at ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:221)</code></p>
<p>lines that indicate a custom file reader is used. Normally with MaMuT we use the classic BDV reader.<br>
If this reader does not work, can you create a new topic with this issue?</p>
<p>thanks</p> ;;;; <p>Thanks <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a> !</p>
<p>Even if big-stitcher works, would it not be faster anyways using the GPU with clij?<br>
We will try with <a class="mention" href="/u/jboixcampos">@JBoixCampos</a> the big-stitcher and see how it works with time-lapse sequences <img src="https://emoji.discourse-cdn.com/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"></p>
<p>Best,</p>
<p>Pablo</p> ;;;; <p>Ok, this is weird.</p>
<p>I’ve made a small modification (not pushed yet): does the plugin work with the following file instead?<br>
<a class="attachment" href="/uploads/short-url/MFM8Omeo4xB49FepSCrmq1SAPS.jar">omero_batch-plugin-1.0.6-SNAPSHOT.jar</a> (50.1 KB)</p> ;;;; <p>A little confused, do the original mrxs files not have aligned channels?</p> ;;;; <p><strong>Hello <a class="mention" href="/u/tinevez">@tinevez</a> ,</strong></p>
<p><strong>My name is Marina Herrero and I’m currently doing my master’s thesis using Mamut. I’ve been trying to open a new Mamut annotation and the error message I am getting is as follows:</strong></p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 54MB of 12057MB (&lt;1%)</p>
<p>ch.systemsx.cisd.hdf5.exceptions.HDF5FileNotFoundException: Path does not exit. (C:\Users\lab185\Desktop\MASTER TFM\Prácticas.\2021-12-08_16.49.16_ATHMP41-NLS-3xYFP_x_membraneRed_Movie2-track-track-crop.h5)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:221)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:177)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:155)<br>
at ch.systemsx.cisd.hdf5.HDF5ReaderConfigurator.reader(HDF5ReaderConfigurator.java:81)<br>
at ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.openForReading(HDF5FactoryProvider.java:55)<br>
at ch.systemsx.cisd.hdf5.HDF5Factory.openForReading(HDF5Factory.java:54)<br>
at bdv.img.hdf5.Hdf5ImageLoader.open(Hdf5ImageLoader.java:183)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:389)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:79)<br>
at bdv.BigDataViewer.initSetupNumericType(BigDataViewer.java:279)<br>
at bdv.BigDataViewer.initSetups(BigDataViewer.java:311)<br>
at fiji.plugin.mamut.SourceSettings.(SourceSettings.java:101)<br>
at fiji.plugin.mamut.NewMamutAnnotationPlugin.run(NewMamutAnnotationPlugin.java:103)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p>
<p><strong>I’ve tried updating, reinstalling the software and plugins from start in another pc and the same error appears. Don’t know what to do, a few months ago it worked perfeclty fine.</strong></p>
<p><strong>I’ve tried oppening an old annotation and same thing:</strong></p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 97MB of 12057MB (&lt;1%)</p>
<p>hdf.hdf5lib.exceptions.HDF5FileInterfaceException: Unable to open file ["f:\jhdf5\c\build\cmake-hdf5-1.10.5\hdf5-1.10.5\src\h5fdsec2.c line 346 in H5FD_sec2_open(): unable to open file: name = ‘C:\Users\lab185\Desktop\MASTER TFM\PrÃ¡cticas\DR4.\DC_CROPPED.h5’, errno = 2, error message = ‘No such file or directory’, flags = 0, o_flags = 0<br>
"]<br>
at hdf.hdf5lib.H5.H5Fis_hdf5(Native Method)<br>
at ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.isHDF5File(HDF5FactoryProvider.java:61)<br>
at ch.systemsx.cisd.hdf5.HDF5Factory.isHDF5File(HDF5Factory.java:106)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:231)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:177)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:155)<br>
at ch.systemsx.cisd.hdf5.HDF5ReaderConfigurator.reader(HDF5ReaderConfigurator.java:81)<br>
at ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.openForReading(HDF5FactoryProvider.java:55)<br>
at ch.systemsx.cisd.hdf5.HDF5Factory.openForReading(HDF5Factory.java:54)<br>
at bdv.img.hdf5.Hdf5ImageLoader.open(Hdf5ImageLoader.java:183)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:389)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:79)<br>
at bdv.BigDataViewer.initSetupNumericType(BigDataViewer.java:279)<br>
at bdv.BigDataViewer.initSetups(BigDataViewer.java:311)<br>
at fiji.plugin.mamut.SourceSettings.(SourceSettings.java:101)<br>
at fiji.plugin.mamut.io.MamutXmlReader.readSourceSettings(MamutXmlReader.java:172)<br>
at fiji.plugin.mamut.LoadMamutAnnotationPlugin.load(LoadMamutAnnotationPlugin.java:103)<br>
at fiji.plugin.mamut.LoadMamutAnnotationPlugin.run(LoadMamutAnnotationPlugin.java:84)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p>
<p><strong>Any help at all would be greatly appreciated.</strong></p>
<p><strong>Thank you very much,</strong></p>
<p><strong>Marina Herrero</strong></p> ;;;; <p>Hi <a class="mention" href="/u/jboixcampos">@JBoixCampos</a> ,</p>
<p>to help you best, it would be great if you could share an example dataset so that others can try potential solutions before proposing them.</p>
<aside class="quote no-group" data-username="JBoixCampos" data-post="1" data-topic="79115">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jboixcampos/40/69482_2.png" class="avatar"> Javier Boix Campos:</div>
<blockquote>
<p>ImageJ relies on the CPU to run its processes, so it can run short of memory</p>
</blockquote>
</aside>
<p>Yeah, when switching to using GPUs, the memory problem becomes harder actually, because GPUs have less memory than common image processing workstations.</p>
<aside class="quote no-group" data-username="JBoixCampos" data-post="1" data-topic="79115">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jboixcampos/40/69482_2.png" class="avatar"> Javier Boix Campos:</div>
<blockquote>
<p>However, there is one function that I have not been able to find in the CLIJ documentation (<a href="https://clij.github.io/">https://clij.github.io/</a>) that I would like to use: stitching.</p>
</blockquote>
</aside>
<p>Stitching has not been implemented in CLIJ. It may also not be necessary (see other options below)</p>
<aside class="quote no-group" data-username="JBoixCampos" data-post="1" data-topic="79115">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jboixcampos/40/69482_2.png" class="avatar"> Javier Boix Campos:</div>
<blockquote>
<p>Specifically, we were using the Grid/Collection Stitching with the Sequential Images configuration</p>
</blockquote>
</aside>
<p>The grid-collection stitching plugin has been further developed over the years. Have you tried <a href="https://imagej.net/plugins/bigstitcher/">big-stitcher</a>? I’m pretty sure it’s what you’re looking for <img src="https://emoji.discourse-cdn.com/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"></p>
<p>Best,<br>
Robert</p> ;;;; <p>I think after using getROI to get the ROI, you can check it’s name or type. I think name since the types are simpler (area, line etc.)</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://qupath.github.io/javadoc/docs/qupath/lib/roi/PolygonROI.html">
  <header class="source">

      <a href="https://qupath.github.io/javadoc/docs/qupath/lib/roi/PolygonROI.html" target="_blank" rel="noopener">qupath.github.io</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://qupath.github.io/javadoc/docs/qupath/lib/roi/PolygonROI.html" target="_blank" rel="noopener">PolygonROI (QuPath 0.4.0)</a></h3>

  <p>declaration: package: qupath.lib.roi, class: PolygonROI</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>untested code from eyeballing the javadocs</p>
<pre><code class="lang-auto">for (annotation in annotations){
print annotation.getROIName().getName()

}
</code></pre> ;;;; <p>Thank you so much <a class="mention" href="/u/wayne">@Wayne</a> for the help</p> ;;;; <p>Thank you very much! Your script helps me a lot!</p> ;;;; <p><a class="attachment" href="/uploads/short-url/cjiF4QhotiKVny9KxZkzkwi1GIl.tif">A1_01_1_1_Transmitted_001.tif</a> (14.3 MB)</p>
<p>Can you try with this one?</p> ;;;; <p>Hi,</p>
<p>ImageJ relies on the CPU to run its processes, so it can run short of memory and in some processes it is not as fast as it could be if it used a GPU. Therefore, CLIJ was developed so that the GPU can be used for various ImageJ processes.</p>
<p>However, there is one function that I have not been able to find in the CLIJ documentation (<a href="https://clij.github.io/" rel="noopener nofollow ugc">https://clij.github.io/</a>) that I would like to use: stitching. ImageJ has several plugins for stitching, such as Grid/Collection Stitching or Pairwise Stitching, but they all use CPU and when working on lots of big files the PC may run out of memory. Specifically, we were using the Grid/Collection Stitching with the Sequential Images configuration, and we were able to see the PC struggling, as you can see in the image below. The CPU is at 100% while the GPU is not being used at all by ImageJ.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f972ee6fb3e52d1123835520ac37dcab51b73d2.jpeg" data-download-href="/uploads/short-url/fVaUhq4KrgBMSeLhLXP9GXesyno.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f972ee6fb3e52d1123835520ac37dcab51b73d2_2_690x471.jpeg" alt="image" data-base62-sha1="fVaUhq4KrgBMSeLhLXP9GXesyno" width="690" height="471" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f972ee6fb3e52d1123835520ac37dcab51b73d2_2_690x471.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f972ee6fb3e52d1123835520ac37dcab51b73d2_2_1035x706.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f972ee6fb3e52d1123835520ac37dcab51b73d2_2_1380x942.jpeg 2x" data-dominant-color="59606C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1311 145 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><a class="mention" href="/u/pj_saez">@pj_saez</a> and I were wondering if there is a way to do large image stitching via GPU instead of CPU using CLIJ or some other library. Does <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a> or anyone know if this possibility exists or if it could be implemented in the future? Thanks for your attention and time <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi,</p>
<p>I am trying to export a registered TIFF stack in 16-bit format from TrakEM2.<br>
I used the script described here: <a href="https://forum.image.sc/t/exporting-registered-tiff-stack-from-trakem2/5770/8" class="inline-onebox">Exporting registered tiff stack from TrakEM2 - #8 by albertcardona</a><br>
However, this is exporting all patches in all layers, even the patches that are made invisible.</p>
<p>In my project, each layer contains 3 patches, which are 3 channels of the same image. I would like to export 1 channel at a time, and thus made only 1 channel visible before exporting. This worked for the built in function ‘make flat image’, but does not work for the script.</p>
<p>Could anyone give information on how to only export the visible patches to a 16-bit TIFF with a script, or another way to reach my goal?</p>
<p>Thank you in advance.</p> ;;;; <p><a class="mention" href="/u/will-moore">@will-moore</a> Ah, my bad. I meant that if we’re using the projection subgroups, the metadata about what is in the main 3D group and what goes into the 2D subgroup could be in the <code>multiscales</code> of the main folder. As in, the <code>multiscales</code> of the whole image has the information that there are 2 OME-Zarr images present in the folder. The one in the main folder and the one in the <code>projections</code> folder.</p>
<p>Though I guess the projections folder also needs to have a <code>multiscales</code> dict and that would need to be in the subfolder, so maybe that suggestion doesn’t make much sense?</p>
<p>My motivation behind it was, that it should be easy e.g. for a reader to know, what types of subgroups exist, so it could selectively load just one of them.</p> ;;;; <p><a class="mention" href="/u/jluethi">@jluethi</a> I understood your example of <code>multiscales</code> list containing 3D and 2D images as an alternative to the <code>projections</code> subgroup?</p> ;;;; <p>Good morning.</p>
<p>I realized that Leica´s .lif coming from a color camera looks strange when I opened them in ImageJ.</p>
<p>It was changing the red and blue channels, so the image resulted in a blueish image:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/768e8624b3c44ac7065a2a55d414d6ce2340a404.jpeg" data-download-href="/uploads/short-url/gUNHgdZgL5ogPYclOyIT90FBhU8.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/768e8624b3c44ac7065a2a55d414d6ce2340a404_2_491x500.jpeg" alt="image" data-base62-sha1="gUNHgdZgL5ogPYclOyIT90FBhU8" width="491" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/768e8624b3c44ac7065a2a55d414d6ce2340a404_2_491x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/768e8624b3c44ac7065a2a55d414d6ce2340a404_2_736x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/768e8624b3c44ac7065a2a55d414d6ce2340a404.jpeg 2x" data-dominant-color="C3CACF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">890×905 75.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>While the expected outcome is:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/101b78cbb2110bd8096364dcef822e3c08feec3c.jpeg" data-download-href="/uploads/short-url/2iuuRcF0YNFx9B0qQP465S8U2F6.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/101b78cbb2110bd8096364dcef822e3c08feec3c_2_491x500.jpeg" alt="image" data-base62-sha1="2iuuRcF0YNFx9B0qQP465S8U2F6" width="491" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/101b78cbb2110bd8096364dcef822e3c08feec3c_2_491x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/101b78cbb2110bd8096364dcef822e3c08feec3c_2_736x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/101b78cbb2110bd8096364dcef822e3c08feec3c.jpeg 2x" data-dominant-color="D0CBC3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">890×905 74.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I resolved this issue by splitting the channels while I open the image and then <code>run("Merge Channels..."</code>, reordering them.</p>
<p>However, I was wondering if there is a way with the Macro extension to change the channel order to create the composite while it opens the file?</p>
<p>Thanks in advance for your help.</p>
<p>I</p> ;;;; <aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/238;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2be12cefa538364d4c40804a4281946d25ac40e1_2_690x238.png" class="thumbnail" width="690" height="238" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2be12cefa538364d4c40804a4281946d25ac40e1_2_690x238.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2be12cefa538364d4c40804a4281946d25ac40e1.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2be12cefa538364d4c40804a4281946d25ac40e1.png 2x" data-dominant-color="338E95"></div>

<h3><a href="https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere" target="_blank" rel="noopener nofollow ugc">Using labeled data in DeepLabCut that was annotated elsewhere</a></h3>

  <p>Official implementation of DeepLabCut: Markerless pose estimation of user-defined features with deep learning for all animals incl. humans - DeepLabCut/DeepLabCut</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Are these instructions still relevant?</p>
<p>Thanks <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>hi <a class="mention" href="/u/george_alex">@George_Alex</a>,</p>
<p>I think the problem is that when you are looking for your second image, you are always starting at the image after the first image. If you want a truly all-to-all (ignoring comparing to self) you should set up your <code>j</code> loop to iterate through the whole of <code>list1</code>.<br>
To prevent running the same image against itself, I would suggest something like an <code>if</code> loop within your <code>j</code> loop to check if <code>i==j</code>.</p>
<p>Should look something like this:</p>
<pre><code class="lang-auto">setBatchMode(true);
file1 = getDirectory("Select a folder containing images");
list1 = getFileList(file1);
n1 = list1.length;
file3 = getDirectory("Output");
for (i=0; i&lt;n1; i++){
	name = substring(list1[i], 0, lengthOf(list1[i])-4); //remove the extension
	open(file1 + list1[i]);
	for (j=0; j&lt;n1; j++){
//-- ADDED IN THIS FUNCTION TO CHECK THAT i DOES NOT EQUAL j
		if (i!=j) {
		open(file1 + list1[j]);
		run("JACoP ", "imga=["+list1[i] + "] imgb=["+list1[j]+"] pearson");
		saveAs("Text", file3 + name + "-" + substring(list1[j], 0, lengthOf(list1[j])-4) +"_Coloc.txt");
		run("Close");
		} //-- CLOSE IF
	}
	
}
run("Close All");
</code></pre>
<p>Hope that makes sense, and that I’ve understood your problem correctly.</p> ;;;; <p>I need the width with an orientation system based on the longest axis of the embryos. The length indeed I measured it with the geodesic diameter. I just used the BoneJ and does exactly what I need. Thank you for your assistance!!</p> ;;;; <p>That’s good to hear <a class="mention" href="/u/will-moore">@will-moore</a> and makes sense that multiscales hasn’t been used much for that in viewers so far.</p>
<blockquote>
<p>In that case, if you had <code>labels/</code> for <em>only</em> the 2D image, I wonder if you could store them without breaking the spec (which doesn’t allow 2D labels under 3D image), or if the spec would need to change to allow that?</p>
</blockquote>
<p>In that case, wouldn’t the labels then only be in the projections folder anyway, i.e. something like this:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── projections    
    │   │   │   │   ├── 0        # zeroth resolution level projection
    │   │   │   │   └── labels
    │   │   │   │       └── 0    # labels corresponding to A/1/0/projections/0
</code></pre>
<p>In that case, the 3D image doesn’t have labels and doesn’t have a labels subgroup. But the projections subgroup does have its own labels subgroup and one can load the 2D images with the 2D labels.<br>
Whether one could (or should be allowed to) load the 2D labels from the projections together with the 3D data is an additional question for me.</p>
<p>Or where would it break the spec if the projection subgroup comes with its own labels that are part of that subgroup?</p> ;;;; <p>obviously it is 2023, I failed to admit that <img src="https://emoji.discourse-cdn.com/twitter/face_with_peeking_eye.png?v=12" title=":face_with_peeking_eye:" class="emoji" alt=":face_with_peeking_eye:" loading="lazy" width="20" height="20"></p> ;;;; <p>Maybe there is a misunderstanding. I thought that for the length, you are using the geodesic diameter. For the width, you are finding the largest circle that will fit into the shape (with the “local thickness”), and take the width as the diameter of that circle. In your original workflow, you had to threshold the image again to find the widest circle (step 3 onwards) - the methods above will just give you the widest circle (ie the width of the shape) without the additional thresholding and re-measuring steps (ie, you can measure directly after step 2).</p> ;;;; <p>Hi <a class="mention" href="/u/dgault">@dgault</a>!</p>
<p>Thanks for picking up on this issue and sorry for the delayed response. I have now updated the Bio-Formats to 6.12 and the issue persists.</p>
<p>I have uploaded some exemplary data from myself on figshare:<br>
FLIM_testdata <a href="https://doi.org/10.6084/m9.figshare.22336594.v1" class="inline-onebox" rel="noopener nofollow ugc">FLIM testdata</a></p>
<p>In the dataset, you will find the lif file (FLIM_testdata.lif) with one image (sample1_slice1) and a folder with tifs exported with LAS X (LASXv3.5.6_exported/tifs) and a few supplementary screenshots.</p>
<p>To re-iteratee: the issue is that the lifetime image exported with LAS X has very different pixel values that the one imported with BioFormats and I can’t quite figure out what conversion/compression is performed on the file and why the values are different. From what I know about the lifetime of the probe that I am using, the values exported with LAS X make more sense (there should be two peaks, one around 1.5 ns and one in the range 4-6 ns).</p>
<p>Do let me know how it looks for you and again many thanks for looking into this!</p>
<p>Marta</p>
<p>Further details:</p>
<p>I export the lifetime image in LAS X (LAS X FLIM FACS Offline, this image was exported with v3.5.6, but I had same issues with v4.6) by right clicking on the image and selecting “Export Raw Image”, two following tif files are saved (LASXv3.5.6_exported/tifs):</p>
<ul>
<li>filename_ch0 - intensity [cnts]</li>
<li>filename_ch1 - lifetime [ns]<br>
These files are uint16 which is fine for counts but bad for lifetime (they have float values). My workaround is to use a conversion factor of 0.001 for lifetime to not loose information (see LASX_ExportRawImage.png). I have exported one file with conversion and one without (“no scaling”).</li>
</ul>
<p>When I import the FLIM_testdata.lif to Fiji using Bio-Formats plugin, I see the following list of files:</p>
<ul>
<li>‘sample1_slice1’,</li>
<li>‘sample1_slice1/FLIM Compressed/Phasor Plot’,</li>
<li>‘sample1_slice1/FLIM Compressed/Pattern Matching Scatter Plot Channel 1’,</li>
<li>‘sample1_slice1/FLIM Compressed/Intensity’,</li>
<li>‘sample1_slice1/FLIM Compressed/Fast Flim’,</li>
<li>‘sample1_slice1/FLIM Compressed/Standard Deviation’,</li>
<li>‘sample1_slice1/FLIM Compressed/Phasor Real’,</li>
<li>‘sample1_slice1/FLIM Compressed/Phasor Imaginary’,</li>
<li>‘sample1_slice1/FLIM Compressed/Phasor Intensity’,</li>
<li>‘sample1_slice1/FLIM Compressed/Phasor Mask’<br>
I believe that the ‘sample1_slice1/FLIM Compressed/Fast Flim’ is the file that should contain the lifetime data. The pixel values that I see for this image (see ‘LASXv3.5.6_exported/Histograms/Histogram_lif_ImageJ_FLIM_testdata_sample1_slice1_FLIMCompressed_FastFlim.png’) are completely different from the ones I see for the ch1 exported with LAS X (‘LASXv3.5.6_exported/Histograms/Histogram_LASXexported_sample1_slice1_ch1.png’).</li>
</ul> ;;;; <p><a class="mention" href="/u/jbehnsen">@jbehnsen</a> <a class="mention" href="/u/emartini">@emartini</a> Something that I didnt mention is that I want the measurement to follow the longest axis of the embryo. Most of the measurements on FIJI except the oriented bounding box they seem to give me a result according to the orientation system of the image. I think its the same with your suggestions? (If I employed them correctly).</p> ;;;; <p><strong>Hello <a class="mention" href="/u/tinevez">@tinevez</a> ,</strong></p>
<p><strong>My name is Marina Herrero and I’m currently doing my master’s thesis using Mamut. I’ve been trying to open a new Mamut annotation and the error message I am getting is as follows:</strong></p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 54MB of 12057MB (&lt;1%)</p>
<p>ch.systemsx.cisd.hdf5.exceptions.HDF5FileNotFoundException: Path does not exit. (C:\Users\lab185\Desktop\MASTER TFM\Prácticas.\2021-12-08_16.49.16_ATHMP41-NLS-3xYFP_x_membraneRed_Movie2-track-track-crop.h5)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:221)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:177)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:155)<br>
at ch.systemsx.cisd.hdf5.HDF5ReaderConfigurator.reader(HDF5ReaderConfigurator.java:81)<br>
at ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.openForReading(HDF5FactoryProvider.java:55)<br>
at ch.systemsx.cisd.hdf5.HDF5Factory.openForReading(HDF5Factory.java:54)<br>
at bdv.img.hdf5.Hdf5ImageLoader.open(Hdf5ImageLoader.java:183)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:389)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:79)<br>
at bdv.BigDataViewer.initSetupNumericType(BigDataViewer.java:279)<br>
at bdv.BigDataViewer.initSetups(BigDataViewer.java:311)<br>
at fiji.plugin.mamut.SourceSettings.(SourceSettings.java:101)<br>
at fiji.plugin.mamut.NewMamutAnnotationPlugin.run(NewMamutAnnotationPlugin.java:103)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p>
<p><strong>I’ve tried updating, reinstalling the software and plugins from start in another pc and the same error appears. Don’t know what to do, a few months ago it worked perfeclty fine.</strong></p>
<p><strong>I’ve tried oppening an old annotation and same thing:</strong></p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 97MB of 12057MB (&lt;1%)</p>
<p>hdf.hdf5lib.exceptions.HDF5FileInterfaceException: Unable to open file ["f:\jhdf5\c\build\cmake-hdf5-1.10.5\hdf5-1.10.5\src\h5fdsec2.c line 346 in H5FD_sec2_open(): unable to open file: name = ‘C:\Users\lab185\Desktop\MASTER TFM\PrÃ¡cticas\DR4.\DC_CROPPED.h5’, errno = 2, error message = ‘No such file or directory’, flags = 0, o_flags = 0<br>
"]<br>
at hdf.hdf5lib.H5.H5Fis_hdf5(Native Method)<br>
at ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.isHDF5File(HDF5FactoryProvider.java:61)<br>
at ch.systemsx.cisd.hdf5.HDF5Factory.isHDF5File(HDF5Factory.java:106)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:231)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:177)<br>
at ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:155)<br>
at ch.systemsx.cisd.hdf5.HDF5ReaderConfigurator.reader(HDF5ReaderConfigurator.java:81)<br>
at ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.openForReading(HDF5FactoryProvider.java:55)<br>
at ch.systemsx.cisd.hdf5.HDF5Factory.openForReading(HDF5Factory.java:54)<br>
at bdv.img.hdf5.Hdf5ImageLoader.open(Hdf5ImageLoader.java:183)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:389)<br>
at bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:79)<br>
at bdv.BigDataViewer.initSetupNumericType(BigDataViewer.java:279)<br>
at bdv.BigDataViewer.initSetups(BigDataViewer.java:311)<br>
at fiji.plugin.mamut.SourceSettings.(SourceSettings.java:101)<br>
at fiji.plugin.mamut.io.MamutXmlReader.readSourceSettings(MamutXmlReader.java:172)<br>
at fiji.plugin.mamut.LoadMamutAnnotationPlugin.load(LoadMamutAnnotationPlugin.java:103)<br>
at fiji.plugin.mamut.LoadMamutAnnotationPlugin.run(LoadMamutAnnotationPlugin.java:84)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p>
<p><strong>Any help at all would be greatly appreciated.</strong></p>
<p><strong>Thank you very much,</strong></p>
<p><strong>Marina Herrero</strong></p> ;;;; <p>In this case, as there is only a single channel element (with SamplesPerPixel set to 3) then I would expect just a single TiffData element also.</p>
<p>Would you be able to provide a link to a small sample file to test what is going wrong?</p> ;;;; <p>Hi <a class="mention" href="/u/ydzhang98">@ydzhang98</a> ,</p>
<p>Do you want to use the GUI or a script ?</p>
<p>With the graphical user interface this should already work. It’s a  pre-requisite that you test it first.</p>
<p>If you want a programmatic way to run ti, here’s the script that I use now (groovy):</p>
<pre><code class="lang-auto">#@File(label = "File with cell to track") f
#@boolean showOutput

#@File(label = "CSV File with spots to track") spotsFile
#@double(style = "format:0.000") default_radius

#@double(label = "LINKING_MAX_DISTANCE", style="format:0.000") linking_max_distance
#@double(label = "GAP_CLOSING_MAX_DISTANCE", style="format:0.000") gap_closing_max_distance
#@int(label = "MAX_FRAME_GAP") max_frame_gap

#@String(label = "Which object is tracked ? (Terra, Telomeres, ...) ") object_tracked

DebugTools.enableLogging("INFO");

imp = IJ.openImage(f.getAbsolutePath());
imp.show()

Model model = new Model();

// Send all messages to ImageJ log window.
// Settings (voxel size, time delay), set up from the ImagePlus
Settings settings = new Settings(imp);
model.setLogger(Logger.IJ_LOGGER);
String fixedPath = FilenameUtils.removeExtension(spotsFile.getAbsolutePath())+"_fixed.csv";
renameColumns(spotsFile.getAbsolutePath(), fixedPath);

CSVImporterDetectorFactory detectFactory = new CSVImporterDetectorFactory();
Map&lt;String, Object&gt; map = detectFactory.getDefaultSettings();
map.put("FILE_PATH", fixedPath);
map.put("X_COLUMN", "x");
map.put("Y_COLUMN", "y");
map.put("Z_COLUMN", "z");//(Object) null);
map.put("FRAME_COLUMN", "t");
map.put("QUALITY_COLUMN", "");
map.put("NAME_COLUMN", "");
map.put("ID_COLUMN", "");
map.put("RADIUS", default_radius);
map.remove("RADIUS_COLUMN");

settings.detectorFactory = detectFactory;
settings.detectorSettings = map;

settings.trackerFactory = new SimpleSparseLAPTrackerFactory();// SparseLAPTrackerFactory();
settings.trackerSettings = LAPUtils.getDefaultSegmentSettingsMap();
settings.trackerSettings.put("LINKING_MAX_DISTANCE", linking_max_distance);      // 0.5 microns displacement may between each timef
settings.trackerSettings.put("GAP_CLOSING_MAX_DISTANCE", gap_closing_max_distance);  // 0.4 microns displacement before and after gap
settings.trackerSettings.put("MAX_FRAME_GAP", max_frame_gap);                 // 2 frames gap max allowed during tracking

// Add ALL the feature analyzers known to TrackMate. They will
// yield numerical features for the results, such as speed, mean intensity etc.
settings.addAllAnalyzers();

TrackMate trackmate = new TrackMate(model, settings);

boolean ok = trackmate.checkInput();
if (!ok) IJ.log(trackmate.getErrorMessage());

ok = trackmate.process(); // Actually performs the tracking
if (!ok) IJ.log(trackmate.getErrorMessage());

SelectionModel selectionModel = new SelectionModel( model );
boolean showOutput = true;
// ---- To display the result:
if (showOutput) {
    // A selection.

    // Read the default display settings.
    DisplaySettings ds = DisplaySettingsIO.readUserDefault();
    // With the line below, we state that we want to color tracks using
    // a numerical feature defined for TRACKS, and that has they key 'TRACK_INDEX'.
    ds.setTrackColorBy( TrackMateObject.TRACKS, "TRACK_INDEX" );

    HyperStackDisplayer displayer =  new HyperStackDisplayer( model, selectionModel, imp, ds );
    displayer.render();
    displayer.refresh();
}

// Echo results with the logger we set at start:
model.getLogger().log(  model.toString() );

String fileTrackMateTelomere = FilenameUtils.removeExtension(f.getAbsolutePath())+"_TM_"+object_tracked+".xml";
TmXmlWriter writer = new TmXmlWriter(new File(fileTrackMateTelomere));
writer.appendModel(model);
settings.trackerSettings.put("LINKING_FEATURE_PENALTIES", new HashMap()); //See https://github.com/trackmate-sc/TrackMate/issues/248
writer.appendSettings(settings);
writer.writeToFile();


IJ.log("Trackmate file written: "+fileTrackMateTelomere);

String fileTrackMateTelomereTracks = FilenameUtils.removeExtension(f.getAbsolutePath())+"_TM_"+object_tracked+"_Tracks.csv";
ExportTracksToXML.export(model, settings, new File(fileTrackMateTelomereTracks));
new ExportStatsTablesAction().execute(trackmate, selectionModel, new DisplaySettings(), null);
TrackTableView tableView = ExportStatsTablesAction.createTrackTables(model, selectionModel, new DisplaySettings());
tableView.getSpotTable().exportToCsv(new File(fileTrackMateTelomereTracks));
IJ.log("Exported tracks");



public static void renameColumns(String inputFilePath, String outputFilePath)  {
    try {
        // Define the CSV format and open the input file for parsing
        CSVFormat format = CSVFormat.DEFAULT.withHeader();
        FileReader reader = new FileReader(inputFilePath);
        CSVParser parser = new CSVParser(reader, format);

        // Define the output CSV format and open the output file for writing
        CSVFormat outputFormat = CSVFormat.DEFAULT.withHeader("y", "x", "t", "z");
        FileWriter writer = new FileWriter(outputFilePath);
        CSVPrinter printer = new CSVPrinter(writer, outputFormat);

        // Iterate over each record in the input file
        for (CSVRecord record : parser) {
            // Get the values of the first three columns
            String y = record.get(0);
            String x = record.get(1);
            String z = record.get(2);

            // Write a new record with the renamed columns and a new 't' column with value 0
            printer.printRecord(y, x, z, "0");
        }

        // Close the input and output files
        parser.close();
        printer.close();
    } catch (Exception e) {
        e.printStackTrace();
    }
}


import fiji.plugin.trackmate.action.ExportTracksToXML;
import fiji.plugin.trackmate.visualization.table.TrackTableView;
import ij.*;
import java.io.File;

import loci.common.DebugTools;
import net.imagej.ImageJ;
import net.imagej.patcher.LegacyInjector;
import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVParser;
import org.apache.commons.csv.CSVPrinter;
import org.apache.commons.csv.CSVRecord;
import org.apache.commons.io.FilenameUtils;

import fiji.plugin.trackmate.visualization.hyperstack.HyperStackDisplayer;
import fiji.plugin.trackmate.Logger;
import fiji.plugin.trackmate.Settings;
import fiji.plugin.trackmate.SelectionModel;
import fiji.plugin.trackmate.gui.displaysettings.DisplaySettingsIO;
import fiji.plugin.trackmate.TrackMate;

import fiji.plugin.trackmate.Model;

import fiji.plugin.trackmate.detection.CSVImporterDetectorFactory;

import fiji.plugin.trackmate.io.TmXmlWriter;

import fiji.plugin.trackmate.gui.displaysettings.DisplaySettings.TrackMateObject;
import fiji.plugin.trackmate.action.ExportStatsTablesAction;
import fiji.plugin.trackmate.gui.displaysettings.DisplaySettings;

import fiji.plugin.trackmate.tracking.jaqaman.SparseLAPTrackerFactory;
import fiji.plugin.trackmate.tracking.jaqaman.SimpleSparseLAPTrackerFactory;
import fiji.plugin.trackmate.tracking.jaqaman.LAPUtils;

import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
</code></pre> ;;;; <p>Hi,</p>
<p>I am also having this issue using bioformats 6.7.0 when I run the command. Please can you advise?</p>
<p>bfconvert -crop “100,100,500,500” -pyramid-resolutions 1 P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff test_pyr_bfconvert.ome.tiff<br>
bf.sh HERE Here<br>
java  -Xmx2G -Dhttp.proxyHost= -Dhttp.proxyPort= -cp /gpfs2/well/leedham/users/brr908/conda/skylake/envs/hyperion/bin:/gpfs2/well/leedham/users/brr908/conda/skylake/envs/hyperion/bin/bioformats_package.jar: loci.formats.tools.ImageConverter -crop 100,100,500,500 -pyramid-resolutions 1 P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff test_pyr_bfconvert.ome.tiff<br>
Output file test_pyr_bfconvert.ome.tiff exists.<br>
Do you want to overwrite it? ([y]/n)<br>
y<br>
P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff<br>
OMETiffReader initializing P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff<br>
Reading IFDs<br>
Populating metadata<br>
[OME-TIFF] → test_pyr_bfconvert.ome.tiff [OME-TIFF]<br>
Switching to BigTIFF (by file size)<br>
Series 0: converted 8/51 planes (15%)<br>
Series 0: converted 16/51 planes (31%)<br>
Series 0: converted 24/51 planes (47%)<br>
Series 0: converted 34/51 planes (66%)<br>
Series 0: converted 42/51 planes (82%)<br>
Series 0: converted 46/51 planes (90%)<br>
Series 0: converted 51/51 planes (100%)<br>
Exception in thread “main” loci.formats.FormatException: Buffer is too small; expected 1327104000 bytes, got 250000 bytes.<br>
at loci.formats.FormatWriter.checkParams(FormatWriter.java:476)<br>
at loci.formats.out.TiffWriter.saveBytes(TiffWriter.java:229)<br>
at loci.formats.out.OMETiffWriter.saveBytes(OMETiffWriter.java:218)<br>
at loci.formats.out.OMETiffWriter.saveBytes(OMETiffWriter.java:208)<br>
at loci.formats.FormatWriter.saveBytes(FormatWriter.java:132)<br>
at loci.formats.ImageWriter.saveBytes(ImageWriter.java:252)<br>
at loci.formats.tools.ImageConverter.convertPlane(ImageConverter.java:800)<br>
at loci.formats.tools.ImageConverter.testConvert(ImageConverter.java:718)<br>
at loci.formats.tools.ImageConverter.main(ImageConverter.java:1095)</p> ;;;; <aside class="quote quote-modified" data-post="1" data-topic="31094">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/victoria_corbit/40/22856_2.png" class="avatar">
    <a href="https://forum.image.sc/t/transferring-labeled-frames-to-new-project/31094">Transferring labeled frames to new project</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    I am working with a well-trained network (~1700 frames labeled, trained over 12 iterations), but for organization/access reasons I need to move the network in order to update it with new frames. I saw a previous topic (<a href="https://forum.image.sc/t/merging-deeplabcut-training-sets/28140" class="inline-onebox">Merging deeplabcut training sets</a>) suggesting that I could just copy over the labeled-data folder into my new project folder to transfer the labeled frames to the training dataset. 
However, when I run create_training_dataset, I get an error related to the ‘Project Scorer’, which I…
  </blockquote>
</aside>

<p>Would following these instructions still be appropriate?</p>
<p>Thanks</p> ;;;; <p>To measure the image:</p>
<ul>
<li>click on the local thickness image</li>
<li>Analyze &gt; Set Measurements… : tick “min &amp; max grey value” (and other measurements you want)</li>
<li>Analyze &gt; Measure (or just press Ctrl+M as a shortcut)</li>
</ul>
<p>From the histogram:</p>
<ul>
<li>click on the local thickness image</li>
<li>Analyze &gt; Histogram</li>
<li>look for the max value of the histogram</li>
</ul>
<p>(Be aware that you might need to do something slightly different when measuring a stack of images)</p> ;;;; <p>Hello Emanuele,</p>
<p>I dont know how to extract the maximum from the local Tickness image. I am practically a selft-taught noob (that maybe makes it clearer <img src="https://emoji.discourse-cdn.com/twitter/laughing.png?v=12" title=":laughing:" class="emoji" alt=":laughing:" loading="lazy" width="20" height="20">). I assume you mean I can get it from the histogram but how?<br>
<a class="attachment" href="/uploads/short-url/99aXvLlXizEuZy9abZXgzacNU5K.tif">emb11_LocThk.tif</a> (9.0 MB)<br>
here an image to show me if you dont mind</p> ;;;; <p>Thank you Julia. How do I get the value from the histogram? or set “get min and max”  on FIJI? I am sorry, maybe it is very basic question. Here an image to show me if you dont mind.<br>
<a class="attachment" href="/uploads/short-url/99aXvLlXizEuZy9abZXgzacNU5K.tif">emb11_LocThk.tif</a> (9.0 MB)</p> ;;;; <p>I could see Model II working OK. In that case, viewing a 2D plate would be similar logic to the current loading of a labels layer. You’d probably have to make some assumptions like “all paths under projections are the same for each Well” etc, so as not to have to load <code>A/1/0/projections/.zattrs</code> for every Well and keep acceptable performance.</p>
<p><a class="mention" href="/u/jluethi">@jluethi</a> We’ve not really made much use of the multiscales list yet - all existing tools assume just a single item. However, your example makes good sense. In that case, if you had <code>labels/</code> for <em>only</em> the 2D image, I wonder if you could store them without breaking the spec (which doesn’t allow 2D labels under 3D image), or if the spec would need to change to allow that?</p> ;;;; <p>The Pull request is still a draft. We are looking at the best way to fix the issue.<br>
Hopefully we will have a release in the coming weeks</p>
<p>Cheers</p>
<p>Jean-Marie</p> ;;;; <p>Hi everyone,<br>
I am trying to write a macro that opens all the images from one folder and then run JACoP and do a pearson correlation assay for all combination. The problem is that after the third iteration it stops telling me that JACoP is not able to find any open images.<br>
I tried to remove the run(“Close”) but still nothing change.<br>
How I can modify my code to run through all combinations?<br>
Thank you very much for your help,</p>
<p>George</p>
<pre><code class="lang-auto">setBatchMode(true);
file1 = getDirectory("Select a folder containing images");
list1 = getFileList(file1);
n1 = list1.length;
file3 = getDirectory("Output");
for (i=0; i&lt;n1; i++){
	name = substring(list1[i], 0, lengthOf(list1[i])-4); //remove the extension
	open(file1 + list1[i]);
	for (j=i+1; j&lt;n1; j++){
		open(file1 + list1[j]);
		run("JACoP ", "imga=["+list1[i] + "] imgb=["+list1[j]+"] pearson");
		saveAs("Text", file3 + name + "-" + substring(list1[j], 0, lengthOf(list1[j])-4) +"_Coloc.txt");
		run("Close");
	}
	
}
run("Close All");
</code></pre> ;;;; <p>Hm, okay. So the probability maps already don’t separate the cells - is that correct? There are options…</p>
<ol>
<li>Would you say that there is a visible boundary between those cells? If so, then you could try to train accordingly in Pixel Classification - adding a “boundary” class to help separate those.</li>
<li>If that doesn’t help, or is not feasible, you could also try to live with it - in object clasifiaction you could add an additional class of “bad segmentations” and exclude those clusters from the analysis.</li>
<li>is there by any chance some additional channel in your data, e.g. on the nuclei (assuming what I am seeing in your example data is full cells and not nuclei :)). Then you could exploit the fact that cells usually only come with one nucleus and try the <em>Hysteresis Thresholding</em> (<a href="https://www.youtube.com/watch?v=_ValtSLeAr0&amp;t=2042s">video turorial</a>, <a href="https://www.ilastik.org/documentation/objects/objects#hysteresis-thresholding">documentation</a>). For this you’d need to train to separate pixel classification projects, one for the whole cell bodies, and one for the nuclei - combining the probability maps in object classification.</li>
</ol>
<p>Hope this helps!</p>
<p>Cheers<br>
Dominik</p> ;;;; <p>Hi again</p>
<p>I ran your Groovy script and get the following error:</p>
<pre><code class="lang-auto">omero.ClientError: Obtained null object proxy
	at omero.client.createSession(client.java:837)
	at omero.gateway.Gateway.createSession(Gateway.java:1034)
	at omero.gateway.Gateway.connect(Gateway.java:295)
	at fr.igred.omero.GatewayWrapper.connect(GatewayWrapper.java:220)
	at fr.igred.omero.GatewayWrapper.connect(GatewayWrapper.java:205)
	at fr.igred.omero.GatewayWrapper$connect.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)
	at New_.run(New_.groovy:10)
	at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)
	at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)
	at org.scijava.plugins.scripting.groovy.GroovyScriptLanguage$1.eval(GroovyScriptLanguage.java:97)
	at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)
	at org.scijava.script.ScriptModule.run(ScriptModule.java:164)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

</code></pre> ;;;; <p>Thank you for the swift response!</p>
<p>I do get a comma in the port text box. I thought that might be causing a problem, but when I remove it, it is immediately put back in when I click out.</p> ;;;; <aside class="quote no-group" data-username="Ioannis" data-post="1" data-topic="79093">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ioannis/40/22446_2.png" class="avatar"> Ioannis Theodorou:</div>
<blockquote>
<ul>
<li>Local thickness (complete process)</li>
</ul>
</blockquote>
</aside>
<p>that should be enough.<br>
You can get the maximum of the Local Thickness image and that is the maximum width.<br>
Not getting why you need to use the bounding box.<br>
Local thickness itself computes/estimates the width of an object.</p>
<p>Maybe something is not clear to me.</p>
<p>Emanuele Martini</p> ;;;; <p>It worked for me as well.</p>
<p>How do I create bug for the original problem?</p> ;;;; <p>Hi <a class="mention" href="/u/mporter">@mporter</a>,</p>
<p>I’m sorry to hear this does not work on your end.<br>
If it works with the OMERO_ij plugin, it might be caused by the library or a formatting problem with the port number: someone once had a problem because the locale imposed a thousand separator, but it was not parsed correctly.</p>
<p>Is there a separator (such as a comma or a space) in the port box of the connection window?<br>
If so, does it work by removing it manually?<br>
Or, on the contrary, should there be one in your locale? Does it work by adding one?</p>
<p>If it does not, or if separators are irrelevant, could you ensure the underlying library is not responsible for this problem, by running this Groovy script, to test the connection?</p>
<pre><code class="lang-groovy">// @String(label="Username") USERNAME
// @String(label="Password", style='password', persist=false) PASSWORD
// @String(label="Host") HOST
// @Integer(label="Port", value=4064) PORT
// @Long(label="Image ID") image_id

import fr.igred.omero.Client;

client = new Client();
client.connect(HOST, PORT, USERNAME, PASSWORD.toCharArray());

image = client.getImage(image_id);
print(image.getName() + "\n");

client.disconnect();
</code></pre>
<p>Edit: The plugin may not handle the formatting correctly actually. It looks like a parsing function is missing, so a problem with the port is probably caused by a superfluous separator. I’ll fix that when I can.</p> ;;;; <p>Hi,<br>
I also want to import the spots only and use another tracker. But I don’t know how to write the jython script that using “TrackMate CSV Importer” to import the spots and then using “TrackMate” to linking them. Do you have any suggestions or thoughts?</p> ;;;; <p>You method seems ok too, but I think you’d need to manually segment the thickness map? If you are only after the maximum thickness of one particle, you could also get that value from the histogram, or set “get min and max” in measurements and measure the thickness image.</p>
<p>If you have multiple particles per image, I would use MorphoLibJ’s “Max inscribed circle/sphere” or the max thickness measurement from BoneJ’s Particle Analyser.</p> ;;;; <p>Dear image.sc community,</p>
<p>its my first post here and probably I am repeating stuff, but I couldnt find anything that would probably solve my issues.</p>
<p>I am doing multiplex immunohistochemistry and using a scanner with .mrxs files. I managed to allign the images using QuPath and ImajeJ on WSI ROI, but my troubles are with TMAs. I have &gt;1000 cores to analyze and the single cores are too big for QuPath to export them to ImageJ for aligning.<br>
Is there a way to align the whole TMA slide somehow, deconvolute and merge the images in somewhat automatic fashion?<br>
I will analyze the merged image afterwars using QuPath. That in itself isnt a problem.</p>
<p>Thank you in advance</p>
<p>Best regards<br>
andreevg</p> ;;;; <p>Hi,</p>
<p>I was wondering if its possible to export previous labelling information from previous projects to a new project?</p>
<p>I saw this post:</p>
<aside class="quote" data-post="1" data-topic="41325">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jjerphan/40/33216_2.png" class="avatar">
    <a href="https://forum.image.sc/t/exporting-labels-without-training/41325">Exporting labels without training</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hi, 
I am using Ilastik 1.3.3post3 to label 3D images. 
I want to export them but it seems that one needs to perform a step of training to be able to then export them on the Prediction Export pane. 
This is long or just impossible on some machines for 3D data. 
Is there a simpler way to be able to get labels without any training step? <img width="20" height="20" src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title="slight_smile" alt="slight_smile" class="emoji"> 
Thank you, 
Julien.
  </blockquote>
</aside>

<p>But I don’t know Ilastik</p>
<p>Is there a way via DeepLabCut?</p>
<p>I saw a function deeplabcut.export_model but I wasn’t sure if this performed the function I wanted…?</p>
<p>Thanks, again</p>
<p>I really appreciate the help <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>GR</p> ;;;; <p>It works!<br>
Thank you for your help.</p>
<p>Natalia</p> ;;;; <p>Hi</p>
<p>I have a user that would greatly benefit from the omero_batch_plugin functionality, but when testing it, I’m having difficulty connecting to our server. I wondered if <a class="mention" href="/u/pierre.pouchin">@pierre.pouchin</a> (or anyone!) might be able to help. I always get the error ‘Port not valid or no internet access’ after a timeout. I get this after launching the batch plugin, clicking ‘Connect’ and filling in server and credentials. I can connect to the server with the OMERO_ij plugin no problem. I am using up to date Fiji with:</p>
<p>omero_ij-5.8.1-all<br>
omero_batch-plugin-1.0.5<br>
simple-omero-client-5.12.1-jar-with-dependencies (also tested version without dependencies)</p>
<p>Many thanks,</p>
<p>Michael.</p> ;;;; <p>Hi,<br>
I am also stuck on this problem. I want to use the detection results by “TrackMate CSV importer” and then link particles by TrackMate, but I don’t know how to write this script. Can you share your script? Thank you so much!</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/2/c2a4ced091867f6420d8d97b80b5bb3e66a4db9a.png" data-download-href="/uploads/short-url/rLTJalYxG5LOX7Chbp1mPuTAdBw.png?dl=1" title="Screenshot 2023-03-27 102028" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2a4ced091867f6420d8d97b80b5bb3e66a4db9a_2_690x170.png" alt="Screenshot 2023-03-27 102028" data-base62-sha1="rLTJalYxG5LOX7Chbp1mPuTAdBw" width="690" height="170" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2a4ced091867f6420d8d97b80b5bb3e66a4db9a_2_690x170.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2a4ced091867f6420d8d97b80b5bb3e66a4db9a_2_1035x255.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2a4ced091867f6420d8d97b80b5bb3e66a4db9a_2_1380x340.png 2x" data-dominant-color="4D5454"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-27 102028</span><span class="informations">1877×463 43.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b05b6ed53016bca921253a3fe45c259da240d4a5.png" data-download-href="/uploads/short-url/pa7VAXHm87GPAufFPdui4QgQrGJ.png?dl=1" title="Screenshot 2023-03-27 102131" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b05b6ed53016bca921253a3fe45c259da240d4a5_2_690x349.png" alt="Screenshot 2023-03-27 102131" data-base62-sha1="pa7VAXHm87GPAufFPdui4QgQrGJ" width="690" height="349" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b05b6ed53016bca921253a3fe45c259da240d4a5_2_690x349.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b05b6ed53016bca921253a3fe45c259da240d4a5_2_1035x523.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b05b6ed53016bca921253a3fe45c259da240d4a5_2_1380x698.png 2x" data-dominant-color="183033"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-27 102131</span><span class="informations">1633×828 28.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I would like to know if the below mentioned method is sound. I am not an image analyst or informatician so I need something that works on FIJI. Also I still dont know how to write macros, I can use the recorder if you find it usefull for your feedback.</p>
<p>Overall I want to find the max. width of the segmented structure on (1). I have 13 of those and each one has a different shape so I need a way to measure length and width without searching for it every time manually. For length I use geodesic diameter from the morpholibJ plugin and I am content with it. But average thickness is not working for my purposes and neither feret diameter or just a simple oriented bounding box.</p>
<p>So the method I used is below.</p>
<ol>
<li>
<p>Segmented blade with MorpholibJ and a marker or manual drawing</p>
</li>
<li>
<p>Local thickness (complete process)</p>
</li>
<li>
<p>with editing contrast, the largest width region(s) is isolated ( right?) .</p>
</li>
<li>
<p>re-segment with morpholibJ</p>
</li>
<li>
<p>analyze with morpholibJ =&gt; oriented bounding box keep width</p>
</li>
</ol>
<p>I am not sure what can introduce bias except maybe the contrast step. But what I get as measurements fit well with my qualitative observations and theory.</p>
<p>Kind regards and many thanks.</p> ;;;; <p>Actually, I wonder whether we could use the <code>multiscales</code> metadata (see <a href="https://ngff.openmicroscopy.org/latest/#multiscale-md" class="inline-onebox" rel="noopener nofollow ugc">Next-generation file formats (NGFF)</a>) to describe a list of the OME-Zarr files in subgroups in option 2. In that way, an OME-Zarr image could have a defined list of all the available subgroups and their description.</p>
<p>Something like:</p>
<pre><code class="lang-auto">{
    "multiscales": [
        {
            "version": "0.5-dev",
            "name": "3D",
            "axes": [
                {"name": "t", "type": "time", "unit": "millisecond"},
                {"name": "c", "type": "channel"},
                {"name": "z", "type": "space", "unit": "micrometer"},
                {"name": "y", "type": "space", "unit": "micrometer"},
                {"name": "x", "type": "space", "unit": "micrometer"}
            ],
            "datasets": [
                {
                    "path": "0",
                    "coordinateTransformations": [{
                        // the voxel size for the first scale level (0.5 micrometer)
                        "type": "scale",
                        "scale": [1.0, 1.0, 0.5, 0.5, 0.5]
                    }]
                },
                {
                    "path": "1",
                    "coordinateTransformations": [{
                        // the voxel size for the second scale level (downscaled by a factor of 2 -&gt; 1 micrometer)
                        "type": "scale",
                        "scale": [1.0, 1.0, 1.0, 1.0, 1.0]
                    }]
                },
                {
                    "path": "2",
                    "coordinateTransformations": [{
                        // the voxel size for the third scale level (downscaled by a factor of 4 -&gt; 2 micrometer)
                        "type": "scale",
                        "scale": [1.0, 1.0, 2.0, 2.0, 2.0]
                    }]
                }
            ],
            "coordinateTransformations": [{
                // the time unit (0.1 milliseconds), which is the same for each scale level
                "type": "scale",
                "scale": [0.1, 1.0, 1.0, 1.0, 1.0]
            }],
            "type": "gaussian",
            "metadata": {
                "description": "the fields in metadata depend on the downscaling implementation. Here, the parameters passed to the skimage function are given",
                "method": "skimage.transform.pyramid_gaussian",
                "version": "0.16.1",
                "args": "[true]",
                "kwargs": {"multichannel": true}
            }
        },
        {
            "version": "0.5-dev",
            "name": "max_projection",
            "axes": [
                {"name": "t", "type": "time", "unit": "millisecond"},
                {"name": "c", "type": "channel"},
                {"name": "y", "type": "space", "unit": "micrometer"},
                {"name": "x", "type": "space", "unit": "micrometer"}
            ],
            "datasets": [
                {
                    "path": "projections/max_projection/0",
                    "coordinateTransformations": [{
                        // the voxel size for the first scale level (0.5 micrometer)
                        "type": "scale",
                        "scale": [1.0, 1.0, 0.5, 0.5]
                    }]
                },
                {
                    "path": "projections/max_projection/1",
                    "coordinateTransformations": [{
                        // the voxel size for the second scale level (downscaled by a factor of 2 -&gt; 1 micrometer)
                        "type": "scale",
                        "scale": [1.0, 1.0, 1.0, 1.0]
                    }]
                },
                {
                    "path": "projections/max_projection/2",
                    "coordinateTransformations": [{
                        // the voxel size for the third scale level (downscaled by a factor of 4 -&gt; 2 micrometer)
                        "type": "scale",
                        "scale": [1.0, 1.0, 2.0, 2.0]
                    }]
                }
            ],
            "coordinateTransformations": [{
                // the time unit (0.1 milliseconds), which is the same for each scale level
                "type": "scale",
                "scale": [0.1, 1.0, 1.0, 1.0]
            }],
            "metadata": {
                "description": "max projection of the 3D data",
                "method": "max_projection function",
                "version": "0.16.1",
                "args": "[true]",
                "kwargs": {"multichannel": true}
            }
        }
    ]
}
</code></pre> ;;;; <p>Yes, those would be the 3 models I’d imagine, thanks for showing them nicely with their tree structures!</p>
<p>I also do like the idea of option 2. It nicely maintains individual Zarr files that stay within the spec. We would need the generalization though that a Zarr file can contain (arbitrary) subgroups consisting of Zarr files again. Could certainly mean some additional work for viewer to decide how to render either the main group or specific subgroups. But that also could be an interesting challenge to tackle, if the community goes that way!<br>
And it’s nice how the metadata on that specific acquisition could just live in the well-level Zarr file, all other ones would likely belong to the same without needing duplication of it. Though with some of the discussions on future metadata directions and them potentially becoming something that can refer to other metadata, maybe that issue becomes less problematic.</p>
<blockquote>
<p>If we would go for option 1), we should think of an additional key next to <code>acquisition</code> . But adding another key on that level will interfere the definition of <code>field</code> , which is a field of view during acquisition.</p>
</blockquote>
<p>I’m anyway proposing we transition away from calling the OME-Zarr images in each well “fields”, and just call them images. See PR here: <a href="https://github.com/ome/ngff/pull/137" class="inline-onebox" rel="noopener nofollow ugc">Generalize well organization in high-content screening: field of view =&gt; image by jluethi · Pull Request #137 · ome/ngff · GitHub</a><br>
We currently store multiple field of views in one image and the HCS spec basically is a collection of OME-Zarr images (whatever images they may be), not necessarily field of views.<br>
In that version, I think having a 2D image and a 3D image is not so problematic. If nesting of Zarr files doesn’t become the standard, I’d think this would be a valid alternative.</p> ;;;; <p>Hello, <a class="mention" href="/u/tinevez">@tinevez</a> <a class="mention" href="/u/setareh">@Setareh</a><br>
Could you please share your new script, I’m also stuck on this problem.<br>
Thanks a lot.</p> ;;;; <p>Hi <a class="mention" href="/u/jpolentes">@JPolentes</a></p>
<p>try the solution marked in this thread: <a href="https://forum.image.sc/t/select-and-measure-multiple-rois-within-the-bigger-roi/40290/4" class="inline-onebox">Select and measure multiple ROIs within the bigger ROI - #4 by Research_Associate</a></p>
<p>Best wishes,<br>
Marie</p> ;;;; <p>I’ve tried Interleaved=“true” with the same results.<br>
Can’t the problem be in the TiffData?<br>
Shouldn’t I have 3, one for each channel but all referring to the same IFD?<br>
Just wondering…</p> ;;;; <p>Hi <a class="mention" href="/u/chin">@chin</a></p>
<p>I’m not sure you’re having the same error as yours states :</p>
<pre><code class="lang-auto">'conda.bat' is not recognized as an internal or external command,operable program or batch file.
</code></pre>
<p>Did you follow the <a href="https://github.com/BIOP/ijl-utilities-wrappers#enable-conda-command-outside-conda-prompt">instructions there to enable use of conda outside conda prompt</a> ?</p>
<p>Best,</p>
<p>Romain</p> ;;;; <p>Done! I have updated the script.<br>
I tested it with my MATLAB installation and it now works as expected:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m">
  <header class="source">

      <a href="https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m" target="_blank" rel="noopener">trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m</a></h4>


      <pre><code class="lang-m">%% TrackMate in MATLAB example.
% We run a full analysis in MATLAB, calling the Java classes.

%% Some remarks.
% For this script to work you need to prepare a bit your Fiji installation
% and its connection to MATLAB.
%
% 1.
% In your Fiji, please install the 'ImageJ-MATLAB' site. This is explained
% here: https://imagej.net/scripting/matlab. Then restart Fiji.
%
% 2.
% Add the /path/to/your/Fiji.app/scripts to the MATLAB path. Either use the
% path tool in MATLAB or use the command:
% &gt;&gt; addpath( '/path/to/your/Fiji.app/scripts' )
%
% 3.
% In MATLAB, first launch ImageJ-MATLAB:
% &gt;&gt; ImageJ
%
</code></pre>



  This file has been truncated. <a href="https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi,</p>
<p>I am running DLC live using the GUI. My camera has 60 fps; however, the saved video file is only at 10fps (A 1 minute recording in real time is only 10s when played through a media player,) My question is about the saved video only (inference rate is fine). Is it possible to record the video at a higher frame rate?</p> ;;;; <p>Hey <a class="mention" href="/u/jluethi">@jluethi</a>,</p>
<p>Thanks for the detailed comments! The three models, in directory tree structure, would then be:</p>
<h2>
<a name="h-1-2d-3d-images-are-separate-fields-1" class="anchor" href="#h-1-2d-3d-images-are-separate-fields-1"></a>1) 2D &amp; 3D images are separate fields</h2>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field =&gt; contains 3D data, with metadata tag
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
    .................
    │   │   ├── 1                 # second "field" =&gt; contains 2D image data, with metadata tag
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
</code></pre>
<h2>
<a name="h-2-2d-images-are-within-a-new-projection-subgroup-2" class="anchor" href="#h-2-2d-images-are-within-a-new-projection-subgroup-2"></a>2) 2D images are within a new projection subgroup</h2>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── labels   
    │   │   │   │   └── 0        # labels corresponding to A/1/0/0
    │   │   │   ├── projections    
    │   │   │   │   ├── 0        # zeroth resolution level projection
    │   │   │   │   └── labels
    │   │   │   │       └── 0    # labels corresponding to A/1/0/projections/0
</code></pre>
<h2>
<a name="h-3-3d-images-can-have-a-projection-subgroup-3" class="anchor" href="#h-3-3d-images-can-have-a-projection-subgroup-3"></a>3) 3D images can have a projection subgroup</h2>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── projections    
    │   │   │   │   └── 0         # zeroth resolution level projection
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
    │   │   │   │   ├── projections 
    │   │   │   │   │   └── 0     # labels corresponding to A/1/0/projections/0
</code></pre>
<hr>
<p>After thinking about all three options I think I am in favor of option 2). This way <code>projections</code> are just a different view of the acquired data, but if we only look at <code>projections</code> it is a valid image which can have its own <code>labels</code> and <code>tables</code>. I think this would also nicely generalize to other types of persisted <code>views</code> e.g. <code>denoised</code> or <code>deconvolved</code> data.<br>
I like this approach, because the <code>field</code> group will always contain the data coming from the acquisition system. In some cases these data are already projected, while in other cases it might only be single 2D planes or it is 3D volumes. In any case what we have in the zarr <code>field</code> would be what is persisted by the system.</p>
<p>If we would go for option 1), we should think of an additional key next to <code>acquisition</code>. But adding another key on that level will interfere the definition of <code>field</code>, which is a field of view during acquisition.</p>
<p>By now I find option 3) very confusing. It elevates <code>labels</code> to be some kind of special sub-group and I think it would become very complicated if different processed data should be saved. And it seems to be in violation of the existing spec.</p>
<hr>
<p>Now, I am trying to imagine how far we could take option 2) by adding more processed data following the same logic:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                            # row
    │   ├── 1                        # column
    │   │   ├── 0                    # zeroth field
    │   │   │   ├── 0                # zeroth resolution level
    │   │   │   ├── labels   
    │   │   │   │   └── 0            # labels corresponding to A/1/0/0
    │   │   │   ├── denoised   
    │   │   │   │   ├── 0            # denoised version of A/1/0/0
    │   │   │   │   ├── projections    
    │   │   │   │   │   ├── 0        # projection of denoised data 
    │   │   │   │   │   └── labels
    │   │   │   │   │       └── 0    # labels of denoised projected data
    │   │   │   │   └── labels
    │   │   │   │       └── 0        # labels of the denoised data
    │   │   │   ├── projections    
    │   │   │   │   ├── 0            # zeroth resolution level projection
    │   │   │   │   └── labels
    │   │   │   │       └── 0        # labels corresponding to A/1/0/projections/0
</code></pre>
<p>Looking forward to where we will end up on this journey!</p> ;;;; <p>hi <a class="mention" href="/u/marie-nkaefer">@Marie-nkaefer</a></p>
<p>I thought of filtering the objects by size to eliminate small false positives, but in fact I also have small real objects sometimes. So this approach is not the right one for me. In fact, there is no point in filtering the objects, and what I would like to know is if there is any code or maybe a plugin that allows us to draw a boundary around multiple ROIs previously detected on an image. We can do this on a single ROI by double-clicking on it. This way, the ROI is selected on the image, and the corresponding ROI is also selected in the ROI manager. I would like to be able to do the same thing with several ROIs at the same time, and get their indexes in the ROI manager in one step.</p> ;;;; <p>Doh!<br>
The MATLAB + TrackMate script hasn’t been updated you are right! I will fix it today</p> ;;;; <p>Hello.</p>
<p>Yes the import paths of trackers haven changed woth v7.10. This is what causes the error on your script.</p>
<p>But the examples in the scripting page linked above have been updated. You can refer to them for the correct paths.</p>
<p>If they don’t work it’s a bug.</p> ;;;; <p>Super interesting to see, thanks for sharing <a class="mention" href="/u/tibuch">@tibuch</a> &amp; <a class="mention" href="/u/imagejan">@imagejan</a><br>
Also looking forward to trying faim-hcs!</p>
<p>I’m personally most interested in the discussion of how we store 2D &amp; 3D representation of the same image data. Given that this isn’t fully defined yet, in <a href="https://fractal-analytics-platform.github.io" rel="noopener nofollow ugc">Fractal</a> we currently store a separate 2D OME-Zarr plate file and 3D OME-Zarr plate file. This makes each plate fully compatible with the current spec, but looses the information about how they are connected. Thus, very interesting to come together on how we store 2D &amp; 3D data in the same OME-Zarr plate.</p>
<p>The complexity arises when we sometimes have 2D only plates, sometimes 3D only plates and sometimes plates where we have both 3D data and 2D projections for the same channels.</p>
<p>We started planning for an approach where we would store each (up to 5D image) separately within the well. In your case above, that would mean having 2 images: A 4 channel images with the 3D representation and a 4 channel image with the 2D representation. They then could have different <code>acquisition</code> keys or a similar key to acquisition (e.g. <code>modality</code>) on the plate level (see <a href="https://ngff.openmicroscopy.org/latest/#plate-md" class="inline-onebox" rel="noopener nofollow ugc">Next-generation file formats (NGFF)</a>). An example layout could look like this:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field =&gt; contains 3D data, with metadata tag
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
    .................
    │   │   ├── 1                 # second "field" =&gt; contains 2D image data, with metadata tag
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
</code></pre>
<p>Let’s call this the “separate images in same plate” layout. In that case, each image still follows existing OME-Zarr logic. But it’s not clearly defined yet what the relationship between images is.</p>
<p>Above, <a class="mention" href="/u/imagejan">@imagejan</a> &amp; <a class="mention" href="/u/tibuch">@tibuch</a> show 2 alternative models for how to store the 2D images, both are within the 3D image, with different interaction models for labels (and I assume then similar logic for the tables PR, see <a href="https://github.com/ome/ngff/pull/64" rel="noopener nofollow ugc">https://github.com/ome/ngff/pull/64</a>). If the 2D image is not its own image, but just a new subgroup, then do the labels (&amp; tables) go within this subgroup, or do they stay in the labels (/tables) folder of the 3D image, but within a separate subfolder there?</p>
<p>Thus, I currently see these 3 models:<br>
I) 2D &amp; 3D images are separate images within an OME-Zarr plate (with some metadata linking them)<br>
II) 2D images are within a new projection subgroup of 3D images (and then are a whole OME-Zarr image within that, e.g. their own labels &amp; tables)<br>
III) 3D images have projection subgroup within each category (i.e. image projection in the /projection folder, label projection in the /labels/projection folder etc.)</p>
<hr>
<p>Trade-offs:<br>
Model I) can stick very closely to the existing specification (just may need additional metadata entries for us to know which images are linked). But the 2D &amp; 3D data are actually in different OME-Zarr images (=&gt; not as intuitively linked). A processing pipeline could easily choose just 2D or 3D images by filtering for such a metadata entry.</p>
<p>Model II) Very interesting further development of what an OME-Zarr image is. If all labels, tables etc. are saved within the projection subgroup, then the main OME-Zarr file would still work the same (ignoring the projection subgroup) and the projection subgroup is actually its own OME-Zarr image =&gt; OME-Zarr images could become nested (=&gt; super interesting thought! The nesting structure then conveys meaning).<br>
Makes 2D projection data somewhat of an addition to the 3D data, not its independent dataset. The thing that worries me a bit about this model is that we could have 2 different 2D plates: One being acquired as 2D, one the result of projections from 3D images. Even though a user may only be interested in the 2D plates, this 2D data can now be stored in 2 different ways: as the main image or as the subgroup.</p>
<p>Model III) This is less general to me, but could be very explicit to say that an image can have different image representation, different label representations, different table representations etc. That could be interesting e.g. when we want to do mixed-modality processing. For example, we detect some larger objects (for us e.g. organoids) in 2D, define a 2D bounding box (=&gt; label &amp; table), but then use this during 3D processing. Adds some complexity though, as labels &amp; tables now also need to do with additional subgroups.</p>
<p>Are there strong opinions on which model will be the correct one for combined 2D &amp; 3D data?</p>
<blockquote>
<p>I think that the labels under an Image must have the same dimensions, so you couldn’t have a projected 2D labels under a 3D image.</p>
</blockquote>
<p>So <a class="mention" href="/u/will-moore">@will-moore</a> this excludes model III? Do you have an opinion on which structure we should be aiming for?</p>
<p>While I have a slight preference for model I), I can live with any of the 3 representations of the data. But I would be strongly in favor of standardizing on one, so that it will be well supported by viewers and our code is exchangeable. All of them would require some adaptation in viewers should they become part of the standard anyway.</p> ;;;; <p><strong>Special thanks to you, the issue has been resolved.</strong></p> ;;;; <p>Hello. I would like to select the annotations whose ROI types are <strong>Polygon</strong>. How could I do via script to achieve this goal</p>
<p>The picture below with red and yellow annotations are objects I would like to choose.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c6ccdad3ab54a046d4449a663ecc7ac5a21dab5.jpeg" data-download-href="/uploads/short-url/k2fXhljq98A2BNXT2hViDNxa31b.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c6ccdad3ab54a046d4449a663ecc7ac5a21dab5_2_690x433.jpeg" alt="image" data-base62-sha1="k2fXhljq98A2BNXT2hViDNxa31b" width="690" height="433" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c6ccdad3ab54a046d4449a663ecc7ac5a21dab5_2_690x433.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c6ccdad3ab54a046d4449a663ecc7ac5a21dab5_2_1035x649.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c6ccdad3ab54a046d4449a663ecc7ac5a21dab5.jpeg 2x" data-dominant-color="C2A1C3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1349×847 113 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thank you for your help in advance.</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c.png" data-download-href="/uploads/short-url/4KsalnNguCoC8jheuVdrAzchU1C.png?dl=1" title="Captura de Tela (17)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c_2_690x387.png" alt="Captura de Tela (17)" data-base62-sha1="4KsalnNguCoC8jheuVdrAzchU1C" width="690" height="387" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c_2_690x387.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c_2_1035x580.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c.png 2x" data-dominant-color="454545"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Captura de Tela (17)</span><span class="informations">1366×768 78 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d.png" data-download-href="/uploads/short-url/eezNKgutRNGJE1ql9kerK1kRPcx.png?dl=1" title="Captura de Tela (16)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d_2_690x387.png" alt="Captura de Tela (16)" data-base62-sha1="eezNKgutRNGJE1ql9kerK1kRPcx" width="690" height="387" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d_2_690x387.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d_2_1035x580.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d.png 2x" data-dominant-color="373636"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Captura de Tela (16)</span><span class="informations">1366×768 76.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db.jpeg" data-download-href="/uploads/short-url/voXtULHxQbJhujyqgWHMvXykkjh.jpeg?dl=1" title="Captura de Tela (15)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db_2_690x387.jpeg" alt="Captura de Tela (15)" data-base62-sha1="voXtULHxQbJhujyqgWHMvXykkjh" width="690" height="387" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db_2_690x387.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db_2_1035x580.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db.jpeg 2x" data-dominant-color="81716D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Captura de Tela (15)</span><span class="informations">1366×768 96.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi, I had the exact same problem and I raised an issue on Github.  Greatly appreciate their effort, the issue was solved promptly.  You should test if it works for you, too.  I basically replaced the VolumeViewer  with the updated version that they just released on latest distribution of ImageJ.  Good luck!</p> ;;;; <p>So, when I run my macro I posted <a href="https://forum.image.sc/t/how-to-properly-count-and-measure-colonies/78451/8">above</a>, it perfectly works.</p>
<p>Have you tried to run exactly that macro (unchanged!) on the image you posted here. It should give you exactly what it shows in the image in my post.</p> ;;;; <p>I am running CellProfiler on a recently updated Windows 11 computer. The program no longer opens and flashes a fast error before closing the console. It was working fine before a recent batch of Windows updates.</p>
<p>“[process exited with code 3221226356 (0xc0000374)]”</p>
<p>This has happened on a couple of computers after a recent windows update. I have tried installing and reinstalling CellProfiler 4.2.1 and 4.2.5. I have also insallted the Visual C++ Redistributable linked on the downloads page. Any help will be appreciated!</p> ;;;; <p>I could not get .OBJ files to show either with 3D Viewer, but when I converted it to STL it worked fine. So I would suggest converting your files to .STL. I used an online converter for it.</p> ;;;; <p>Sure, thank you. Here is the original<br>
Best regards<br>
Luigi</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000.jpeg" data-download-href="/uploads/short-url/lUM4sXeN8MjmqGpT15eDoW33KAU.jpeg?dl=1" title="CT_e-4.jpg" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg" alt="CT_e-4.jpg" data-base62-sha1="lUM4sXeN8MjmqGpT15eDoW33KAU" width="375" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_750x1000.jpeg 2x" data-dominant-color="5D4019"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">CT_e-4.jpg</span><span class="informations">1200×1600 346 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/overcook6402">@Overcook6402</a>,</p>
<p>My guess is that you need to apply the contrast change. <code>setMinAndMax(min, max)</code> only makes a visual contrast adjustment on screen.<br>
It might work if you add <code>run("Apply LUT");</code> after that.<br>
But I could not currently test this.</p> ;;;; <p>Hi <a class="mention" href="/u/luigi_marongiu">@Luigi_Marongiu</a> ,</p>
<p>The only way to adapt the macro in a helpful way we would need the original image. The macro I posted was created for the posted image from you which most likely is different from the original and therefore the macro does not do exactly what it is supposed to do.</p> ;;;; <p>Hi <a class="mention" href="/u/nadeem">@Nadeem</a> I develop QuPath using an M1 Mac… so it’s especially important to me that QuPath has good support for Macs <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>There is an Apple Silicon version of QuPath, but there are a few limitations and complications described at <a href="https://qupath.readthedocs.io/en/0.4/docs/intro/installation.html#apple-silicon" class="inline-onebox">Installation — QuPath 0.4.3 documentation</a></p>
<p>However I believe everything should just work if you download the Intel version of QuPath instead. That’s the simplest solution, since it means you don’t really have to think about compatible problems. It runs a bit more slowly than you have the Apple Silicon version because it needs to run through <a href="https://support.apple.com/en-gb/HT211861">Rosetta 2</a>, but I think it isn’t very noticeable – I only really know there is a difference between I did a speed comparison once.</p>
<p>Note that you might not need to convert MRXS images to ome.tiff, because QuPath supports OpenSlide and OpenSlide supports <em>some</em> .mrxs files. There is more info at <a href="https://qupath.readthedocs.io/en/0.4/docs/intro/formats.html#mrxs-3d-histech" class="inline-onebox">Supported image formats — QuPath 0.4.3 documentation</a></p> ;;;; <p>Thank you but I still cannot get anything from the image.<br>
I tried with the following macro:</p>
<pre><code class="lang-auto">open("CT_e-4.tif");
run("Gaussian Blur...", "sigma=2.50");
setAutoThreshold("Default no-reset");
//run("Threshold...");
setAutoThreshold("Minimum dark no-reset");
setAutoThreshold("Default dark no-reset");
//setThreshold(155, 255);
setOption("BlackBackground", true);
run("Convert to Mask");
run("Close");
run("Convert to Mask");
run("Watershed");
saveAs("Tiff", "/watershed.tif");
run("Undo");
setAutoThreshold("Default dark no-reset");
//run("Threshold...");
//setThreshold(0, 128);
run("Convert to Mask");
run("Close");
run("Invert");
run("Watershed");
run("Undo");
run("Find Maxima...", "prominence=10 output=[Point Selection]");
saveAs("Tiff", "maxima.tif");
</code></pre>
<p>I get only one dot. Is there a video tutorial more advanced than this?<br>
<a href="https://www.youtube.com/watch?v=zwjXbeiviH0" rel="noopener nofollow ugc">https://www.youtube.com/watch?v=zwjXbeiviH0</a></p>
<p>Thank you.</p>
<p><a class="attachment" href="/uploads/short-url/dEBp3eDxyfLkuuObdd7QsL1ID7S.tif">maxima.tif</a> (1.83 MB)</p>
<p><a class="attachment" href="/uploads/short-url/qxoEu366qbfr9HQyQoeDxrzMgC9.tif">watershed.tif</a> (1.83 MB)</p> ;;;; <p>Hello</p>
<p>I am currently scanning slides using a Panoramic 3dhistech slide scanner with 4 channels and Z stack. I was able to convert the file from MRXS to ome.tiff format, which allows me to analyze it using Qupath.<br>
I am now in the process of ordering a new computer and considering the Apple Mac Mini M2 because of its impressive features. However, I have never used a Mac system before and was wondering if you could offer any advice or if anyone has experience analyzing large files on a Mac.</p> ;;;; <p>Hi <a class="mention" href="/u/jni">@jni</a>,</p>
<p>In fact I did try already.<br>
However, stereo.compute() does not recognize markers properly, using RGB system directly. I believe it’s because markers get a very similar pixel’s gradient than its neighborhood, as the other objects and the scenario, once they are converted from RGB to grayscale.</p>
<p>That’s the reason I thought that converting them to CMYK and  running images in stereo.compute(CMYK_L,CMYK_R) would return a better mapping, identifying markers and their (x,y) coordinates accurately.</p>
<p>an example of grayscale has been attached.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912.png" data-download-href="/uploads/short-url/c6PLLWtIllF9QrIrHD7LLfK1xyq.png?dl=1" title="Screen Shot 2023-03-26 at 11.38.56 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912_2_433x500.png" alt="Screen Shot 2023-03-26 at 11.38.56 AM" data-base62-sha1="c6PLLWtIllF9QrIrHD7LLfK1xyq" width="433" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912_2_433x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912.png 2x" data-dominant-color="B6CEC7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-26 at 11.38.56 AM</span><span class="informations">642×740 105 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>another example of an image with the results of stereo.compute() has been attached.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e.png" data-download-href="/uploads/short-url/hnBx0o0SSeAQjeAdXss79ESOnDg.png?dl=1" title="Screen Shot 2023-03-26 at 11.27.07 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e_2_431x500.png" alt="Screen Shot 2023-03-26 at 11.27.07 AM" data-base62-sha1="hnBx0o0SSeAQjeAdXss79ESOnDg" width="431" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e_2_431x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e.png 2x" data-dominant-color="676767"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-26 at 11.27.07 AM</span><span class="informations">482×558 47.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Anyway, for that reason, I converted from RGB to CMYK previously. So that, I could get a better contrast between markers, arm, and scenario (i.e. markers in magenta).<br>
As you can see in this image below</p>
<p>Image in the CMYK system<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/04b983e6ca1217c9907990b9717820589eee7464.png" alt="Screen Shot 2023-03-25 at 11.08.44 PM" data-base62-sha1="FNndob3PC7fsEoDWIis6Xx3y3a" width="351" height="349"></p>
<p>However, converting from CMYK to grayscale system then running stereo.compute(CMYK_L, CMYK_R) returns an error as indicated.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48.png" data-download-href="/uploads/short-url/nHfP3LVL3hvfTazlJ1BH0r5CWSc.png?dl=1" title="Screen Shot 2023-03-26 at 11.43.18 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_690x184.png" alt="Screen Shot 2023-03-26 at 11.43.18 AM" data-base62-sha1="nHfP3LVL3hvfTazlJ1BH0r5CWSc" width="690" height="184" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_690x184.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_1035x276.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_1380x368.png 2x" data-dominant-color="F7F7F7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-26 at 11.43.18 AM</span><span class="informations">2732×730 91.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The error is related to dimension, as CMYK has one more dimension such as (640,480),4, while RGB has only the first two, as in (640,480).</p>
<p>How do we calculate disparity when the input is in the CMYK system?</p>
<p>Best wishes,</p> ;;;; <p>It seems that the path changed a bit ago.<br>
“from fiji.plugin.trackmate.tracking.jaqaman import SparseLAPTrackerFactory” fixed the error.<br>
I learned that I should refer to “<a href="https://imagej.net/plugins/trackmate/scripting/scripting" class="inline-onebox" rel="noopener nofollow ugc">Scripting TrackMate</a>”, and many other examples are outdated.</p>
<p>I am not sure if I can use the Trackmate functions using ImageJ-MATLAB without the compatibility issue. I appreciate all the effort of the Trackmate team. Thank you!</p> ;;;; <p>Nice,</p>
<p>Thanks <a class="mention" href="/u/dnmason">@dnmason</a>!</p>
<p>Not sure why I remembered that’s how this line works…<br>
Anyway, great advice!</p>
<p>Cheers,<br>
Daniel</p> ;;;; <p>Hi <a class="mention" href="/u/daniel_waiger">@Daniel_Waiger</a>,</p>
<p>The problem is that you’re not passing a full path string to <code>File.getParent</code>, only a name without extension.</p>
<p>The function you need is <code>File.directory</code> which will then return the path to the most recently opened file. If you want to step up to the parent, you can then use <code>File.getParent</code> as in your original code.</p>
<p>Hope that helps!</p> ;;;; <p>Just a short update on this: I looked into adding support for the transformations with ome.zarr file format, but I decided to wait till ome.zarr v0.5 will be released (which will include significant more transformation functionality).</p>
<p>But I extended the current code, in order to throw an error message when trying to add a transformation for ome.zarr, see <a href="https://github.com/mobie/mobie-utils-python/pull/94" class="inline-onebox">Start implementing transformation support for ome.zarr by constantinpape · Pull Request #94 · mobie/mobie-utils-python · GitHub</a>.</p> ;;;; <p>By the way, I have already asked the question on SO.<br>
<a href="https://stackoverflow.com/questions/75822503/display-mesh-in-obj-file-format-with-imagej" rel="noopener nofollow ugc">https://stackoverflow.com/questions/75822503/display-mesh-in-obj-file-format-with-imagej</a></p> ;;;; <p>0</p>
<p>I am using Fiji-ImageJ 1.53q.</p>
<p>I used “3D Viewer” plugin and tried to display the following simple mesh via a .obj file but I don’t see any cube getting visualized. The log window just displayed the filepath and if I double click on it, it opens another window and displays textual information of <span class="hashtag">#vertices</span> and <span class="hashtag">#faces</span>. What am I missing?</p>
<h1>
<a name="h-8-vertices-6-faces-1" class="anchor" href="#h-8-vertices-6-faces-1"></a>8 vertices, 6 faces</h1>
<p>v -1.000000 -1.000000 1.000000<br>
v 1.000000 -1.000000 1.000000<br>
v 1.000000 1.000000 1.000000<br>
v -1.000000 1.000000 1.000000<br>
v -1.000000 -1.000000 -1.000000<br>
v 1.000000 -1.000000 -1.000000<br>
v 1.000000 1.000000 -1.000000<br>
v -1.000000 1.000000 -1.000000<br>
f 1 2 3 4<br>
f 8 7 6 5<br>
f 2 1 5 6<br>
f 3 2 6 7<br>
f 4 3 7 8<br>
f 1 4 8 5</p> ;;;; <p>Hi Konrad</p>
<p>Thanks for your reply.</p>
<p>I didn’t success to run deeplab cut with Ubuntu.</p>
<p>I will try with a new computer in windows 10 with gpu nvidia quadro RTX 4000.</p>
<p>Before I will begin which version of :</p>
<p><strong>Python, tensorflow , cuda and cudnn should be installed.</strong></p>
<p>Thanks so much</p>
<p>Silvia</p> ;;;; <p>Hi, below the download button on the homepage there is a link to the <a href="https://qupath.readthedocs.io/en/stable/docs/intro/installation.html">installation notes</a> in case there are problems installing QuPath:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9.png" data-download-href="/uploads/short-url/usRxiUEFZVsuyFu58iZthYvnpsl.png?dl=1" title="Screenshot 2023-03-26 at 09.31.34"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_516x375.png" alt="Screenshot 2023-03-26 at 09.31.34" data-base62-sha1="usRxiUEFZVsuyFu58iZthYvnpsl" width="516" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_516x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_774x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_1032x750.png 2x" data-dominant-color="EBEAE5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-26 at 09.31.34</span><span class="informations">1300×944 191 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/ctrueden">@ctrueden</a> and fellow scripters,<br>
I’m trying to get the path of an image to use later with another plugin in my macro.<br>
<strong>The issue is that it prints the path as ‘0’ instead of the expected string</strong><br>
Does anyone have an idea/experience with that bug?<br>
I tried different locations on my PC<br>
<a class="attachment" href="/uploads/short-url/jAYJEJcyiBejcmVZRHTItuSAoSn.tif">IDA-OCS 10-4.tif</a> (4.1 MB)<br>
Since the image’s original location is a google drive folder, a local folder didn’t solve the issue.</p>
<hr>
<p>Snippet:</p>
<pre><code class="lang-auto">run("Open...");
run("8-bit");
run("Set Scale...", "distance=267.5 known=100 unit=microns global");
imgName = getTitle();
orgName = File.getNameWithoutExtension(imgName);
print(imgName);
print(orgName);
path = File.getParent(orgName);
print(path);



/*
Pre-processing of the image: Cropping the image's central region, renaming it, and enhancing the contrast for the segmentation part.
 */
makeRectangle(311, 148, 712, 672);
run("Duplicate...", " ");
rename(orgName + "-CLAHE");
newName = getTitle();
print(newName);

Etc... etc...
</code></pre>
<p><strong>Log:</strong><br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f91ef66afe11e216b703c668a41cee63c8f35e10.png" alt="image" data-base62-sha1="zxPe58mjP0r8iL5giLgZtuQspCo" width="206" height="197"></p>
<p><strong>Original Image:</strong><br>
<a class="attachment" href="/uploads/short-url/jAYJEJcyiBejcmVZRHTItuSAoSn.tif">IDA-OCS 10-4.tif</a> (4.1 MB)</p>
<p>Thanks and cheers for any help,<br>
Daniel</p> ;;;; <p><a class="mention" href="/u/iuri">@iuri</a>, have you tried passing in the RGB image? I don’t know what the <code>StereBM_create</code> function does internally, but if it accepts RGB images, it probably uses the color information to find marker correspondences.</p>
<p>I don’t have any experience with this kind of data but this example in the scikit-image gallery might help:</p>
<p><a href="https://scikit-image.org/docs/stable/auto_examples/transform/plot_fundamental_matrix.html" class="onebox" target="_blank" rel="noopener">https://scikit-image.org/docs/stable/auto_examples/transform/plot_fundamental_matrix.html</a></p>
<p>As you can see in the example, the approach is divided into finding the corresponding markers, and then matching them to find the projections. You can therefore use whatever approach you want to find your markers, including e.g. thresholding the red channel only. However, you’ll need some kind of feature to describe the markers so that the <code>match_descriptors</code> function can find matches.</p> ;;;; <p>Hello there,<br>
Reading image as RGB and converting them to CMYK resulted in a better color definition to identify markers in an actuator. As you can see them in red, in the image attached.</p>
<p>Now, they must be sterilized, as in:<br>
‘’‘’<br>
stereo = cv.StereoBM_create(numDisparities=16, blockSize=15)<br>
disparity = stereo.compute(CMYK_L, CMYK_R)<br>
‘’‘’</p>
<p>However , the following error occurs, because CMYK has a fourth dimension (i.e. K), causing calculations to break.</p>
<p>error: OpenCV(4.7.0) /io/opencv/modules/calib3d/src/stereobm.cpp:1173: error: (-210:Unsupported format or combination of formats) Both input images must have CV_8UC1 in function ‘compute’</p>
<p>How can images be converted from CMYK to grayscale directly?<br>
Should I use another color system?<br>
or even better, does stereo.compute()  support CMYK system?</p>
<p>Converting them back to RGB is not am option/solution. Because markers get lost when the image is converted from RGB to grayscale.</p>
<p>Markers get similar gray as the scenario, making it hard, to any openCV lib or segmentation algorithm such as Cany, to track marker’s coordinates (x,y) accurately.</p>
<p>Do I need to convert them to another system? which one?</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/04b983e6ca1217c9907990b9717820589eee7464.png" alt="Screen Shot 2023-03-25 at 11.08.44 PM" data-base62-sha1="FNndob3PC7fsEoDWIis6Xx3y3a" width="351" height="349"></p> ;;;; <p>Thanks a lot.</p> ;;;; <p>If you search the forum you will find a few posts all pointing to the docs where the error message is explained <a href="https://forum.image.sc/t/qupath-damaged-after-ventura-mac-update/73517/2" class="inline-onebox">QuPath damaged after Ventura Mac Update - #2 by petebankhead</a></p> ;;;; <p>The following macro is meant to convert images from a .lif file into tiffs, ask for brightness adjustments and apply those, it’s meant to save the raw (unadjusted) tiff, the adjusted tiff, and an adjusted montage of the channels within each image.</p>
<p>It does all of that except applying the brightness adjustments to the adjusted tiff. Oddly, it does correctly apply it to the montage…</p>
<pre><code class="lang-auto">run("Bio-Formats Macro Extensions");
GetTime();
setBatchMode(true); 
input = getDirectory("Input directory, folder where your .lif file is and nothing else");
output = getDirectory("Output directory, where you'd like your adjusted .tiff files to go");

run("Input/Output...", "jpeg=100 gif=-1 file=.xls use_file copy_row save_column save_row");
Dialog.create("File type");
Dialog.addString("File suffix: ", ".tif", 5);
Dialog.addNumber("Ch1:", 0);
Dialog.addNumber("Ch1:", 65535);
Dialog.addNumber("Ch2:", 0);
Dialog.addNumber("Ch2:", 65535);
Dialog.addNumber("Ch3:", 0);
Dialog.addNumber("Ch3:", 65535);
Dialog.addNumber("Ch4:", 0);
Dialog.addNumber("Ch4:", 65535);
Dialog.addNumber("Ch5:", 0);
Dialog.addNumber("Ch5:", 65535);
Dialog.show();
suffix = Dialog.getString();
Range1 = Dialog.getNumber();
Range2 = Dialog.getNumber();
Range3 = Dialog.getNumber();
Range4 = Dialog.getNumber();
Range5 = Dialog.getNumber();
Range6 = Dialog.getNumber();
Range7 = Dialog.getNumber();
Range8 = Dialog.getNumber();
Range9 = Dialog.getNumber();
Range10 = Dialog.getNumber();

BC_range = newArray(Range1,Range2,Range3,Range4,Range5,Range6,Range7,Range8,Range9,Range10);

print("Brightness and contrast range is: ");
Array.print(BC_range);
print("Blue","Green","Red","Grays","Yellow");

suffix = ".lif";

processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
}

function processFile(input, output, file) {
	Ext.setId(input + file);
	Ext.getSeriesCount(count);
	print("Processing: " + input + file+ " - "+d2s(count,0)+" images");

	for (f=0;f&lt;count;f++) {
		openLif(input+file,f);
		print("Saving to: " + output);
		listImages();
	}
}

function openLif(input,f){
	Ext.setSeries(f);
	run("Bio-Formats Importer", "open=[" +input +"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_" + d2s(f+1,0));
}

//Create string "image_1 image_2 image_3 image_4...."
function seriesN(num){
	str = "";
	for (i=0; i&lt;num; i++){
		ii = i+1;
        str = str+"image_"+ii+" ";
     }
     return str;
}

//Save
function listImages(){
	imageList = getList("image.titles");
	if (imageList.length==0)
	     print("No image windows are open");
	else {
    	print("Image windows:");
     	for (i=0; i&lt;imageList.length; i++){
        	print("   "+imageList[i]);
        	// TODO: Add condition for saving unadjusted files
                saveTiff(imageList[i]);
                
                // Process the files as per the second script
                processTiff(output, output, imageList[i]);
     	}
	}
	print("");
}


function saveTiff(winName){
	selectWindow(winName); 
	saveAs("Tiff", output + winName);
	close();
}

function processTiff(input, output, file) {

	print("Processing: " + input + file);

	open(input+file+".tif");
	renderColor(file);
	brightnessNcontrast(file);
	deleteSlices(file);

	saveAs("Tiff", output + file + "_BCadj");
	rename(file);
	
	RGBmerge(file);
	scaleBar();
	makeMontage(file);

	//selectWindow("Montage1to5");
	//saveAs("Jpeg", output + file + "_montage1to5");
	selectWindow("MontageHorizontal");
	saveAs("Jpeg", output + file + "_BCmont");

	print("Saving to: " + output);
	run("Close All");
}



// Give colors for each slice
function renderColor(file){
	color = newArray("Blue","Green","Red","Grays","Yellow");
	//color = newArray("Blue","Green","Red");

	run("Make Composite", "display=Color");
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		run(color[i]);
	}
}
// Change brightness and contrast
function brightnessNcontrast(file){
	//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		setMinAndMax(BC_range[2*i],BC_range[2*i+1]);
	}
	
}

// Delete slices. You want to delete slice a, b
function deleteSlices(file){
	deleteA = 0;
	deleteB = 0;
	//deleteB &gt; deleteA. deleteB should be larger than deleteA

	print("deleting the slice "+deleteA+" and "+deleteB);
	selectWindow(file+".tif");
	if(deleteB&gt;0){
		setSlice(deleteB);
		run("Delete Slice", "delete=channel");
		}
	if(deleteA&gt;0){
		setSlice(deleteA);
		run("Delete Slice", "delete=channel");
	}	
}

// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.
function RGBmerge(file){
	run("Duplicate...", "title=4channels duplicate");
	run("RGB Color");
		
	selectWindow(file);
	run("Duplicate...", "title=Merge duplicate");
	Stack.setDisplayMode("composite");
	run("RGB Color");
	run("Copy");

	selectWindow("4channels (RGB)");
	setSlice(nSlices);
	run("Add Slice"); 
	run("Paste"); 

	close("4channels");
	close("Merge");
	close("Merge (RGB)");
	
	// "4channels (RGB)" is made.
}

// Add scale bar
function scaleBar(){
	setSlice(nSlices);
	run("Set Scale...", "distance=311.0016 known=100 pixel=1 unit=µm");
	//run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] hide");
	run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] bold");
}

function makeMontage(input){
	//selectWindow("4channels (RGB)");
	//run("Make Montage...", "columns=1 rows=5 scale=0.25 border=2");
	//rename("Montage1to5");
	selectWindow("4channels (RGB)");
	run("Make Montage...", "columns="+nSlices+" rows=1 scale=0.5 border=2");
	rename("MontageHorizontal");
}
function GetTime(){
     MonthNames = newArray("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec");
     DayNames = newArray("Sun", "Mon","Tue","Wed","Thu","Fri","Sat");
     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);
     TimeString ="Date: "+DayNames[dayOfWeek]+" ";
     if (dayOfMonth&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+dayOfMonth+"-"+MonthNames[month]+"-"+year+"\nTime: ";
     if (hour&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+hour+":";
     if (minute&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+minute+":";
     if (second&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+second;
     print(TimeString);
}
</code></pre> ;;;; <p>Hi,<br>
I can’t load QuPath-0.4.3-MAC.pkg file on my MacBook Pro Intel. “Cannot open because it is damaged” note appears on my screen. Can you help me solving this problem. And also I want to open bif files on my Mac. Can I open them?</p> ;;;; <p>Hi!</p>
<p>I have a plugin carrying a bunch of different acquistions. In particular some acquisitions can be stopped if certain criterion are reached (e.g. device property value corresponding to an activation laser in localization microscopy). The acquisition are started with <code>runAcquisitionWithSettings</code>:</p>
<pre data-code-wrap="Java"><code class="lang-plaintext">AcquisitionManager acqManager = studio.acquisitions();
Datastore store = acqManager.runAcquisitionWithSettings(seqBuilder.build(), false);
</code></pre>
<p>In the past (MM 2.0.0), I kind of hack my way around stopping the acquisition using the following call:</p>
<pre data-code-wrap="Java"><code class="lang-plaintext">((DefaultAcquisitionManager) studio.acquisitions()).getAcquisitionEngine().stop(true);
</code></pre>
<p>Since <a href="https://github.com/micro-manager/micro-manager/commit/b126fcf0adacda9970b90fb161faa384fce50d4f" rel="noopener nofollow ugc">b126fcf0</a> the method <code>getAcquisitionEngine</code> is private and the snippet breaks.</p>
<p>The other exposed method, <a href="https://github.com/micro-manager/micro-manager/blob/9eca07ec58cac6d3f76b221f1e75e040b5b751d2/mmstudio/src/main/java/org/micromanager/acquisition/internal/DefaultAcquisitionManager.java#L173" rel="noopener nofollow ugc"><code>haltAcquisition</code></a>, opens a dialog and thus requires user intervention. This is not suitable for automated acquisitions such as ours.</p>
<p>Is there any way to stop an on-going acquisition without dialog?</p>
<p>Thanks!<br>
tagging <a class="mention" href="/u/nicost">@nicost</a> <a class="mention" href="/u/henrypinkard">@henrypinkard</a> <a class="mention" href="/u/marktsuchida">@marktsuchida</a></p> ;;;; <p>If you export the masks from QuPath, you can choose to create a border region, and rather than the usual choice of making that border region a different, third value, you could choose to make it the same as the background value.</p>
<p>Alternatively you could take the ROI for all of the cells and erode it one pixel, but that seems like way more processing power and time, and still requires exporting the masks.</p> ;;;; <p>The link and video shows you how to create annotations from a trained pixel classifier. In your image you haven’t saved the classifier. Maybe you didn’t create a project either? Not sure.</p> ;;;; <p>Thanks . I got it here, I had some folders on the external HD and other desktops on the computer. I was wondering if in addition there was somewhere in QuPath to delete as well, but no need.</p> ;;;; <p>Hi guys! I am currently performing nuclei segmentation using DAPI channel of multiplex immunofluorescence tumor biopsy images. My goal is to perform the segmentation with various open source software to compare and contrast them.</p>
<p>I have a working pipeline with semi-optimized parameters in QuPath but have one issue: For standardization across platforms, I require there to be a space between detected objects (nuclei). Is there a way to do this in QuPath ?</p>
<p>For example in the screenshot below, it can be seen that the objects are overlapping<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0b0c95024be35a1437cd6fea316e98469aaeaf.png" data-download-href="/uploads/short-url/aHRtWyBvoqMeu1WKtbpZj4ySCkf.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0b0c95024be35a1437cd6fea316e98469aaeaf.png" alt="image" data-base62-sha1="aHRtWyBvoqMeu1WKtbpZj4ySCkf" width="498" height="500" data-dominant-color="545454"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">608×610 16.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>There is a module that does this object shrinking in CellProfiler and was wondering if there was an equivalent in QuPath, or if someone has any ideas if I can export this to another program to achieve my desired results?</p>
<p>Thanks!</p> ;;;; <p>I am trying to use the ImageJ plugin TrackMate using a script.</p>
<p>First, I tried to run TrackMate using ImageJ-MATLAB following the example in “<a href="https://imagej.net/plugins/trackmate/scripting/using-from-matlab" class="inline-onebox" rel="noopener nofollow ugc">Using TrackMate from MATLAB</a>”, but the script failed to import “SparseLAPTrackerFactory”. It seems that the other parts were imported.</p>
<pre><code class="lang-auto">%% The import lines, like in Python and Java
import java.lang.Integer
import ij.IJ
import fiji.plugin.trackmate.TrackMate
import fiji.plugin.trackmate.Model
import fiji.plugin.trackmate.Settings
import fiji.plugin.trackmate.SelectionModel
import fiji.plugin.trackmate.Logger
import fiji.plugin.trackmate.features.FeatureFilter
import fiji.plugin.trackmate.detection.LogDetectorFactory
import fiji.plugin.trackmate.tracking.sparselap.SparseLAPTrackerFactory
import fiji.plugin.trackmate.tracking.LAPUtils
import fiji.plugin.trackmate.gui.displaysettings.DisplaySettingsIO
import fiji.plugin.trackmate.gui.displaysettings.DisplaySettings
import fiji.plugin.trackmate.visualization.hyperstack.HyperStackDisplayer
</code></pre>
<p>Then, I try to use ImageJ macro using Jython. I saved the script below (language: python). But the script makes the same error for “SparseLAPTrackerFactory”</p>
<pre><code class="lang-auto"># from ij import IJ
from ij import IJ, ImagePlus, ImageStack
from fiji.plugin.trackmate import Model
from fiji.plugin.trackmate import Settings
from fiji.plugin.trackmate import TrackMate
from fiji.plugin.trackmate.detection import LogDetectorFactory
from fiji.plugin.trackmate.detection import DogDetectorFactory
from fiji.plugin.trackmate.tracking.sparselap import SparseLAPTrackerFactory
</code></pre>
<p>I have reinstalled Fiji and MATLAB, but it was not resolved. Could you please comment on this issue or provide an example that works in the latest version?</p>
<blockquote>
<p>OS: Windows 11<br>
ImageJ - Version 1.54c 6 March 2023<br>
MATLAB 2023a</p>
</blockquote> ;;;; <p>Thank you！<br>
I have trained a pixel classifier and detected tumor areas. But it seems that they  can only guide me to annotate. I can not keep them and still can not make batch annotations.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5.jpeg" data-download-href="/uploads/short-url/22YcP00fiDXT5rZERMiZXqQMOxv.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_690x301.jpeg" alt="image" data-base62-sha1="22YcP00fiDXT5rZERMiZXqQMOxv" width="690" height="301" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_1380x602.jpeg 2x" data-dominant-color="855983"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1797×785 259 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Thank you so much, no worries. I tried and I’m waiting on a response.</p> ;;;; <p>Sorry, I don’t have any other resources about this. Perhaps you can try contacting the authors?</p> ;;;; <p>You wrote a macro code to read the x-y values from a .csv file and draw the related curve on the initial calibrated image. Here is the code:</p>
<p>xpoints = Table.getColumn(“x_points”);<br>
ypoints = Table.getColumn(“y_points”);<br>
toUnscaled(xpoints, ypoints);<br>
makeSelection(“freeline”, xpoints, ypoints);</p>
<p>I need to edit the code. So, I can input two different x-y values from two .csv files and draw both of them. So, I would be able to compare two curves. I also need to be able to change the color and thickness of the drawn curves. For example, one curve with yellow dash lines, and the other continuous thick black line. Could you please guide me on how I can change the code for this purpose?</p>
<p>Thank you very much.</p> ;;;; <p>Hi <a class="mention" href="/u/weichen">@weichen</a> ,</p>
<p>we had a discussion earlier about potential strategies in these threads:</p><aside class="quote quote-modified" data-post="2" data-topic="41816">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/haesleinhuepf/40/16931_2.png" class="avatar">
    <a href="https://forum.image.sc/t/clij2-blockwise-analysis/41816/2">CLIJ2 Blockwise analysis</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hey <a class="mention" href="/u/rfrs">@rfrs</a>, 
what kind of denoising / analysis where you thinking of? I’m also working with light sheet data a lot and we use <a href="https://clij.github.io/clij2-docs/reference_downsampleSliceBySliceHalfMedian">downsampleSliceBySliceHalfMedian</a> in routine to reduce our amount of data while imaging. 
To fit your data into GPU memory of your computer, you have several strategies: 

Process it slice by slice using <a href="https://clij.github.io/clij2-docs/reference_pushCurrentSlice">pushCurrentZSlice</a>. There is also an <a href="https://github.com/clij/clij2-docs/blob/335fb08749225be3b522f4c20fcd78cc06bd2496/src/main/macro/push_pull_slices.ijm">example macro</a> for this.
Process tiles using the methods <a href="https://clij.github.io/clij2-docs/reference_pushTile">pushTile</a> and <a href="https://clij.github.io/clij2-docs/reference_pullTile">pullTile</a> as also discussed in <a href="https://forum.image.sc/t/segmentation-of-islets-in-fluorescence-image/35358/6">this thread</a>. Please note that pushTil…
  </blockquote>
</aside>

<aside class="quote quote-modified" data-post="6" data-topic="35358">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/haesleinhuepf/40/16931_2.png" class="avatar">
    <a href="https://forum.image.sc/t/segmentation-of-islets-in-fluorescence-image/35358/6">Segmentation of islets in fluorescence image</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Awesome! Let me know how it goes! For working with hugh images, there are two new commands in CLIJx: pushTile and pullTile which allow you to process a huge image piece wise. 
Here is some example code: 
<a href="https://github.com/clij/clij2-docs/blob/master/src/main/macro/processTiles3D.ijm">https://github.com/clij/clij2-docs/blob/master/src/main/macro/processTiles3D.ijm</a> 
for (x = 0; x &lt; numTilesX; x++) {
	for (y = 0; y &lt; numTilesY; y++) {
		for (z = 0; z &lt; numTilesZ; z++) {
			Ext.CLIJx_pushTile(original, x, y, z, tileWidth, tileHeight, tileDepth, margin, margin, margin);
			
			Ex…
  </blockquote>
</aside>

<p>If you work with Python, you could also use <a href="https://www.dask.org/">dask</a> for processing the data tile-by-tile:<br>
<a href="https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiled_nuclei_counting_quick.html" class="onebox" target="_blank" rel="noopener">https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiled_nuclei_counting_quick.html</a></p>
<p>Furthermore, cropping and downsampling are often alternatives for answering a scientific question without processing every single pixel.</p>
<p>Let us know if this helps!</p>
<p>Best,<br>
Robert</p> ;;;; <p>Hi everyone,</p>
<p>I am trying to build a macro that will allow me to subdivide an ROI into smaller ROIs to do a more in-depth spatial analysis of signal intensity and the number of stained cells within a brain region. I managed to build a macro that allows me to draw a rectangular ROI and subdivide it into smaller rectangle ROI:</p>
<p><strong>Script</strong></p>
<p>// Clear the ROI Manager<br>
roiManager(“reset”);</p>
<p>// Get the current selection coordinates<br>
getSelectionCoordinates(x, y);</p>
<p>// Draw a rectangle around the current selection<br>
makeRectangle(x[0], y[0], x[2] - x[0], y[2] - y[0]);<br>
roiManager(“add”);</p>
<p>// Prompt the user to enter the number of subdivisions<br>
numDivisions = getNumber(“Enter the number of subdivisions:”, 2);</p>
<p>// Calculate the size of each subdivision<br>
subWidth = (x[2] - x[0]) / numDivisions;<br>
subHeight = (y[2] - y[0]) / numDivisions;</p>
<p>// Create sub-ROIs and add them to the ROI Manager<br>
for (i = 0; i &lt; numDivisions; i++) {<br>
for (j = 0; j &lt; numDivisions; j++) {<br>
subX = x[0] + i * subWidth;<br>
subY = y[0] + j * subHeight;<br>
makeRectangle(subX, subY, subWidth, subHeight);<br>
roiManager(“add”);<br>
}<br>
}</p>
<p><strong>However</strong>, I have not been able to build a script that allows me to subdivide an irregular ROI into smaller ROIs of the equal area by dividing the original ROI into a different shape with smaller ROIs. I came up with this script, but it is giving me an error with the ROI manager command… Does anyone have an idea to pass this issue?</p>
<p><strong>Script</strong></p>
<p>// Get user-defined input for a number of sub-ROIs<br>
n = getNumber(“Enter number of sub-ROIs:”, 2);</p>
<p>// Get original ROI<br>
roiManager(“Show All”);<br>
waitForUser(“Select the ROI to subdivide.”);<br>
<em><strong>original = roiManager(“getCurrentRoi”);</strong></em></p>
<p>// Calculate the area of the original ROI<br>
originalArea = original.getStatistics().area;</p>
<p>// Calculate area of each sub-ROI<br>
subROIarea = originalArea / n;</p>
<p>// Initialize variables<br>
startX = original.getBounds().x;<br>
startY = original.getBounds().y;<br>
width = original.getBounds().width;<br>
height = original.getBounds().height;</p>
<p>// Determine dimensions of each sub-ROI based on the desired area<br>
subROIsideLength = sqrt(subROIarea);<br>
subROInumSides = 6; // Change this to specify number of sides in polygon</p>
<p>// Subdivide original ROI into smaller ROIs<br>
for (i = 0; i &lt; n; i++) {<br>
subROICenterX = startX + (i % (width / subROIsideLength)) * subROIsideLength + subROIsideLength / 2;<br>
subROICenterY = startY + (Math.floor(i / (width / subROIsideLength))) * subROIsideLength + subROIsideLength / 2;<br>
subROIvertices = <span class="chcklst-box fa fa-square-o fa-fw"></span>;<br>
for (j = 0; j &lt; subROInumSides; j++) {<br>
subROIvertices[2 * j] = subROICenterX + subROIsideLength / 2 * Math.cos(2 * Math.PI * j / subROInumSides);<br>
subROIvertices[2 * j + 1] = subROICenterY + subROIsideLength / 2 * Math.sin(2 * Math.PI * j / subROInumSides);<br>
}<br>
subROI = new PolygonRoi(subROIvertices, Roi.POLYGON);<br>
roiManager(“add”, subROI);<br>
}</p>
<p>// Remove original ROI from ROI Manager<br>
roiManager(“select”, 0);<br>
roiManager(“delete”);</p>
<p>Thank you in advance for any help that you all can give me</p> ;;;; <p>Thank you very much!</p> ;;;; <p>You would delete the folder wherever it is in the operating system you are using. There is nothing special about deleting a project folder. It gets deleted like any other file/folder. If you forgot where you created the project, you can find that by right clicking on the images in the project and navigating to the project folder.</p> ;;;; <p>Hello all! I tried to gaussian fit a synthetic PSF to get its xy and z resolution. I got some numbers but something doesn’t look right. Can I have some suggestions please?</p>
<p>This is the synthetic PSF I generated using PSF generator.</p><aside class="onebox googledrive" data-onebox-src="https://drive.google.com/file/d/1fNDTqw0XosmV5LuasyhMyH0xPsfOpWeb/view?usp=sharing">
  <header class="source">

      <a href="https://drive.google.com/file/d/1fNDTqw0XosmV5LuasyhMyH0xPsfOpWeb/view?usp=sharing" target="_blank" rel="noopener nofollow ugc">drive.google.com</a>
  </header>

  <article class="onebox-body">
      <a href="https://drive.google.com/file/d/1fNDTqw0XosmV5LuasyhMyH0xPsfOpWeb/view?usp=sharing" target="_blank" rel="noopener nofollow ugc"><span class="googledocs-onebox-logo g-drive-logo"></span></a>



<h3><a href="https://drive.google.com/file/d/1fNDTqw0XosmV5LuasyhMyH0xPsfOpWeb/view?usp=sharing" target="_blank" rel="noopener nofollow ugc">PSF BW.tif</a></h3>

<p>Google Drive file.</p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>import matplotlib.pyplot as plt<br>
import numpy as np<br>
from skimage import filters<br>
from skimage import measure</p>
<p>PSF_img = io.imread(‘PSF BW.tif’)<br>
thresh= filters.threshold_otsu(PSF_img)<br>
mask = PSF_img&gt;thresh<br>
labeled_mask = measure.label(mask)<br>
props = measure.regionprops_table(labeled_mask,properties=[‘centroid’,<br>
‘axis_minor_length’,‘axis_major_length’])</p>
<p><span class="hashtag">#PSF</span> center<br>
z0=int(props.centroid[0])<br>
y0=int(props.centroid[2])</p>
<p><a class="hashtag" href="/tag/intensity">#<span>intensity</span></a> profile on the x<br>
x_line =PSF_img[z0,y0]<br>
x = np.arange(len(x_line))</p>
<h1>
<a name="gaussian-function-1" class="anchor" href="#gaussian-function-1"></a>Gaussian function</h1>
<p>def func(x, a, x0, sigma):<br>
return a<em>np.exp(-(x-x0)**2/(2</em>sigma**2))</p>
<p>from scipy.optimize import curve_fit<br>
popt, pcov = curve_fit(func, x, x_line)<br>
print(popt)</p>
<p>I’m expecting roughly a = 1, x0 = 127 or 128, sigma 1. But the fitted a,x0 are way off, and sigma is almost twice what I get from using line profile gaussian fitting in Fiji. 106nm or roughly 1 pixel.</p><aside class="onebox googledrive" data-onebox-src="https://drive.google.com/file/d/1HTKmyenEsz0w65GWGxUiz0xWqx2WEmaz/view?usp=sharing">
  <header class="source">

      <a href="https://drive.google.com/file/d/1HTKmyenEsz0w65GWGxUiz0xWqx2WEmaz/view?usp=sharing" target="_blank" rel="noopener nofollow ugc">drive.google.com</a>
  </header>

  <article class="onebox-body">
    
<div class="aspect-image" style="--aspect-ratio:690/362;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f6d19e3047f5205f19822ff4f1f4fd0a4c4c834_2_690x362.png" class="thumbnail" width="690" height="362" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f6d19e3047f5205f19822ff4f1f4fd0a4c4c834_2_690x362.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f6d19e3047f5205f19822ff4f1f4fd0a4c4c834_2_1035x543.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/f/5f6d19e3047f5205f19822ff4f1f4fd0a4c4c834.png 2x" data-dominant-color="DFE1E8"></div>

<h3><a href="https://drive.google.com/file/d/1HTKmyenEsz0w65GWGxUiz0xWqx2WEmaz/view?usp=sharing" target="_blank" rel="noopener nofollow ugc">Screen Shot 2023-03-25 at 3.27.39 PM.png</a></h3>

<p>Google Drive file.</p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Can someone point out what’s wrong with my fitting?</p> ;;;; <p>Hi,</p>
<p>After downgrading cellpose version from 2.1.1 to 2.0, I got the same error as Daniel posted.</p>
<pre><code class="lang-auto">.......
.......
Requirement already satisfied: importlib-metadata in c:\users\offline2\.conda\envs\cellpose\lib\site-packages (from numba-&gt;cellpose==2.0.0) (4.12.0)
Requirement already satisfied: colorama in c:\users\offline2\.conda\envs\cellpose\lib\site-packages (from tqdm-&gt;cellpose==2.0.0) (0.4.5)
Requirement already satisfied: zipp&gt;=0.5 in c:\users\offline2\.conda\envs\cellpose\lib\site-packages (from importlib-metadata-&gt;numba-&gt;cellpose==2.0.0) (3.8.1)
Installing collected packages: cellpose
  Attempting uninstall: cellpose
    Found existing installation: cellpose 2.1.1
    Uninstalling cellpose-2.1.1:
      Successfully uninstalled cellpose-2.1.1
Successfully installed cellpose-2.0

(cellpose) C:\Users\offline2&gt;

</code></pre>
<p>BIOP cellpose</p>
<pre><code class="lang-auto">cyto_channel:1:nuclei_channel:2
C:\Users\offline2\AppData\Local\Temp\cellposeTemp\C1-40x-1-1-t1.tif
Cellpose version is set to:2.0
[cmd.exe /C CALL conda.bat activate C:\Users\offline2\.conda\envs\cellpose &amp; python -Xutf8 -m cellpose --dir C:\Users\offline2\AppData\Local\Temp\cellposeTemp --pretrained_model cyto --chan 1 --chan2 2 --diameter 0 --flow_threshold 0.4 --cellprob_threshold 0.0 --verbose --save_tif --no_npy --use_gpu]
'conda.bat' is not recognized as an internal or external command,
operable program or batch file.
C:\Users\offline2\AppData\Local\Programs\Python\Python39\python.exe: No module named cellpose
Runner C:\Users\offline2\.conda\envs\cellpose exited with value 1. Please check output above for indications of the problem.
java.lang.NullPointerException
	at ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:230)
	at ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)
	at org.scijava.command.CommandModule.run(CommandModule.java:196)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

</code></pre>
<p>Best,<br>
chin</p> ;;;; <p>The SUM slice operation calculates the sum of pixel values across all slices of an image. As a result, the final pixel intensity can be very large and may exceed the maximum value that can be represented by a 16-bit format. Therefore, it is necessary to use a 32-bit format to store the final pixel intensity.</p> ;;;; <h2>
<a name="sara-i-found-it-easier-to-create-a-small-python-script-python-at-least-399-installation-install-cuda-i-am-using-cuda-116-httpsdocsnvidiacomcudacuda-installation-guide-microsoft-windowsindexhtml-pip-install-cellpose-pip-uninstall-torch-pip-install-torch-f-httpsdownloadpytorchorgwhltorch_stablehtml-1" class="anchor" href="#sara-i-found-it-easier-to-create-a-small-python-script-python-at-least-399-installation-install-cuda-i-am-using-cuda-116-httpsdocsnvidiacomcudacuda-installation-guide-microsoft-windowsindexhtml-pip-install-cellpose-pip-uninstall-torch-pip-install-torch-f-httpsdownloadpytorchorgwhltorch_stablehtml-1"></a>Sara,<br>
I found it easier to create a small Python script.<br>
python at least 3.9.9<br>
Installation:<br>
Install CUDA (I am using Cuda 11.6)<br>
<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html" class="inline-onebox" rel="noopener nofollow ugc">cuda-installation-guide-microsoft-windows 12.1 documentation</a><br>
pip install cellpose<br>
pip uninstall torch<br>
pip install torch -f <a href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener nofollow ugc">https://download.pytorch.org/whl/torch_stable.html</a>
</h2>
<p>import cv2  # pip install opencv-python if needed<br>
import skimage  # pip install scikit-image if needed<br>
from skimage.color import rgb2gray, label2rgb<br>
from skimage.measure import label, regionprops, regionprops_table<br>
import cellpose  # pip install cellpose<br>
import cellpose.models<br>
import matplotlib.pyplot as plt  # pip install matplotlib<br>
import matplotlib.patches as mpatches<br>
from matplotlib.patches import Rectangle<br>
import torch</p>
<p>image = cv2.imread(filename)<br>
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br>
channels = [0, 0]  # IF YOU HAVE GRAYSCALE<br>
labels, flows, style, diams = model_cyto.eval(gray, diameter=average_diameter_pixs, channels=channels)<br>
…</p> ;;;; <p>Hi, Robert<br>
I have a similar related question:<br>
In case of large images that exceeds the capacity of GPU memory.<br>
Are there any functions in CLIJ can allow lazy- loading/processing?<br>
(Something like “Using virtual stack” in the Bioformat importer)</p> ;;;; <p>I am working on a project doing pose estimation for dogs, however I am wondering, what is the best way to generalize the model to work on all dogs? Not just the dogs that I am training on?</p>
<p>Is there any documentation on this or best practices I can follow?</p>
<p>Thanks!</p> ;;;; <p>Good morning . How do I delete a QuPath project?</p> ;;;; <p>Hi <a class="mention" href="/u/suica46">@Suica46</a></p>
<p>I checked with one of our experts and it is had to tell what is going on here.<br>
Here is his response:</p>
<hr>
<p>The error seems to come from here:</p>
<p><a href="https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FZEISS%2Flibczi%2Fblob%2F9a716c2122eae58e69bad28bf1b51919b0e84769%2FSrc%2FlibCZI%2Fdecoder_wic.cpp%23L235&amp;data=05%7C01%7C%7C1af84ee4eabb4de27b0d08db2c795baa%7C28042244bb514cd680347776fa3703e8%7C0%7C0%7C638152671952735072%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=NIM%2BEebE7m6GsDRAgyWouch9czmxVgHl811Wrn77Xk4%3D&amp;reserved=0" rel="noopener nofollow ugc">https://github.com/ZEISS/libczi/blob/9a716c2122eae58e69bad28bf1b51919b0e84769/Src/libCZI/decoder_wic.cpp#L235</a></p>
<p>and it is a problem with constructing the WIC-decoder for JPGXR-compressed data (which is a Windows-component).</p>
<p>Reasons could be… just guessing… either some configuration-issue or COM-apartment-intricacy. Or the data is bogus.</p>
<p>Ideas/Question:</p>
<ul>
<li>Does the same operation work ok if running single-threaded? Or – how sure are we that the problem is related to flask/concurrency at all?</li>
<li>libCZI could be configured to not use WIC but its own JPGXR-decoder. On Windows and by default, it uses the Windows-codec, but on Linux it uses its own implementation. One could configure things to use the “bundled” decoder on Windows instead, and this would rule out “configuration- and COM-issues”.</li>
</ul> ;;;; <p>Thanks! One more question.<br>
When I use the threshold and size filter when doing the objection classification, The red signal is merged together so that the object identification can not distinguish the merged cells one by one. If increase the threshold and smooth choice, some small staining cells will be ignored. If decrease the threshold and smooth choice, the cells will merge into one object. Is there any solution for this?</p> ;;;; <p>Hi, I tried to use the latest version of FIJI-ImageJ, and it works well.<br>
<a class="attachment" href="/uploads/short-url/2uv5oWJvj4n0XZsRZlSb5FUszr7.tif">test.tif</a> (1.3 MB)</p> ;;;; <p>Yes, this is a related post<br>
merci</p> ;;;; <p>Thanks Sara<br>
This solution is not enough when working with my images</p> ;;;; <p>Hi everyone,</p>
<p>Does the pyimageJ package (<a href="https://github.com/imagej/pyimagej" class="inline-onebox" rel="noopener nofollow ugc">GitHub - imagej/pyimagej: Use ImageJ from Python</a>) have the functionality that is described here (<a href="https://imagej.net/plugins/diameterj" class="inline-onebox" rel="noopener nofollow ugc">DiameterJ</a>) in the diameterJ plugin (particularly the fiber diameter calculation functionality). If so, please can someone point me to a tutorial for this? Thanks so much!</p> ;;;; <p>Hello, masters, I want to use [deeplabcut.benchmark.metrics.calc_map_from_obj(<em>eval_results_obj</em> , <em>h5_file</em> , <em>metadata_file</em> , <em>oks_sigma=0.1</em> , <em>margin=0</em> , <em>symmetric_kpts=None</em> , <em>drop_kpts=None</em> )]to obtain mAP, but I don’t understand eval very well <em>eval_results_obj</em> , <em>h5_file</em> , <em>metadata_file</em> ,  What do the three parameters “file” specifically refer to? Where should I find these three files or how do I prepare them.</p> ;;;; <p>Open the .txt file in a text editor. Change all “|” characters to commas. Change the first line to “,x,y,”. Save as a .csv (comma-separated values) file. Open the .csv file as a table and run this macro:</p>
<pre><code class="lang-auto">newImage("Untitled", "8-bit black", 250, 250, 1);
xpoints = Table.getColumn("x");
ypoints = Table.getColumn("y");
makeSelection("polyline", xpoints, ypoints);
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/893ac821c41b528c86982d0938aa46c7e03e8d5c.png" data-download-href="/uploads/short-url/jzZlSaLc3BK7pSpOUrCucouEL8g.png?dl=1" title="Screenshot"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/893ac821c41b528c86982d0938aa46c7e03e8d5c_2_562x500.png" alt="Screenshot" data-base62-sha1="jzZlSaLc3BK7pSpOUrCucouEL8g" width="562" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/893ac821c41b528c86982d0938aa46c7e03e8d5c_2_562x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/893ac821c41b528c86982d0938aa46c7e03e8d5c.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/893ac821c41b528c86982d0938aa46c7e03e8d5c.png 2x" data-dominant-color="959591"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot</span><span class="informations">574×510 143 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><a class="attachment" href="/uploads/short-url/wGLeIL1VBrMGlRQQj8puK47CdrA.csv">table.csv</a> (588 Bytes)</p> ;;;; <p>Thanks! It is really puzzling for me as well.</p> ;;;; <p>Hi Johanna, I calibrated it on another photo and added it using “set scale”.</p> ;;;; <p>That worked perfect on the image! One quick question - can this work on a stack of images (3D stack) to generate a plane?</p>
<p>Thank you thank you!</p>
<aside class="onebox googledrive" data-onebox-src="https://drive.google.com/file/d/1YbHxfkkiZX50ngwadYaNad3KycWxiugY/view?usp=sharing">
  <header class="source">

      <a href="https://drive.google.com/file/d/1YbHxfkkiZX50ngwadYaNad3KycWxiugY/view?usp=sharing" target="_blank" rel="noopener nofollow ugc">drive.google.com</a>
  </header>

  <article class="onebox-body">
    
<div class="aspect-image" style="--aspect-ratio:690/362;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae_2_690x362.jpeg" class="thumbnail" width="690" height="362" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae_2_690x362.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae_2_1035x543.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae.jpeg 2x" data-dominant-color="101010"></div>

<h3><a href="https://drive.google.com/file/d/1YbHxfkkiZX50ngwadYaNad3KycWxiugY/view?usp=sharing" target="_blank" rel="noopener nofollow ugc">getHelp3d.tif</a></h3>

<p>Google Drive file.</p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>This would the be the stack I’m referring to.</p> ;;;; <p>Those look like the mesophyll cells underneath the surface. What projection distance did you use?</p> ;;;; <p>Not relevant to the QuPath scalebar, but the usual option right now <a href="https://forum.image.sc/t/changing-the-colour-of-the-scale-bar-in-qupath/58427/3" class="inline-onebox">Changing the colour of the Scale bar in QuPath - #3 by petebankhead</a></p> ;;;; <p>Same here, I tried on two different browsers (edge and Opera), and I got the same result, error 403.</p>
<p>I hope the team can fix the link.</p>
<p>Best regards,</p>
<p>Tiago</p> ;;;; <p>A possible change…<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7.png" data-download-href="/uploads/short-url/qw8399HYdgVVyGhWIcMAxxhKL0H.png?dl=1" title="How to detect the surface" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_345x242.png" alt="How to detect the surface" data-base62-sha1="qw8399HYdgVVyGhWIcMAxxhKL0H" width="345" height="242" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_345x242.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_517x363.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_690x484.png 2x" data-dominant-color="F8F2F3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">How to detect the surface</span><span class="informations">965×679 63.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e.png" data-download-href="/uploads/short-url/2f7S2kL1ngkvVdDPXRszYpAwdKS.png?dl=1" title="Profil de surface" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e_2_690x316.png" alt="Profil de surface" data-base62-sha1="2f7S2kL1ngkvVdDPXRszYpAwdKS" width="690" height="316" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e_2_690x316.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e.png 2x" data-dominant-color="5D5C5C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Profil de surface</span><span class="informations">909×417 270 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-auto">macro "How to detect the surface"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
//--------------------------------
// Start batch mode
setBatchMode(true);
//Copy and select
orig=getImageID();
run("Duplicate...", "title=1");
close("\\Others");
run("Duplicate...", "title=2");
h=getHeight();
w=getWidth();
//--------------------------------
// Start processing
run("Invert");
imageCalculator("Subtract create", "2","1");
selectWindow("Result of 2");
run("Convert to Mask");
run("Invert");
doWand(0, 0, 11.0, "Legacy");
setBackgroundColor(255,255,255);
run("Clear", "slice");
setBackgroundColor(0,0,0);
run("Clear Outside");
doWand(0,h/2, 11.0, "Legacy");
//run("Fill Holes");
//run("Select None");
//run("Open");
run("Median", "radius=2");
//--------------------------------
x=newArray();
y=newArray();
find_scale=h/255;
for(i=0;i&lt;w;i++){
makeLine(i,0,i,h);
profile=getProfile();
Array.getStatistics(profile, min, max, mean, stdDev) ;
x[i]=i;
y[i]=find_scale*mean;
}
//--------------------------------
Fit.doFit("Straight Line", x, y);
a=d2s(Fit.p(0),6);
b=d2s(Fit.p(1),6);
//--------------------------------
x1=0;
y1=Fit.f(x1);
x2=getWidth();
y2=Fit.f(x2);
selectImage("1");
makeLine(x1,y1,x2,y2);
//--------------------------------
// End of batch mode
setBatchMode(false);
//--------------------------------
Fit.plot;
//--------------------------------
// End of processing
//--------------------------------

//--------------------------------
exit("It's over!");
}

</code></pre> ;;;; <aside class="quote no-group" data-username="GopherConfocal" data-post="3" data-topic="79034">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/gopherconfocal/40/51778_2.png" class="avatar"> Mary E Brown:</div>
<blockquote>
<p>all the money I spent was for naught?!?</p>
</blockquote>
</aside>
<p>Your full GPU is detected. You can fill all 16 GB with image data, which is necessary anyway to process the image. Just as an example: For applying a Gaussian blur to a 4GB 32-bit image, a total of 16 GB is necessary: 4 GB for the input image, 2x 4GB for temporary intermediate results, and 4GB for the result.</p>
<p>Addendum: You can debug memory consumption using the <code>GPU memory display</code> in the menu <code>Plugins &gt; CLIJ2 (ImageJ on GPU)</code>:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0b89cd044ac30993239dd856ff22564b8c837bde.gif" alt="14" data-base62-sha1="1E4ukcLLnq82iGl4cPKiL3Qcy0m" width="690" height="350" class="animated"></p> ;;;; <p>Hi all,</p>
<p>I tried to download icy for windows and it told me it is forbidden-failed each time I try to download. Please help.</p>
<p>Thx</p> ;;;; <p>So CliJ cannot detect my full GPU even though I have the latest GeForce driver and all the money I spent was for naught?!?</p> ;;;; <p>That’s not the issue. Ignore that part, was a misprint here.</p> ;;;; <p>Hi <a class="mention" href="/u/gopherconfocal">@GopherConfocal</a> ,</p>
<aside class="quote no-group" data-username="GopherConfocal" data-post="1" data-topic="79034">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/gopherconfocal/40/51778_2.png" class="avatar"> Mary E Brown:</div>
<blockquote>
<p>I in fact have a 16 GB NVIDIA GeForce RTX 3080 laptop GPU. Why does the plugin only detect 3.9 GB fand how do I get the plugin to detect the full 16 GB? Thank you for your help.</p>
</blockquote>
</aside>
<p>The reason is a limitation of the GPU and/or its driver. I presume it is related to how chips are soldered on the board of the GPU. But anway, if you were able to load a 16 GB image into your GPU, you couldn’t do anything with it because it has no memory left for a processing result. Thus, the 4 GB limitation isn’t too bad practically.</p>
<p>You can find out the limitations of your GPU using this little macro:</p>
<pre><code class="lang-auto">run("CLIJ2 Macro Extensions", "cl_device=");
Ext.CLIJ2_clInfo();
</code></pre>
<p>In case of my GPU, global memory size is 4 GB and the maximum image size is 1 GB:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/6/46711cf1f5277a56c37e83d6eb2c918afbd58a2e.png" data-download-href="/uploads/short-url/a39LwEECcfS0KjgGN5Z1IsUANVY.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/6/46711cf1f5277a56c37e83d6eb2c918afbd58a2e.png" alt="image" data-base62-sha1="a39LwEECcfS0KjgGN5Z1IsUANVY" width="690" height="447" data-dominant-color="EFEFEE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">791×513 7.97 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Let me know if this answers your question!</p>
<p>Best,<br>
Robert</p> ;;;; <aside class="quote no-group" data-username="Radu_Vrabie" data-post="4" data-topic="79021">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/radu_vrabie/40/20171_2.png" class="avatar"> Radu Vrabie:</div>
<blockquote>
<p>Can you tell me if the surface from the table is the surface area of the tissue thresholded for -29 + 150 HU? And that the average intesity is thea mean from that table.</p>
</blockquote>
</aside>
<p>Correct.</p>
<p>In the segmented region the minimum voxel intensity is -29 HU, maximum is 150 HU and the average is 30.5 HU.</p>
<p>The surface area of the segmented region is a 3D surface area (includes the top and bottom surface and side). If you want to get the cross-section area of a single slice then you need to divide the value by 2, or for a simpler and more accurate computation, you can multiply the number of voxels by the pixel size. For example, if pixel size in the image slice is 0.2mm x 0.2mm and number of voxels is 21400 then the cross-sectional area is 21400 x 0.8mm x 0.8mm =13696mm2. You can see the pixel size in the Volume Information section in Volumes module.</p>
<p>Note that you can get cross-sectional area along each slice in a 3D segmentation using “Segment Cross-Section Area” module (provided by Sandbox extension).</p> ;;;; <p>I’m getting the following error whenever I try to run a CliJ function on my GPU:<br>
CLIJ2 Warning: You’re creating an image with size 4.4 gigabytes, which exceeds your GPUs capabilities (max 3.9 gigabytes).<br>
CLIJ Error: Creating an image or kernel failed because your device ran out of memory.<br>
You can check memory consumption in CLIJ2 by calling these methods from time to time and see which images live in memory at specific points in your workflow:  Ext.CLIJ2_reportMemory(); // ImageJ Macro  print(clij2.reportMemory()); // Java/groovy/jythonFor support please contact the CLIJ2 developers via the forum on <a href="https://image.sc">https://image.sc</a> .<br>
Therefore, please report the complete error message, the code snippet or workflow you were running, an example image if possible and details about your graphics hardware.</p>
<p>I in fact have a 16 GB NVIDIA GeForce RTX 3080 laptop GPU. Why does the plugin only detect 3.9 GB fand how do I get the plugin to detect the full 16 GB? Thank you for your help.</p> ;;;; <p><a class="mention" href="/u/rlprice410">@rlprice410</a></p>
<p>For 16-bit images:</p>
<blockquote>
<p>When displayed, the intensity of each pixel that is written in the image file is converted into the <em>grayness</em> of that pixel on the screen. How these intensities are interpreted is specified by the image type. From the <a href="https://imagej.nih.gov/ij/docs/concepts.html">Basic concepts page</a>:</p>
<blockquote>
<p>16-bit and 32-bit grayscale images are not directly displayable on computer monitors, which typically can show only display 256 shades of gray. Therefore, the data are mapped to 8-bit by windowing. The window defines the range of gray values that are displayed: values below the window are made black, while values above the window are white. The window is defined by minimum and maximum values that can be modified using Image▷Adjust▷<a href="https://imagej.nih.gov/ij/docs/guide/146-28.html#sub:Brightness/Contrast...%5BC%5D">Brightness/Contrast… [C]↑</a>.<br>
It may happen that the initial windowing performed by ImageJ on these high bit–depth (or HDR<a href="https://imagej.nih.gov/ij/docs/guide/146-Nomenclature.html#nom-hdr">[?]</a>) images is suboptimal. Please note that windowing does not affect image data</p>
</blockquote>
</blockquote> ;;;; <p>You’re very friendly for a Sith Lord <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
This isn’t exactly the solution you requested, but if you go to preferences (gear wheel on top right) and type scale bar, you can make the lines and text thicker. That can help readibility.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8ed372e52afe03f6c8905c44387eebf3632e8837.png" data-download-href="/uploads/short-url/knuPk19DpcvQ3IFr8VNRNsTQojJ.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8ed372e52afe03f6c8905c44387eebf3632e8837.png" alt="image" data-base62-sha1="knuPk19DpcvQ3IFr8VNRNsTQojJ" width="690" height="270" data-dominant-color="A3A3A1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">815×319 11.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>If your goal is to measure the number of nuclei per spot in an H&amp;E image of a Visium-processed sample, Squidpy can already do this through their default tools. The <a href="https://squidpy.readthedocs.io/en/stable/auto_examples/image/compute_segmentation_features.html">Extract Segmentation Features</a> demo shows how you can use <a href="https://squidpy.readthedocs.io/en/stable/api/squidpy.im.segment.html#squidpy.im.segment">squidpy.im.segment</a> to run a segmentation model of your choice (watershed, but I’ve also got it to work with StarDist)</p>
<p><code>segmentation_label </code> will show the number of detected nuclei per spot. Other exported features include average  and std channel (R, G, B) intensity.</p>
<p>Unlike QuPath, Squidpy has yet to implement stain deconvolution. Meaning, segmentation will undoubtably be poorer in Squidpy as your only options for the built-in watershed segmentation are to pass in just one channel, or the RGB mean. Calling a pre-trained StarDist H&amp;E model in Squidpy might yield better results.</p> ;;;; <p>Hello<br>
Great tool QuPath!<br>
Is it possible to make the scalebar more contrast?<br>
Scaleabar visualization capabilities with the latest updates are wonderful, but this is not enough to be clearly visible on screenshots with a structure: letters and stripes are lost on RGB-balanced histogram images.<br>
Thin contrasting black/white (reverse to the main color) edging, as well as the ability to control transparency would significantly improve the situation<br>
May the force be with us</p> ;;;; <aside class="quote no-group" data-username="Radu_Vrabie" data-post="4" data-topic="79021">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/radu_vrabie/40/20171_2.png" class="avatar"> Radu Vrabie:</div>
<blockquote>
<p>Can you tell me if the surface from the table is the surface area of the tissue thresholded for -29 + 150 HU? And that the average intesity is thea mean from that table.</p>
</blockquote>
</aside>
<p>Correct.</p>
<p>In the segmented region the minimum voxel intensity is -29 HU, maximum is 150 HU and the average is 30.5 HU.</p>
<p>The surface area of the segmented region is a 3D surface area (includes the top and bottom surface and side). If you want to get the cross-section area of a single slice then you need to divide the value by 2, or for a simpler and more accurate computation, you can multiply the number of voxels by the pixel size. For example, if pixel size in the image slice is 0.2mm x 0.2mm and number of voxels is 21400 then the cross-sectional area is 21400 x 0.8mm x 0.8mm =13696mm2. You can see the pixel size in the Volume Information section in Volumes module.</p>
<p>Note that you can get cross-sectional area along each slice in a 3D segmentation using “Segment Cross-Section Area” module (provided by Sandbox extension).</p> ;;;; <p>I understand that it can be reset, but I’m trying to understand how the default display range window is calculated. It’s not the same as the autocontrast either, because if I do B&amp;C &gt; Reset &gt; Auto after opening the file, the Auto Contrast Display Range is 551 - 7423 for the blue image above.</p> ;;;; <p>Hello,</p>
<p>I am analyzing large multiplexed images (14000x14000 pixels and 24 channels). I am counting the number of cells (roughly 250,000 cells) using IdentifyPrimaryObjects in one channel that is stained with DAPI. Then I create a cytoplasm mask using ExpandOrShrinkObjects then subsequently MaskObjects. I am measuring the intensity of all 24 channels using MeasureObjectIntensity for each cell in both cellular compartments (cytoplasm and nucleus). When I run ExportToDatabase, the file is created; however, it fails to ever populate with the data nor does the pipeline ever finish. I can pause and stop Cellprofiler using the buttons in the cellprofiler GUI, so it isn’t actually “frozen” in the traditional sense. If I instead create 24 cellprofiler pipelines, each of which measures one channel and exports the cytoplasmic and nuclear data for this one channel, the program works fine; however, the problem arises when all channels are analyzed at once. This to me indicates that it isn’t a problem with my data but some limitation of the ExportToDatabase module. I am very interested in getting this issue resolved; however, I have no way to troubleshoot since no error occurs. Is there a limitation for ExportToDatabase that I am not aware of? This is highly disruptive to my workflow as I would like to analyze several sets of these multiplexed images.<br>
<a class="attachment" href="/uploads/short-url/rNcph0XFDsl8fBsKZbBB4d0y9Bq.cpproj">CyCIF analysis.cpproj</a> (4.5 MB)</p> ;;;; <p>Hi,</p>
<p>With your image, try this:</p>
<pre><code class="lang-auto">//This macro requires the BAR plugin
run("Set Measurements...", "area centroid perimeter bounding stack display redirect=None decimal=3");
Title=getTitle();
getDimensions(width, height, channels, slices, frames);
run("Duplicate...", "title=Blurred");
run("Spectrum");
run("Gaussian Blur...", "sigma=2");
run("Find Edges");

run("Subtract...", "value=25");

CoordinateX=newArray();
CoordinateY=newArray();

for (i = 10; i &lt; width; i=i+width/10) {
	selectWindow("Blurred");
	makeLine(i, 0, i, height, 5);
	run("Plot Profile");
	run("Find Peaks", "min._peak_amplitude=5 min._peak_distance=0 min._value=[] max._value=[] list");
	YList=newArray();
	for (j = 0; j &lt; height; j++) {
		Y=getResult("X1", j, "Plot Values");
		YList=Array.concat(YList,Y);
	}
	YList=Array.deleteValue(YList, NaN);
	Array.print(YList);
	if(YList.length==2){
		CoordinateX=Array.concat(CoordinateX,i);
		CoordinateY=Array.concat(CoordinateY,YList[1]);
	}
	close("Plot Values");
	close("Plot of Blurred");
	close("Peaks in Plot of Blurred");

}
Last=CoordinateX.length-1;
selectWindow("Blurred");

makeLine(CoordinateX[0], CoordinateY[0], CoordinateX[Last], CoordinateY[Last]);

run("Measure");


</code></pre>
<p>You should get this image<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2dcee1ab8fb9dc6411a799672c75d01d324e3745.png" alt="image" data-base62-sha1="6xeJ7D33t6o3pv5osTVjzkgcn6l" width="657" height="301"><br>
and a table of result with the angle of the line and its coordinates</p>
<p>M.</p> ;;;; <p>From Wayne on ImageJ list :<br>
This bug is fixed in the ImageJ 1.54d22 daily build.</p>
<p>-wayne</p> ;;;; <p>It looks like the polyline specifically won’t work for some subpixel angles. For example, you can break the first line by making the xpoints 11.5, 11.5.</p>
<p>Haven’t figured it out entirely, but it seems fine for pixel values.</p> ;;;; <p>Cool, I will try to see if it is a problem later !!<br>
Thank you Eugene<br>
Have a good week end</p> ;;;; <p>Yes, I saw that with the original code, because it isn’t making a selection for you to “Add” in the next line.</p> ;;;; <p>Actually the Error message is : The active image does not have a selection</p> ;;;; <p>Hi Eric,</p>
<p>I’ve changed Roi type to polygon, not polyline and it seems fine with it.<br>
The best way would be to debug whole ImageJ, but I do not have access to it right now.</p>
<p>Cheers,<br>
Eugene</p> ;;;; <p>Hi MRA,<br>
Thanks for your answer but I have shorter distance working</p> ;;;; <p>Hi Eugene,<br>
Do you mean that with the value in the macro below it is working for you ?<br>
I set the decimal number to 9 and it did not change anything</p>
<p>xpoints2=newArray(11.572,11.594);<br>
ypoints2=newArray(51.733,51.986);</p>
<p>makeSelection( “polyline”, xpoints2, ypoints2 );<br>
roiManager(“Add”);</p> ;;;; <p>Interesting.</p>
<blockquote>
<p>Your second line is much less than a pixel.</p>
</blockquote>
<p>The first line is even shorter, but it works.</p>
<p>Some kind of rounding up error?<br>
Also,</p>
<pre><code class="lang-auto">
makeSelection( "polygon", xpoints2, ypoints2 );
</code></pre>
<p>seems to work fine.</p> ;;;; <p>With the downloaded image and scaled inputs I get<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717.jpeg" data-download-href="/uploads/short-url/77KcoHSMeKNMVhI0xeBZ0gQPJOv.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_690x466.jpeg" alt="image" data-base62-sha1="77KcoHSMeKNMVhI0xeBZ0gQPJOv" width="690" height="466" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_690x466.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_1035x699.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_1380x932.jpeg 2x" data-dominant-color="C3C3C3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1557×1053 90.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
also in 1.53t</p>
<p>Your font size and the “cm” in the first window do look a bit odd though. Not sure why.</p> ;;;; <p>I did it<br>
the same problem persist!</p> ;;;; <p>I think the script is fine, the values are wrong.</p>
<pre><code class="lang-auto">newImage("Untitled", "8-bit white", 200, 200, 1);

xpoints=newArray(12,22);
ypoints=newArray(51,62);


makeSelection( "polyline", xpoints, ypoints );
        roiManager("Add");


xpoints2=newArray(21,11);
ypoints2=newArray(51,11);

makeSelection( "polyline", xpoints2, ypoints2 );
        roiManager("Add");


</code></pre>
<p>works for me. And is easier to see since the lines have length. The sub-pixel values seem to be the issue. Your second line is much less than a pixel.</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c.png" data-download-href="/uploads/short-url/eh0uGUYyYNa6KB8xa3VZ72wBJjC.png?dl=1" title="3 D slicer segmentation" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c_2_690x333.png" alt="3 D slicer segmentation" data-base62-sha1="eh0uGUYyYNa6KB8xa3VZ72wBJjC" width="690" height="333" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c_2_690x333.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c_2_1035x499.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c.png 2x" data-dominant-color="353338"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">3 D slicer segmentation</span><span class="informations">1336×646 70.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Sorry for bothering you sir. Can you tell me if the surface from the table is the surface area of the tissue thresholded for -29 + 150 HU? And that the average intesity is thea mean from that table. Thanks a lot sir!</p> ;;;; <p>I had let my Ilastik analysis on the side for a few weeks to take care of other projects, but I tried with logfile today and it works perfectly <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
Thank you !!</p> ;;;; <p>Thank you for the suggestions. I’ll give it a try! The Zarr hint was really helpful!</p> ;;;; <p>You would have to reinstall deeplabcut also since this is probably where it also happened</p> ;;;; <aside class="quote no-group" data-username="henrypinkard" data-post="2" data-topic="79027">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/henrypinkard/40/30258_2.png" class="avatar"> Henry Pinkard:</div>
<blockquote>
<p>If you only want to modify pixels and keep the dtype the same and not make any modifications to metadata, you could probably do this is a somewhat hacky way where you overwrote the pixel data at each place it is stored in the file</p>
</blockquote>
</aside>
<p>That is possible with the <a href="https://pypi.org/project/tifffile/" rel="noopener nofollow ugc">tifffile</a> library, e.g.:</p>
<pre><code class="lang-python">import tifffile
import zarr

store = tifffile.imread('NDTiff3.2_multichannel_NDTiffStack.tif', mode='r+', aszarr=True)
z = zarr.open(store, mode='r+')
z[:, 1, :, 16:32, 24:42] = 100
store.close()
</code></pre>
<p>Support for NDTiffStorage in tifffile is relatively new. In any case, make a copy of the folder containing the ndtiff TIFF and index files before. Some metadata, like ranges and precision, might get out of sync with the data. Tifffile does not handle pyramids, tiled frames, or overlapping frames as the <a href="https://pypi.org/project/ndtiff/" rel="noopener nofollow ugc">ndtiff</a> library.</p> ;;;; <p>Hi All,</p>
<p>I have a list of coordinates from which I want to do polyline in the roiManager. It does not work on all the list.</p>
<p>I reproduce the problem in the following macro where the first arrays works but the 2nd arrays does’t.</p>
<p>I can’t figure out  why .</p>
<p>Any ideas ?</p>
<pre><code class="lang-auto">//Macro starts

newImage("Untitled", "8-bit black", 200, 200, 1);

xpoints=newArray(12,12);
ypoints=newArray(51.733,51.986);


makeSelection( "polyline", xpoints, ypoints );
        roiManager("Add");


xpoints2=newArray(11.572,11.594);
ypoints2=newArray(51.733,51.986);

makeSelection( "polyline", xpoints2, ypoints2 );
        roiManager("Add");


// Macro End
</code></pre>
<p>–</p>
<p>Eric Denarier<br>
Grenoble Institut des Neurosciences<br>
Inserm U1216<br>
Chemin Fortuné Ferrini<br>
38700 La Tronche<br>
France</p>
<p>Tél :33 (0)4 565 205 23</p> ;;;; <p>The only official way to write NDTiff right now is through the Java library. This is doable through python, but will be somewhat slow (100-200 MB/s). If you only want to modify pixels and keep the dtype the same and not make any modifications to metadata, you could probably do this is a somewhat hacky way where you overwrote the pixel data at each place it is stored in the file. If you look through the <code>dataset.index</code> field, you should be able to find out where each image is stored and then use python file IO to overwrite the pixels at each location</p> ;;;; <p>Hi <a class="mention" href="/u/rlprice410">@rlprice410</a> . If you Reset your Brightness/Contrast settings, the full display range should correspond to the histogram.</p> ;;;; <aside class="quote no-group" data-username="f.goerlitz" data-post="3" data-topic="78881">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>Can I also use the Multi-Cam utility within Aquisition?</p>
</blockquote>
</aside>
<p>Yes</p>
<aside class="quote no-group" data-username="f.goerlitz" data-post="3" data-topic="78881">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>Is there a way to report progress out of a running Aquisition in sequencing mode (at which image in the sequence it is) to use as a progress bar in the GUI? If not, I could use the DAQ device to do this, however that would not be as nice.</p>
</blockquote>
</aside>
<p>Only by monitoring images being pulled off the camera. The whole point of hardware triggering is that the computer is out of the loop.</p>
<aside class="quote no-group" data-username="f.goerlitz" data-post="3" data-topic="78881">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>Is there a way to interrupt/stop a running Acquistion in sequencing mode?</p>
</blockquote>
</aside>
<p>Yes in theory you can abort it like any other acquisition but whether that works depends on the device adapters of the specific hardware you’re using</p> ;;;; <p>I don’t know these functions, but one thing that I can see is that there is a ; missing after print(stage_x).</p> ;;;; <p>I’d recommend looking through this section of the docs <a href="https://qupath.readthedocs.io/en/0.4/docs/tutorials/pixel_classification.html" class="inline-onebox">Pixel classification — QuPath 0.4.3 documentation</a></p> ;;;; <aside class="quote no-group" data-username="Delpierre" data-post="3" data-topic="78907">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/d/f05b48/40.png" class="avatar"> Delpierre:</div>
<blockquote>
<p><code>Interleaved="false" BigEndian="true"</code></p>
</blockquote>
</aside>
<p>Shouldn’t that be <code>Interleaved="true"</code> for 24-bit RGB? <code>BigEndian=</code> should match the TIFF header although I’m not sure it matters in this case.</p> ;;;; <p>You can do this in <a href="https://www.slicer.org" rel="noopener nofollow ugc">3D Slicer</a> by <a href="https://slicer.readthedocs.io/en/latest/user_guide/modules/dicom.html#read-dicom-files-into-the-scene" rel="noopener nofollow ugc">loading the DICOM image</a>, segmenting the region using <a href="https://slicer.readthedocs.io/en/latest/user_guide/modules/segmenteditor.html" rel="noopener nofollow ugc">Segment Editor module</a>, and computing average intensity using <a href="https://slicer.readthedocs.io/en/latest/user_guide/modules/segmentstatistics.html" rel="noopener nofollow ugc">Segment Statistics module</a>.</p> ;;;; <p>Hi gugs,<br>
When facing such a WSI, how can I make batch annotation rather than annotating these small tumor areas one by one? Could you please tell me if there are some efficient methods? Thanks!<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b.jpeg" data-download-href="/uploads/short-url/o8YsJqWCcAMG6hsf8X27jb31jlx.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b_2_535x500.jpeg" alt="image" data-base62-sha1="o8YsJqWCcAMG6hsf8X27jb31jlx" width="535" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b_2_535x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b_2_802x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b.jpeg 2x" data-dominant-color="AE67AB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">899×839 228 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>apologies in reviving this topic <a class="mention" href="/u/nicost">@nicost</a> <a class="mention" href="/u/nanthony21">@nanthony21</a> <a class="mention" href="/u/mdj97">@mdj97</a> <a class="mention" href="/u/german.camargo">@german.camargo</a></p>
<p>I followed the advice for <a href="https://micro-manager.org/NikonTi2" rel="noopener nofollow ugc">NikonTi2 - Micromanager</a> with MM2nightly <a href="https://download.micro-manager.org/nightly/2.0/Windows/MMSetup_64bit_2.0.1_20230321.exe" rel="noopener nofollow ugc">MMSetup_64bit_2.0.1_20230321.exe</a> and Ti2 Control 2.70.101.</p>
<p>However, when starting the device configuration wizard, it says NikonTi2 (not available)<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a.png" data-download-href="/uploads/short-url/eGhDw9nAIPW6HwkIzCnsWTdKesW.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_690x354.png" alt="image" data-base62-sha1="eGhDw9nAIPW6HwkIzCnsWTdKesW" width="690" height="354" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_690x354.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_1035x531.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_1380x708.png 2x" data-dominant-color="818080"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1567×804 51 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Would one of you maybe have a pointer for me how to continue?</p>
<p>Thank you for your support!</p> ;;;; <p>Hi,</p>
<p>I have been playing with the Micro-Magellan recently. The performance of the NDTiff is very impressive, and the Dask array was pretty handy. But so far, I could only use it for storing the raw data. I am trying to do batch processing and normalization to the images (no changes to the dimension or dtype) and possibly save them back as NDTiff. Before I do too much hacking, is there already an easy way to make this happen? I tried to directly change the tiff files and save them back will modify the header files. Rewriting the metadata didn’t help. Excuse me if I didn’t read the documents carefully enough.<br>
Thank you!</p>
<p>Yehe</p> ;;;; <p>yes true it is perfect !</p> ;;;; <p>I just checked this with a different channel and am getting the same window effect on the Display Range as above. I also opened the file in Python and the min / max intensity values are the ones displayed in the histogram, so there is a default window or scaling being applied to the Display Range.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb.jpeg" data-download-href="/uploads/short-url/mSzdWsfL82AcujoxpHHYWskgYST.jpeg?dl=1" title="ImageJ_DisplayRange_2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb_2_542x500.jpeg" alt="ImageJ_DisplayRange_2" data-base62-sha1="mSzdWsfL82AcujoxpHHYWskgYST" width="542" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb_2_542x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb_2_813x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb.jpeg 2x" data-dominant-color="4D4D31"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">ImageJ_DisplayRange_2</span><span class="informations">900×830 70.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The input image is 16-bit, so maybe this has something to do with it?</p> ;;;; <p>Thank you so much, Pete. <img src="https://emoji.discourse-cdn.com/twitter/grinning.png?v=12" title=":grinning:" class="emoji" alt=":grinning:" loading="lazy" width="20" height="20"></p> ;;;; <p>In that case, I’ll leave my response to Mark’s video as well <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> <a href="https://forum.image.sc/t/contruct-voronoi-polygons-from-a-set-of-points-groovy-script/70998/13" class="inline-onebox">Contruct Voronoi polygons from a set of points (groovy script) - #13 by petebankhead</a></p> ;;;; <p>Hi <a class="mention" href="/u/surgdocc">@Surgdocc</a> there’s some info on this at <a href="https://qupath.readthedocs.io/en/0.4/docs/concepts/classifications.html#ignored-classifications" class="inline-onebox">Classifications — QuPath 0.4.3 documentation</a></p> ;;;; <p>A warm thanks to both of you for your inputs, we will make it work!</p>
<p>I propose to continue to discuss the cryoscope option here, and the python-microscope on the github issue.<br>
We tried to use the code on Linux, loading the <code>libLinkamSDK.so</code> from our path (both of the versions we have).</p>
<p>In both cases we have a segfault during <code>CryoStage.Connect()</code> when calling <code>linkamProcessMessageCommon()</code></p> ;;;; <p>What is the difference between classes with or without a ***** after the class names？Thanks！<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18bbeac08f1d89dd0bd90762fcd41084afc51331.png" alt="image" data-base62-sha1="3wO4jzx8RocEYHmXgggRuHv7wyt" width="269" height="424"></p> ;;;; <p>Sounds good. I’ll just leave this here too in case it is relevant. <a href="https://forum.image.sc/t/qupath-for-spatial-trans/78657/3" class="inline-onebox">QuPath for spatial trans - #3 by Mark_Zaidi</a></p> ;;;; <aside class="quote no-group quote-modified" data-username="Research_Associate" data-post="10" data-topic="79017">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p>selectObjects( getAnnotationObjects().findAll{ it.getName() == “ATACCAGGTGAGCGAT-1” } )</p>
</blockquote>
</aside>
<p>Indeed, as each spot is annotated with an unique barcode, I won’t have repetitive object names on my slide.</p>
<p>Thank you very much for the shortened code. Elegant!</p>
<p>Best,</p>
<p>Yuxuan</p> ;;;; <p>I post the .txt for those who can help me.<br>
x  y<br>
|61.5|11.5|<br>
|77.5|13.5|<br>
|45.5|15.5|<br>
|67.5|17.5|<br>
|31.5|25.5|<br>
|104.5|25.5|<br>
|38.5|32.5|<br>
|108.5|34.5|<br>
|28.5|49.5|<br>
|147.5|56.5|<br>
|156.5|65.5|<br>
|49.5|66.5|<br>
|106.5|71.5|<br>
|65.5|74.5|<br>
|93.5|74.5|<br>
|20.5|75.5|<br>
|99.5|83.5|<br>
|38.5|85.5|<br>
|171.5|87.5|<br>
|35.5|99.5|<br>
|141.5|101.5|<br>
|155|101.5|<br>
|96.5|104.5|<br>
|182.5|105.5|<br>
|49.5|107.5|<br>
|118.5|110.5|<br>
|158.5|116.5|<br>
|183.5|122.5|<br>
|44.5|128.5|<br>
|150.5|131.5|<br>
|137.5|135.5|<br>
|58.5|139.5|<br>
|83.5|146.5|<br>
|152.5|151.5|<br>
|173.5|153.5|<br>
|107.5|157.5|<br>
|62.5|165.5|<br>
|186|169.5|<br>
|85.5|173.5|<br>
|101|180.5|<br>
|136.5|182.5|<br>
|107.5|188.5|<br>
|98.5|192.5|<br>
|158.5|196.5|<br>
|109.5|198.5|</p> ;;;; <p>Thanks for you quick answer David.<br>
That’s making good sense. I should have thought about this.</p>
<p>Here is the OME header that I used:</p>
<pre><code class="lang-auto">  &lt;Image ID="Image:0"&gt;
    &lt;Pixels Type="uint8" SignificantBits="8" Interleaved="false" BigEndian="true" DimensionOrder="XYCZT" ID="Pixels:0" SizeX="2448" SizeY="2048" SizeC="3" SizeZ="1" SizeT="1" PhysicalSizeX="2.886029" PhysicalSizeY="3.449707" PhysicalSizeZ="1.000000"&gt;
      &lt;Channel ID="Channel:0:0" SamplesPerPixel="3"&gt;
        &lt;LightPath /&gt;
      &lt;/Channel&gt;
      &lt;TiffData /&gt;
    &lt;/Pixels&gt;
  &lt;/Image&gt;

</code></pre>
<p>The IFD (only one) contains the 24bit RGB tiff.</p>
<p>When I try to open with the BioFormat plugin in ImageJ, I’m getting this error message:</p>
<pre><code class="lang-auto">ImageJ 1.53e; Java 1.8.0_172 [64-bit]; Windows 10 10.0; 49MB of 24372MB (&lt;1%)
 
java.lang.IllegalStateException: Too early in import process: current step is FILE, but must be after STACK
	at loci.plugins.in.ImportProcess.assertStep(ImportProcess.java:778)
</code></pre>
<p>Any idea?</p>
<p>Note that the file itself opens just ok in ImageJ.</p> ;;;; <p>Hopefully someone can help you, but I think they will need more information.</p> ;;;; <p>Have you tried either <code>Ridge detection</code> or filters for edge detection <a href="https://forum.image.sc/t/image-denoise-after-edge-detection/49285" class="inline-onebox">image denoise after edge detection</a></p> ;;;; <p>Hello guys.<br>
Can you help me to calculet the mean density values from a  specific area of a CT image?<br>
Thanks a lot</p> ;;;; <p>I think that was covered here <a href="https://forum.image.sc/t/qupath-cellpose-cell-objects/65691" class="inline-onebox">QuPath - Cellpose - Cell objects</a><br>
and here <a href="https://forum.image.sc/t/combining-cellpose-and-stardist-detections-into-cells/78225/5" class="inline-onebox">Combining Cellpose and StarDist detections into cells - #5 by ChrisStarling</a><br>
You can use the same ROI for the nucleus and cytoplasm if you only have one ROI, or maybe use <code>null</code>.</p> ;;;; <p>One note, you are currently only finding one example of an object with that name, even if more then one exists. Is that expected?</p>
<p>The whole thing could also be written as<br>
<code>selectObjects( getAnnotationObjects().findAll{ it.getName() == "ATACCAGGTGAGCGAT-1" } )</code><br>
Without any imports or other variable definitions. As an option.</p>
<p>It would also select all objects with that particular name string.</p> ;;;; <p>Dear Qupath user, I try to convert my detections (by cellpose) into cells . Is there any script to do it ? I need probably to erode my detections before as Cellpose detect the cytoplasmic (cyto2 model) of my cells. My cell are quite round and little so the cytoplasmic is really around the nucleus (T and  B cells).Thanks fro your help.</p> ;;;; <p>Thank you Yau Mun, Chris and Kind MicroscopyRA!</p>
<p>The code now runs very smoothly.<br>
Here is it:</p>
<pre><code class="lang-auto">import qupath.lib.objects.PathObject;
import qupath.lib.objects.PathAnnotationObject;

// Get the current project
var currentProject = getProject();

// Get all annotation objects in the project
var annotationObjects = getAnnotationObjects()

// Find the annotation object with the specified name
var annotationObject = annotationObjects.find { obj -&gt;
  obj.getName() == "ATACCAGGTGAGCGAT-1"
};

// Select the annotation object in the view
if (annotationObject) {
  selectObjects(annotationObject);
} else {
  println("Annotation object not found");
}
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/5/f5dca33ee4cc5d945495be49140222f064fa3fc1.jpeg" data-download-href="/uploads/short-url/z4ZH2UxDO81qKMbhsPveaapAhAl.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/5/f5dca33ee4cc5d945495be49140222f064fa3fc1.jpeg" alt="image" data-base62-sha1="z4ZH2UxDO81qKMbhsPveaapAhAl" width="514" height="500" data-dominant-color="C78D9F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">621×604 41.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thank you all again for the swift replies! Good luck in your research.</p>
<p>Best,</p>
<p>Yuxuan</p> ;;;; <aside class="quote no-group" data-username="ym.lim" data-post="7" data-topic="79017">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png" class="avatar"> Yau Mun Lim:</div>
<blockquote>
<p>You technically can loop through all entries in a project, use <code>hierarchy.getAnnotationObjects()</code> to get the objects from each image hierarchy, and add them to a list to collect all objects from each entry…</p>
</blockquote>
</aside>
<p>Yep, exactly. That’s one way of iterating through next images.</p>
<p>And selectObjects should work though I don’t think it is part of getViewer()</p> ;;;; <aside class="quote no-group" data-username="Research_Associate" data-post="3" data-topic="79017">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p>I don’t think there is a way to get all of the objects in a whole project, just within an image hierarchy</p>
</blockquote>
</aside>
<p>You technically can loop through all entries in a project, use <code>hierarchy.getAnnotationObjects()</code> to get the objects from each image hierarchy, and add them to a list to collect all objects from each entry…</p>
<aside class="quote no-group" data-username="Research_Associate" data-post="3" data-topic="79017">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p>I also do not think there is a selectObject() function at all</p>
</blockquote>
</aside>
<p>There is <code>selectObjects()</code> which should work in place of <code>selectObject()</code>.</p> ;;;; <p>Currently the class of annotation is set to be “Spot” as shown below:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/7368c54a969b2c614564d0bff47536e7d70975ce.png" alt="image" data-base62-sha1="gsXnvK3WRYcXItOcVfQEh2zAnme" width="589" height="338"></p>
<p>The Name and Class are different here.</p> ;;;; <p>Do the annotations have a class, or could the name be set as the class?</p>
<p>Or is the listed name actually the class?</p> ;;;; <p>Hi <a class="mention" href="/u/zy112">@zy112</a> . How did you determine the scaling of your image?</p> ;;;; <p>This now looks right!!  Hopefully that’s all I’ll need <img src="https://emoji.discourse-cdn.com/twitter/crossed_fingers.png?v=12" title=":crossed_fingers:" class="emoji" alt=":crossed_fingers:" loading="lazy" width="20" height="20"></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c.jpeg" data-download-href="/uploads/short-url/rxbdxqRjNTfIMTKuFpUiCtZskWM.jpeg?dl=1" title="bigstitcher result" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_564x500.jpeg" alt="bigstitcher result" data-base62-sha1="rxbdxqRjNTfIMTKuFpUiCtZskWM" width="564" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_564x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_846x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_1128x1000.jpeg 2x" data-dominant-color="11160A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">bigstitcher result</span><span class="informations">1214×1075 62.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Huw</p> ;;;; <p>Hi, there!</p>
<p>After Chris’ suggestion, my first error is gone. But like you suggested, there is no selectObject() method indeed.</p>
<pre><code class="lang-auto">ERROR: It looks like you've tried to access a method that doesn't exist.


ERROR: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getViewer() is applicable for argument types: () values: [] in QuPathScript at line number 17

</code></pre>
<p>which points to this line</p>
<pre><code class="lang-auto">// Select the annotation object in the view
if (annotationObject) {
  getViewer().selectObject(annotationObject);
}
</code></pre>
<p>Best,<br>
Yuxuan</p> ;;;; <aside class="quote no-group" data-username="ChrisStarling" data-post="2" data-topic="79017">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png" class="avatar"> ChrisStarling:</div>
<blockquote>
<p>This is your problem:</p>
</blockquote>
</aside>
<p>Agreed, I don’t think there is a way to get all of the objects in a whole project, just within an image hierarchy. getAnnotationObjects will work for the current image. You would need to iterate through subsequent images to process the whole project, which is usually done by “Run for project”  <a href="https://qupath.readthedocs.io/en/stable/docs/scripting/workflows_to_scripts.html#running-a-script-for-multiple-images" class="inline-onebox">Workflows to scripts — QuPath 0.4.3 documentation</a></p>
<p>I also do not think there is a selectObject() function at all, when checking the javadocs. How was this script generated?</p> ;;;; <p>This is your problem:</p>
<aside class="quote no-group" data-username="YuxuanXie" data-post="1" data-topic="79017">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/977dab/40.png" class="avatar"> Yuxuan Xie:</div>
<blockquote>
<pre><code class="lang-auto">var annotationObjects = currentProject.getAllObjects().findAll { obj -&gt;
  obj instanceof PathAnnotationObject
</code></pre>
</blockquote>
</aside>
<p>Try something like:</p>
<pre><code class="lang-auto">var annotationObjects = getAnnotationObjects()
</code></pre> ;;;; <p>Hi <a class="mention" href="/u/wayne">@Wayne</a>,</p>
<p>I am confused about the differences between the outputs of the 3 different ways of filtering the FFT:</p>
<pre><code class="lang-auto">run("Comparing Lengths");
run("FFT");

//clear central vertical line
makeRectangle(254, 0, 5, 512);
setBackgroundColor(0, 0, 0);
run("Clear", "slice");

run("Create Mask");
run("Invert");

//Inverse FFT
selectWindow("FFT of Comparing Lengths-1");
run("FFT");

// FFT using filter
selectWindow("Comparing Lengths-1");
run("Duplicate...", " ");
run("Custom Filter...", "filter=Mask");
rename("FFT using mask as filter");
run("Tile");

// FFT using filter on 8-bit
selectWindow("Comparing Lengths-1");
run("Duplicate...", " ");
run("8-bit");
run("Custom Filter...", "filter=Mask");
rename("FFT using mask as filter on 8-bit input");
run("Tile");
</code></pre>
<p>Outputs:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea.jpeg" data-download-href="/uploads/short-url/cqGqsIUEPnQAbgiiOKTz2facKfM.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_690x149.jpeg" alt="image" data-base62-sha1="cqGqsIUEPnQAbgiiOKTz2facKfM" width="690" height="149" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_690x149.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_1035x223.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_1380x298.jpeg 2x" data-dominant-color="7F7F7F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">2057×447 126 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Is this working as intended?<br>
I am confused by the outputs of the <code>run("Custom Filter...", "filter=Mask");</code> commands.</p>
<p>Thanks a lot</p> ;;;; <p>Hi There,</p>
<p>I am measuring numbers of nuclei per spot in one H&amp;E image of 10X visium datasets. As I have nearly 4000 different annotation objects, is there any way I can select and focus on the object I want via searching for its “Name” or “Object ID”?</p>
<p>And this is my [quote=“Yuxuan Xie, post:1, topic:79017, full:true, username:YuxuanXie”]<br>
Hi There,</p>
<p>I am measuring numbers of nuclei per spot in one H&amp;E image of 10X visium datasets. As I have nearly 4000 different annotation objects, is there any way I can select and focus on the object I want via searching for its “Name” or “Object ID”?</p>
<p>This is my attempt for the task.</p>
<pre><code class="lang-auto">import qupath.lib.objects.PathObject;
import qupath.lib.objects.PathAnnotationObject;

// Get the current project
var currentProject = getProject();

// Get all annotation objects in the project
var annotationObjects = currentProject.getAllObjects().findAll { obj -&gt;
  obj instanceof PathAnnotationObject
}.collect { obj -&gt;
  obj.getPathObject()
}.findAll { obj -&gt;
  obj instanceof PathObject
};

// Find the annotation object with the specified name
var annotationObject = annotationObjects.find { obj -&gt;
  obj.getName() == "ATACCAGGTGAGCGAT-1"
};

// Select the annotation object in the view
if (annotationObject) {
  getViewer().selectObject(annotationObject);
} else {
  println("Annotation object not found");
}
</code></pre>
<p>But it comes the errors as such.</p>
<pre><code class="lang-auto">ERROR: It looks like you've tried to access a method that doesn't exist.


ERROR: No signature of method: qupath.lib.projects.DefaultProject.getAllObjects() is applicable for argument types: () values: [] in QuPathScript at line number 8

</code></pre>
<p>Any suggestions? Many thanks!</p>
<p>Yuxuan</p> ;;;; <p>Thanks,<br>
I’ll run some tests after the weekend,and post back my results.</p>
<p>Cheers!</p> ;;;; <p>Hi Konrad,</p>
<p>I performed filterpredictions with windowlength = 11 and filtertype = ‘median’.</p>
<p>Everything worked well. It does look like it filters maybe too well… ?</p>
<p>I’ll have a play with a windowlength = 9 as you also suggested when my video fps is 120.</p>
<p>Just with your statement the other day:</p>
<p>“The only thing that comes to mind is that the video you’re using for testing has been almost fully used as a training dataset”</p>
<p>I’m currently only training the model with the same video I’m analysing as I’m still learning and don’t have a lot of videos to add to the model.</p>
<p>Does that mean when using the same video for training and analysing that the likelihood scores will always be 1?</p>
<p>I did this with a single animal project and never score of 1 for a frame - is it different for multi-animal projects?</p>
<p>Moving forward, should I start adding different videos to perform the extract frames and label frames functions before starting the training so hopefully I still obtain high likelihood scores but the model is being trained by frames from various videos?</p>
<p>If so, how many frames do you recommend labelling when frames are sourced from different videos?</p>
<p>Thanks, again</p> ;;;; <p>That line in <code>django-redis 4.8</code> that’s causing the error <a href="https://github.com/jazzband/django-redis/blob/2b4a21620c6a7127e0d0800f3e05a010ff7b4db4/django_redis/util.py#L7" class="inline-onebox">django-redis/util.py at 2b4a21620c6a7127e0d0800f3e05a010ff7b4db4 · jazzband/django-redis · GitHub</a></p>
<p>I think you need to upgrade to <code>django-redis==5.0.0</code> which is the current requirement for omero-web:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68">
  <header class="source">

      <a href="https://github.com/ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68" target="_blank" rel="noopener">ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68</a></h4>



    <pre class="onebox"><code class="lang-py">
      <ol class="start lines" start="58" style="counter-reset: li-counter 57 ;">
          <li>        "django-pipeline==2.0.7",</li>
          <li>        "django-cors-headers==3.7.0",</li>
          <li>        "whitenoise&gt;=5.3.0",</li>
          <li>        "gunicorn&gt;=19.3",</li>
          <li>        "omero-marshal&gt;=0.7.0",</li>
          <li>        "Pillow",</li>
          <li>    ],</li>
          <li>    include_package_data=True,</li>
          <li>    tests_require=["pytest"],</li>
          <li>    extras_require={</li>
          <li class="selected">        "redis": ["django-redis==5.0.0"],</li>
          <li>    },</li>
          <li>)</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/mmongy">@mmongy</a><br>
That looks OK. I assume you’ve restarted omero-web as part of that update?<br>
Did you create the “public” user with correct password, and can you login as that user normally via CLI, Insight (and web)?<br>
I wonder if your browser has cached a previous session. Can you try a different browser or “incognito” mode etc?</p>
<p>Will</p> ;;;; <p>…attempting to solve my own problem here, I tried adding .flipVertical(true) to the opm builder, which appears to have worked:</p>
<pre><code class="lang-auto">#@File id (label="Selected File")
#@File save_dir (label="Save Location", style="directory")
#@Integer downsample (label="Downsample Factor", value=1)

import ch.epfl.biop.operetta.OperettaManager
import ij.gui.Roi
import ij.IJ

def opm = new OperettaManager.Builder()
									.setId( id )
									.setSaveFolder( save_dir )
									.flipVertical(true)
									.build();
</code></pre>
<p>I’ll try stitching the tiles that has produced and see if it’s right.</p> ;;;; <p>I modified the proposed macro on the BiG website into (add the three dots after “Pure Denoise”):</p>
<pre><code class="lang-auto">run("URL...", "url=http://bigwww.epfl.ch/algorithms/denoise/dataset/Noisy-Test-Data.tif");
rename("Input PureDenoise Macro");

// Slow (multi-channnel(10)/redundant(4)/individual parameters estimation)
selectWindow("Input PureDenoise Macro");
run("PureDenoise ...", "parameters='10 4' estimation='Auto Individual' ");
rename("Auto-Individual nf=10 cs=3");
</code></pre>
<p>And it worked in FiJi.<br>
B</p> ;;;; <p>I fix the SSL error using information of this website:</p><aside class="onebox githubissue" data-onebox-src="https://github.com/conda/conda/issues/8273">
  <header class="source">

      <a href="https://github.com/conda/conda/issues/8273" target="_blank" rel="noopener nofollow ugc">github.com/conda/conda</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/conda/conda/issues/8273" target="_blank" rel="noopener nofollow ugc">Can't connect to HTTPS URL because the SSL module is not available</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2019-02-13" data-time="13:16:12" data-timezone="UTC">01:16PM - 13 Feb 19 UTC</span>
      </div>

        <div class="date">
          closed <span class="discourse-local-date" data-format="ll" data-date="2019-02-13" data-time="14:29:52" data-timezone="UTC">02:29PM - 13 Feb 19 UTC</span>
        </div>

      <div class="user">
        <a href="https://github.com/SandorSzalma1" target="_blank" rel="noopener nofollow ugc">
          <img alt="SandorSzalma1" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/6798456939bee592339b0c3aeebffab35a56cb80.png" class="onebox-avatar-inline" width="20" height="20">
          SandorSzalma1
        </a>
      </div>
    </div>

    <div class="labels">
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">## Current Behavior
Environment: Windows 2016 Server
Installer: https://repo.a<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">naconda.com/archive/Anaconda3-2018.12-Windows-x86_64.exe

with the fresh installation neither conda nor pip are able to work

### Steps to Reproduce
Start Anaconda Prompt
`conda search conda` gives an error message:
**Can't connect to HTTPS URL because the SSL module is not available**

## Environment Information
 conda version : 4.5.12

## Resolution
I did a trace with Process Monitor.
D:\Anaconda3\DLLs\_ssl.pyd search for the OpenSSL DLLs but in the wrong/current location!
As they are not found the search goes to C:\Windows\System32 where we have the same DLLs, installed by an other application, but with a different version. :-(

The DLLs delivered by Anaconda3 are located here:
D:\Anaconda3\Library\bin

My workaround:
I have copied the following files
```
libcrypto-1_1-x64.*
libssl-1_1-x64.*
```
from D:\Anaconda3\Library\bin to D:\Anaconda3\DLLs.

And it works as a charm!</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>I have copied the following files</p>
<p>libcrypto-1_1-x64.*<br>
libssl-1_1-x64.*</p>
<p>from Anaconda3\Library\bin to D:\Anaconda3\DLLs.</p>
<p>now i can do the step 9, but the same error when i run the test script.</p>
<p>I deleted the environment and i created everything again error is the same:</p>
<p>(DLC) C:\repos\DeepLabCut\examples&gt;python testscript_multianimal.py<br>
Loading DLC 2.3.2…<br>
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)<br>
Traceback (most recent call last):<br>
File “testscript_multianimal.py”, line 12, in <br>
import deeplabcut<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut_<em>init</em>_.py”, line 37, in <br>
from deeplabcut.create_project import (<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\create_project_<em>init</em>_.py”, line 12, in <br>
from deeplabcut.create_project.demo_data import load_demo_data<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\create_project\demo_data.py”, line 16, in <br>
from deeplabcut.utils import auxiliaryfunctions<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\utils_<em>init</em>_.py”, line 11, in <br>
from deeplabcut.utils.auxfun_multianimal import *<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\utils\auxfun_multianimal.py”, line 34, in <br>
from deeplabcut.utils import auxiliaryfunctions, conversioncode<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\utils\auxiliaryfunctions.py”, line 31, in <br>
from deeplabcut.pose_estimation_tensorflow.lib.trackingutils import TRACK_METHODS<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\pose_estimation_tensorflow_<em>init</em>_.py”, line 17, in <br>
from deeplabcut.pose_estimation_tensorflow.datasets import *<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets_<em>init</em>_.py”, line 13, in <br>
from .pose_deterministic import DeterministicPoseDataset<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets\pose_deterministic.py”, line 17, in <br>
from deeplabcut.utils.auxfun_videos import imread, imresize<br>
File “C:\Users\sie\anaconda3\envs\DLC\lib\site-packages\deeplabcut\utils\auxfun_videos.py”, line 25, in <br>
import cv2<br>
ImportError: DLL load failed while importing cv2: No se puede encontrar el módulo especificado.</p> ;;;; <p>Before I had ubuntu working in this pc using the same wifi!<br>
I will try, what do you think?</p> ;;;; <p>Hi <a class="mention" href="/u/cellkai">@CellKai</a>,</p>
<blockquote>
<p>It sound like an odd question, but how would you recommend to clear a whole mouse brain sample so that enough autofluorescence remains?</p>
</blockquote>
<p>No idea actually. most of the clearing I’ve done has been with CLARITY, in which I always found lots of autofluroescence.</p>
<p>I forgot one pretty important suggestion though - you could try another BrainGlobe atlas. The original Allen mouse atlases have a serial two-photon reference image that doesn’t look very similar to iDISCO brains. There are two others you could use that have an iDISCO lightsheet reference image:</p>
<ul>
<li>
<code>perens_lsfm_mouse_20um</code> - This is a 20um resolution atlas from <a href="https://link.springer.com/article/10.1007/s12021-020-09490-8">Perens et al. 2020</a>. It has the same annotations as the Allen, but with an iDISCO reference image. The downside is that it’s in a different coordinate space as the Allen, due to clearing-induced tissue deformation. This should improve registration, but it will mean that any features in your images would need to be transformed to the Allen CCF if you wanted to analyse them alongside other data.</li>
<li>
<code>kim_dev_mouse_idisco_10um</code> - This is an as-yet <a href="https://data.mendeley.com/datasets/2svx788ddf/1">upublished 10um resolution atlas from the Kim lab</a>. It has an iDISCO template, but in the same coordinate space as the Allen. It has different annotations as the Allen, but because it’s in the same coordinate space, converting between the two shouldn’t be hard.</li>
</ul>
<p>We haven’t tested these atlases much for iDISCO registration, but theoretically they should perform better than the original Allen mouse atlas.</p> ;;;; <p>Hi <a class="mention" href="/u/wayne">@Wayne</a>,</p>
<p>This new function solves the issue.<br>
Thank you very much for your valuable support.</p>
<p>Best regards,</p>
<p>Maro</p> ;;;; <p>Hi <a class="mention" href="/u/rlprice410">@rlprice410</a>,</p>
<p>when I open the B&amp;C-tool, the min. and max. display values correspond to the min. and max. values in the histogram.<br>
Are you sure that the shown B&amp;C tool actually corresponds to the image. When you have the tool open, it will not update what it displays, when the image changes. You need to activate the tool window to get the display for the current image.<br>
The min. and max. values are actually the minimum and maximum intensity values in the image.</p>
<p>Best,<br>
Volker</p> ;;;; <aside class="quote no-group" data-username="Zoeyzhengg" data-post="3" data-topic="78678">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/zoeyzhengg/40/69138_2.png" class="avatar"> Zoeyzhengg:</div>
<blockquote>
<p>How can I distinguish the red signal from the red+green signal when doing object classification? What kind of object feature selection I should use?</p>
</blockquote>
</aside>
<p>for this the intensity features could work well (they are computed per-channnel). So if you use something like <em>Mean Intensity</em> will include this information.</p> ;;;; <p>Hi everyone,</p>
<p>I’m developing a program to read czi files with pylibCZIrw and libczi. I try to build my program in flask with multiprocessing. However, if I open the multithreading in flask, the pylibCZIrw cannot read czi file correctly even if I add a lock when calling the pylibczirw.czi.read() function. Like the following image.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/48d4da512ff1c852151151077faa5bd5087d818b.png" alt="Error Info" data-base62-sha1="aoipBaFgivOP448qsJv7y5wXbV9" width="681" height="91"></p>
<p>Does the pylibczirw support multithreading? The doc of libczi told me that this library can be used in multithreading, so I think this may be caused by the pylibczirw or _pylibczirw.pyd?</p>
<p>Thanks all.</p> ;;;; <p>I think the same SSL error might actually be preventing proper installation of the environment dependencies that’s why this is happening</p> ;;;; <p>Hi everyone,</p>
<p>This is my first time using the new maDLC 2.3.0 and my task is too see whether the new version is suitable for our laboratory to analyze our social direct behaviour videos. I have to analyze videos where there are two rats in an open field arena and one of them is marked on the fur. I am using the GUI interface of the program.</p>
<p>The training and the evaluation of the training run smoothly without any errors, my problems arise after I try to analyze a new video. Since this is a test project for now, I wanted to analyze only a 3 minute long video. I checked the boxes that I would like to create new labelled videos, plot the rajectories and save the csv file.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6.png" data-download-href="/uploads/short-url/2FPpmxmOCs9pIY7JEExSgiFzq5g.png?dl=1" title="Képernyőkép 2023-03-21 100710" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_690x351.png" alt="Képernyőkép 2023-03-21 100710" data-base62-sha1="2FPpmxmOCs9pIY7JEExSgiFzq5g" width="690" height="351" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_690x351.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_1035x526.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_1380x702.png 2x" data-dominant-color="242E39"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Képernyőkép 2023-03-21 100710</span><span class="informations">1920×979 37.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The analyzation runs and finishes, I did not see any major error massages, just some smaller ones, but I have attached an image about it. However, after it finishes succesfully, there are no plots, no video, no h5 nor csv file created, only two pickle files.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73dc8eb0cd3c81ffb626fc9ad89f2a0c86742935.png" data-download-href="/uploads/short-url/gwXrY6AA1bG1ckQGAKoQ8Lc94vb.png?dl=1" title="Képernyőkép 2023-03-21 110909" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73dc8eb0cd3c81ffb626fc9ad89f2a0c86742935.png" alt="Képernyőkép 2023-03-21 110909" data-base62-sha1="gwXrY6AA1bG1ckQGAKoQ8Lc94vb" width="690" height="333" data-dominant-color="202020"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Képernyőkép 2023-03-21 110909</span><span class="informations">1917×927 71.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/8/d88fbfe8f329bb21552c4560cb91785db1f7c4b4.png" data-download-href="/uploads/short-url/uTN7bcGje4vl8PTWuEJprml4Kyg.png?dl=1" title="Képernyőkép 2023-03-21 110946" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/8/d88fbfe8f329bb21552c4560cb91785db1f7c4b4.png" alt="Képernyőkép 2023-03-21 110946" data-base62-sha1="uTN7bcGje4vl8PTWuEJprml4Kyg" width="690" height="97" data-dominant-color="1D1D1D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Képernyőkép 2023-03-21 110946</span><span class="informations">1917×272 20 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Now I have read in other forum questions, that in the case of the new maDLC the csv and h5 files are created after the ‘refine tracklets’ step. That is why I decided to try to go to the next step. Since in the GUI of the 2.3.0 the ‘Create video’ is the next step, I tried that first, however neither the skeleton building nor the create video option works. In the case of the video creation I tried with filtered and unfiltered version, as well, but same error comes up, that it cannot find the file. After that I tried to jump to the refine tracklets option, but it does not work either, as it cannot find h5 file for it.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/6/96072a54aae5255a224c830331e77aa549677ba9.png" data-download-href="/uploads/short-url/lpcYhoMINYsVGsCbp0ECT6STmjf.png?dl=1" title="Képernyőkép 2023-03-21 111024" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/6/96072a54aae5255a224c830331e77aa549677ba9.png" alt="Képernyőkép 2023-03-21 111024" data-base62-sha1="lpcYhoMINYsVGsCbp0ECT6STmjf" width="690" height="277" data-dominant-color="1C1C1C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Képernyőkép 2023-03-21 111024</span><span class="informations">1920×773 58.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>It seems to me that there are major output files missing and I do not know why. I tried to run the analyzation again and again, also with different video, but it is the same everytime.</p>
<p>I have limited programming knowledge, thus I may have missed something important, so if you have any further questions, I will try my best to answer it. <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Thank you for your help in advance, I am rather desperate to find the solution.</p> ;;;; <p>(dlc) C:\repos\DeepLabCut\conda-environments&gt;python -m deeplabcut<br>
Loading DLC 2.3.2…<br>
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)<br>
Traceback (most recent call last):<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\runpy.py”, line 185, in _run_module_as_main<br>
mod_name, mod_spec, code = <em>get_module_details(mod_name, <em>Error)<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\runpy.py”, line 144, in <em>get_module_details<br>
return <em>get_module_details(pkg_main_name, error)<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\runpy.py”, line 111, in <em>get_module_details<br>
<strong>import</strong>(pkg_name)<br>
File "C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut_<em>init</em></em>.py", line 37, in <br>
from deeplabcut.create_project import (<br>
File "C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\create_project_<em>init</em></em>.py", line 12, in <br>
from deeplabcut.create_project.demo_data import load_demo_data<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\create_project\demo_data.py”, line 16, in <br>
from deeplabcut.utils import auxiliaryfunctions<br>
File "C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\utils_<em>init</em></em>.py", line 11, in <br>
from deeplabcut.utils.auxfun_multianimal import *<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\utils\auxfun_multianimal.py”, line 34, in <br>
from deeplabcut.utils import auxiliaryfunctions, conversioncode<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\utils\auxiliaryfunctions.py”, line 31, in <br>
from deeplabcut.pose_estimation_tensorflow.lib.trackingutils import TRACK_METHODS<br>
File "C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\pose_estimation_tensorflow_<em>init</em></em>.py", line 17, in <br>
from deeplabcut.pose_estimation_tensorflow.datasets import *<br>
File "C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets_<em>init</em></em>.py", line 13, in <br>
from .pose_deterministic import DeterministicPoseDataset<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets\pose_deterministic.py”, line 17, in <br>
from deeplabcut.utils.auxfun_videos import imread, imresize<br>
File “C:\Users\sie\anaconda3\envs\dlc\lib\site-packages\deeplabcut\utils\auxfun_videos.py”, line 25, in <br>
import cv2<br>
ImportError: DLL load failed while importing cv2: No se puede encontrar el módulo especificado.</p> ;;;; <p>I assume you don’t know the identity of the animals so you shouldn’t assemble with identity. More bodyparts will definitely help</p> ;;;; <p>Can you test if DLC works now? It just doesn’t have GPU support but should work overall.</p>
<p>This error is because of your institution proxy, SSL certificate of your institution has to be added to anaconda.</p>
<p>You can circumvent it by running it on a guest WiFi or using your phone hotspot - I will work on a guide about adding SSL certificates to Anaconda since this issues has been popping a lot lately.</p> ;;;; <p>My university clusters rely on Singularity instead of Docker, so if any of this could be fixed with the helper package <code>deeplabcut-docker</code>, please let me know.</p>
<p>My local cluster relies for every new Jupyter notebook on a freshly created token to access the server.<br>
Unfortunately the docker container does not seem to create such a token. This means I get to the login page of the Jupyter server, but cannot go any further.</p>
<p>Is there a way to force the token generation?<br>
I already contacted my IT department a few days ago, but so far I haven’t heard back.</p>
<pre><code class="lang-auto">$singularity run deeplabcut-2.2.1.1.sif
[I 09:26:45.496 NotebookApp] Serving notebooks from local directory: /home/&lt;university&gt;/&lt;department&gt;/&lt;userid&gt;
[I 09:26:45.496 NotebookApp] Jupyter Notebook 6.4.12 is running at:
[I 09:26:45.496 NotebookApp] http://&lt;node&gt;:8888/
[I 09:26:45.496 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).

</code></pre> ;;;; <p>Thank you. With the sources you pointed out I could get the hardware trigering running in a nice simple way. I have a few follow up questions you might be able to help me with:</p>
<ol>
<li>Can I also use the Multi-Cam utility within Aquisition?</li>
<li>Is there a way to report progress out of a running Aquisition in sequencing mode (at which image in the sequence it is) to use as a progress bar in the GUI? If not, I could use the DAQ device to do this, however that would not be as nice.</li>
<li>Is there a way to interrupt/stop a running Acquistion in sequencing mode?</li>
</ol>
<p>Best wishes<br>
Fred</p> ;;;; <p>Hi <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a></p>
<p>i did until the step 9 without error.</p>
<p>(DLC) C:\repos\DeepLabCut\conda-environments&gt;conda install -c conda-forge cudnn<br>
Collecting package metadata (current_repodata.json): failed</p>
<p>CondaSSLError: OpenSSL appears to be unavailable on this machine. OpenSSL is required to<br>
download and install packages.</p>
<p>Exception: HTTPSConnectionPool(host=‘<a href="http://conda.anaconda.org" rel="noopener nofollow ugc">conda.anaconda.org</a>’, port=443): Max retries exceeded with url: /conda-forge/win-64/current_repodata.json (Caused by SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”))</p> ;;;; <p>Hi <a class="mention" href="/u/cary">@Cary</a>, welcome to the forum!</p>
<p>The error message is quite informative. In order to select a window you need to know its name. There is a separate function that allows you to find the name of the currently active window: <code>getTitle()</code>. Here is a short example of how you might use it:</p>
<pre><code class="lang-auto">//-- Open a new image
newImage("example", "RGB noise", 100, 100, 1);
//-- find the name of the window and record it into a variable
title=getTitle();
//print(title);
run("Split Channels");
//-- Select the Red window
selectWindow(title + " (red)");
</code></pre>
<p>Note how you can concatenate the original name with a string (in this case <code>" (red)"</code>) using a plus sign (<code>+</code>).</p>
<p>It also helps others when you add code to a post, if you can use the toolbar button to format your code (or use triple back ticks <code>```</code> as above), and this really helps with readability.</p>
<p>Hope that helps!</p> ;;;; <p>just as an update, the pairwise registration is now working again with Big_Stitcher-1.1.3 / multiview-reconstruction version: 1.2.8 <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/smile.png?v=12" title=":smile:" class="emoji" alt=":smile:" loading="lazy" width="20" height="20"></p>
<p><a class="mention" href="/u/oburri">@oburri</a></p>
<p>I noticed though an odd behavior. When using<br>
<code> Plugins › BigStitcher › Batch Processing › Calculate pairwise shifts ...</code></p>
<p>A second instance of Fiji is started, and the previous instance crashes.</p>
<p>The pairwise shift calculation finishes successfully, but afterwards its necessary to close/kill both instances and then restart Fiji</p> ;;;; <p>Hi <a class="mention" href="/u/adamltyson">@adamltyson</a> ,</p>
<p>thanks for your reply! I indeed have only this one channel.</p>
<p>We used iDisco clearing with ECi, and in general the autofluorescence in the sample was very weak. While thats usually a good thing, I think thats likely the issue in this case.</p>
<p>It sound like an odd question, but how would you recommend to clear a whole mouse brain sample so that enough autofluorescence remains?</p> ;;;; <p>HI <a class="mention" href="/u/daniel_waiger">@Daniel_Waiger</a> ,</p>
<p>I didn’t test “yet” the 2.2 version.</p>
<p>Two things to troubleshoot :</p>
<p>A. can you <code>conda activate cellpose</code> and :</p>
<ul>
<li>
<p>use <code>conda list</code> and paste here the results (to double check that cellpose is installed)</p>
</li>
<li>
<p>run the very same line output from the console : <code>python -Xutf8 -m cellpose --dir C:\Users\Daniel\AppData\Local\Temp\cellposeTemp --pretrained_model cyto2_omni --chan 1 --diameter 12 --flow_threshold 0.4 --cellprob_threshold 0.0 --verbose --save_tif --no_npy --use_gpu</code><br>
(to see if this works , just check that you have an image in the <em>directory</em> )</p>
</li>
<li>
<p>I saw that you have the argument <code>cyto2_omni</code> , is omnipose installed too ?</p>
</li>
</ul>
<p>B. make a new env with cellpose version 2.0.0 (as mentionned above I didn’t test the 2.2 yet)</p>
<p>Best,</p>
<p>Romain</p> ;;;; <p>Thank you for your suggestion. Yes, I only use 3 bodyparts now. I will try to use 7 bodyparts.<br>
If I want to obtain the coordinates of all frames without any blank frames, does this mean that I should label all body parts, even if they are occluded?</p>
<p>Are you referring to “Assemble with ID only”?<br>
I think l set it as true in shuffle1-4, and false in shuffle5.</p> ;;;; <h3>
<a name="display-range-calculation-1" class="anchor" href="#display-range-calculation-1"></a>Display Range Calculation</h3>
<p>I’m trying to understand how the default “Display Range” that is shown in the Brightness &amp; Contrast window or under Image &gt; Show Info… are calculated.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b.png" data-download-href="/uploads/short-url/zrrWRjB3qwQGTlJ3QXZz3vsjZ7J.png?dl=1" title="ImageJ_DisplayRange" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b_2_690x371.png" alt="ImageJ_DisplayRange" data-base62-sha1="zrrWRjB3qwQGTlJ3QXZz3vsjZ7J" width="690" height="371" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b_2_690x371.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b_2_1035x556.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b.png 2x" data-dominant-color="3B3B50"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">ImageJ_DisplayRange</span><span class="informations">1093×588 202 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h4>
<a name="imagej-documentation-2" class="anchor" href="#imagej-documentation-2"></a>ImageJ Documentation</h4>
<p>The ImageJ documentation says the following, but this is related to the scaling / mapping. It is unclear to me how these minimum and maximum display values are actually obtained.</p>
<blockquote>
<p>The two numbers under the plot are the minimum and maximum displayed pixel values. These two values define the display range, or ‘window’. ImageJ displays images by linearly mapping pixel values in the display range to display values in the range 0–255.</p>
</blockquote> ;;;; <p>Hi CellProfiler community,</p>
<p>I am trying to count the number of co-positive nuclei. However, some speckles in my DAPI channel are being detected as nuclei, which is skewing my co-positive percentages.<br>
There is a stark difference in intensity (but not size) between nuclei and speckles. I have tried setting pixel intensity thresholds within the ‘IdentifyPrimaryObject’ function, but this does not filter out the DAPI speckles.<br>
However, I have found that the ‘FindMaxima’ function is able to pick up these speckles very well (blue particles in image). Is there any way to filter out all the particles that are detected using ‘FindMaxima’?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d.png" data-download-href="/uploads/short-url/2l2DhxTkMVAnpDhOYqgstblX4Ul.png?dl=1" title="image3" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_234x500.png" alt="image3" data-base62-sha1="2l2DhxTkMVAnpDhOYqgstblX4Ul" width="234" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_234x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_351x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d.png 2x" data-dominant-color="191919"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image3</span><span class="informations">385×820 90.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>If not, any other suggestions for removing speckles based on their higher intensity are very much welcome.</p>
<p>Thank you in advance.</p> ;;;; <p>Sorry for the slow reply, and thanks for your support. I ran into various issues still and wasn’t able to put in a lot of time to sort this out. I switched to using SLEAP, which installed very easily including GPU support, and I was able to generate the data I need that way. I still intend to come back to DLC in the future, to compare results and to decide which system SLEAP or DLC will work better for me for processing a lot of additional data in the future.</p>
<p>I don’t understand the python/CUDA underpinnings that cause the installation process (in my hands) to be so delicate/difficult for DLC. SLEAP seems to use the same/similar libraries under the hood, yet installing (with GPU) was trivially easy for me. I know I’m not the first user to run into version / GPU issues with DLC, so maybe there’s an opportunity for some transfer learning here <img src="https://emoji.discourse-cdn.com/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"></p>
<p>I hope it won’t be too long before I get a chance to try to fix my DLC setup and get up and running with your latest code again.</p> ;;;; <p>Hi <a class="mention" href="/u/maro">@Maro</a>,</p>
<p>The ImageJ 1.54d21 daily build adds a  RoiManager.selectPosition(c,z,t) macro function.</p>
<p>Here is an updated version of your macro that uses this new function:</p>
<pre><code class="lang-auto">for (slice=1; slice&lt;=nSlices; slice++) {
   showStatus(slice+"/"+nSlices);
   RoiManager.selectPosition(0,slice,0);
   roiManager("Combine");
   roiManager("Add");
   roiManager("delete");
}
Roi.remove;
roiManager("Show All with labels");
</code></pre> ;;;; <p><a>Uploading: 屏幕截图 2023-03-24 102325.png…</a><br>
<a>Uploading: 屏幕截图 2023-03-24 102226.png…</a></p>
<p>run(“Split Channels”);<br>
selectWindow(“title.jpg (red)”);<br>
setAutoThreshold(“Default dark”);<br>
//run(“Threshold…”);<br>
//setThreshold(70, 255);<br>
setOption(“BlackBackground”, false);<br>
run(“Convert to Mask”);</p>
<p>Can you help me see what’s wrong with my script? Why always report an error:<br>
No window with the title "title.jpg(red)"found.</p> ;;;; <h3>
<a name="sample-image-andor-code-1" class="anchor" href="#sample-image-andor-code-1"></a>Sample image and/or code</h3>
<p>I’ve uploaded a sample image of an OCT image I took. As you can tell, there are some spheres that are of interest to me. Also to note, the substrate is on a tilt (this is one image in a 3D stack). Since we can readily see with our own eyes the air-substrate interface (the continuous surface near the top of the image - its higher on the left and lower on the right), how could I get ImageJ or FIJI to identify the plane at which the surface is?</p>
<p><a class="attachment" href="/uploads/short-url/lIcbaIjeloGqWXTReYn7tXWSwSC.tif">gethelp2.tif</a> (163.9 KB)</p>
            <div class="onebox imgur-album">
              <a href="https://imgur.com/a/y1QKYcu" target="_blank" rel="noopener nofollow ugc">
                <span class="outer-box" style="width:600px">
                  <span class="inner-box">
                    <span class="album-title">[Album] Imgur: The magic of the Internet</span>
                  </span>
                </span>
                <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/2/b26226bb00127689188e7770557360abe99487d7.jpeg" title="Imgur: The magic of the Internet" height="315" width="600">
              </a>
            </div>

<p>Here is my image.</p>
<h3>
<a name="background-2" class="anchor" href="#background-2"></a>Background</h3>
<p>This is a sample of silver particles in 1% agarose gel imaged with OCT.</p>
<h3>
<a name="analysis-goals-3" class="anchor" href="#analysis-goals-3"></a>Analysis goals</h3>
<p>I’d like to measure and identify the plane of the surface, so I can map an equation for it. My analysis requires knowing the depth of the particles from the surface.</p>
<h3>
<a name="challenges-4" class="anchor" href="#challenges-4"></a>Challenges</h3>
<p>I’ve been identifying particles on the surface and then using them to calculate the equation for the surface. Is there something easier that I’m overlooking? There is also a lot of noise in the image, so the interface is not as readily apparently if I plot profile of one line…can there be an average that I can make?</p> ;;;; <p>I am trying control the z stage of a Nikon Ti with Micro manager 2.<br>
However, whenever I tried to move the z stage, it crashed the software. I was able to control the z stage with the TIControl software from Nikon, though. The<br>
nikon driver version is 2.5.0.1</p>
<p>Also, I see on the micro manager page saying that I shouldn’t use a USB 3 ports. However, all USB ports on my computer are USB 3. Am I suppose to find a USB to PCI card just for this?</p>
<p>The error log is attached.<br>
<a class="attachment" href="/uploads/short-url/hWFKUFOF14teuqkWBMYt6Gt1qSJ.txt">hs_err_pid10604.txt</a> (48.2 KB)</p> ;;;; <p>Hi,</p>
<p>Recently I found a scale bar that I just saved did not look right, and this issue kept happening a few times, so I am here to see if anyone knows I did something wrong.</p>
<p>The example image I am using is this:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5.jpeg" data-download-href="/uploads/short-url/vKMFGyuDAlpmBCEij3cJTEcVlyt.jpeg?dl=1" title="NoScaling" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_690x439.jpeg" alt="NoScaling" data-base62-sha1="vKMFGyuDAlpmBCEij3cJTEcVlyt" width="690" height="439" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_690x439.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_1035x658.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_1380x878.jpeg 2x" data-dominant-color="C0C0C0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">NoScaling</span><span class="informations">1460×930 135 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
It is 1460*930 on my laptop.<br>
And I use Analyze&gt;Set Scale as 232 pixel / 1 cm as this:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b3206a40f113bad1ba9e10011715ce40443bd7f6.jpeg" alt="SS1" data-base62-sha1="pyCUNEiWLGPLOoMwouUg7IuwG0K" width="311" height="362"><br>
Then, I use Analyze&gt;Tools&gt;Scale bar:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41798f69e04af3af1d048177c24b169070b243c0.jpeg" alt="SB1" data-base62-sha1="9ldtA4AM7EblOpiRLX3qfXtpCA8" width="301" height="459"><br>
The scale bar looks like this:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0.jpeg" data-download-href="/uploads/short-url/dJTnI0CB0D7312olAwjCskaC2cg.jpeg?dl=1" title="WithScaleBar0" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0_2_690x481.jpeg" alt="WithScaleBar0" data-base62-sha1="dJTnI0CB0D7312olAwjCskaC2cg" width="690" height="481" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0_2_690x481.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0.jpeg 2x" data-dominant-color="C4C4C4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">WithScaleBar0</span><span class="informations">927×647 61.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
If you measure the scale bar in this figure it already does not live up to my expectation that the scale bar should reflect 1 cm. When I measure it using shortcut M, I am told this is actually 0.69 cm, and when I save it as jpg and reopen it, it measures 160 px which is exactly 232*0.69.</p>
<p>I wonder if there is anything I might have messed up or if this is some weird bug. My ImageJ seems to be ImageJ 1.53t. Any comment or help is appreciated!</p>
<p>Best,<br>
Zhengyu</p> ;;;; <p>Hi Antonio,<br>
What software did you use to generate the RGB? What is the source of the mIF file?</p> ;;;; <p>Sorry, I should mention, an RGB image is 8 bit. So, you can only trust if it your original data was also 3 channel, 8 bit.</p> ;;;; <p>Hi <a class="mention" href="/u/oburri">@oburri</a> <a class="mention" href="/u/romainguiet">@romainGuiet</a>,</p>
<p>Me again, on another PC.<br>
HP Z620, XEON E5-2650 V0<br>
32GB DDR3<br>
1660Ti<br>
WIN10<br>
PYTHON 3.8<br>
Minimal cellpose (no GUI) 2.2<br>
pytorch==1.12.0 cudatoolkit=11.3</p>
<p><strong>I followed all the instructions, including the environment variables page.</strong><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/9/29db30d4a3b403648878f57b6464adb750e0ada5.png" data-download-href="/uploads/short-url/5YhbNu8bU9aO3eozvF0ghvGfnYV.png?dl=1" title="2023-03-23_22h26_40" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/9/29db30d4a3b403648878f57b6464adb750e0ada5.png" alt="2023-03-23_22h26_40" data-base62-sha1="5YhbNu8bU9aO3eozvF0ghvGfnYV" width="492" height="375" data-dominant-color="1D1D1D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">2023-03-23_22h26_40</span><span class="informations">672×512 6.59 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I get this error since the morning <img src="https://emoji.discourse-cdn.com/twitter/face_with_spiral_eyes.png?v=12" title=":face_with_spiral_eyes:" class="emoji" alt=":face_with_spiral_eyes:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/face_with_spiral_eyes.png?v=12" title=":face_with_spiral_eyes:" class="emoji" alt=":face_with_spiral_eyes:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/face_with_spiral_eyes.png?v=12" title=":face_with_spiral_eyes:" class="emoji" alt=":face_with_spiral_eyes:" loading="lazy" width="20" height="20"><br>
How should I proceed?<br>
I suspect this line in the log window:<br>
<strong>C:\Users\Daniel\anaconda3\envs\cellpose\python.exe: No module named cellpose</strong></p>
<pre><code class="lang-auto">cyto_channel:1:nuclei_channel:0
C:\Users\Daniel\AppData\Local\Temp\cellposeTemp\IDA-OCS10-4-CLAHE-t1.tif
Cellpose version is set to:2.0
[cmd.exe /C CALL conda.bat activate C:\Users\Daniel\anaconda3\envs\cellpose &amp; python -Xutf8 -m cellpose --dir C:\Users\Daniel\AppData\Local\Temp\cellposeTemp --pretrained_model cyto2_omni --chan 1 --diameter 12 --flow_threshold 0.4 --cellprob_threshold 0.0 --verbose --save_tif --no_npy --use_gpu]
C:\Users\Daniel\anaconda3\envs\cellpose\python.exe: No module named cellpose
Runner C:\Users\Daniel\anaconda3\envs\cellpose exited with value 1. Please check output above for indications of the problem.
java.lang.NullPointerException
	at ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:230)
	at ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)
	at org.scijava.command.CommandModule.run(CommandModule.java:196)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

</code></pre>
<p>Thanks in advance for the advice!</p>
<p>Daniel</p> ;;;; <p>It would help to know what the original file type is, and what the Server type listed in the Image tab. Some images, like 4 channel images from MRXS files, can only be rendered as RGB because that is the only option due to how difficult 3DHISTECH has made working with their files. You may want to look up one of the many posts on the forum about dealing with or converting such files into a usable format if that is the case.</p>
<p>Otherwise, as <a class="mention" href="/u/smcardle">@smcardle</a> says, once a 4 channel or more image is saved as RGB, much of the data is gone, and you will need to go back to the original data.</p> ;;;; <p>If your original image had exactly 3 channels and they were pseudo-colored red, green, and blue, you can work with the data exactly as is. Unfortunately, if your image had more channels than that, once it has been converted down to a 3 channel RGB, there is no way to undo it. Information has been lost and QuPath cannot recover it. <img src="https://emoji.discourse-cdn.com/twitter/frowning.png?v=12" title=":frowning:" class="emoji" alt=":frowning:" loading="lazy" width="20" height="20"></p> ;;;; <p>Thank you Sara! I think you are right and they have been converted to RGB, even if at the beginning I save them as fluorescence… do you know if there is a way to convert back ?</p> ;;;; <aside class="quote no-group" data-username="cstoltzfus" data-post="3" data-topic="78966">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cstoltzfus/40/32335_2.png" class="avatar"> Dr. Caleb Stoltzfus:</div>
<blockquote>
<p>In the cytomap plot in the bottom left there is an options button, inside that sub menu you can flip the Y axis so the X-Y display of the positions of the cells will match the actual image.</p>
</blockquote>
</aside>
<p>Not entirely related to the thread, but I didn’t know this and I’m glad I do now!</p> ;;;; <p>I can edit this with some code later. But the memory usage increase is quite manageable with 64GB of system memory. Turning on shading does slow it way down though.</p> ;;;; <p>Hi, I am trying to extract stage coordinates from a .vsi file using Bio-formats Macro Extensions.<br>
I get “Macro error” when trying to read the stage positions. Any ideas what could be wrong?</p>
<p>file = File.openDialog(“Select a File”);<br>
run(“Bio-Formats Macro Extensions”);<br>
Ext.setId(file);<br>
Ext.getPixelsPhysicalSizeZ(sizeZ); print(sizeZ); - this wprks fine<br>
Ext.getPlanePositionX(stage_x , 0); print(stage_x)<br>
Ext.close();</p> ;;;; <p>Hello,</p>
<p>I am currently trying to manually seed and segment a stack taken of an Arabidopsis leaf, and I’m having trouble getting clearly defined cell borders. When I project the signal onto the mesh, it doesn’t look like the clean lines given in the example images from the manual. It looks like this:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c.png" data-download-href="/uploads/short-url/6m8i1lYmf3hyqlqohUtyYrRAkxe.png?dl=1" title="Screenshot 2023-03-23 115757" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_297x375.png" alt="Screenshot 2023-03-23 115757" data-base62-sha1="6m8i1lYmf3hyqlqohUtyYrRAkxe" width="297" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_297x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_445x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_594x750.png 2x" data-dominant-color="606060"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-23 115757</span><span class="informations">646×814 242 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I can still see where the cell borders are, but when I place seeds onto the mesh the watershed segmentation doesn’t seem to be able to accurately find them:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e.png" data-download-href="/uploads/short-url/3831JItPSjHHOwDpQgJvYfkthoO.png?dl=1" title="Screenshot 2023-03-23 115704" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_201x250.png" alt="Screenshot 2023-03-23 115704" data-base62-sha1="3831JItPSjHHOwDpQgJvYfkthoO" width="201" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_201x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_301x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_402x500.png 2x" data-dominant-color="58550F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-23 115704</span><span class="informations">653×809 282 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>If I draw around each cell with the seeds, the borders are still very jagged and often don’t match up with the cell outlines:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6.png" data-download-href="/uploads/short-url/2PoHrbHlsc1uOJTwnnEySePjUvI.png?dl=1" title="Screenshot 2023-03-23 120127" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_195x250.png" alt="Screenshot 2023-03-23 120127" data-base62-sha1="2PoHrbHlsc1uOJTwnnEySePjUvI" width="195" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_195x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_292x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_390x500.png 2x" data-dominant-color="342E4F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-23 120127</span><span class="informations">635×811 287 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I have tried subdividing the mesh multiple times before seeding, along with subdividing adaptive near borders after segmentation, and I still can’t get the borders to be clearly defined like in the manual. I’m guessing the problem has something to do with the original projected signal but I’m not sure what to do about it.</p>
<p>Does anyone have any suggestions?<br>
Thank you!</p> ;;;; <p>Hi Eddy,</p>
<p>for RGB the pixel type will be 8bit, you will have a single channel which will then have SamplesPerPixel set to 3. So each pixel will have 3 bytes, I have a small sample below that may help:</p>
<pre><code class="lang-auto">   &lt;Image ID="Image:0" Name="color.tiff"&gt;
      &lt;Pixels BigEndian="true" DimensionOrder="XYCZT" ID="Pixels:0" Interleaved="false" SignificantBits="8" SizeC="3" SizeT="1" SizeX="512" SizeY="512" SizeZ="1" Type="uint8"&gt;
         &lt;Channel ID="Channel:0:0" SamplesPerPixel="3"&gt;
            &lt;LightPath/&gt;
         &lt;/Channel&gt;
         &lt;MetadataOnly/&gt;
      &lt;/Pixels&gt;
   &lt;/Image&gt;
</code></pre> ;;;; <p>Hey Antonio-</p>
<ol>
<li>Check your Image tab to make sure QuPath is reading this as a fluorescence image</li>
<li>Check your input data to make sure the file has all the channels and hasn’t been converted to a flattened RGB</li>
</ol> ;;;; <p>If you are looking to capture a specific population of cells you can clearly see in the image you can manually gate on the channel intensity of the cells as you would with flow cytometry data. I go over this workflow a little in one of the examples: <a href="https://cstoltzfus.com/posts/2021/06/CytoMAP%20Demo/" class="inline-onebox">Simplified CytoMAP Workflow - Dr. Caleb R. Stoltzfus</a></p>
<p>When I do this with my samples I like to have the original image open, a plot of the position of the cells in X,Y, and the plot with my cell gate. The I will adjust my cell gate, save the population, refresh or re-plot the cell positions, then compare their distribution to the original image to see if I am roughly capturing the cells I need to quantify.</p>
<p>One thing to note when doing this is that most image display software flips the images when they display them. In the cytomap plot in the bottom left there is an options button, inside that sub menu you can flip the Y axis so the X-Y display of the positions of the cells will match the actual image.</p> ;;;; <aside class="quote no-group" data-username="moseyic" data-post="3" data-topic="77764">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/8491ac/40.png" class="avatar"> moseyic:</div>
<blockquote>
<p>As an aside, I’ve found that the volume renderer in PyVista is butter smooth. I assume its downsampling in the background since 1024x1024x1024 is a pretty large volume.</p>
</blockquote>
</aside>
<p>This surprises me, but I’m not very familiar with PyVista. Do you have some sample code I can use? Memory usage exploded to &gt;25 GB when I tried to do this with a 1024 cube of random values.</p> ;;;; <p>Hi, I have a project with MIF images and I would like to use Qupath, however after that I created my project I’m not able to see the right channels in the “brightness and contrast” section. Do you know how to fix this? thank you<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403.jpeg" data-download-href="/uploads/short-url/2ZEujHjfWoQlEHPm92DhoSlhGMP.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_690x440.jpeg" alt="image" data-base62-sha1="2ZEujHjfWoQlEHPm92DhoSlhGMP" width="690" height="440" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_690x440.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_1035x660.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_1380x880.jpeg 2x" data-dominant-color="131618"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1642×1048 129 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>So thing is that channels() is a QuPath thing. Allowing you to send the channels you want.<br>
cellposeChannels() corresponds to the <code>--chan</code> and <code>--chan1</code> flags from the cellpose command line.</p>
<p>I do not have a specific use case in mind, though for example you might send an RGB image to cellpose (by not setting anything on the channels() ) and then want cellpose to only use the red channel, so you’d use <code>cellposeChannels(1,0)</code></p>
<p>Hope this clears it up a little bit.</p>
<p>Oli</p> ;;;; <p>Greetings everyone,<br>
I am trying to measure the volume of a retinal lesions in an image obtained using OCT. Each section parameters (X and Z parameters) and the distance between each scans are available. I studied a few paper and it seems that Cavalieri principle is the main method that have been used to measure the volume using the area (by ImageJ). I just wanted to be sure that the proposed method is the accurate method so I can measure the volume of lesions.<br>
My scans of retinal lesion are as follows:<br>
3D scan:<br>
<a class="attachment" href="/uploads/short-url/5uN5exmHInCI63aiEKQbiVA91ta.tif">3D 3.tif</a> (1.7 MB)<br>
2D scan:<br>
<a class="attachment" href="/uploads/short-url/uXdfX7ayKhmzm6emGJUmBhG5mXS.tif">2D scan.tif</a> (451.1 KB)<br>
Proposed method to measure the retinal lesions:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d.jpeg" data-download-href="/uploads/short-url/zgp1CslsE5VEt18cScN5OSpnpk9.jpeg?dl=1" title="Picture1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d_2_690x419.jpeg" alt="Picture1" data-base62-sha1="zgp1CslsE5VEt18cScN5OSpnpk9" width="690" height="419" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d_2_690x419.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d_2_1035x628.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d.jpeg 2x" data-dominant-color="8F9192"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Picture1</span><span class="informations">1224×744 161 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899.jpeg" data-download-href="/uploads/short-url/1QMiTo2LrRg6MRrw0S2fKgpI0id.jpeg?dl=1" title="Picture2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899_2_690x268.jpeg" alt="Picture2" data-base62-sha1="1QMiTo2LrRg6MRrw0S2fKgpI0id" width="690" height="268" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899_2_690x268.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899_2_1035x402.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899.jpeg 2x" data-dominant-color="CACBCC"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Picture2</span><span class="informations">1224×476 163 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
It would be great if you could give me your opinion or any suggestions.<br>
Thank you</p> ;;;; <p>I don’t think the ImageJ viewer is showing isosurfaces. That looks more like a sophisticated (2D+) transfer function to me. I’d be excited to take up some work on this for napari and/or vispy <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20">.</p>
<p>This is a relevant review of transfer functions: <a href="https://www.diva-portal.org/smash/get/diva2:954156/FULLTEXT01.pdf" rel="noopener nofollow ugc">https://www.diva-portal.org/smash/get/diva2:954156/FULLTEXT01.pdf</a></p> ;;;; <p>The MPII dataset actually labels more points on the human body than I have labeled on the exoskeleton data. Is there any other way to combine the models? Also, shouldn’t the MPII model be able to label a human body without an exoskeleton on its own (without combining the model with my exoskeleton model)? I am confused why the MPII model isn’t performing well on the human body alone. Thank you.</p> ;;;; <p>It’s set to false. but i have a feeling thats overriden.</p> ;;;; <p>Hello <a class="mention" href="/u/julianhn">@JulianHn</a>,</p>
<p>It’s exactly what I had in mind, espacially to have a thinner height for the bar, as the default one is a bit large sometimes.<br>
Thank you for the PR !</p>
<p>Rémy.</p> ;;;; <p>If in your data you have points that label the human that are the same points that would be labeled in the MPII dataset, use this subset from the MPII and change the names to match the ones in your data.</p> ;;;; <p>Thanks <a class="mention" href="/u/will-moore">@will-moore</a> !<br>
Indeed, it is a small fix ; now, it perfectly works.</p>
<blockquote>
<p>You could also add a loop in to process all selected Images, if you want to allow that?</p>
</blockquote>
<p>This is what I planed to do</p> ;;;; <p>The body part names are not the same since it also has points on the exoskeleton. Would it still be possible to combine the two models? Thank you.</p> ;;;; <p>Hello everyone,<br>
I installed OMERO.web and everything worked fine. Then I installed  OMERO.iviewer from the omero.web virtual environment and installation successfully completed.</p>
<p>I followed this instructions:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://pypi.org/project/omero-iviewer/">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6ef85bcf60dcca703f2a4b057ca0fefbdf0668c6.png" class="site-icon" width="32" height="30">

      <a href="https://pypi.org/project/omero-iviewer/" target="_blank" rel="noopener nofollow ugc">PyPI</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f97026709e67b2111b465be6427519ead928642.webp" class="thumbnail onebox-avatar" width="300" height="300">

<h3><a href="https://pypi.org/project/omero-iviewer/" target="_blank" rel="noopener nofollow ugc">omero-iviewer</a></h3>

  <p>A Python plugin for OMERO.web</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>When I tried to load OMERO.web again from my browser It did not work anymore.</p>
<p>(base) [omero-web@bigbang root]$ pip freeze<br>
appdirs==1.4.4<br>
asgiref==3.4.1<br>
async-timeout==4.0.2<br>
certifi==2022.12.7<br>
charset-normalizer==2.0.12<br>
concurrent-log-handler==0.9.20<br>
Django==3.2.18<br>
django-cors-headers==3.7.0<br>
django-pipeline==2.0.7<br>
django-redis==4.8.0<br>
future==0.18.3<br>
gunicorn==20.1.0<br>
idna==3.4<br>
importlib-metadata==4.8.3<br>
numpy==1.19.5<br>
omero-iviewer==0.12.0<br>
omero-marshal==0.8.0<br>
omero-py==5.13.1<br>
omero-web==5.19.0<br>
packaging==21.3<br>
Pillow==7.1.1<br>
portalocker==2.7.0<br>
pyparsing==3.0.9<br>
pytz==2022.7.1<br>
PyYAML==6.0<br>
redis==3.4.1<br>
requests==2.27.1<br>
sqlparse==0.4.3<br>
typing_extensions==4.1.1<br>
urllib3==1.26.14<br>
whitenoise==5.3.0<br>
zeroc-ice==3.6.5<br>
zipp==3.6.0</p>
<p>This is my config</p>
<p>(base) [omero-web@bigbang root]$ /opt/omero/web/venv3/bin/omero config get<br>
omero.glacier2.IceSSL.ProtocolVersionMax=TLS1_2<br>
omero.glacier2.IceSSL.Protocols=TLS1_0,TLS1_1,TLS1_2<br>
omero.web.application_server=wsgi-tcp<br>
omero.web.apps=[“omero_iviewer”]<br>
omero.web.caches={“default”: {“BACKEND”: “django_redis.cache.RedisCache”,“LOCATION”: “redis://127.0.0.1:6379/0”}}<br>
omero.web.middleware=[{“index”: 1, “class”: “django.middleware.common.BrokenLinkEmailsMiddleware”}, {“index”: 2, “class”: “django.middleware.common.CommonMiddleware”}, {“index”: 3, “class”: “django.contrib.sessions.middleware.SessionMiddleware”}, {“index”: 4, “class”: “django.middleware.csrf.CsrfViewMiddleware”}, {“index”: 5, “class”: “django.contrib.messages.middleware.MessageMiddleware”}, {“index”: 6, “class”: “django.middleware.clickjacking.XFrameOptionsMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}, {“index”: 0, “class”: “whitenoise.middleware.WhiteNoiseMiddleware”}]<br>
omero.web.open_with=[[“Image viewer”, “webgateway”, {“supported_objects”: [“image”], “script_url”: “webclient/javascript/ome.openwith_viewer.js”}], [“omero_iviewer”, “omero_iviewer_index”, {“supported_objects”: [“images”, “dataset”, “well”], “script_url”: “omero_iviewer/openwith.js”, “label”: “OMERO.iviewer”}]]<br>
omero.web.server_list=[[“localhost”, 4064, “omero”]]<br>
omero.web.session_engine=django.contrib.sessions.backends.cache<br>
omero.web.viewer.view=omero_iviewer.views.index</p>
<p>(base) [omero-web@bigbang root]$ /opt/omero/web/venv3/bin/omero web start</p>
<p>0 static files copied to ‘/opt/omero/web/omero-web/var/static’, 593 unmodified, 2 post-processed.<br>
Clearing expired sessions. This may take some time… Traceback (most recent call last):<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/cache/<strong>init</strong>.py”, line 39, in create_connection<br>
backend_cls = import_string(backend)<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/utils/module_loading.py”, line 17, in import_string<br>
module = import_module(module_path)<br>
File “/usr/lib64/python3.6/importlib/<strong>init</strong>.py”, line 126, in import_module<br>
return _bootstrap._gcd_import(name[level:], package, level)<br>
File “”, line 994, in _gcd_import<br>
File “”, line 971, in _find_and_load<br>
File “”, line 955, in _find_and_load_unlocked<br>
File “”, line 665, in _load_unlocked<br>
File “”, line 678, in exec_module<br>
File “”, line 219, in _call_with_frames_removed<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django_redis/cache.py”, line 7, in <br>
from .util import load_class<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django_redis/util.py”, line 7, in <br>
from django.utils.encoding import smart_text, python_2_unicode_compatible<br>
ImportError: cannot import name ‘python_2_unicode_compatible’</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br>
File “manage.py”, line 75, in <br>
execute_from_command_line(sys.argv)<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/<strong>init</strong>.py”, line 419, in execute_from_command_line<br>
utility.execute()<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/<strong>init</strong>.py”, line 413, in execute<br>
self.fetch_command(subcommand).run_from_argv(self.argv)<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/base.py”, line 354, in run_from_argv<br>
self.execute(*args, **cmd_options)<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/base.py”, line 393, in execute<br>
self.check()<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/base.py”, line 423, in check<br>
databases=databases,<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/checks/registry.py”, line 76, in run_checks<br>
new_errors = check(app_configs=app_configs, databases=databases)<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/checks/caches.py”, line 63, in check_file_based_cache_is_absolute<br>
cache = caches[alias]<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/utils/connection.py”, line 62, in <strong>getitem</strong><br>
conn = self.create_connection(alias)<br>
File “/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/cache/<strong>init</strong>.py”, line 43, in create_connection<br>
) from e<br>
django.core.cache.backends.base.InvalidCacheBackendError: Could not find backend ‘django_redis.cache.RedisCache’: cannot import name ‘python_2_unicode_compatible’</p>
<p>Can anypone help me to solve this issue?</p>
<p>Thank you very much in advance</p> ;;;; <p>If the names of bodyparts for human are the same in your dataset and the MPII dataset you’re using you can combine them, retrain the model and try it out. Ideally it would just label only the human when there is no exoskeleton and human + exoskeleton when the person is wearing one.</p> ;;;; <p>I am also labeling the human in my data.</p> ;;;; <p>It is comparing to the values in the dataset which is created from the labeled data. Do you have <code>cropping</code> set to <code>true</code> in the lower part of the config?</p> ;;;; <p>I get the same results !<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/934254049c2187093c8da73d4fff0cb99b77af8e.png" data-download-href="/uploads/short-url/l0IikvQKdwsEmDF5ocZHsv3XlpQ.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/934254049c2187093c8da73d4fff0cb99b77af8e.png" alt="image" data-base62-sha1="l0IikvQKdwsEmDF5ocZHsv3XlpQ" width="690" height="55" data-dominant-color="F3E7E7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1004×81 3.26 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
I just want to know what values is DLC comparing to the predictions to ?</p> ;;;; <p>Awesome, we just tested the thing and everything seems to work ! Thanks a lot for this super fast reaction <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"></p> ;;;; <p>So you want to track both the human and exoskeleton analyzed with two separate models? Or are you also labelling the human in your data?</p> ;;;; <aside class="quote no-group" data-username="Leo_Mv" data-post="1" data-topic="78966">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/leo_mv/40/22830_2.png" class="avatar"> Leonard:</div>
<blockquote>
<p>However, the clustering detects fewer CD8+ T cell subsets than are visibly present in the original IHC image.</p>
</blockquote>
</aside>
<p>I am not certain what the problem is (and have no real experience with Histocat), but I do want to emphasize that CytoMAP clustering is based purely off of numerical values passed to it, which tend not to represent the stain distribution that we see very well. The fact that most analyses are 2D makes matters even worse. Depending on the software, there is often a “cytoplasmic expansion” stage which is not necessarily context dependent which will poll a larger area than the tight cytoplasms of lymphocytes, leading to them being visible to the eye, but not showing up well as “average intensities” for a given ROI in a spreadsheet.</p>
<p>Cheers,<br>
Mike</p> ;;;; <p>Hi <a class="mention" href="/u/saurabhm">@saurabhm</a>,</p>
<p>Hm, strange. Which update sites did you switch on in Fiji?</p>
<p>You will minimally need to activate the following ones for this to work.</p>
<ul>
<li>bv3dbox</li>
<li>clij</li>
<li>clij2</li>
<li>clijx-assistant</li>
<li>clijx-assistant-extensions</li>
<li>3D ImageJ Suite</li>
</ul>
<p>And standard-wise</p>
<ul>
<li>Fiji</li>
<li>ImageJ</li>
<li>Java 8</li>
</ul>
<p>are also active</p>
<p>However, the error message does not really point to any of those as far as I can see. You also run exactly the same Fiji version I have. Potentially try to reinstall Fiji again and add those update sites only. Then try to run one of the BioVoxxel functions or the Voronoi Threshold Labeler directly again.</p>
<p>I tried a fresh Fiji installation with the update and the functions work on my end.</p> ;;;; <p>Thank you so much! I don’t why but my macro recorder didn’t show that command.</p> ;;;; <p>What data do you get when you run <code>deeplabcut.analyze_time_lapse_frames</code> on one of the folders in labeled-data?</p> ;;;; <p>Also, looking quickly at the paper, it seems this line:</p>
<pre><code class="lang-python">    newVal = (intersections[i] - intersections[i - 1]) * (i+1)
</code></pre>
<p>ought to be:</p>
<pre><code class="lang-python">    newVal = (intersections[i] - intersections[i - 1]) * i
</code></pre>
<p>(I edited my snippet above to reflect this)</p> ;;;; <p>How do i get the groundtruth to be on the same scale as the predictions ? How does DLC do it (the anotations are relative to the original size)?</p> ;;;; <p>Rescale != crop</p>
<p>If they were croped when creating the training set the coordinates should be relative to croping, not the whole image. Same with the evaluation since the evaluation is run on the images that are in the labeled-data folder.</p> ;;;; <p><a class="mention" href="/u/ghriann">@Ghriann</a> welcome to the forum! I pm’d your login credentials and your site should be good to go <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> let me know if you need anything else.</p> ;;;; <p>Brilliant Chris, thankyou. I will let you know how I get on.</p> ;;;; <p><a class="mention" href="/u/domonkos_nh">@Domonkos_NH</a> welcome to the forum! I pm’d your login credentials and your site should be good to go <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> let me know if you need anything else.</p> ;;;; <p>Also, if you use the same Type and IDs inputs as in e.g. <a href="https://github.com/ome/omero-scripts/blob/88219a3323685a575f92e689222b91b4e590da3c/omero/util_scripts/Channel_Offsets.py#L268" class="inline-onebox">omero-scripts/Channel_Offsets.py at 88219a3323685a575f92e689222b91b4e590da3c · ome/omero-scripts · GitHub</a> then the currently selected Image(s) will be filled-in when the script dialog is shown.</p>
<p>You could also add a loop in to process all selected Images, if you want to allow that?</p> ;;;; <p>The labeled data I have consists of the person wearing an exoskeleton, so I was hoping this dataset would improve performance on the human body so I can track the exoskeleton and human body. Will my labeled frames of the person wearing an exoskeleton lead to improvement in the human data test? Thank you.</p> ;;;; <p><a class="mention" href="/u/julianhn">@JulianHn</a>: I think you got the wrong PR: this one… <a href="https://github.com/ome/omero-figure/pull/504" class="inline-onebox">Add option to adjust Scalebar width by JulianHn · Pull Request #504 · ome/omero-figure · GitHub</a></p> ;;;; <p>Could you please provide me with your input image/sequence and parameters that you are using? I cannot reproduce it on images I have.</p> ;;;; <p>i don’t know the original size of the videos as i got the data already annotated in dlc format.</p> ;;;; <p>It’s not clear in the images but yes i’m sure. Also the images are cropped to 288x162 in the config file except the last video. but even when i rescale the groundtruth to that size the error is still high compared to images and csv files.</p> ;;;; <p>Just like you normally would label data for the project. Or copy paste from a different project the folder with labeled frames and add the video to the project</p> ;;;; <p>Hi Josh, thanks for the answer, but:</p>
<aside class="quote group-team" data-username="joshmoore" data-post="2" data-topic="78910">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/joshmoore/40/1634_2.png" class="avatar"> Josh Moore:</div>
<blockquote>
<p>The guest user is used internally to load non-sensitive information. It has special handling (and restrictions) so I wouldn’t suggest using it as the public user. i.e. this is likely your root problem.</p>
<p>Instead, create a new user (named “public”, “PUBLIC”, etc.) and configure it for public access.</p>
</blockquote>
</aside>
<p>Here is the content of my ansible playbook.yml for omero-web:</p>
<pre><code class="lang-auto">- hosts: localhost
  roles:
  - role: ome.omero_web
    omero_web_config_set:
      omero.web.public.enabled: true
      omero.web.public.server_id: 1
      omero.web.public.user: public
      omero.web.public.password: "{{ omero_web_public_password }}"
      omero.web.public.url_filter: "^/(webadmin/myphoto/|webclient/\
      (?!(action|logout|annotate_(file|tags|comment|rating|map)|\
      script_ui|ome_tiff|figure_script))|\
      webgateway/(?!(archived_files|download_as)))"
  - role: ome.java


  vars:
    ice_version: "3.6"
    ice_install_devel: False
    ice_install_python: False
    ice_python_wheel: https://github.com/ome/zeroc-ice-py-centos7/releases/download/0.1.0/zeroc_ice-3.6.4-cp27-cp27mu-linux_x86_64.whl
    omero_web_systemd_setup: False
    omero_web_setup_nginx: False
    # These defaults can be overriden at runtime
    omero_web_public_password: public
    omero_web_config_set:
      omero.web.application_server.host: 0.0.0.0
      # Allow connecting to different minor releases
      # When https://github.com/openmicroscopy/openmicroscopy/pull/5913 is
      # released this can be removed
      omero.web.check_version: "false"
      omero.web.server_list: [[omero, 4064, omero]]
      omero.web.secure: "true"

</code></pre>
<p>I didn’t used “guest” for public user. I have still the same error.</p>
<p>Marc.</p> ;;;; <p>Hi, here are two examples of macro commands that run Particle Tracker in percentile and absolute mode:</p>
<pre><code class="lang-auto">// Percentile
run("Particle Tracker 2D/3D", "radius=3 cutoff=0.001 per/abs=0.501 link=2 displacement=10 dynamics=Brownian");

// Absolute
run("Particle Tracker 2D/3D", "radius=3 cutoff=0.001 per/abs=50 absolute link=2 displacement=10 dynamics=Brownian");
</code></pre>
<p>As you may notice, the only difference is adding ‘absolute’ keyword to command (and this is exactly what macro recorder is creating in my case).</p> ;;;; <p>No I did not. If I should do that, how would I go about doing that since that has a separate csv and h5 file? Thank you.</p> ;;;; <p>Dear Jan-Marie,</p>
<p>As Sebastian Besson suggested in the linked post, I have looked at the Blitz logs and it seems it is precisely what we are suffering from. There are no 'remove servant ’ event logs resulting from that search! We are glad that it’s a problem well on its way to being solved.</p>
<p>Would you happen to know when these patches will be available for download at <a href="https://www.openmicroscopy.org/omero/downloads/" class="inline-onebox" rel="noopener nofollow ugc">OMERO Downloads | Open Microscopy Environment (OME)</a> ? It would allow us to ask our users to update their OMERO.insight.</p>
<p>Bests,<br>
Rodrigo</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10.jpeg" data-download-href="/uploads/short-url/jqyvkpYjHAx0ZoeS1B24L8RUmXu.jpeg?dl=1" title="Screenshot 2023-03-23 130124" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_517x293.jpeg" alt="Screenshot 2023-03-23 130124" data-base62-sha1="jqyvkpYjHAx0ZoeS1B24L8RUmXu" width="517" height="293" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_517x293.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_775x439.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_1034x586.jpeg 2x" data-dominant-color="BFBFDB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-23 130124</span><span class="informations">1246×707 178 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
This is the result for that script so you might want to make <code> "thresholdPositive1"</code> something like 0.1 rather than 0.2…</p> ;;;; <p>Something like:</p>
<pre><code class="lang-auto">setImageType('BRIGHTFIELD_H_DAB')
setColorDeconvolutionStains('{"Name" : "H-DAB estimated", ' + 
          '"Stain 1" : "Hematoxylin", "Values 1" : "0.8228 0.52632 0.21444", ' +
          '"Stain 2" : "DAB", "Values 2" : "0.32346 0.46972 0.82142", ' +
          '"Background" : " 232 232 231"}')
                            
createFullImageAnnotation(true)

runPlugin('qupath.imagej.detect.cells.PositiveCellDetection', 
            '{"detectionImageBrightfield": "Hematoxylin OD", ' +
            ' "requestedPixelSizeMicrons": 0.5, ' +
            ' "backgroundRadiusMicrons": 8.0, ' +
            ' "medianRadiusMicrons": 0.0, ' +
            ' "sigmaMicrons": 1.5, ' +
            ' "minAreaMicrons": 10.0, ' +
            ' "maxAreaMicrons": 400.0, ' +
            ' "threshold": 0.1, ' +
            ' "maxBackground": 2.0, ' +
            ' "watershedPostProcess": true, ' +
            ' "excludeDAB": false, ' +
            ' "cellExpansionMicrons": 8.0, ' +
            ' "includeNuclei": true, ' +
            ' "smoothBoundaries": true, ' +
            ' "makeMeasurements": true, ' +
            ' "thresholdCompartment": "Cytoplasm: DAB OD mean", ' +
            ' "thresholdPositive1": 0.2, ' +
            ' "thresholdPositive2": 0.4, ' +
            ' "thresholdPositive3": 0.6, ' +
            '"singleThreshold": true}')
            
println("Done!")

</code></pre>
<p>should work. You will need to check <em>all</em> the variables are correct as I have not tested it on your image. Try working out the variables using one image through the GUI then plug them in the script where needed.<br>
The colour vectors are something that can be worked out using the preprocessing options and then play around with cell detection until you are getting good results.</p>
<p>(Edited to correct script)</p> ;;;; <p>Are you sure these are sorted with the same index?</p> ;;;; <p>Hey,</p>
<p>since we were actually having similar wishes this thread and Issue have prompted me to actually look into implementing it. PR Draft is linked against the GitHub issue. <a href="https://github.com/ome/omero-figure/pull/497" class="inline-onebox" rel="noopener nofollow ugc">Default figure filename to YYYY-MM-DD_hh-mm-ss by Tom-TBT · Pull Request #497 · ome/omero-figure · GitHub</a><br>
<a class="mention" href="/u/rdornier">@Rdornier</a> : Is this what you had in mind? (Ofc still work to do)</p>
<p>// Julian</p> ;;;; <p>Its a close-up of the image (with the less important bit, such as glands) removed. I have uploaded an example for you. We have 1000s of these types of images.<br>
<a class="attachment" href="/uploads/short-url/xGshjjrNDFCWPWoHkBH4AQV8GWU.tif">Tester image DAB.tif</a> (10.4 MB)</p> ;;;; <p>Is your image of the whole slide with whitespace around it or of a close-up of the tissue so the whole image has cells?</p> ;;;; <p>Here the global scale is at 0.8.</p> ;;;; <p>Hi Paul,<br>
To batch-process images requires you to write a script. Most of the things you do to an image will appear in the workflow tab and you can use this to generate a script.</p>
<p>Your workflow will look something like this:</p>
<p>Correct colour vectors<br>
Annotate tissue area<br>
Run positive cell detection</p>
<p>There are plugins for all of these so it should be simple to script. Once you have done all of this on one image you can generate the script and run for project. Assuming all slides were stained and scanned at the same time the variables should work across all images.</p> ;;;; <p>Thanks, Konrad.</p>
<p>I give the filterpredictions another attempt with your recommendations tomorrow.</p>
<p>Thanks</p> ;;;; <p>Hi Fiona, thanks for replying. I am looking to count over the whole image.</p> ;;;; <p>Hi Paul,</p>
<p>Welcome to the forum!<br>
It sounds like you need to create a script for your process that can then be run across all the image in the project. Check out <a href="https://qupath.readthedocs.io/en/0.4/docs/scripting/index.html">workflows and scripts</a> section for an explanation of how to do this.</p>
<p>As for the initial annotation are you looking to have it:</p>
<ol>
<li>Over the whole image</li>
<li>Around the tissue</li>
<li>In a specific place the same across each image</li>
<li>A variable position and size for each image</li>
</ol> ;;;; <p>Hello,</p>
<p>I would like to create an update site for our lab, associated with a new user.</p>
<p>Site : ZG-Scripts<br>
User : Ghriann</p>
<p>Many thanks !</p> ;;;; <p>Hi <a class="mention" href="/u/rdornier">@Rdornier</a></p>
<p>This small fix you need is to use <code>rlong</code> instead of <code>rstring</code> for the <code>IDs</code> input:</p>
<pre><code class="lang-auto">description="Image ID.").ofType(rlong(0)),
</code></pre>
<p>You can also improve the usability of the script by returning the new Image so that users can find it:</p>
<pre><code class="lang-auto">from omero.rtypes import rstring, rlong, robject
...

def do_max_intensity_projection(conn, script_params):

  # return the image...
  return new_image

...

        image = do_max_intensity_projection(conn, script_params)
        client.setOutput("Message", rstring("Created Image:" + image.name))
        client.setOutput("Image", robject(image._obj))

</code></pre>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/5936c82a225f1111abc7caddf7268dba5c3b1717.png" alt="Screenshot 2023-03-23 at 12.24.30" data-base62-sha1="cJdRBgIoO8jRV0oI9TBTmxT2jeD" width="403" height="97"></p> ;;;; <p>I’m creating a macro to do batch processing using MOSAIC Particle Tracker but I haven’t been able to set the intensity threshold to absolute in the macro,  it always assumes that is the percentile.  I was wondering if anyone knew the correct command to  do so.<br>
Thank you for your time <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>It seems that the prediction in the h5 file have diffrent size than the groundtruth. Here for example the pixel error is about 3 pixels, but the values for predictions and groundtruth are very far off.</p>
<p>preditions :<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94473aef17240d374124162d9c0ab8286cfde32e.png" data-download-href="/uploads/short-url/l9Jh5ATZMDiOLsLrRCLve3os27Q.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94473aef17240d374124162d9c0ab8286cfde32e.png" alt="image" data-base62-sha1="l9Jh5ATZMDiOLsLrRCLve3os27Q" width="690" height="331" data-dominant-color="E8E9EA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">770×370 18.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
groundtruth :<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f075f59332002060b5e55f38bd88399f072f2848.png" data-download-href="/uploads/short-url/yjdjO8jeDwGX8pmfkHjaIDRXXnO.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f075f59332002060b5e55f38bd88399f072f2848.png" alt="image" data-base62-sha1="yjdjO8jeDwGX8pmfkHjaIDRXXnO" width="690" height="316" data-dominant-color="EBEBEB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">771×354 11 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Selection in MorphoGraphX is done by rendering all the triangles off screen with unique colors without shading, and then querying what color is under the mouse pointer when you click.</p>
<p>If selection gives random results there are a couple possibilities.</p>
<ol>
<li>
<p>The mesh is too large. If the number of faces is greater than 2^24 (8bit rgb), then some triangles will get the same color. The number of triangles will be roughly 2x the number of vertices.</p>
</li>
<li>
<p>Multisampling. If the graphics driver has multisampling turned on, or anything else that could affect the unique color, it could cause problems.</p>
</li>
</ol>
<p>If it is 1) I would expect the problem to occur on any machine, although there is no guarantee you would have the problem with the same cells. If it is 2) then all meshes on that machine would be affected, and potentially all selection tools, although again there is no guarantee that it would manifest the same way for different meshes/tools.</p> ;;;; <p>It can be corrupted data, wrong tensorflow version, optimizer problem etc.</p>
<p>Best thing to do would be to run a small project of your own, with your own data. Especially if you’re using  data annotated automatically with the testscript.</p> ;;;; <p>I am looking to run a simple positive cells selection on 1000s of DAB-stained images in Qupath. I have figured out how to do one image but cannot seem to apply this to batch processing. I would like to count the entire image but can’t seem to do so without it asking for an annotation/area, and then I cant seem to apply the same annotation automatically across multiple images.</p>
<p>Anyone have any ideas?</p> ;;;; <pre><code class="lang-auto">**Note:** 
This step will only work if the links to the new videos are included in the config.yaml file (see
permission issues [here](https://github.com/DeepLabCut/DeepLabCut/issues/1181) and [here]
(https://stackoverflow.com/a/65504258)). Otherwise, you will just duplicate the previous training
dataset, and spend several days training a new model that turns out to be exactly the same as 
the old one. It happened to a friend of mine.
</code></pre>
<p>When you <code>extract_outlier_frames</code> the video from which you extracted the frames is added to the <code>config.yaml</code> (won’t happen if you’re not running the terminal as administrator - which you always should be doing).</p>
<p>After merging, a new dataset is created with new iteration. It’s not a huge issue if you train from scratch - the model would get better, but yes it makes more sense to train from a snapshot since you don’t have to train for as many iterations.</p>
<p>The actual issue in your case was more likely that the iteration was never updated (since you say that you had to remove the <code>evaluation-results</code>) - hence you actually were constantly training without using the refined data. To avoid issues like this you can follow the workflow as explained in the DLC cookbook: <a href="https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html" class="inline-onebox">DeepLabCut User Guide (for single animal projects) — DeepLabCut</a></p> ;;;; <p>Hi Mark</p>
<p>I would like to request an update site for a FIJI plugin we use for FRAP analysis.</p>
<p>Name: FRAP_kovacs<br>
User: Domonkos_NH</p>
<p>Thanks,<br>
Domonkos</p> ;;;; <p>Are you using only 3 bodyparts per animal? Using more would definitely improve performance. Also, are you potentially running with <code>identity_only</code> set to <code>True</code>?</p> ;;;; <p>With 120 fps you can make the window larger, even 9 or 11 - just remove the file with filtered predictions before you run it again.</p>
<p>The only thing that comes to mind is that the video you’re using for testing has been almost fully used as a training dataset</p> ;;;; <p>Did you add any of your own labeled frames to this training dataset?</p> ;;;; <p>Could you follow those steps copy pasting the commands:</p>
<ol>
<li>Open Anaconda Prompt as administrator</li>
<li><code>cd C:\</code></li>
<li><code>mkdir repos</code></li>
<li><code>git clone https://github.com/DeepLabCut/DeepLabCut.git</code></li>
<li><code>cd DeepLabCut\conda-environments</code></li>
<li>Go to <code>C:\repos\DeepLabCut\conda-environments</code> in your file explorer, open the <code>DEEPLABCUT.yaml</code> file and modify it so it looks like this and save:</li>
</ol>
<pre><code class="lang-auto"># DEEPLABCUT.yaml

#DeepLabCut2.0 Toolbox (deeplabcut.org)
#© A. &amp; M.W. Mathis Labs
#https://github.com/DeepLabCut/DeepLabCut
#Please see AUTHORS for contributors.

#https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#Licensed under GNU Lesser General Public License v3.0
#
# DeepLabCut environment
# FIRST: INSTALL CORRECT DRIVER for GPU, see https://stackoverflow.com/questions/30820513/what-is-the-correct-version-of-cuda-for-my-nvidia-driver/30820690
#
# install: conda env create -f DEEPLABCUT.yaml
# update:  conda env update -f DEEPLABCUT.yaml
name: DLC
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.8
  - pip
  - ipython
  - jupyter
  - nb_conda
  - ffmpeg
  - pip:
    - deeplabcut[gui,tf]
</code></pre>
<ol start="7">
<li>Back in the terminal run <code>conda env create -f DEEPLABCUT.yaml</code>
</li>
<li><code>activate DLC</code></li>
<li><code>conda install -c conda-forge cudnn</code></li>
<li>cd <code>C:\repos\DeepLabCut\examples</code>
</li>
<li><code>python testscript_multianimal.py</code></li>
</ol>
<p>Let me know if everything runs correctly. If you don’t have <code>git</code> in your base env just run <code>conda install git</code> before step 4</p> ;;;; <p>After looking at this problem some more I realize that part of the problem is that when you decide to save the images as a stack you loose almost all the metadata. But if you instead choose to save as individual images you get a ton of metadata both embedded in each image as well as saved as a separate txt file.</p>
<p>I guess a possible workaround is to make a script that uses MDA to record individual images, and then have ImageJ post-hoc in the same script create an image stack and save the metadata to a results table.</p> ;;;; <p>Yes, i am as administrator!</p>
<p>I would like that works with me too.</p>
<p>I am starting to think to remove windows and start from the beginning in ubuntu!</p> ;;;; <p>When you’re creating the environment, are you running the terminal as administrator?</p>
<p>I tried installing from scratch to reproduce this, but it just works no matter the way I install, cloning the repo, installing from the repo, installing via pip, building from scratch, I just don’t get any errors</p> ;;;; <blockquote>
<p>If you want I can start a github issue and give you a sample .ply file.</p>
</blockquote>
<p>yes sure!</p> ;;;; <p>I thank you very much for trying to help me.<br>
I think I have found what was wrong thanks to Guillermo Hidalgo Gadea website <a href="https://guillermohidalgogadea.com/openlabnotebook/refining-your-dlc-model/" class="inline-onebox" rel="noopener nofollow ugc">Refining your DeepLabCut Model – Learning from Mistakes | Guillermo Hidalgo Gadea</a><br>
I was concerned by “Step 4: Learning from Mistakes”. In fact I kept using pretrained resnet_50n instead of using my own pretrained model so my analysis started from the beginning each time.<br>
Now that I have changed the “init_weights” in the “pose_cfg.yaml” like Guillermo Hidalgo Gadea explains,  I have noticed improvement in my refinement.</p> ;;;; <p>By default there is no rescaling happening, so the evaluation is the model’s performance on the 1:1 scale with the labeled data.</p> ;;;; <p>Yes! In SNT, Sholl profiles are obtained through dedicated parsers for images, SWC files, or preexisting tabular data. You can use a TabularParser to directly access the CSV data, as you would do using pandas. Here is an example:</p>
<pre><code class="lang-python">from sc.fiji.snt.analysis.sholl.parsers import TabularParser

# Documentation Resources: https://imagej.net/plugins/snt/scripting
# Latest SNT API: https://javadoc.scijava.org/SNT/

table_path = "/Users/gabriellafricklas1/Downloads/testSholl.csv"
parser = TabularParser(table_path, 'Radius', 'Inters.') # file path, radius column heading, counts column heading (case sensitive)
profile = parser.getProfile() # access the Sholl profile object of (r, inters.) pairs. An exception is thrown if no valid profile exists (e.g., if column headings were invalid)
intersections = profile.counts() # access intersection counts directly

vector = list()
for i in range(1, len(intersections)):
    newVal = (intersections[i] - intersections[i - 1]) * i
    if newVal &lt; 0:
        vector.append(0)
    else:
        vector.append(newVal)
BI = sum(vector)
print('BI', BI)
</code></pre>
<p>I’m surprised we don’t include that branching index’ in the default pool of metrics. You could use Fiji’s script-editor autocompletion (<kbd>Ctrl</kbd> + <kbd>Space</kbd>) to check which <a href="https://javadoc.scijava.org/SNT/index.html?sc/fiji/snt/analysis/sholl/math/package-summary.html">methods</a> are available:</p>
<pre><code class="lang-python">from sc.fiji.snt.analysis.sholl.math import LinearProfileStats 
stats = LinearProfileStats(profile)
stats. # Press Ctrl+Space here
</code></pre> ;;;; <p>On its own I didn’t think it would, but grayscale inverted if the background is sufficiently average, should work nicely as an augmentation method - I don’t think it’s part of imgaug or tensorpack though, so there would be some actual augmented data creation before creating a training dataset</p> ;;;; <h3>
<a name="sample-image-andor-code-1" class="anchor" href="#sample-image-andor-code-1"></a>Sample image and/or code</h3>
<p>A confocal stack was segmented in 3D. A mesh was created. When cells are selected using the “select connected area tool” the wrong cell is often selected instead. It is not a neighbour cell.</p>
<h3>
<a name="analysis-goals-2" class="anchor" href="#analysis-goals-2"></a>Analysis goals</h3>
<p>Accurate segmentation</p>
<h3>
<a name="challenges-3" class="anchor" href="#challenges-3"></a>Challenges</h3>
<p>We have tried 3 different machines and multiple different versions of MGX. The bug exists on all new machines purchased in the last year. Only some cells are affected, these cells are always affected. They can be selected if we use an old laptop. We tried with and without CUDA, the windows and linux version of MGX.</p> ;;;; <p>Hello <a class="mention" href="/u/will-moore">@will-moore</a>,</p>
<p>I wanted to test the script you’ve created to make a Maximum intensity projection of an image.<br>
I rewrite it a bit to be able to upload it on our server but I get stuck with this error that always happen.<br>
<a class="attachment" href="/uploads/short-url/edSmE56P7wevUSj8YFdbA1yZ3MU.zip">mip.zip</a> (1.6 KB)</p>
<p>Do you have an idea on where it is coming from ?<br>
Thank you,</p>
<p>Rémy.</p>
<pre><code class="lang-auto">WARNING:omero.gateway:ValueError on &lt;class 'omero.gateway.OmeroGatewaySafeCallWrapper'&gt; to &lt;bd82e55b-8a75-4979-acfb-742b82fed7feomero.api.IPixels&gt; copyAndResizeImage(('37505', object #0 (::omero::RInt)
{
    _val = 2912
}, object #0 (::omero::RInt)
{
    _val = 2912
}, object #0 (::omero::RInt)
{
    _val = 1
}, object #0 (::omero::RInt)
{
    _val = 1
}, range(0, 1), None, False, &lt;ServiceOptsDict: {'omero.client.uuid': 'bd82e55b-8a75-4979-acfb-742b82fed7fe', 'omero.session.uuid': '55f7839e-9caf-4aac-9497-facd9beac5ac', 'omero.group': '253'}&gt;), {})
Traceback (most recent call last):
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 4856, in __call__
    return self.f(*args, **kwargs)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero_api_IPixels_ice.py", line 893, in copyAndResizeImage
    return _M_omero.api.IPixels._op_copyAndResizeImage.invoke(self, ((imageId, sizeX, sizeY, sizeZ, sizeT, channelList, methodology, copyStats), _ctx))
ValueError: invalid value for argument 1 in operation `copyAndResizeImage'
ERROR:omero.gateway:Failed to setPlane() on rawPixelsStore while creating Image
Traceback (most recent call last):
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 3864, in createImageFromNumpySeq
    image, dtype = createImage(plane, channelList)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 3790, in createImage
    rint(sizeT), channelList, None, False, self.SERVICE_OPTS)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 4859, in __call__
    return self.handle_exception(e, *args, **kwargs)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 4856, in __call__
    return self.f(*args, **kwargs)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero_api_IPixels_ice.py", line 893, in copyAndResizeImage
    return _M_omero.api.IPixels._op_copyAndResizeImage.invoke(self, ((imageId, sizeX, sizeY, sizeZ, sizeT, channelList, methodology, copyStats), _ctx))
ValueError: invalid value for argument 1 in operation `copyAndResizeImage'
Traceback (most recent call last):
  File "./script", line 113, in &lt;module&gt;
    run_script()
  File "./script", line 105, in run_script
    message = do_max_intensity_projection(conn, script_params)
  File "./script", line 65, in do_max_intensity_projection
    sourceImageId=image_id, channelList=clist, dataset=dataset)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 3893, in createImageFromNumpySeq
    raise exc
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 3864, in createImageFromNumpySeq
    image, dtype = createImage(plane, channelList)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 3790, in createImage
    rint(sizeT), channelList, None, False, self.SERVICE_OPTS)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 4859, in __call__
    return self.handle_exception(e, *args, **kwargs)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py", line 4856, in __call__
    return self.f(*args, **kwargs)
  File "/opt/omero/server/venv3/lib64/python3.6/site-packages/omero_api_IPixels_ice.py", line 893, in copyAndResizeImage
    return _M_omero.api.IPixels._op_copyAndResizeImage.invoke(self, ((imageId, sizeX, sizeY, sizeZ, sizeT, channelList, methodology, copyStats), _ctx))
ValueError: invalid value for argument 1 in operation `copyAndResizeImage'
</code></pre> ;;;; <p>Hi,</p>
<p>I am having issues clustering cell sub-populations using Cytomap. I have IHC images which I segment using Histocat and then carry out the spatial analysis in Cytomap. For my analysis, I am trying to identify different CD8+ T cell subsets in the lymph node. Some subsets (e.g. CXCR5+ CD8+ T cells) are rarely found in my IHC images, and when I run the cell clustering in cytomap this and other rare subsets are being underrepresented when I compare the cytomap output to the original IHC image.</p>
<p>I have tried running the clustering repeatedly using different normalisation and clustering parameters. Normally I first cluster my cells into either CD8+ and CD8- cells as a means of excluding unwanted cell populations. This first step is able to accurately detect all CD8+ T cells. I then proceed to cluster the CD8+ T cells into different sub-populations. However, the clustering detects fewer CD8+ T cell subsets than are visibly present in the original IHC image.</p>
<p>Is there a way I can improve the clustering so that the cell detection in Cytomap is representative of the original IHC staining. I am trying to ensure my analysis is accurate and robust enough for me to submit my work for publication.</p>
<p>Thanks,</p>
<p>Leonard</p> ;;;; <p>By the way, is there a solution available for the issue with deeplabcut not being able to label all body parts, as shown in the figure?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997.png" data-download-href="/uploads/short-url/pXLX9Q3jQi7ZVTlZcJHXEqO7DV5.png?dl=1" title="ID FULL P2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_254x500.png" alt="ID FULL P2" data-base62-sha1="pXLX9Q3jQi7ZVTlZcJHXEqO7DV5" width="254" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_254x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_381x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_508x1000.png 2x" data-dominant-color="A2A488"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">ID FULL P2</span><span class="informations">508×1000 257 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Also, despite having over 1k frames of training data, the results from shuffle 1 to 5 have not shown significant improvement. Since shuffle 2, I have manually selected frames with unusual body states.</p> ;;;; <p>Hi, I would like to download Cellpose plugin and try it for my images.<br>
However when I click on the download link it refers me to Github repository.<br>
I am not into programming so much, I would like to have this very basic question that how I can add it to my cell profiler?<br>
thanks a lot</p> ;;;; <p>That’s so helpful thank you <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> One more thing if you can help me ! I was trying to recreate the training and test errors in .csv file  but the prediction seems to be rescaled. I tried to rescale the groundtruth using the ‘gobal_scale’ attribute in pose_conf.yaml but it seems not to be enough as i get much higher error. How do i get the predictions and the groundtruth to the same scale ?</p> ;;;; <p>Hi Wayne,</p>
<p>Here is a RoiSet generated for a 300/512/200 frames/x/y time series.<br>
Thanks a lot for looking into this.<br>
<a class="attachment" href="/uploads/short-url/zibvDjbAKlCruC5Vpsjn18bSSCw.zip">Maro_RoiSet.zip</a> (7.8 MB)</p>
<p>Maro</p> ;;;; <p>Thank you for your response. Yes, it is possible to save only one layer.</p>
<p>I have also come across a point of confusion. After analyzing the video, the video obtained from using “analyze videos” seems to be different from the video obtained from using “create videos”. I am unsure if I have missed any parameters.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/a/9a403698426f973d612b66865081a75bc8906f98.png" data-download-href="/uploads/short-url/m0z6CfLY2vjgEr7rlO1jLCsDVfa.png?dl=1" title="ID FULL P1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_254x500.png" alt="ID FULL P1" data-base62-sha1="m0z6CfLY2vjgEr7rlO1jLCsDVfa" width="254" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_254x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_381x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_508x1000.png 2x" data-dominant-color="A2A488"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">ID FULL P1</span><span class="informations">508×1000 188 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Furthermore, the first ten frames of the .csv file obtained through “analyze videos” are blank, just like the video obtained in “create videos”.</p> ;;;; <p><a class="mention" href="/u/nicokiaru">@NicoKiaru</a></p>
<p>I see that in BIOP Operetta Importer plugin, in the GUI there are now options to flip images horizontally, vertically or both. Many many thanks for these!  I seem to need the vertical flip for our data from the Opera Phenix.<br>
My question is can these options be used in the groovy scripts?  If so, how?  I tried to look at the javadoc of OperettaManager, but I’m getting a 404. I don’t know any groovy or java scripting so a short example would be super helpful.</p>
<p>Many thanks for any help.<br>
Huw</p> ;;;; <p>Hey <a class="mention" href="/u/vitalijvictor">@Vitalijvictor</a></p>
<p>Really sorry I missed your post here. I personally only do these kind of experiments in Groovy. However, if you check the Github page for creating extensions: <a href="https://github.com/qupath/qupath/wiki/Creating-extensions" class="inline-onebox" rel="noopener nofollow ugc">Creating extensions · qupath/qupath Wiki · GitHub</a> – You will see:</p>
<pre><code class="lang-auto">	public void installExtension(QuPathGUI qupath) {
	}

</code></pre>
<p>which I suspect is where you would add your tab initialisation. And since my Groovy code is modelled after things happening in the QuPath GUI JAVA code, I’m pretty sure it’s more or less directly translatable back to JAVA.</p>
<p>That would actually be a fun experiment to try. If you get there first, would you mind posting a minimal extension code? Otherwise I’ll try to get to it when I can, but I can’t make promises right now, sorry.</p>
<p>Kind regards,<br>
Egor</p> ;;;; <p>Hi,</p>
<p>I’m trying to record some metadata for a multidimensional aquisition to an IJ results table. I have created a beanshell script by hacking together various code I have found online, but it is not working. Can someone please help me figure out what I am doing wrong? Here is my script so far:</p>
<pre><code class="lang-auto">// Load the Micro-Manager core
import org.micromanager.MMStudio;
import org.micromanager.acquisition.Acquisition;
import org.micromanager.acquisition.MDA;
import org.micromanager.utils.ReportingUtils;
import ij.measure.ResultsTable;

// Create an instance of the Micro-Manager core
MMStudio mm = MMStudio.getInstance();

// Set up the acquisition settings
MDA mda = new MDA();
mda.numFrames = 25;
mda.numChannels = 1;
mda.numSlices = 1;
mda.intervalMs = 0;
mda.save = true;
mda.rootName = "my_image_stack_";
mda.channelNames = new String[] {"Channel1"};

// Create a results table to store the time tag data
ResultsTable rt = new ResultsTable();
rt.setPrecision(0);

// Start the acquisition
mm.acquisitions().runAcquisition(mda, true);

// Loop through each frame and add the time tag to the results table
for (int i = 0; i &lt; mda.numFrames; i++) {
    double timeTag = mm.acquisitions().getTimeStamp(i);
    rt.incrementCounter();
    rt.addValue("Frame", i+1);
    rt.addValue("TimeTag", timeTag);
}

// Show the results table
rt.show("Time Tags");

// Report the completion of the acquisition
ReportingUtils.showMessage("Acquisition complete!");
</code></pre> ;;;; <aside class="quote no-group" data-username="mmongy" data-post="1" data-topic="78910">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/51bf81/40.png" class="avatar"> Marc Mongy:</div>
<blockquote>
<p>What is the purpose of the “guest” user?</p>
</blockquote>
</aside>
<p>The guest user is used internally to load non-sensitive information. It has special handling (and restrictions) so I wouldn’t suggest using it as the public user. i.e. this is likely your root problem.</p>
<p>Instead, create a new user (named “public”, “PUBLIC”, etc.) and configure it for public access.</p>
<aside class="quote no-group" data-username="mmongy" data-post="1" data-topic="78910">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/51bf81/40.png" class="avatar"> Marc Mongy:</div>
<blockquote>
<p>How do enable the “default” group?</p>
</blockquote>
</aside>
<p>I’m not exactly sure what you are referring to, but two possibilities:</p>
<ul>
<li>Each user has a <em>default</em> group, but it need not be named “default” and is whatever group they are a member of first.</li>
<li>The “default” group I know of is related to LDAP based login. When a user gets created via LDAP, a choice must be made of which group to put them in, and one of the mechanisms by default puts them in the “default” group which is automatically created.</li>
</ul>
<p>~Josh</p> ;;;; <p>Hi Jan</p>
<p>Thank you for your suggestions.</p>
<p>I added the correct image dimensions in image properties as you pointed out. I am not sure why they were not recorded properly in imagej.</p>
<p>I installed BioVoxxel but when i try to run “Voroni Threshold”, I get the following error.</p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Linux 4.15.0-109-generic; 473MB of 42199MB (1%)</p>
<p>java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception<br>
at net.imagej.legacy.LegacyService.runLegacyCompatibleCommand(LegacyService.java:308)<br>
at net.imagej.legacy.DefaultLegacyHooks.interceptRunPlugIn(DefaultLegacyHooks.java:166)<br>
at ij.IJ.runPlugIn(IJ.java)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at ij.IJ.run(IJ.java:319)<br>
at ij.IJ.run(IJ.java:330)<br>
at ij.macro.Functions.doRun(Functions.java:703)<br>
at ij.macro.Functions.doFunction(Functions.java:99)<br>
at ij.macro.Interpreter.doStatement(Interpreter.java:281)<br>
at ij.macro.Interpreter.doStatements(Interpreter.java:267)<br>
at ij.macro.Interpreter.run(Interpreter.java:163)<br>
at ij.macro.Interpreter.run(Interpreter.java:93)<br>
at ij.macro.Interpreter.run(Interpreter.java:107)<br>
at ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)<br>
at ij.IJ.runMacro(IJ.java:158)<br>
at ij.IJ.runMacro(IJ.java:147)<br>
at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)<br>
at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)<br>
at net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)<br>
at net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)<br>
at net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)<br>
at org.scijava.script.ScriptModule.run(ScriptModule.java:164)<br>
at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)<br>
at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>
at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)<br>
at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)<br>
at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>
at java.lang.Thread.run(Thread.java:750)<br>
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception<br>
at java.util.concurrent.FutureTask.report(FutureTask.java:122)<br>
at java.util.concurrent.FutureTask.get(FutureTask.java:192)<br>
at net.imagej.legacy.LegacyService.runLegacyCompatibleCommand(LegacyService.java:304)<br>
… 30 more<br>
Caused by: java.lang.RuntimeException: Module threw exception<br>
at org.scijava.module.ModuleRunner.call(ModuleRunner.java:127)<br>
… 6 more<br>
Caused by: java.lang.NullPointerException<br>
at de.biovoxxel.bv3dbox.gui.BV_VoronoiThresholdLabelingGUI.cancel(BV_VoronoiThresholdLabelingGUI.java:409)<br>
at org.scijava.module.ModuleRunner.cleanupAndBroadcastCancelation(ModuleRunner.java:185)<br>
at org.scijava.module.ModuleRunner.run(ModuleRunner.java:157)<br>
at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>
… 6 more</p>
<p>I tried with different imagej versions, but the error still persists. I also cannot use any of the other functions from the biovoxxel toolbar from the plugin menu.</p>
<p>I have not put up your suggestion or the error on the list, wasn’t sure how to, but we can now correspond on the old list.</p>
<p>Sincerely<br>
Saurabh</p> ;;;; <p>Hi there,</p>
<blockquote>
<p>Can I script making the ID of the annotation the path class name?</p>
</blockquote>
<p>We can certainly try that, it’s one line to change in the <code>rings.eachWithIndex</code> loop.</p>
<p><em>However!</em> All the NDPView generated NDPA files I’ve checked have <code>&lt;ndpviewstate id="some_numerical_value"&gt;</code>  as an <em>unique numerical value</em>, which increases for each annotation. Are you absolutely sure HALO requires this to be changed to be the PathClass?</p>
<p>No harm done in trying I guess, so in my <code>ndp_write()</code> function, find this loop:</p>
<pre><code class="lang-auto">rings.eachWithIndex { ring, index -&gt;
                def annot = [:]
                if (index == 0) {
                    annot['title'] = pathObject.getName()
                    annot['details'] = pathObject.getPathClass()
                    annot['color'] = '#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)
                    isFirst = false
                } else {
                    annot['title'] = "clear"
                    annot['details'] = "clear"
                    annot['color'] = '#000000'
                }

                annot['id'] = ++ndpIndex
</code></pre>
<p>and change the line</p>
<ul>
<li><code>annot['id'] = ++ndpIndex</code></li>
</ul>
<p>to read</p>
<ul>
<li><code>annot['id'] = pathObject.getPathClass()</code></li>
</ul>
<p>I cannot test this at my end and strongly suspect the NDPA files would broken in NDPView. But do try and let me know how you get on!</p>
<p><strong>Thinking about it,</strong> another thing you can try first is to simply swap which of the <code>pathName</code> or <code>pathClass</code> go in ‘title’ and ‘details’.</p>
<p>This is worth trying (same <code>rings.eachWithIndex</code> loop talked about before but <code>pathClass</code> now goes into the annotation title instead of the <code>pathName</code>):</p>
<pre><code class="lang-auto">            rings.eachWithIndex { ring, index -&gt;
                def annot = [:]
                if (index == 0) {
                    annot['title'] = pathObject.getPathClass()
                    annot['details'] = pathObject.getName()
                    annot['color'] = '#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)
                    isFirst = false
                } else {
</code></pre>
<p>Cheers,<br>
Egor</p> ;;;; <p>I appreciate that this question is solved, but you may also find the <a href="https://imagej.nih.gov/ij/developer/macro/functions.html#replace" rel="noopener nofollow ugc"><code>replace</code> function</a> useful. Something like:</p>
<pre><code class="lang-auto">filePath="path/folder_mask/images_cropped/file.png") ;
print(filePath);
newPath=replace(filePath,"images_cropped","masks_cropped");
print(newPath);
</code></pre>
<p>The advantage here being that if your filenames in <code>images_cropped</code> and <code>masks_cropped</code> are the same, then you don’t have to rebuild the string based on a hardcoded length in <code>substring</code>.</p>
<p>Hope that helps!</p> ;;;; <p>Yes, keep the “pick key-value” option.</p> ;;;; <p>Hi Saurabh,</p>
<p>I would recommend to <a href="https://imagej.net/software/fiji/">install Fiji</a> and test the <a href="https://biovoxxel.github.io/bv3dbox/">BioVoxxel 3D Box</a>.</p>
<p>One thing I realized… could it be that your z-scaling (voxel size in z) is incorrect? It seems to be 312-times smaller than the x/y pixel scaling. If that is incorrectly recorded by your imaging device, any 3D segemtnation will be problematic.<br>
[image]</p>
<p>Furthermore, it is in cm which seems odd.</p>
<p>Here a macro which gave me the objects in 3D while not perfectly separated due to the scaling problem. You will first need to follow all the installation instructions on the pages above to b able to run this.</p>
<pre><code class="lang-auto">run("Enhance Contrast...", "saturated=0.01 normalize process_all use");
run("Voronoi Threshold Labler (2D/3D)", "filtermethod=Gaussian filterradius=3.0 backgroundsubtractionmethod=None backgroundradius=1.0 histogramusage=full thresholdmethod=Li fillholes=2D separationmethod=[Maxima Spheres] spotsigma=7.0 maximaradius=7.0 volumerange=0-Infinity excludeonedges=false outputtype=Labels stackslice=10 applyoncompleteimage=false processonthefly=false");
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150.jpeg" data-download-href="/uploads/short-url/y24U8k59pSHiYEdGON9DYtV2d7a.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_690x264.jpeg" alt="image" data-base62-sha1="y24U8k59pSHiYEdGON9DYtV2d7a" width="690" height="264" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_690x264.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_1035x396.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_1380x528.jpeg 2x" data-dominant-color="2F2B2A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1626×624 173 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Attached also a GIF showing the objects in 3D with a guessed z-dimension size to enable proper display.</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/1447fcd2e90d64aa54c27449da1ab618969e21af.gif" alt="VTL_vacuole_s1-100_original-1-1.tif-1" data-base62-sha1="2TpMusbqgmX566FEg9zHF4vFOFh" width="342" height="342" class="animated"></p>
<p>Hope this get’s you kick-started.</p> ;;;; <p>Dear All</p>
<p>I am trying to segment objects (vacuoles) fluorescently marked at their periphery and imaged as Z-stacks.</p>
<p>I’m currently doing this slice-by-slice, but this yields poor results (there is a lot of noise within slices + slice-to-slice variability).</p>
<p>// Example processing for one slice<br>
run(“Enhance Contrast”, “saturated=0.35”);<br>
run(“Invert”);<br>
run(“Subtract Background…”, “rolling=1000 light sliding”);<br>
makeRectangle(0, 0, 2048, 2048);<br>
run(“Sharpen”);<br>
run(“Gaussian Blur…”, “sigma=0.7”);<br>
run(“Enhance Contrast…”, “saturated=0.20 normalize”);<br>
run(“8-bit”);<br>
run(“Auto Local Threshold”, “method=Phansalkar radius=15 parameter_1=0.08 parameter_2=0 white”);<br>
run(“Invert”);</p>
<p>I’m wondering about 3D segmentation packages / functions / protocols you would recommend to improve this, or even recommendations that could help me improve the 2D processing.</p>
<p>I copy below a link to the original data (green channel) + current segmentation (red channel) in a stack.</p>
<p>Cropped version (512 x 490): <a href="https://drive.google.com/file/d/1AlSyynf_GAQ_gfpOzhGjpXxQeWfsqtNv/view?usp=share_link" class="inline-onebox" rel="noopener nofollow ugc">composite_masks-red_original-green_crop.tif - Google Drive</a></p>
<p>Original stack (2k x 2k): <a href="https://drive.google.com/file/d/1Khge0NMWaPDbcWPpFwicb5oc_VzMZ2DU/view?usp=sharing" class="inline-onebox" rel="noopener nofollow ugc">vacuole_s1-100_original_1.tif - Google Drive</a></p>
<p>Thank you for your help and input</p>
<p>Sincerely,<br>
Saurabh<br>
PhD student,<br>
Weizmann Institute</p> ;;;; <p>Hello,<br>
thank you, that solved it <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>thank you very much for letting me know, I will keep your pointers in mind. appreciate the help!</p> ;;;; <p>Hi,</p>
<p>I have a quick question about cellpose. Can someone explain the difference between the lines :</p>
<p>.channels(“DAPI”, “CY3”)<br>
.cellposeChannels( channel1, channel2 )</p>
<p>And why should I use one instead of the other ? Or both ?<br>
I coulndt find the information on the forum, maybe I missed it, sorry!</p>
<p>Thx <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
Malo</p> ;;;; <p>Hello Will,</p>
<p>Thanks for your answer.</p>
<blockquote>
<p>It wasn’t expected that you’d have duplicate labels on a single panel.</p>
</blockquote>
<p>True</p>
<blockquote>
<p>E.g. if creating labels from Tags, it could skip creating a label if it exists already?</p>
</blockquote>
<p>This is a good idea, yes. Avoiding duplicate labels on the figure.</p>
<blockquote>
<p>I can see this gets more confusing if they are mixed with existing Tags, but if we simply avoid creating duplicates, will that fix most of the issues you’re seeing?</p>
</blockquote>
<p>Indeed</p>
<blockquote>
<p>Import all key-values → labels sounds like a useful feature that shouldn’t be too hard to do. Created <a href="https://github.com/ome/omero-figure/issues/501" rel="noopener nofollow ugc">Create Labels from ALL Key-Value pairs · Issue #501 · ome/omero-figure · GitHub</a></p>
</blockquote>
<p>Perfect. This option will be useful. I guess you will also keep the option to pick only key-value, right ?</p>
<blockquote>
<p>Added to <a href="https://github.com/ome/omero-figure/issues/486" rel="noopener nofollow ugc">Label markdown formatting does too much · Issue #486 · ome/omero-figure · GitHub</a></p>
</blockquote>
<p><img src="https://emoji.discourse-cdn.com/twitter/ok_hand.png?v=12" title=":ok_hand:" class="emoji only-emoji" alt=":ok_hand:" loading="lazy" width="20" height="20"></p> ;;;; <p>If you want I can start a github issue and give you a sample .ply file. I made the segmentations. I’m mainly curious what the color patterns are from. Everything works ok if I use the mesh.c() command with the associated meshes.</p> ;;;; <blockquote>
<p>I found a possible work-around that might help…<br>
If you select the bigger ROI and “Copy” it, then delete it and delete others behind it, then you can “Paste” to add it back.</p>
</blockquote>
<p>Thanks for the trick !</p> ;;;; <p>Interested in gaining valuable insights on how to choose the right imaging solution for your Medical / Life science application?</p>
<p>Join this webinar titled <strong>“Digital Imaging Solutions for Life Science Applications”</strong>.</p>
<p><a href="https://www.e-consystems.com/webinars/digital-imaging-solutions-for-life-science-applications.asp" rel="noopener nofollow ugc"><strong>REGISTER FOR THE WEBINAR »</strong></a></p>
<p>Tuesday, March 28, 2023, 10.00 am – 11.00 am PST</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Role of cameras in life science applications</li>
<li>5 key factors involved in choosing an imaging solution</li>
<li>Updates on e-con Systems’ latest camera solutions</li>
</ul>
<p>Don’t miss this exclusive opportunity to listen to thought leaders in the embedded vision space!</p>
<p><a href="https://www.e-consystems.com/webinars/digital-imaging-solutions-for-life-science-applications.asp" rel="noopener nofollow ugc"><strong>SAVE MY SEAT »</strong></a></p>
<p><em>Not available on March 28? Don’t worry - you can still register,</em> <em>as they will share the complete webinar recording with all registrants…</em></p> ;;;; <p>We haven’t worked that out yet <a class="mention" href="/u/jjfrackowiak">@jjfrackowiak</a>. You should keep using this for now. My guess is that at some point we’ll have a way to:</p>
<ul>
<li>remove all layer controls from the viewer</li>
<li>create arbitrary buttons (including whatever controls you want) in various widgets on the viewer</li>
</ul>
<p>and those two combined will give you similar functionality to this. But, that doesn’t exist yet so this is still the best answer, even despite the warning.</p> ;;;; <p>Thanks for the suggestion: I created an issue at <a href="https://github.com/ome/omero-figure/issues/503" class="inline-onebox">Scalebar line thickness · Issue #503 · ome/omero-figure · GitHub</a></p> ;;;; <p>I mean how to classify the cells into red signal and the red+green signal?</p> ;;;; <p>Hi k-dominik,</p>
<p>Thanks for your advice! I still have one more question. How can I distinguish the red signal from the red+green signal when doing object classification? What kind of object feature selection I should use?</p> ;;;; <p>Hi <a class="mention" href="/u/belen_benitez">@Belen_Benitez</a>,</p>
<p>Post the .txt file and I will try to write a macro that creates an ROI from the x,y coordinates in it.</p> ;;;; <p>Hi <a class="mention" href="/u/research_associate">@Research_Associate</a>,</p>
<p>Thank you very much for the explanation. I believe the error was due to something wrong in my QuPath or ImageJ build then. I don’t mind to do things manually if necessary. Maybe a clean install may help.</p> ;;;; <p>Hi <a class="mention" href="/u/ym.lim">@ym.lim</a> ,</p>
<p>I will for sure have a try. Once again thank you very much.</p> ;;;; <p>yes, the data array associated to the vertices is called “RGB”, but these are “direct” colors (not associated to a scalar mapping), probably the result of a segmentation.<br>
I’ll try to implement that in the <code>k3d</code> backend so the two renderings will match.</p> ;;;; <p>Hi <a class="mention" href="/u/tibuch">@tibuch</a>,<br>
Using MicroMetaApp to create the json file is indeed a good option. I’m pulling in <a class="mention" href="/u/caterina">@Caterina</a> and her team, the developers of MicroMetaApp, to provide further advice. I’m not sure whether they already have the full workflow to insert the json into an OME-Zarr but they can definitely give you more insight.<br>
Best,<br>
Damir</p> ;;;; <aside class="quote no-group" data-username="Mareike_Daubert" data-post="1" data-topic="78936">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mareike_daubert/40/67083_2.png" class="avatar"> Mareike Daubert:</div>
<blockquote>
<p>open ( "folder_mask + “masks_cropped/” + name + ". png “” ) ;</p>
</blockquote>
</aside>
<p>Despite the misleading error, it looks like it might actually be the quotes, not the parenthesis? Try:</p>
<pre><code class="lang-auto">open(folder_mask+"masks_cropped/"+name+".png"); 
</code></pre> ;;;; <p>The output image should have the same dimensions regardless of line width–it will just take the maximum value along the width. Could you share an example image that illustrates what you’re trying to track, and where you’re selecting your line?</p>
<p>For comparison, I opened the Mitosis sample image (File &gt; Open samples &gt; Mitosis (5D stack), and selected a line on the image (left image). When I set the width to 1, I got the kymograph at the top right (which is pretty dim since I didn’t place my line well), and when I set it to 15, I got the kymograph at the bottom right, which manages to capture more signal from the red dots which my thinner line didn’t. They looked similar straight out of the plugin because they auto-contrasted, but when set to display the same values, you can see the max-projected one (width=15) is much brighter.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/694c42f7b41f48a9b470df5ade5c0fb60789ef22.png" alt="image" data-base62-sha1="f1vwuEQEgfjpMYHLttF8FwKh6a6" width="361" height="325"><br>
You should have this example image too if you’d like to test the plugin on this.</p> ;;;; <p>Hi Konrad,</p>
<p>I tried to execute filterpredictions but because I’ve previously done it using the default settings: median filtertype and windowlength = 5 it won’t let me do it again</p>
<p>It says it is skipping’ the command, but what you suggested is the default setting anyway.</p>
<p>A filtered CSV was created and when comparing with the analyze_video CSV file you can see the coordinates have been smoothed in the filtered file (especially first frame)</p>
<p>Any other suggestions why I’m getting likelihood scores of 1 for all body parts and frames?</p>
<p>Thanks <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Appreciate the help.</p>
<p>Neither adjustment seems to change the fact that the “_BCadj” tiff that is saved, which is supposed to reflect the brightness changes requested, is identical to the raw tiff. The saved montage continues to correctly reflect the brightness changes.</p> ;;;; <p>Hello,<br>
I have a .txt file with x;y coordinates. I need these coordinates to be captured in a binary image that I have. My question is how can I create a ROI with these coordinates and then select it to place these points on the image (using ImageJ).<br>
I hope you can help me.</p> ;;;; <p>Main resources aside from the primary readthedocs page and all of the forum post examples are probably<br>
Pete’s gists as examples: <a href="https://gist.github.com/petebankhead" class="inline-onebox">petebankhead’s gists · GitHub</a><br>
The javadocs which I think are linked through the scripting interface now <a href="https://qupath.github.io/javadoc/docs/" class="inline-onebox">Overview (QuPath 0.4.0)</a><br>
And a kind of guide/example thing here <a href="https://www.imagescientist.com/image-analysis#scripting" class="inline-onebox">Image Analysis Topics — Image Scientist</a></p>
<p>Also some examples of older code here, but most of them are not as relevant these days <a href="https://gist.github.com/Svidro" class="inline-onebox">Svidro’s gists · GitHub</a></p> ;;;; <p>I’ve actually had that problem in the past and taken similar approaches (waits, while loops etc).</p>
<p>I tried that here with a wait of over 30 seconds (when the ilastik classification takes &lt; 5 when not in batch mode or run from command line) to no avail. I didn’t bother with a while loop as I figured it would just get into an infitite loop but I’ll give it a shot today just to see what happens.</p>
<p>I realsied I neglected to answer your other question, the only time it works (and works consistently I might add) is when I run the macro from an already open instance of FIJI and with batch mode set to false.</p> ;;;; <p>It looks like the saved training snapshot was corrupted, so once I retrained I did not get the error and I was able to analyze new videos! However, the performance is not very good. I actually got better performance from a much smaller training set so I was hoping the MPII dataset would be better since it is much larger. I have attached a sample video I have analyzed using the MPII dataset. Do you have any suggestions regarding improvement of performance? Thank you very much.<br>
</p><div class="video-container">
    <video width="100%" height="100%" preload="metadata" controls="">
      <source src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a18b2c44f26bb0a35f21f2544f20fea5f761a65.mp4">
      <a href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a18b2c44f26bb0a35f21f2544f20fea5f761a65.mp4" rel="noopener nofollow ugc">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a18b2c44f26bb0a35f21f2544f20fea5f761a65.mp4</a>
    </source></video>
  </div><p></p> ;;;; <p>I am trying to run the reaching-mackenzie example code given in deeplabcut repo. It worked fine for 10000 iterations, then changed the number of iterations to 300000 and tried rerunning it. The code I used is as follows :</p>
<pre><code class="lang-auto">path_config_file = r'D:\000_DeepLabCut\D1\DeepLabCut\examples\Reaching-Mackenzie-2018-08-30\config.yaml'
deeplabcut.create_training_dataset(path_config_file,Shuffles=[1])

# initiate training
deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)
</code></pre>
<p>Training continued for several thousand iterations but then got interrupted throwing error as follows :</p>
<pre><code class="lang-auto">InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values
	 [[{{node train_op/CheckNumerics}}]]

During handling of the above exception, another exception occurred:
</code></pre>
<p>And further also mentioned details as :</p>
<pre><code class="lang-auto">
InvalidArgumentError: Graph execution error:

Detected at node 'train_op/CheckNumerics' defined at (most recent call last):
</code></pre>
<p>I am adding an image of the complete error with this for further details. What I found confusing is that, it worked perfectly fine for the first time when I run it with 10000 iterations but then threw errors when I tried to rerun the same jupyter notebook.</p>
<p>I am adding complete error output below  :</p>
<pre><code class="lang-auto">&gt; --------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:1378, in BaseSession._do_call(self, fn, *args)
   1377 try:
-&gt; 1378   return fn(*args)
   1379 except errors.OpError as e:

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:1361, in BaseSession._do_run.&lt;locals&gt;._run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1360 self._extend_graph()
-&gt; 1361 return self._call_tf_sessionrun(options, feed_dict, fetch_list,
   1362                                 target_list, run_metadata)

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:1454, in BaseSession._call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1452 def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,
   1453                         run_metadata):
-&gt; 1454   return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
   1455                                           fetch_list, target_list,
   1456                                           run_metadata)

InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values
	 [[{{node train_op/CheckNumerics}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
Cell In[2], line 9
      4 deeplabcut.create_training_dataset(path_config_file,Shuffles=[1])
      6 # initiates training. 
      7 # you may edit the number of iterations using 'maxiters' parameter
----&gt; 9 deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\training.py:223, in train_network(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)
    212         train(
    213             str(poseconfigfile),
    214             displayiters,
   (...)
    219             allow_growth=allow_growth,
    220         )  # pass on path and file name for pose_cfg.yaml!
    222 except BaseException as e:
--&gt; 223     raise e
    224 finally:
    225     os.chdir(str(start_path))

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\training.py:212, in train_network(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)
    209         from deeplabcut.pose_estimation_tensorflow.core.train import train
    211         print("Selecting single-animal trainer")
--&gt; 212         train(
    213             str(poseconfigfile),
    214             displayiters,
    215             saveiters,
    216             maxiters,
    217             max_to_keep=max_snapshots_to_keep,
    218             keepdeconvweights=keepdeconvweights,
    219             allow_growth=allow_growth,
    220         )  # pass on path and file name for pose_cfg.yaml!
    222 except BaseException as e:
    223     raise e

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\core\train.py:283, in train(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)
    280     current_lr = lr_gen.get_lr(it - start_iter)
    281     lr_dict = {learning_rate: current_lr}
--&gt; 283 [_, loss_val, summary] = sess.run(
    284     [train_op, total_loss, merged_summaries], feed_dict=lr_dict
    285 )
    286 cum_loss += loss_val
    287 train_writer.add_summary(summary, it)

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:968, in BaseSession.run(self, fetches, feed_dict, options, run_metadata)
    965 run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None
    967 try:
--&gt; 968   result = self._run(None, fetches, feed_dict, options_ptr,
    969                      run_metadata_ptr)
    970   if run_metadata:
    971     proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:1191, in BaseSession._run(self, handle, fetches, feed_dict, options, run_metadata)
   1188 # We only want to really perform the run if fetches or targets are provided,
   1189 # or if the call is a partial run that specifies feeds.
   1190 if final_fetches or final_targets or (handle and feed_dict_tensor):
-&gt; 1191   results = self._do_run(handle, final_targets, final_fetches,
   1192                          feed_dict_tensor, options, run_metadata)
   1193 else:
   1194   results = []

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:1371, in BaseSession._do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1368   return self._call_tf_sessionprun(handle, feed_dict, fetch_list)
   1370 if handle is None:
-&gt; 1371   return self._do_call(_run_fn, feeds, fetches, targets, options,
   1372                        run_metadata)
   1373 else:
   1374   return self._do_call(_prun_fn, handle, feeds, fetches)

File ~\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\client\session.py:1397, in BaseSession._do_call(self, fn, *args)
   1392 if 'only supports NHWC tensor format' in message:
   1393   message += ('\nA possible workaround: Try disabling Grappler optimizer'
   1394               '\nby modifying the config for creating the session eg.'
   1395               '\nsession_config.graph_options.rewrite_options.'
   1396               'disable_meta_optimizer = True')
-&gt; 1397 raise type(e)(node_def, op, message)

InvalidArgumentError: Graph execution error:

Detected at node 'train_op/CheckNumerics' defined at (most recent call last):
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\runpy.py", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\runpy.py", line 87, in _run_code
      exec(code, run_globals)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel_launcher.py", line 17, in &lt;module&gt;
      app.launch_new_instance()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\traitlets\config\application.py", line 1043, in launch_instance
      app.start()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelapp.py", line 711, in start
      self.io_loop.start()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tornado\platform\asyncio.py", line 215, in start
      self.asyncio_loop.run_forever()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\asyncio\base_events.py", line 570, in run_forever
      self._run_once()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\asyncio\base_events.py", line 1859, in _run_once
      handle._run()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\asyncio\events.py", line 81, in _run
      self._context.run(self._callback, *self._args)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 510, in dispatch_queue
      await self.process_one()
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 499, in process_one
      await dispatch(*args)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 406, in dispatch_shell
      await result
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 729, in execute_request
      reply_content = await reply_content
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\ipkernel.py", line 411, in do_execute
      res = shell.run_cell(
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\zmqshell.py", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 2961, in run_cell
      result = self._run_cell(
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3016, in _run_cell
      result = runner(coro)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\async_helpers.py", line 129, in _pseudo_sync_runner
      coro.send(None)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3221, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3400, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3460, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File "C:\Users\aselvite\AppData\Local\Temp\ipykernel_18272\1865657941.py", line 9, in &lt;module&gt;
      deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\training.py", line 212, in train_network
      train(
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\core\train.py", line 238, in train
      learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\core\train.py", line 117, in get_optimizer
      train_op = slim.learning.create_train_op(loss_op, optimizer)
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tf_slim\learning.py", line 436, in create_train_op
      return training.create_train_op(
    File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tf_slim\training\training.py", line 468, in create_train_op
      total_loss = array_ops.check_numerics(total_loss,
Node: 'train_op/CheckNumerics'
LossTensor is inf or nan : Tensor had NaN values
	 [[{{node train_op/CheckNumerics}}]]

Original stack trace for 'train_op/CheckNumerics':
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel_launcher.py", line 17, in &lt;module&gt;
    app.launch_new_instance()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\traitlets\config\application.py", line 1043, in launch_instance
    app.start()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelapp.py", line 711, in start
    self.io_loop.start()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tornado\platform\asyncio.py", line 215, in start
    self.asyncio_loop.run_forever()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\asyncio\base_events.py", line 570, in run_forever
    self._run_once()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\asyncio\base_events.py", line 1859, in _run_once
    handle._run()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\asyncio\events.py", line 81, in _run
    self._context.run(self._callback, *self._args)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 510, in dispatch_queue
    await self.process_one()
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 499, in process_one
    await dispatch(*args)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 406, in dispatch_shell
    await result
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\kernelbase.py", line 729, in execute_request
    reply_content = await reply_content
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\ipkernel.py", line 411, in do_execute
    res = shell.run_cell(
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\ipykernel\zmqshell.py", line 531, in run_cell
    return super().run_cell(*args, **kwargs)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 2961, in run_cell
    result = self._run_cell(
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3016, in _run_cell
    result = runner(coro)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\async_helpers.py", line 129, in _pseudo_sync_runner
    coro.send(None)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3221, in run_cell_async
    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3400, in run_ast_nodes
    if await self.run_code(code, result, async_=asy):
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\IPython\core\interactiveshell.py", line 3460, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "C:\Users\aselvite\AppData\Local\Temp\ipykernel_18272\1865657941.py", line 9, in &lt;module&gt;
    deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\training.py", line 212, in train_network
    train(
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\core\train.py", line 238, in train
    learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\core\train.py", line 117, in get_optimizer
    train_op = slim.learning.create_train_op(loss_op, optimizer)
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tf_slim\learning.py", line 436, in create_train_op
    return training.create_train_op(
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tf_slim\training\training.py", line 468, in create_train_op
    total_loss = array_ops.check_numerics(total_loss,
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\ops\gen_array_ops.py", line 1220, in check_numerics
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 795, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File "C:\Users\aselvite\Anaconda3\envs\DEEPLABCUT\lib\site-packages\tensorflow\python\framework\ops.py", line 3798, in _create_op_internal
    ret = Operation(
</code></pre>
<p>Could you <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a> please help in this regard, like what is the probable cause of this error and how come it worked at first but threw error in second run.</p> ;;;; <p>You could use <code>show_info</code> from <code>napari.utils.notifications</code> for showing info in popup in napari viewer.</p>
<p>Another option is to use the logging module <a href="https://docs.python.org/3/library/logging.html" class="inline-onebox" rel="noopener nofollow ugc">logging — Logging facility for Python — Python 3.11.2 documentation</a> which allows control if printed to the console.</p> ;;;; <p>Hi all,<br>
I just bought a monochrome and color 5 Mp Mightex CMOS cameras. They are very inexpensive and I was hoping they would work with MM. So far I can’t get them to work with the Mightex driver that is included with MM 1.4.22 (Win10 64-bit).</p>
<p>Mightex ships a utility app that does successfully talk to the camera, so I know I have the Mightex drivers installed properly. If anyone knows anything more, I’d be grateful!</p>
<p>Cheers,<br>
Jeff Hardin<br>
Dept. of Integrative Biology<br>
Univ. of Wisconsin-Madison</p> ;;;; <p>Thank you, Konrad. I tried inverted video, but this did not work. Regards! | P</p> ;;;; <blockquote>
<p>ping <a class="mention-group notify" href="/groups/ome">@ome</a> : how complex would be the update to kryo 5.4.0 ?</p>
</blockquote>
<p>Thanks for mentioning this, <a class="mention" href="/u/nicokiaru">@NicoKiaru</a>. There are now two pull requests open to evaluate updating kryo:</p>
<ul>
<li><a href="https://github.com/ome/bioformats/pull/3967" class="inline-onebox">Update kryo to 5.4.0 by melissalinkert · Pull Request #3967 · ome/bioformats · GitHub</a></li>
<li><a href="https://github.com/ome/ome-common-java/pull/74" class="inline-onebox">Update kryo to 5.4.0 by melissalinkert · Pull Request #74 · ome/ome-common-java · GitHub</a></li>
</ul> ;;;; <p>Sounds good.</p>
<p>I’ll give it a go.</p>
<p>I previously performed filterpredictions but left the default setting. A median filter was applied previously but I’ll state 3 - 5 next time.</p>
<p>FYI - my video was created with 120 fps if that changes anything…?</p> ;;;; <p><a class="mention" href="/u/research_associate">@Research_Associate</a> … you are a genius man! that was the answer, I changed and all work!<br>
Thank you so much!!!</p>
<p>P.D: Do you have a extensive documentation or maybe manual about qupath methods and functions? I would like to explore in deep the scripting part</p>
<p>best</p>
<p>Luis</p> ;;;; <p>Additionally, the confocal listserv gets quite a lot of activity if you have not already tried there. And I think a wider variety of people tend to check it, though I’d prefer the microlist forum got more action.<br>
<a href="https://lists.umn.edu/cgi-bin/wa?SUBED1=CONFOCALMICROSCOPY&amp;A=1" class="onebox" target="_blank" rel="noopener">https://lists.umn.edu/cgi-bin/wa?SUBED1=CONFOCALMICROSCOPY&amp;A=1</a></p> ;;;; <p>Fantastic. I will post there, too.</p> ;;;; <p>Hi <a class="mention" href="/u/ekatz">@ekatz</a></p>
<p>I cannot help you here, as I am not knowledgeable about hardware, however I am interested in learning who is knowledgeable about Spinning Disk systems, as I have long standing questions regarding the best way to calculate theoretical PSFs for Spinning disk, so just really following the question, as a side note you may also want to try the micro-forum <a href="https://forum.microlist.org/">https://forum.microlist.org/</a> related forum.</p>
<p>Brian</p> ;;;; <p>Sorry for the late reply,</p>
<p>This solution worked great! Thank you.</p> ;;;; <p>Hello,<br>
I noticed that the SUM SLICE z projection change my 16-bit image to a 32-bit result image. I could not find the answer in the Z project of ImageJ.<br>
I was wondering why is the reason since the average or Maximum maintain the 16 bits.</p>
<p>Thank you</p> ;;;; <p>Thank you very much. I will try to adjust it this way.</p> ;;;; <p>Thank you very much. I tried to set the scale on ImageJ but I need a reference that I cannot get from the Olympus metadata that I see on QUpath. So, I exported it as OME tiff and I got the information, is it Ok for the values exported as OME tiff?<br>
Thanks</p> ;;;; <p>I am looking to purchase a new spinning disk system to replace our aging CV7000. Does anyone have a favorite system to work with 384-well formats?</p>
<p>Thanks</p> ;;;; <p>This might also be relevant: <a href="https://qupath.readthedocs.io/en/0.4/docs/advanced/exporting_annotations.html#binary-labeled-images" class="inline-onebox">Exporting annotations — QuPath 0.4.3 documentation</a></p> ;;;; <p>How large are the images? That sounds like a fairly standard import process (see some of the cellpose import scripts for cells where each object created is based on a set of pixels with a specific integer value) if the images are small, otherwise you might need something like</p>
<aside class="quote quote-modified" data-post="1" data-topic="71579">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/smcardle/40/17938_2.png" class="avatar">
    <a href="https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579">Script for generating double threshold classifier</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hi All, 
Here’s a script I wrote that will hopefully be useful to some people. 
Normally, if I need to find double-positive regions, I run two pixel classifiers, then use Java geometry functions to find the object intersections. This works, but if there are many small detections, it can be quite slow. The script below uses ImageOps to apply a threshold to two different channels and writes it as a single pixel classifier. Then, the “colocalized” objects can be created directly through normal QuPa…
  </blockquote>
</aside>
 ;;;; <p>This is working great! I added the single line <code>RMSE.to_csv(resultfilename[:-3]+"_Dists.csv")</code> after the RMSE is calculated within the <code>evaluate_network</code> function, and quickly found a frame that had been seriously mislabeled previously due to what looks like an accidental ctrl-C <img src="https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12" title=":sweat_smile:" class="emoji" alt=":sweat_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Recent post suggest the stardist script is working assuming you have the right versions of QuPath and the StarDist plugin. <a href="https://forum.image.sc/t/qupath-stardist-global-normalisation-for-deconvolved-channels-in-v-0-4-x/78814/6" class="inline-onebox">QuPath StarDist global normalisation for deconvolved channels in v.0.4.x - #6 by ym.lim</a><br>
Can you confirm which versions you have?</p>
<p>Also not sure if some of the error is missing as I don’t see a line number listed. Always better to post the errors as text, not a screenshot since they can’t be word searched or copied.</p> ;;;; <p>I’m trying to run the following script which was barely modified from the one given to me in the QuPath documentation. I’m getting the following error. I’m new to image processing so please forgive me if there is an obvious answer.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8.png" data-download-href="/uploads/short-url/r9x7aS3jsgXi54nl7wrD67a2bwI.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_690x291.png" alt="image" data-base62-sha1="r9x7aS3jsgXi54nl7wrD67a2bwI" width="690" height="291" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_690x291.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_1035x436.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_1380x582.png 2x" data-dominant-color="F9F9F9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1755×741 53.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi all,<br>
I am quite new to QuPath.<br>
I would like to know wether exist an automated  way to get annotations from an image which has pixels values in range [0, 255]. At each different pixel values should correspond a different label.</p> ;;;; <p>Hi Cayla, unfortunately, it outputs the same image with a width of 1 no matter the line width I select.</p> ;;;; <aside class="quote group-team" data-username="tinevez" data-post="2" data-topic="78933">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tinevez/40/21_2.png" class="avatar"> Jean-Yves Tinevez:</div>
<blockquote>
<p>BTW look at the results: our forum is one of the cleanest and most respectful. On top of being super useful.</p>
</blockquote>
</aside>
<p>On that note, I have bumped a few more people up to Leader status when I see them answering a lot of questions in particular topics, so that they can do things like add tags, edit formatting (code especially, those dastardly curly quotes), split topics, etc. The last two yesterday were <a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a> and <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a> for Napari and DeepLabCut respectively.</p>
<p>Not only does it give them a bit more utility when providing help, hopefully it makes them feel appreciated as well.</p>
<p>Cheers,<br>
Mike</p> ;;;; <p><a class="mention" href="/u/petebankhead">@petebankhead</a> maybe it would be worth having cell detection create cell objects even when there is no cell expansion by using the same ROI for the cytoplasm and nucleus? Seems like this has come up before and confuses people. Alternatively, a warning within the Cell Detection dialog saying that 0 expansion will not create cell objects.</p> ;;;; <aside class="quote no-group" data-username="Luis_Cano" data-post="3" data-topic="78931">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/luis_cano/40/50413_2.png" class="avatar"> Luis Cano:</div>
<blockquote>
<p>don’t get results</p>
</blockquote>
</aside>
<p>What results are you expecting? It looks like you currently don’t have any cell objects (just nuclei since you didn’t use a cell expansion), so the second part of your script should not do anything.</p>
<p>You could try getDetectionObjects() instead since you are not using cells (which need both a cytoplasm and nucleus).</p> ;;;; <p>Hey <a class="mention" href="/u/el_pollo_diablo">@EL_Pollo_Diablo</a>, hey <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a>,</p>
<p>thanks for replying so fast! That was exactly the advise I am looking for, so I will try now my best to make it work. I will come back to you if I made it work and let you know how it went. Thanks for giving me guidance!</p>
<p>Best,</p>
<p>Malte</p> ;;;; <p>Many bots seem to like replying to old threads with many views - this is a signal I will try to look out for ( <a class="mention" href="/u/mike_nelson">@Mike_Nelson</a> 's example above looks consistent with this ).</p>
<p>We might consider more actively discouraging replies to very long topics and prefer starting new topics that link to related ones. This could remove the bot signal above, but would keep the forum a little more tidy. (Edit: this hasn’t really been a big issue outside a few crazy long threads)</p> ;;;; <p>Hello,<br>
I have a question regarding how to open files in a macro, especially if the path and filename are concatenated from previously defined substrings.<br>
Specifically, my files are located in a folder with two subfolders called images_cropped and masks_cropped. I would like to open one file from images_cropped and automatically open the corresponding file with the same name from masks_cropped.<br>
First, I am extracting the image name (without the file ending) and the path to my file in images_cropped. I believe I have succeeded in this. I am however stuck in the subsequent step. My idea is to concatenate several substrings to create the path to the file in masks_cropped. However, I receive the error:</p>
<blockquote>
<p>Error:		‘)’ expected in line 10:<br>
open ( "folder_mask + “masks_cropped/” + name + ". png “” ) ;</p>
</blockquote>
<p>Which I understand to mean that a parenthesis is not closed somewhere, but I can for the life of me not figure out where.<br>
The macro runs fine if I replace this line with the path to an exemplary picture.</p>
<p>My code looks like this:</p>
<blockquote>
<p>//extract information about the file<br>
folder = getDirectory(“image”);<br>
file = getTitle;<br>
end = lengthOf(file) - 4;<br>
name = substring(file, 0, end);</p>
</blockquote>
<p>-4 to remove the last 4 characters, i.e. the file ending</p>
<blockquote>
<p>//open image for masking<br>
end_mask = lengthOf(folder) -15;<br>
folder_mask = substring(folder, 0, end_mask);<br>
open(“folder_mask + “masks_cropped/” + name + “.png””);</p>
</blockquote>
<p>I remove the last 15 characters as my files are located in the folder images_cropped and I want to move one down in the directory structure. There may be more elegant solutions, but it works for me so far.</p>
<blockquote>
<p>//create mask<br>
selectWindow(name + “.png”);<br>
run(“Create Selection”);<br>
run(“Create Mask”);</p>
<p>//apply mask to other image<br>
selectWindow(name + “.jpg”);<br>
run(“Restore Selection”);<br>
run(“Invert”);<br>
setBackgroundColor(0, 0, 0);<br>
run(“Clear”, “slice”);<br>
run(“Select None”);<br>
saveAs(“Bmp”, folder + file + “masked.bmp”);</p>
</blockquote> ;;;; <p>Hi <a class="mention" href="/u/maltemederacke">@MalteMederacke</a> ,</p>
<p>I’m just adding some information to <a class="mention" href="/u/el_pollo_diablo">@EL_Pollo_Diablo</a>’s excellent advice.</p>
<p>I think rechunking is not necessary as you don’t have chunks in the first place. I recommend processing the image tile-by-tile with overlap using dask’s <code>map_overlap</code> (<a href="https://docs.dask.org/en/stable/generated/dask.array.map_overlap.html">documentation</a>).</p>
<aside class="quote no-group" data-username="MalteMederacke" data-post="1" data-topic="78924">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/maltemederacke/40/52522_2.png" class="avatar"> Malte Mederacke:</div>
<blockquote>
<p>What would be the best way to chunk my data?</p>
</blockquote>
</aside>
<p>Use a chunk size that fits into your GPU memory as many times as features you process (plus one, the result). Note: one voxel is four bytes large in this context. Consider testing it on <a href="https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification">napari-apoc</a> interactively with a small image, which has a memory usage estimator (7):</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960.png" data-download-href="/uploads/short-url/1UdPqffwZIAU04nIBLgZjNjsqPK.png?dl=1" title=""><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960_2_301x499.png" alt="" data-base62-sha1="1UdPqffwZIAU04nIBLgZjNjsqPK" role="presentation" width="301" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960_2_301x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960.png 2x" data-dominant-color="40454B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename"></span><span class="informations">420×696 83.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<aside class="quote no-group" data-username="MalteMederacke" data-post="1" data-topic="78924">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/maltemederacke/40/52522_2.png" class="avatar"> Malte Mederacke:</div>
<blockquote>
<p>How does Dask interact with openCL?</p>
</blockquote>
</aside>
<p>Dask may not care, however, the GPU might have hickups. If you use <a href="https://github.com/haesleinhuepf/apoc">APOC</a> which is based on <a href="https://github.com/clEsperanto/pyclesperanto_prototype">clesperanto</a>, I recommend calling this code snippet to prevent issues with multi-threading (<a href="https://github.com/clEsperanto/pyclesperanto_prototype/issues/163">read why</a>):</p>
<pre><code class="lang-auto">import pyclesperanto_prototype as cle
cle.set_wait_for_kernel_finish(True)
</code></pre>
<p>You find a notebook demonstrating the entire procedure here:<br>
<a href="https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiling_images_with_overlap.html">https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiling_images_with_overlap.html</a></p>
<p>Let us know if this works for you!</p>
<p>Best,<br>
Robert</p> ;;;; <aside class="quote group-team" data-username="tinevez" data-post="2" data-topic="78933">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tinevez/40/21_2.png" class="avatar"> Jean-Yves Tinevez:</div>
<blockquote>
<p>We will fight and win <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
</blockquote>
</aside>
<p>I see an AI cold war coming… <img src="https://emoji.discourse-cdn.com/twitter/joy.png?v=12" title=":joy:" class="emoji" alt=":joy:" loading="lazy" width="20" height="20"></p> ;;;; <p>We will fight and win <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
Thanks for the heads up. I have seen and banned similar things recently, and now that we are aware we will be more cautious.</p>
<p>BTW look at the results: our forum is one of the cleanest and most respectful. On top of being super useful.</p> ;;;; <p>Hi,</p>
<p>When trying to add NeuroCyto-LUTs through the Fiji updater, I encounter an HTTP response code 403 for certain LUTs.</p>
<p>If I open my browser and go to <a href="https://sites.imagej.net/NeuroCyto-LUTs/luts/" class="inline-onebox" rel="noopener nofollow ugc">Index of /NeuroCyto-LUTs/luts</a>, I’m able to manually download the file, but I didn’t want to do this multiple times.</p>
<p><strong>I’ve found that a couple iterations of closing Fiji and attempting to update NeuroCyto-LUTs through the Fiji updater will eventually get them all.</strong></p>
<p>I hope this helps someone who encounters a similar situation.</p>
<p>See below for the first problematic file’s error messages.</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4279ac060e97f7428271c3fd7c4587cdd997a04b.png" alt="Fiji-updater-403" data-base62-sha1="9u4c1SAFbkeDAzWLwbM4D2pxrV9" width="536" height="135"></p>
<pre><code class="lang-auto">[ERROR] java.io.IOException: Server returned HTTP response code: 403 for URL: https://sites.imagej.net/NeuroCyto-LUTs/luts/JDM_Grays%20g=0.25.lut-20220424170324
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at sun.net.www.protocol.http.HttpURLConnection$10.run(HttpURLConnection.java:1967)
	at sun.net.www.protocol.http.HttpURLConnection$10.run(HttpURLConnection.java:1962)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1961)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1521)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1505)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:268)
	at net.imagej.updater.util.Downloader.download(Downloader.java:117)
	at net.imagej.updater.util.Downloader.start(Downloader.java:98)
	at net.imagej.updater.Installer.start(Installer.java:154)
	at net.imagej.ui.swing.updater.UpdaterFrame.install(UpdaterFrame.java:627)
	at net.imagej.ui.swing.updater.UpdaterFrame$13.run(UpdaterFrame.java:564)
Caused by: java.io.IOException: Server returned HTTP response code: 403 for URL: https://sites.imagej.net/NeuroCyto-LUTs/luts/JDM_Grays%20g=0.25.lut-20220424170324
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1917)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1505)
	at sun.net.www.protocol.http.HttpURLConnection.getHeaderField(HttpURLConnection.java:3078)
	at java.net.HttpURLConnection.getHeaderFieldDate(HttpURLConnection.java:552)
	at java.net.URLConnection.getLastModified(URLConnection.java:559)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getLastModified(HttpsURLConnectionImpl.java:436)
	at net.imagej.updater.util.Downloader.download(Downloader.java:108)
	... 4 more
</code></pre> ;;;; <p>hi <a class="mention" href="/u/research_associate">@Research_Associate</a> ,<br>
Thanks for your comment, I would like to share with you the error message , but I don’t have one… the script run but I don’t get results … and as you say is  a mistake at the moment to copy the lympho class the original was exactly the same to predefine variable</p> ;;;; <p>Hi <a class="mention" href="/u/maltemederacke">@MalteMederacke</a> ,</p>
<p>soooo, without having seen your data, one thing I could think of would be the following: You say that you have your image data as a dask array, right? You could <a href="https://docs.dask.org/en/stable/generated/dask.array.rechunk.html" rel="noopener nofollow ugc">re-chunk</a>  the array into blocks of a given size (in MB) which you could set that it definitely fits into your GPU.</p>
<p>As for training the classifier, you can probably load dask arrays into napari natively and create an annotation. You could then use the annotation to train the classifier from code. When it comes to applying the classifier, you could write yourself a little helper function that applies the classifier to each chunk in your array.</p>
<p>I found an example on how to do this <a href="https://github.com/dask/dask/issues/7589" rel="noopener nofollow ugc">here</a>.</p>
<p>Hope this helps!</p>
<p>PS: As for how the GPU is used (According to the <a href="https://docs.dask.org/en/stable/gpu.html" rel="noopener nofollow ugc">dask documentation</a>):  <em>“Dask doesn’t need to know that these functions use GPUs. It just runs Python functions. Whether or not those Python functions use a GPU is orthogonal to Dask. It will work regardless.”</em> Fingers crossed <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"></p> ;;;; <p>I’ve tested, and there’s an incompatibility with bio-formats memoization, which is using kryo:</p>
<p>Bio-formats relies on kryo 4.0.2:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/ome/bioformats/blob/32c42f07691d11dc0b863081a2226baf3ac858a6/pom.xml#L50">
  <header class="source">

      <a href="https://github.com/ome/bioformats/blob/32c42f07691d11dc0b863081a2226baf3ac858a6/pom.xml#L50" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/ome/bioformats/blob/32c42f07691d11dc0b863081a2226baf3ac858a6/pom.xml#L50" target="_blank" rel="noopener">ome/bioformats/blob/32c42f07691d11dc0b863081a2226baf3ac858a6/pom.xml#L50</a></h4>



    <pre class="onebox"><code class="lang-xml">
      <ol class="start lines" start="40" style="counter-reset: li-counter 39 ;">
          <li>&lt;project.rootdir&gt;${basedir}&lt;/project.rootdir&gt;</li>
          <li>&lt;imagej1.version&gt;1.51r&lt;/imagej1.version&gt;</li>
          <li>&lt;jgoodies-forms.version&gt;1.7.2&lt;/jgoodies-forms.version&gt;</li>
          <li>&lt;logback.version&gt;1.2.9&lt;/logback.version&gt;</li>
          <li>&lt;ome-stubs.version&gt;5.3.2&lt;/ome-stubs.version&gt;</li>
          <li>&lt;lwf-stubs.version&gt;${ome-stubs.version}&lt;/lwf-stubs.version&gt;</li>
          <li>&lt;mipav-stubs.version&gt;${ome-stubs.version}&lt;/mipav-stubs.version&gt;</li>
          <li>&lt;ome-metakit.version&gt;5.3.5&lt;/ome-metakit.version&gt;</li>
          <li>&lt;metakit.version&gt;${ome-metakit.version}&lt;/metakit.version&gt;</li>
          <li>&lt;slf4j.version&gt;1.7.30&lt;/slf4j.version&gt;</li>
          <li class="selected">&lt;kryo.version&gt;4.0.2&lt;/kryo.version&gt;</li>
          <li>&lt;testng.version&gt;6.8&lt;/testng.version&gt;</li>
          <li>&lt;ome-common.version&gt;6.0.14&lt;/ome-common.version&gt;</li>
          <li>&lt;ome-model.group&gt;org.openmicroscopy&lt;/ome-model.group&gt;</li>
          <li>&lt;ome-model.version&gt;6.3.2&lt;/ome-model.version&gt;</li>
          <li>&lt;ome-poi.version&gt;5.3.7&lt;/ome-poi.version&gt;</li>
          <li>&lt;ome-mdbtools.version&gt;5.3.2&lt;/ome-mdbtools.version&gt;</li>
          <li>&lt;ome-jai.version&gt;0.1.3&lt;/ome-jai.version&gt;</li>
          <li>&lt;ome-codecs.version&gt;0.4.4&lt;/ome-codecs.version&gt;</li>
          <li>&lt;jxrlib.version&gt;0.2.4&lt;/jxrlib.version&gt;</li>
          <li>&lt;xalan.version&gt;2.7.2&lt;/xalan.version&gt;</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>And since it’s a major version change, there’s unfortunately an incompatibility showing up with the new parent pom that has kryo <code>5.4.0</code>. Here’s the (first?) error showing up with bio-formats memoization:</p>
<pre><code class="lang-auto">Caused by: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/Kryo$DefaultInstantiatorStrategy
	at loci.formats.Memoizer$KryoDeser.&lt;init&gt;(Memoizer.java:124)
	at loci.formats.Memoizer.getDeser(Memoizer.java:753)
	at loci.formats.Memoizer.saveMemo(Memoizer.java:954)
	at loci.formats.Memoizer.setId(Memoizer.java:697)
</code></pre>
<p>(I’ve deleted the memo file before opening the file)</p>
<p><a class="mention" href="/u/petebankhead">@petebankhead</a> also uses bio-formats memoization in QuPath with kryo 4.0.2</p>
<p>ping <a class="mention-group notify" href="/groups/ome">@ome</a> : how complex would be the update to kryo 5.4.0 ?</p>
<p>(and as a result, ABBA do not work because it is using memoization, I do not know if other parts are affected)</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad.png" data-download-href="/uploads/short-url/gilyGCekyUbY6uq8Fnk1wiz3bK5.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad_2_690x362.png" alt="image" data-base62-sha1="gilyGCekyUbY6uq8Fnk1wiz3bK5" width="690" height="362" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad_2_690x362.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad_2_1035x543.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad.png 2x" data-dominant-color="F6C2C3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1179×620 56.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
I wanted to point out to <a class="mention-group notify" href="/groups/team">@team</a> the linking of an unrelated OCR product from a freshly created account into a post about reading CZI images. It also makes use of text that is forum relevant like mentioning Fiji.</p>
<p>Sigh. Might be chatbot supported to target specific forums…</p>
<p>Not so cheers,<br>
Mike</p> ;;;; <p>Searching for other examples of this error message <a href="https://forum.image.sc/t/hierarchy-is-null-cannot-invoke-can-anyone-help/78488/4" class="inline-onebox">Hierarchy is null - Cannot invoke - Can anyone help? - #4 by petebankhead</a><br>
<a href="https://forum.image.sc/t/i-cannot-open-the-qupath-file-due-to-the-error-note-hierarchy-is-null-what-can-i-do/68160/4" class="inline-onebox">I cannot open the QuPath file due to the error note "hierarchy is null". What can I do? - #4 by Research_Associate</a><br>
Etc.<br>
If there is a backup file in the folders described above you can try and use that instead.</p> ;;;; <p>the same!</p>
<p>(DEEPLABCUT) PS C:\WINDOWS\system32&gt; python -m deeplabcut<br>
Loading DLC 2.3.2…<br>
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)<br>
Traceback (most recent call last):<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\runpy.py”, line 185, in _run_module_as_main<br>
mod_name, mod_spec, code = <em>get_module_details(mod_name, <em>Error)<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\runpy.py”, line 144, in <em>get_module_details<br>
return <em>get_module_details(pkg_main_name, error)<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\runpy.py”, line 111, in <em>get_module_details<br>
<strong>import</strong>(pkg_name)<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut_<em>init</em></em>.py", line 37, in <br>
from deeplabcut.create_project import (<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\create_project_<em>init</em></em>.py", line 12, in <br>
from deeplabcut.create_project.demo_data import load_demo_data<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\create_project\demo_data.py”, line 16, in <br>
from deeplabcut.utils import auxiliaryfunctions<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils_<em>init</em></em>.py", line 11, in <br>
from deeplabcut.utils.auxfun_multianimal import *<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils\auxfun_multianimal.py”, line 34, in <br>
from deeplabcut.utils import auxiliaryfunctions, conversioncode<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils\auxiliaryfunctions.py”, line 31, in <br>
from deeplabcut.pose_estimation_tensorflow.lib.trackingutils import TRACK_METHODS<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow_<em>init</em></em>.py", line 17, in <br>
from deeplabcut.pose_estimation_tensorflow.datasets import *<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets_<em>init</em></em>.py", line 13, in <br>
from .pose_deterministic import DeterministicPoseDataset<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets\pose_deterministic.py”, line 17, in <br>
from deeplabcut.utils.auxfun_videos import imread, imresize<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils\auxfun_videos.py”, line 25, in <br>
import cv2<br>
ImportError: DLL load failed while importing cv2: No se puede encontrar el módulo especificado.<br>
(DEEPLABCUT) PS C:\WINDOWS\system32&gt;</p> ;;;; <p>What your image shows is not the pixel classifier running, that is a preview (and is not necessary at all, for visualization purposes only). It is also only being run in tiles where there is an annotation, the tiles are just quite large at the resolution you are zoomed out to.</p>
<p>When you run the classifier, it will generally run on the selected annotations, and you can select annotations by class as shown in another recent post <a href="https://forum.image.sc/t/hes-cell-classification-using-measure-features/78931" class="inline-onebox">HES cell classification using measure features</a></p> ;;;; <p>It would help if you shared the error message, but one thing that should be an error is that <code>linfo</code> in your if statement is not defined. I suspect that should be <code>lymph</code></p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3.jpeg" data-download-href="/uploads/short-url/tVxTU9r4iRbSUqdctsiBOu1oY15.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3_2_602x500.jpeg" alt="image" data-base62-sha1="tVxTU9r4iRbSUqdctsiBOu1oY15" width="602" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3_2_602x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3_2_903x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3.jpeg 2x" data-dominant-color="E9EBEA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1192×989 155 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I don’t see anything associated with the vertex. In the ply file it just has x,y,z,r,g,b The color is the same for each vertex.</p> ;;;; <p>Hi Rémy,</p>
<p>The behaviour you’re seeing is a feature that is designed to collapse all “identical” labels together so they can be edited together. The idea is that if you’ve got several panels with e.g. “DAPI” label coming from channel names, you can select all the panels and do a single edit of all the labels to “DNA”. It wasn’t expected that you’d have duplicate labels on a single panel.</p>
<p>I wonder if we should try harder NOT to allow duplicate labels in the first place - or at least warn users. E.g. if creating labels from Tags, it could skip creating a label if it exists already? Created <a href="https://github.com/ome/omero-figure/issues/502" class="inline-onebox">Duplicate Labels can't be edited independently · Issue #502 · ome/omero-figure · GitHub</a></p>
<ol>
<li>
<p>I thought about “Select only the Tag we want to Show” when creating labels from Tags, but you might have different Tags coming from different panels and the UI for picking Tags for each panel would have just looked like the Figure itself - so it was simpler to create labels for All tags, then just remove the ones you don’t want.<br>
I can see this gets more confusing if they are mixed with existing Tags, but if we simply avoid creating duplicates, will that fix most of the issues you’re seeing?</p>
</li>
<li>
<p>Import all key-values → labels sounds like a useful feature that shouldn’t be too hard to do. Created <a href="https://github.com/ome/omero-figure/issues/501" class="inline-onebox">Create Labels from ALL Key-Value pairs · Issue #501 · ome/omero-figure · GitHub</a></p>
</li>
<li>
<p>I think what you’re seeing with the underscores is that the label is being interpreted as <code>markdown</code> formatting, so the <code>_text_</code> is getting turned into <em>italics</em>! This can be escaped with <code>\_text_</code> which will be displayed as _no-italics_. But I guess this isn’t very user-friendly - will need to think about how to disable this…  Added to <a href="https://github.com/ome/omero-figure/issues/486" class="inline-onebox">Label markdown formatting does too much · Issue #486 · ome/omero-figure · GitHub</a></p>
</li>
</ol>
<p>Cheers,</p> ;;;; <blockquote>
<p><a class="mention" href="/u/ctrueden">@ctrueden</a>, <a class="mention" href="/u/gselzer">@gselzer</a> Is there a parent pom scijava associated to the new version ? <code>34.1.0</code> ?</p>
</blockquote>
<p>Yup <a class="mention" href="/u/nicokiaru">@NicoKiaru</a>, <code>34.1.0</code> is the pom-scijava in <a href="https://github.com/fiji/fiji/blob/b15de263ec712c76781b575311498da3f26504e1/pom.xml#L8">the newest Fiji</a>, bumped from <code>34.0.0</code> in Fiji 2.10</p> ;;;; <p>Hi everyone</p>
<p>I’m working with HES images, I used cell detection to get all cells in an area (see image) . My next step is to subclassify all detections according some features i.e. Nucleus Circularity.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88.jpeg" data-download-href="/uploads/short-url/ip31MVcE9Ak4SM8h0VZGC9DCgje.jpeg?dl=1" title="Capture.PNG" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88_2_662x500.jpeg" alt="Capture.PNG" data-base62-sha1="ip31MVcE9Ak4SM8h0VZGC9DCgje" width="662" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88.jpeg 2x" data-dominant-color="8A5574"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Capture.PNG</span><span class="informations">1118×844 284 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I create this piece of code</p>
<pre><code class="lang-auto">selectObjectsByClassification("area");
runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImageBrightfield":"Hematoxylin OD","requestedPixelSizeMicrons":0.5,"backgroundRadiusMicrons":8.0,"backgroundByReconstruction":true,"medianRadiusMicrons":0.0,"sigmaMicrons":1.5,"minAreaMicrons":10.0,"maxAreaMicrons":400.0,"threshold":0.1,"maxBackground":2.0,"watershedPostProcess":true,"cellExpansionMicrons":0.0,"includeNuclei":true,"smoothBoundaries":true,"makeMeasurements":true}')
lymph = getPathClass('lymph') # the subclass that i want to create 
ignore = getPathClass('ignore') # cells to ignore 
for (cell in getCellObjects()) {
    m1 = measurement(cell, 'Nucleus: Circularity')
    if(m1 &gt; 0.85)
        cell.setPathClass(linfo)
    else
        cell.setPathClass(ignore)
    }
fireHierarchyUpdate()
</code></pre>
<p>but I don’t have any results… anyone could help me with that please…</p> ;;;; <aside class="quote no-group" data-username="f.goerlitz" data-post="1" data-topic="78881">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>So, where do I put <strong>fun</strong>? Can I put it in the post_camera_hook_fn (after putting the camera into external triggering mode), as shown in the code below?</p>
</blockquote>
</aside>
<p>Yes. Have you seen <a href="https://pycro-manager.readthedocs.io/en/latest/performance_guide.html#fast-acquisition-with-hardware-triggering" rel="noopener nofollow ugc">this page</a>?</p>
<aside class="quote no-group" data-username="f.goerlitz" data-post="1" data-topic="78881">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<ul>
<li>Is the post_camera_hook_fun only called once before the Acquisition if any loaded device is in sequencing mode (such as the DemoStage) or camera in external trigger mode?</li>
<li>Currently, the DAQ sequence only contains the signal for one z-stack, but we would like to acquire a time series of z-stacks. Can micromanager deal with the time sequencing using the order argument in the Aqcuisition “order=‘tz’” vs “order=‘tz’”? Is there a hook function which is called after each time point?</li>
</ul>
</blockquote>
</aside>
<p>It will be called once for each sequence of acquisition events. Whether events are sequenced is determined by 1) If they were all submitted at once in a single call to <code>acq.acquire(events)</code> and 2) whether the hardware (z stage etc) can support a sequence of that length. If you want everything to be sequenced in one go, submit everything in a single call to acquire. if you want to ensure that it gets broken up into multiple sequences (e.g. one for each z stack), make multiple calls to acquire.</p>
<aside class="quote no-group" data-username="f.goerlitz" data-post="1" data-topic="78881">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>Can micromanager deal with the time sequencing using the order argument in the Aqcuisition “order=‘tz’” vs “order=‘tz’”? Is there a hook function which is called after each time point?</p>
</blockquote>
</aside>
<p>Do you mean in the <code>multi_dimensional_acq_events</code> function? I’d encourage you to read a bit more about <a href="https://pycro-manager.readthedocs.io/en/latest/acq_events.html#customized-acquisition-events" rel="noopener nofollow ugc">what this function is doing under the hood</a> because understanding individual acquisition events should make this make more sense</p> ;;;; <p>nice <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
that looks like a bunch of cells, you can check if there is any vertex-associated data or celldata in your mesh with:<br>
<code>msh</code><br>
and / or<br>
<code>msh.print()</code></p> ;;;; <p><a class="mention" href="/u/ctrueden">@ctrueden</a>, <a class="mention" href="/u/gselzer">@gselzer</a> Is there a parent pom scijava associated to the new version ? <code>34.1.0</code> ?</p> ;;;; <p>Hi Danielle</p>
<p>Thanks!!!</p>
<p>Im going to try with Fiji</p> ;;;; <p>Hi,</p>
<p>No need to apologize, I am super-happy for this fast answer. As I understand it, the main issue is with the older version of bioimageio.core that the model is requesting, not an ilastik issue.</p>
<p>For completeness: I ‘messed’ around by copying older bioimageio.core versions (acquired through pip install bioimageio.core version==0.4.7) and copying them into the bioimageio folder of ilastik, trying to fool ilastik in having an older version of the package. I did not succeed.</p> ;;;; <p>Thanks for that - I created an issue at <a href="https://github.com/ome/omero-figure/issues/500" class="inline-onebox">Small ROIs lost behind bigger ROIs · Issue #500 · ome/omero-figure · GitHub</a></p>
<p>I would like it to “just work” by automatically having the smaller shapes move to the front, so they can be clicked, instead of needing to find space and build a list for clicking on.</p>
<p>I found a possible work-around that might help…<br>
If you select the bigger ROI and “Copy” it, then delete it and delete others behind it, then you can “Paste” to add it back.</p> ;;;; <p>Hi <a class="mention" href="/u/jpolentes">@JPolentes</a></p>
<p>Just a thought as there is no ROI filtering according to criteria as far as I am aware of - happy to stand corrected.<br>
Convert ROIs/instance mask to binary mask, use particle analyzer to filter by size and generate new ROIs.</p>
<p>Also, see this post: <a href="https://forum.image.sc/t/fiji-roi-filtering/25973" class="inline-onebox">Fiji ROI filtering</a></p>
<p>Does this help at all?</p>
<p>Best wishes,<br>
Marie</p> ;;;; <p>It looks like HALO reads the ID as the annotation name which is just a number in the case of my annotations while the name (ie the path class is listed under details). Can I script making the ID of the annotation the path class name?</p> ;;;; <p>It should be fixed now.</p>
<p>Grids do not have any notion of Z position, so if you want them to run at a specific height, use set the position yourself, either through a z-stack with one slice or by moving to the correct position.  Or you can use a surface, which does have Z position</p> ;;;; <p>Hello <a class="mention" href="/u/dimivanhecke">@DimiVanhecke</a>,</p>
<p>I’m really sorry you run into this problem. It’s really a bit of an annoying one, but mainly related to development being very fast in one of the libraries we use (<code>bioimageio...</code>). The preferred solution here (as in sustainable) would be to download the latest stable release <a href="https://www.ilastik.org/download.html">from our website</a> (1.4.0 without the rc, you’re running 1.4.0rc6). I verified this pretrained model works there. Of course I could modify and upload it for you, but this would only solve the problem for this one network (<a href="https://oc.embl.de/index.php/s/9uSKUDzVDXKkPKZ">here it is in any case</a>).</p>
<p>Cheers, and sorry again for running into this.</p>
<p>Dominik</p> ;;;; <p>Ok, it looks as though the windows were not opening at all, this is due to the line <code>setBatchMode(true);</code><br>
If you remove that line you should see the windows open and close as it runs through the processes.</p>
<p>The last change is that the selectWindow command in deleteSlices will also need the .tif suffix added as below:<br>
<code>selectWindow(file+".tif");</code></p>
<p>With those 2 changes I was able to run the script successfully to completion.</p> ;;;; <p>Hi <a class="mention" href="/u/maro">@Maro</a>, <a class="mention" href="/u/research_associate">@Research_Associate</a>,</p>
<p>There is no easy way to increase the ROI group limit of 255. Use the ROI Manager’s More&gt;&gt;Save command to create a ZIP file of the ROIs, so I can reproduce the problem, and I will try to find a work around.</p> ;;;; <p>ImageJ Ops is looking better than ever! Example scripts seem to run well, and the Ops I tried running worked as expected! Thanks <a class="mention" href="/u/ctrueden">@ctrueden</a>!</p> ;;;; <p>Works great now!</p>
<p>The first time I ran the notebook, nothing showed up, but when I re-run it everything seems great.</p>
<p>Here is another question.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732.jpeg" data-download-href="/uploads/short-url/lFXV0OVHX3S7Vdx2rzXxI7Ffp8C.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_690x433.jpeg" alt="image" data-base62-sha1="lFXV0OVHX3S7Vdx2rzXxI7Ffp8C" width="690" height="433" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_690x433.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_1035x649.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_1380x866.jpeg 2x" data-dominant-color="41583F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1205 184 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I’m doing pretty much the same thing, except I am using a PLY file. In the vtk display the meshes are just solid colors.</p>
<p>Do you know what is being shown here? It looks pretty cool.</p> ;;;; <p>Hi David,</p>
<p>I’m using 3.0.15. It’s been a while since I did anything with this SDK or with ctypes in general, so I’m not sure why mine would or wouldn’t work… I had assumed that a c_bool was 1 bit.</p>
<p>Some of the structs that are in my driver are just left over from when I was testing the SDK. After I got the stage to work (i.e. moving the motors, setting condenser intensity, reading temperatures), I left it at that. The python-microscope/microscope code is much more sophisticated so it could be that the ConnectionStatus struct doesn’t actually work for me, but that I can successfully ignore it because of the crude driver.</p> ;;;; <p>Hopefully this is fixed in the dev version:</p>
<pre><code class="lang-auto">pip install -U git+https://github.com/marcomusy/vedo.git
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e.png" data-download-href="/uploads/short-url/wWbgPZFHfOjDEcm4e4owVucryPA.png?dl=1" title="Screenshot from 2023-03-22 15-14-50"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e_2_690x497.png" alt="Screenshot from 2023-03-22 15-14-50" data-base62-sha1="wWbgPZFHfOjDEcm4e4owVucryPA" width="690" height="497" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e_2_690x497.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e_2_1035x745.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e.png 2x" data-dominant-color="84858A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-22 15-14-50</span><span class="informations">1121×809 170 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>thanks for reporting the issue.</p> ;;;; <p>Hi,<br>
I trained stardist model and it work wonders in napari and imageJ plugins but when I used in python script it does very bad job of segmenting cells. I believe its to do with normalisation but I have tried everything but can’t fix the issue.</p>
<p>Details: Model is trained on RGB images [‘YXC’] . I use it to predict RBG images and tried both type of normalisation</p>
<pre><code class="lang-auto">def get_stardist_model(model_name='stardist_model',directory= './models/'):
    model = StarDist2D(None, name=model_name, basedir=directory)
    return model

def stardist_predict(img):       
    model = get_stardist_model()
     #axis_norm = (0,1) #normalise independently 
    axis_norm = (0,1,2) #normalise jointly
    img_norm = normalize(img, 1,99.8, axis=axis_norm)
    labels, details = model.predict_instances(img_norm)
    return labels, details
</code></pre>
<p>Can anyone help me figure the issue please?</p>
<p>regards<br>
Atif</p> ;;;; <p>Hi Mart</p>
<p>Thank you for sharing the code (both in this answer and the Cryoscope project in general).</p>
<p>I’ve looked at how you handle the bit-fields for the flags (which is at the root of this issue) and I’m surprised the way you do it works correctly. You’re doing:</p>
<pre><code class="lang-auto">class ConnectionStatus(Structure):
    _fields_ = [("flags", c_bool*32), ("value", c_uint32)]
</code></pre>
<p>which maps to a struct with 32 <code>_Bool</code>s (each taking 8bit) <em>and</em> one <code>uint32_t</code>. That’s a total of 288 bits (32*8 + 32). Instead, the SDK takes a union of one <code>uint32_t</code> and one struct with 32 bit-fields. The union should have a size of 32 bits.</p>
<p>I would guess that the real value of the flags is merged into <code>ConnectionStatus.flags[:4]</code></p>
<p>But maybe you’re working with a different SDK version (we have 3.0.0100 and 3.0.15)?</p> ;;;; <p>Hi,</p>
<p>Thanks <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a> for bringing a super quick pixel classifier to napari!</p>
<p>I have following issue: I have relatively big 3D image data ~(1100, 4090,4090) where I want to perform semantic pixel classification on. This is of course way to big to fit on any GPU. That’s why I tried to use dask chunking to only load parts of the image, but I want to cover still a meaningful area, as I expect to have intensity differences along Z at from the sides to center. How would I approach such a task using<br>
<span class="hashtag">#napari-accelerated-pixel-and-object-classification</span>.<br>
I already got a way around using relatively strong downsampling and a CPU based pixel classifier, but I am not quite happy with it, as the downsampling leads to resolution issues. Additionally, I will have to do bigger images in the future, so I think I need to figure something out.</p>
<p>How does Dask interact with openCL? What would be the best way to chunk my data? Sorry, I am a little bit lost here.</p>
<p>Best,</p>
<p>Malte</p> ;;;; <p>Hi Tomas,</p>
<p>Check here: <a href="https://imagej.net/software/fiji/downloads" class="inline-onebox" rel="noopener nofollow ugc">Fiji Downloads</a><br>
Here some background &amp; tutorials: <a href="https://imagej.nih.gov/ij/docs/examples/index.html" class="inline-onebox" rel="noopener nofollow ugc">Tutorials and Examples</a></p>
<p>Your question is somewhat broad still - check this video to get going: <a href="https://www.youtube.com/watch?v=FiFwxoxOmNo" class="inline-onebox" rel="noopener nofollow ugc">ImageJ Analysis: Length Measurement, Area Measurement and Thresholding - YouTube</a><br>
And some more: <a href="https://www.youtube.com/@manchesterbioimaging/videos" class="inline-onebox" rel="noopener nofollow ugc">Manchester Bioimaging - YouTube</a></p>
<p>Aside from ImageJ / Fiji there are quite some open source options (<a href="https://imagescience.org/software/" class="inline-onebox" rel="noopener nofollow ugc">ImageScience.Org - Software</a>), though I think Fiji will be good for what you need. Both the image processing as well as automation.</p> ;;;; <p>Hi everyone,</p>
<p>Does anybody knows a way to select multiple ROIs by clicking on them or by tracing a selection area around them on an image, and then get the indexes (in the ROI Manager) of the selected ROIs?<br>
I forgot to precise that i would like to do it in a *.ijm macro.</p>
<p>I found a way to do it on a single ROI, but I’m stuck with the “multiple” selection, which is actually essential for my problem.</p>
<p>To complete my problem if it was necessary to understand the challenge: I use Cellpose to detect objects on images and get measurements on them, which are sent in an excel file. But I also detect false positives, which are in fact just background with noise in it. But that’s not the problem, because I developed a little tool that loads the ROIs from Cellpose onto the image; then I can click on a false positive ROI, and the tool sends me the ROI index in a text document. Then I just have to retrieve the list of objects to neglect in the excel file. The real problem is that sometime I have to select a lot of small artefactual ROIs, and this is REALLY boring/complicated/time consuming, depending on the case. That’s why it would be very useful to be able to delineate several ROIs and get all their indexes at the same time.<br>
I hope someone could help me with this.</p>
<p>Thank you!!</p> ;;;; <p>Thanks very much for posting this for testing <a class="mention" href="/u/ctrueden">@ctrueden</a>. I learned my lesson during the last update when I didn’t test and the changes broke somethings for us.</p>
<p>This time, I am happy to report, I didn’t get any errors or have any problems (I only tested v 2.11.0)! <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"></p> ;;;; <p>I think you can use: <a href="https://github.com/DeepLabCut/DeepLabCut/blob/072c040e6119dd592d0b4fdb289c633e75a09325/deeplabcut/pose_estimation_tensorflow/core/evaluate.py#L217" class="inline-onebox">DeepLabCut/evaluate.py at 072c040e6119dd592d0b4fdb289c633e75a09325 · DeepLabCut/DeepLabCut · GitHub</a> with <code>returnjustfns=False</code> to get the data you need.</p>
<p><a class="mention" href="/u/tahayassine">@tahayassine</a> Using this function and saving the results to a pickle file. This will give you the pickle that is saved for maDLC models with both predicitons and annotations matched to an image</p> ;;;; <p>In that case, I am aiming to wrap up the PR soon. We had a community meeting today where the polygon lasso tool was discussed and I will implement the things mentioned during the discussions there. The current branch does have the RDP algorithm enabled by default. If you would like to work from this branch for now with it disabled I can point you to how to do that.</p> ;;;; <p>I think I get it now ! Thank you so much for helping me <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>I am new to Elephant and Mastodon (but liking them very much already).</p>
<p>One unexpected situation I’d see is the output of the prediction (Elephant &gt; Detection &gt; Predict Spots) being visible at a smaller scale (but shape is preserved) compare to the BDV display. I have attached a screenshot of the BDV here.</p>
<p>To reproduce this observation:</p>
<ol>
<li>Mastodon &gt; open h5/xml</li>
<li>Plugins &gt; Elephant &gt; Detection &gt; Reset Detection Model</li>
<li>Initialize the model with “Versatile” pre-trained model or default settings</li>
<li>Plugins &gt; Elephant &gt; Detection &gt; Predict Spots</li>
</ol>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548.png" data-download-href="/uploads/short-url/rYLdWjSGAynXnNrGyBBonjwSdCM.png?dl=1" title="Elephant_PredictSpots" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548_2_581x499.png" alt="Elephant_PredictSpots" data-base62-sha1="rYLdWjSGAynXnNrGyBBonjwSdCM" width="581" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548_2_581x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548.png 2x" data-dominant-color="262626"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Elephant_PredictSpots</span><span class="informations">656×564 81.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The image file used here is a 3D + time image stack saved in h5/xml pair. The xml file contains the XYZ size and unit. The Elephant docker was installed (on Linux) before Mastodon beta-26 was released. Elephant client is installed on another computer with the latest Fiji/ImageJ (versions below).</p>
<p>May I know if I have missed a step here?</p>
<p>Thank you very much, Catherine</p>
<p>Versions:<br>
Java 1.8.0_322 (64-bit)<br>
Mastodon beta-26<br>
Elephant (updated 0.4.2)</p> ;;;; <p>I’m just reading it with <code>pandas</code> - <code>pd.read_hdf(path_to_file)</code></p> ;;;; <p>The problem I have that the h5 file contain the video names and the images names (all the videos have images named from 0001 to 0500) while in the dataset there’s more than 8000 images. then i find the predictions with no indication to which image they belong.</p> ;;;; <p>It must be taken into account that:</p>
<p>The tubes can have different sizes, shapes and thicknesses</p>
<p>The main objective is to automate the process</p> ;;;; <p><a class="mention" href="/u/hiroalchem">@hiroalchem</a> the plugins above might solve your problem “out of the box”, but if you want you can use scikit-image for this. The general data structure that you need is called a <a href="https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_rag.html#sphx-glr-auto-examples-segmentation-plot-rag-py">region adjacency graph</a>! You can <a href="https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_boundary_merge.html#sphx-glr-auto-examples-segmentation-plot-boundary-merge-py">merge hierarchically</a> or you can <a href="https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_rag_mean_color.html#sphx-glr-auto-examples-segmentation-plot-rag-mean-color-py">threshold the graph</a>. The thresholding or merging is done by the “weight” attributes on the edges, but you change it to be the count easily:</p>
<pre><code class="lang-python">rag = graph.rag_mean_boundary(labels_image, edge_image)  # edge_image can be dummy e.g. np.ones_like

for u, v, data in rag.edges(data=True):
    data['weight'] = data['count']
</code></pre>
<p><code>rag</code> is a subclass of a <a href="https://networkx.org/documentation/stable/reference/index.html">networkx graph</a>, so you can look there for all the ways you can manipulate it! <img src="https://emoji.discourse-cdn.com/twitter/smiley.png?v=12" title=":smiley:" class="emoji" alt=":smiley:" loading="lazy" width="20" height="20"></p>
<p>I think the RAG code in scikit-image hasn’t been very battle tested, so if the API is a bit clunky, please raise an issue so we can improve it!</p> ;;;; <p>Thank you so much for the help ! Can i ask how do you visualize the h5 file like that ?</p> ;;;; <p>Jittering is normal. You can run <code>filterpredictions</code> with a small median filter to smoothen them out. (With videos in 30 fps window 3-5 should be fine)</p> ;;;; <p>And to get the ground truths you can simply concat all the <code>CollectedData</code> files in your labeled-data and sort both with the same index</p> ;;;; <p>For the most part, the detections look like they are located over the bodypart locations during the video (create_labeled_video).</p>
<p>They slightly jitter over the bodypart locations when the individuals are moving but it looks ok (from my understanding)</p> ;;;; <p>First think I thought when I saw this, given that it’s not using OpenCL but OpenGL, just like napari/vispy.</p> ;;;; <p>Oh, I get it now. The pickles are created for multianimal projects. In your case with single animal the <code>h5</code> file should contain the results for the model predictions indexed with video name and frame number (extracted frame file name from labeled-data). Like so:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048fa6e88c9f1d433c9e8365ff886349b2ede1c7.png" data-download-href="/uploads/short-url/ElGkQpKsv23SlrPHTXZbaia1wP.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048fa6e88c9f1d433c9e8365ff886349b2ede1c7.png" alt="image" data-base62-sha1="ElGkQpKsv23SlrPHTXZbaia1wP" width="690" height="227" data-dominant-color="353941"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1431×471 29.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Dear,</p>
<p>I have exactly the same problem, with the same model (joyful-deer). I am using the latest version of ilastik (1.4, 20 February 2023) with exactly the same error (see below). iLastik has version 0.5.5, which is higher than the highest required version (0.4.7).</p>
<p>Small add-on: k-dominik mentioned a possible bioimageio.core version issue (I see mentions of ‘marshmallow’ in the log). I tried/hoped to circumvent the network io error (network as in ‘the internet’?) by downloading the zip file and drag and drop it directly in the box in ilastik. It also failed with the same error. So I guess ‘network’ was intended as in ‘a neural network’.</p>
<p>Thank you!</p>
<p>ERROR 2023-03-22 13:17:50,533 excepthooks 1192 15436 Unhandled exception in thread: ‘MainThread’<br>
ERROR 2023-03-22 13:17:50,533 excepthooks 1192 15436 Traceback (most recent call last):<br>
File “C:\Program Files\ilastik-1.4.0rc6-gpu\lib\site-packages\ilastik\applets\neuralNetwork\modelStateControl.py”, line 282, in onModelInfoRequested<br>
model_info = load_raw_resource_description(model_uri)<br>
File “C:\Program Files\ilastik-1.4.0rc6-gpu\lib\site-packages\bioimageio\spec\io_.py”, line 177, in load_raw_resource_description<br>
raw_rd = schema.load(data)<br>
File “C:\Program Files\ilastik-1.4.0rc6-gpu\lib\site-packages\marshmallow\schema.py”, line 723, in load<br>
data, many=many, partial=partial, unknown=unknown, postprocess=True<br>
File “C:\Program Files\ilastik-1.4.0rc6-gpu\lib\site-packages\marshmallow\schema.py”, line 909, in _do_load<br>
raise exc<br>
marshmallow.exceptions.ValidationError: {‘format_version’: [‘Must be one of: 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7.’]}</p> ;;;; <p>Hi there,</p>
<p>really glad you’ve found the scripts useful! <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<blockquote>
<p>It seems the name of the annotations (ie what I have set the class as) isn’t coming with it when I then upload the annotations into HALO</p>
</blockquote>
<p>So from QuPath to NDPA, I do:</p>
<ul>
<li>Name → annot[‘title’]</li>
<li>PathClass → annot[‘details’]</li>
<li>displayed color → annot[‘color’]</li>
</ul>
<pre><code class="lang-auto">                    annot['title'] = pathObject.getName()
                    annot['details'] = pathObject.getPathClass()
                    annot['color'] = '#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)
</code></pre>
<p>Are you saying these three bits of information don’t get translated over to the NDPA file or that HALO doesn’t use them?</p>
<p>Unfortunately, the PathClass as details was a bit of a hack that I wasn’t sure other programs would be able to make use of.</p>
<p>And when you edit a contour in NDPView, there isn’t much more information you can add:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c7582adc1a0e9184d94f0d82cc54a76ebe9deca.jpeg" data-download-href="/uploads/short-url/dbVGV9PpZEmKDKtQfCKqmBGu30K.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c7582adc1a0e9184d94f0d82cc54a76ebe9deca.jpeg" alt="image" data-base62-sha1="dbVGV9PpZEmKDKtQfCKqmBGu30K" width="425" height="500" data-dominant-color="8D8E91"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">438×515 50.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>You can see this info in the XML file:</p>
<pre><code class="lang-xml">	&lt;ndpviewstate id=""&gt;
		&lt;title/&gt;
		&lt;details&gt;Front&lt;/details&gt;
		&lt;coordformat&gt;nanometers&lt;/coordformat&gt;
		&lt;lens&gt;0.445623&lt;/lens&gt;
		&lt;x&gt;25920&lt;/x&gt;
		&lt;y&gt;22784&lt;/y&gt;
		&lt;z&gt;0&lt;/z&gt;
		&lt;showtitle&gt;0&lt;/showtitle&gt;
		&lt;showhistogram&gt;0&lt;/showhistogram&gt;
		&lt;showlineprofile&gt;0&lt;/showlineprofile&gt;
		&lt;annotation type="freehand" displayname="AnnotateFreehand" color="#00ff00"&gt;
			&lt;measuretype&gt;0&lt;/measuretype&gt;
			&lt;closed&gt;1&lt;/closed&gt;
			&lt;pointlist&gt;
				&lt;point&gt;
</code></pre>
<p>There’s an additional <code>displayname="AnnotateFreehand"</code> tag but I’ve decided to leave it alone.</p>
<p>In Visiopharm, I think (but am probably wrong), the colour of the the outline is used to separate which contour type gets translated to which ROI / Label type.</p>
<p>I don’t have HALO unfortunately so I don’t know what it’s able to read and how. Would GeoJSON help? I wrote these functions (the first code works fine with a recent &gt;0.4.0 QuPath) modelled on saving NDPA files: <a href="https://forum.image.sc/t/issue-with-accented-names-with-my-annotations-geojson-import-export-script-attached/74621" class="inline-onebox">Issue with accented names with my annotations GeoJSON import/export script (attached)</a></p>
<p>If HALO can read GeoJSON, this would be less of a hack than using NDPA as an exchange format.</p>
<p>Cheers,<br>
Egor</p> ;;;; <p>Here’s what i get with plotting set to True.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/a/fa176acbc999beb4ec61fb17b4552684e7ad10ab.png" data-download-href="/uploads/short-url/zGpxoECohP0bXNYUucjGjFY5TaH.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/a/fa176acbc999beb4ec61fb17b4552684e7ad10ab.png" alt="image" data-base62-sha1="zGpxoECohP0bXNYUucjGjFY5TaH" width="690" height="245" data-dominant-color="F4F4F5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">981×349 20.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>As a side note, I wonder how hard it would be to align two image layers in napari by grabbing the <em>textures</em> that are already on the GPU… <img src="https://emoji.discourse-cdn.com/twitter/thinking.png?v=12" title=":thinking:" class="emoji" alt=":thinking:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/exploding_head.png?v=12" title=":exploding_head:" class="emoji" alt=":exploding_head:" loading="lazy" width="20" height="20"></p> ;;;; <p>That is really cool <a class="mention" href="/u/martgf">@MartGF</a>! As someone who frequently rants against monopolies I particularly love that you implemented it in OpenGL and not CUDA! <img src="https://emoji.discourse-cdn.com/twitter/joy.png?v=12" title=":joy:" class="emoji" alt=":joy:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/heart.png?v=12" title=":heart:" class="emoji" alt=":heart:" loading="lazy" width="20" height="20"></p> ;;;; <p>Can you show a screenshot of what is created inside the evaluation-results folder? Seems odd</p> ;;;; <p>I’m using DLC 2.3.2. I tried to do run the evaluation again with no success.</p> ;;;; <p>Thanks so much !! I will try.</p> ;;;; <p>Hi， I need your help, because my English is not very good, I hope this paragraph can clearly express my problem. Now I want to connect my piezoconcept with MM 2.0, and according to Piezoconcept, I have succeeded, thank you very much. I now want to use my XBox controller to take control of the shift console, piezoconcept, but I, according to the Web site, have not succeeded.<a href="https://micro-manager.org/ASI_Gamepad_Plugin" rel="noopener nofollow ugc">https://micro-manager.org/ASI_Gamepad_Plugin</a><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb538f85e198b32a4a3b5484fb731350e57bfaa2.png" data-download-href="/uploads/short-url/t0HXatOcWuYbFss3DFwzjXjmBI6.png?dl=1" title="图片" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb538f85e198b32a4a3b5484fb731350e57bfaa2.png" alt="图片" data-base62-sha1="t0HXatOcWuYbFss3DFwzjXjmBI6" width="624" height="500" data-dominant-color="686B6E"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">图片</span><span class="informations">808×647 30.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
I don’t know what the problem is. Hope you see this post.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bdd874d2be8cf4109930fc90625bee90878f252.png" data-download-href="/uploads/short-url/1GXSeTyhdwpIG9rL4f5N3StpRF8.png?dl=1" title="图片" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bdd874d2be8cf4109930fc90625bee90878f252.png" alt="图片" data-base62-sha1="1GXSeTyhdwpIG9rL4f5N3StpRF8" width="624" height="500" data-dominant-color="69696C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">图片</span><span class="informations">808×647 34.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Is that the reason?<br>
Thank you so much!  <img src="https://emoji.discourse-cdn.com/twitter/love_letter.png?v=12" title=":love_letter:" class="emoji" alt=":love_letter:" loading="lazy" width="20" height="20"></p> ;;;; <p>What if you run evaluation again? Which version of DLC are you on?</p> ;;;; <p>Can you <code>pip install --upgrade opencv-python==4.6.0.66</code>?</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17.jpeg" data-download-href="/uploads/short-url/nCuMTdroB8zXhs90PuV09OJGVUz.jpeg?dl=1" title="Intestinal tissue" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_690x271.jpeg" alt="Intestinal tissue" data-base62-sha1="nCuMTdroB8zXhs90PuV09OJGVUz" width="690" height="271" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_690x271.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_1035x406.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_1380x542.jpeg 2x" data-dominant-color="CBBFBF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Intestinal tissue</span><span class="informations">1900×747 206 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Hello out there!</p>
<p>I am about to begin a PhD project on intestinal health in piglets, and is therefore interested in using QuPath for image analysis as much as possible. I am currently using some control samples from our archive to train in using different features in QuPath. I have created a thresholder that identifies the tissue samples and created annotations named “Tissue”. Then I have trained a few pixel classifiers to identify the different layers of tissue within the gut segments, but when running the classifiers on my samples, it takes quite a lot of machine power, and I was wondering if it is possible to tell QuPath to only run the classifier within the selected annotations and not on the entire image? - When I choose the region for loading my pixel classifier to “Any annotation ROI” it still runs the classifier across almost the entire image, including a large amount of irrelevant background. <img src="https://emoji.discourse-cdn.com/twitter/see_no_evil.png?v=12" title=":see_no_evil:" class="emoji" alt=":see_no_evil:" loading="lazy" width="20" height="20"></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d.jpeg" data-download-href="/uploads/short-url/ys8BGymgcGtNgj7svA2n0qcjwGp.jpeg?dl=1" title="Classifier" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d_2_690x392.jpeg" alt="Classifier" data-base62-sha1="ys8BGymgcGtNgj7svA2n0qcjwGp" width="690" height="392" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d_2_690x392.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d_2_1035x588.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d.jpeg 2x" data-dominant-color="8B7FB7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Classifier</span><span class="informations">1353×769 166 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Following the pixel classification process, I have been able to create objects only based on the annotations, so the system works, and therefore this request is mainly for time-optimizing purposes <img src="https://emoji.discourse-cdn.com/twitter/smiley.png?v=12" title=":smiley:" class="emoji" alt=":smiley:" loading="lazy" width="20" height="20"></p>
<p>Any comments or suggestions will be highly appreciated! <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>opencv-python 4.7.0.72</p> ;;;; <p><a class="mention" href="/u/ep.zindy">@EP.Zindy</a> hope all is well! The script has been working great but I just have one question. It seems the name of the annotations (ie what I have set the class as) isn’t coming with it when I then upload the annotations into HALO. Is there something I can amend to have this as part of the output?</p> ;;;; <p>I just checked an older dlc project i had. There’s no pickle files in that folder also.</p> ;;;; <p>yes Konrad.</p> ;;;; <p>In the <code>project_folder\evaluation-results\iteration-#\etc.</code>?</p> ;;;; <p>… I meant to add, openslide is probably getting ICC profile support this year, so perhaps this issue will go away.</p> ;;;; <p>That’s an <code>opencv</code> error. Can you check <code>opencv</code> versions in the env?</p> ;;;; <p><a class="mention" href="/u/guiwitz">@guiwitz</a><br>
thanks, this line of code indeed solves it, don’t fully understand why yet <img src="https://emoji.discourse-cdn.com/twitter/thinking.png?v=12" title=":thinking:" class="emoji" alt=":thinking:" loading="lazy" width="20" height="20"> :- <img src="https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12" title=":sweat_smile:" class="emoji" alt=":sweat_smile:" loading="lazy" width="20" height="20">.</p>
<p><a class="mention" href="/u/wmv1992">@wmv1992</a><br>
Oh, I hadn’t seen this yet, thanks. I am biologist, who is using python and napari for image analysis. My initial idea was to make a tool, which would allow me to select several labels in a label layer using shapes and I wanted a lasso tool, which would make it easier to precisely select labels, and then plot some graphs based on the selection. A bit like napari-clusters-plotter plugin but the other way around.</p>
<p>I would love to make a plugin but I don’t yet really have the programming skills and time to get into the plugin creation.<br>
Yannick</p> ;;;; <p>Hey Konrad, All i get inside the file of the specific shuffle is h5 files that i talked about for every snapshot and a csv file for results on that specific shuffle. There’s no pickle files. I used the gui at first maybe that has something to do with it.</p> ;;;; <p>Hi everyone,</p>
<p>I’m currently trying to add a public user to our OMERO appliance prototype, to display public data. I tried to follow the official procedure (<a href="https://omero.readthedocs.io/en/stable/sysadmins/public.html" class="inline-onebox" rel="noopener nofollow ugc">Publishing data using OMERO.web — OMERO documentation</a>). It went OK (i use Docker containers with Ansible installation, and i added/adjusted the corresponding lines to get them compatible with 01-default-webapps.omero):</p>
<p>“”"</p>
<h1>
<a name="public-user-set-1" class="anchor" href="#public-user-set-1"></a>Public user set</h1>
<p>config set – omero.web.public.enabled True<br>
config set – omero.web.public.user ‘guest’<br>
config set – omero.web.public.password ‘guest’<br>
config set – omero.web.public.get_only true<br>
<a class="hashtag" href="/tag/config">#<span>config</span></a> set – omero.web.public.url_filter ‘^/webgateway’<br>
config set – omero.web.public.url_filter ‘^/(webadmin/myphoto/|webclient/(?!(script_ui|ome_tiff|figure_script))|webgateway/(?!(archived_files|download_as))|iviewer|api|3Dscript)’<br>
config set – omero.web.public.server_id 1<br>
“”"</p>
<p>The line “config set – omero.web.public.url_filter ‘^/webgateway’” didn’t changed anything to the main frontend, however, the line “config set – omero.web.public.url_filter ‘^/(webadmin/myphoto/|webclient/(?!(script_ui|ome_tiff|figure_script))|webgateway/(?!(archived_files|download_as))|iviewer|api|3Dscript)’” added an option to connect as a public user on the frontend:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e.png" data-download-href="/uploads/short-url/eLKEC05o5xALjpEJVhUGTvJKdEa.png?dl=1" title="Screenshot 2023-03-22 at 11-03-49 OMERO.web - Login" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_690x282.png" alt="Screenshot 2023-03-22 at 11-03-49 OMERO.web - Login" data-base62-sha1="eLKEC05o5xALjpEJVhUGTvJKdEa" width="690" height="282" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_690x282.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_1035x423.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_1380x564.png 2x" data-dominant-color="E0E4EA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-22 at 11-03-49 OMERO.web - Login</span><span class="informations">1637×670 40.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>However, by clicking on it, i have an error screen:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a.png" data-download-href="/uploads/short-url/jCg2lpsOOdi9Si8fQH2N0okKeM2.png?dl=1" title="Screenshot 2023-03-22 at 11-10-19 OMERO.web - support" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_690x284.png" alt="Screenshot 2023-03-22 at 11-10-19 OMERO.web - support" data-base62-sha1="jCg2lpsOOdi9Si8fQH2N0okKeM2" width="690" height="284" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_690x284.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_1035x426.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_1380x568.png 2x" data-dominant-color="F2F2F2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-22 at 11-10-19 OMERO.web - support</span><span class="informations">1637×675 52.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>With this message:</p>
<p>“”"<br>
Traceback (most recent call last):</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/django/core/handlers/exception.py”, line 47, in inner<br>
response = get_response(request)</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/django/core/handlers/base.py”, line 181, in _get_response<br>
response = wrapped_callback(request, *callback_args, **callback_kwargs)</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/decorators.py”, line 538, in wrapped<br>
retval = f(request, *args, **kwargs)</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/decorators.py”, line 597, in wrapper<br>
context = f(request, *args, **kwargs)</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/webclient/views.py”, line 575, in load_template<br>
return _load_template(request=request, menu=menu, conn=conn, url=url, **kwargs)</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/webclient/views.py”, line 495, in _load_template<br>
leaders, members = conn.getObject(“ExperimenterGroup”, active_group).groupSummary()</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py”, line 3271, in getObject<br>
result = self.getQueryService().findByQuery(</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py”, line 5102, in <strong>getattr</strong><br>
obj = self._obj or self._getObj()</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py”, line 5033, in _getObj<br>
self._obj = self._create_func()</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py”, line 5009, in cf<br>
obj = getattr(self._conn.c.sf, self._func_str)()</p>
<p>File “/opt/omero/web/venv3/lib/python3.8/site-packages/omero_API_ice.py”, line 758, in getQueryService<br>
return _M_omero.api.ServiceFactory._op_getQueryService.invoke(self, ((), _ctx))</p>
<p>omero.SecurityViolation: exception :<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/b/eb1c03e7926625ea7fc12539c025467ea8166d82.png?v=12" title=":omero:" class="emoji emoji-custom" alt=":omero:" loading="lazy" width="20" height="20">:SecurityViolation<br>
{<br>
serverStackTrace =<br>
serverExceptionClass =<br>
message = Access denied to guest user: omero.api.IQuery<br>
}</p>
<p>&lt;WSGIRequest: GET ‘/webclient/’&gt;<br>
“”"</p>
<p>So, what may be the root cause?</p>
<p>Also, i have two others questions, partially related to the problem:</p>
<ul>
<li>What is the purpose of the “guest” user? It seems to be different to a public user (i noticed that by looking in the various examples contained in <a href="https://github.com/orgs/ome/repositories" class="inline-onebox" rel="noopener nofollow ugc">Open Microscopy Environment · GitHub</a>)</li>
<li>How do enable the “default” group? It is normally created by default, but in my installation, it is disabled, and i can’t figured how to create it (in fact, i’m not even supposed to create it).</li>
</ul>
<p>Best regards and thanks by advance, Marc.</p> ;;;; <p>(base) PS C:\WINDOWS\system32&gt; conda activate DEEPLABCUT<br>
(DEEPLABCUT) PS C:\WINDOWS\system32&gt; python -m deeplabcut<br>
Loading DLC 2.3.2…<br>
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)<br>
Traceback (most recent call last):<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\runpy.py”, line 185, in _run_module_as_main<br>
mod_name, mod_spec, code = <em>get_module_details(mod_name, <em>Error)<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\runpy.py”, line 144, in <em>get_module_details<br>
return <em>get_module_details(pkg_main_name, error)<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\runpy.py”, line 111, in <em>get_module_details<br>
<strong>import</strong>(pkg_name)<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut_<em>init</em></em>.py", line 37, in <br>
from deeplabcut.create_project import (<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\create_project_<em>init</em></em>.py", line 12, in <br>
from deeplabcut.create_project.demo_data import load_demo_data<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\create_project\demo_data.py”, line 16, in <br>
from deeplabcut.utils import auxiliaryfunctions<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils_<em>init</em></em>.py", line 11, in <br>
from deeplabcut.utils.auxfun_multianimal import *<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils\auxfun_multianimal.py”, line 34, in <br>
from deeplabcut.utils import auxiliaryfunctions, conversioncode<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils\auxiliaryfunctions.py”, line 31, in <br>
from deeplabcut.pose_estimation_tensorflow.lib.trackingutils import TRACK_METHODS<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow_<em>init</em></em>.py", line 17, in <br>
from deeplabcut.pose_estimation_tensorflow.datasets import *<br>
File "C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets_<em>init</em></em>.py", line 13, in <br>
from .pose_deterministic import DeterministicPoseDataset<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\pose_estimation_tensorflow\datasets\pose_deterministic.py”, line 17, in <br>
from deeplabcut.utils.auxfun_videos import imread, imresize<br>
File “C:\Users\sie\anaconda3\envs\DEEPLABCUT\lib\site-packages\deeplabcut\utils\auxfun_videos.py”, line 25, in <br>
import cv2<br>
ImportError: DLL load failed while importing cv2: No se puede encontrar el módulo especificado.<br>
(DEEPLABCUT) PS C:\WINDOWS\system32&gt;</p> ;;;; <p>Thank you!</p> ;;;; <p>I’ll take a look at the code. Maybe the situation with <code>n_tracks</code> changing isn’t handled well down the line when labeled video is being created.</p> ;;;; <p>Please use the offical DLC Cookbook: <a href="https://deeplabcut.github.io/DeepLabCut/docs/recipes/installTips.html#installation-on-ubuntu-20-04-lts" class="inline-onebox">Installation Tips — DeepLabCut</a></p> ;;;; <p>I’ve run into this too. Leica use very large embedded ICC profiles, you can see them with <code>tiffinfo</code>:</p>
<pre><code class="lang-auto">$ tiffinfo  H2022013912S1-1-22_0e16f82f-9ca3-7b7f-6b6b-d360ad89bf1e_100821.svs
=== TIFF directory 0 ===
TIFF Directory at offset 0x59ccc0be (1506590910)
  Subfile Type: (0 = 0x0)
  Image Width: 149882 Image Length: 86028 Image Depth: 1
  Tile Width: 256 Tile Length: 256
  Bits/Sample: 8
  Compression Scheme: JPEG
  Photometric Interpretation: YCbCr
  Samples/Pixel: 3
  Planar Configuration: single image plane
  ImageDescription: Aperio Leica Biosystems GT450 v1.0.1 
149882x86028 [0,0,149882x86028] (256x256) JPEG/YCC Q=91|AppMag = 40|Date = 05/25/2022|Exposure Scale = 0.000001|Exposure Time = 8|Filtered = 3|Focus Offset = 0.000000|Gamma = 2.2|Left = 40.077018737793|MPP = 0.264565|Rack = 1|ScanScope ID = SS45050|Slide = 10|StripeWidth = 4096|Time = 10:08:22|Time Zone = GMT+0200|Top = 23.170736312866
  ICC Profile: &lt;present&gt;, 13113264 bytes
</code></pre>
<p>You can see this slide is from a GT450, and includes a 13 megabyte (!!! GOOD LORD !!! this is insane) profile.</p>
<p>libvips uses openslide to load this type of TIFF, and openslide has no ICC profile support, so unfortunately you don’t see the profile if you do a default load. You can see it if you force libvips to use its standard TIFF loader:</p>
<pre><code class="lang-auto">$ python3
Python 3.10.7 (main, Mar 10 2023, 10:47:39) [GCC 12.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import pyvips
&gt;&gt;&gt; x = pyvips.Image.tiffload("H2022013912S1-1-22_0e16f82f-9ca3-7b7f-6b6b-d360ad89bf1e_100821.svs")
&gt;&gt;&gt; len(x.get("icc-profile-data"))
13113264
&gt;&gt;&gt; 
</code></pre>
<p>Applying the profile in the standard way does indeed fix the colour.</p> ;;;; <p>It will be easiest to make a separate model, since the one you have is heavily biased towards white rats. But with more and more data you can think of combining it into one model (just keep the bodypart number and names the same between both projects).</p>
<p>An interesting thing to try out might be color inversion on the video - it’s something I alwyas wanted to try out, but don’t have the data (since this could potentially become a good augmentation method for training models for both black and white rodents)</p>
<p>There is also an approach like the one used in superanimal models, but I guess that requires way more data - and I haven’t read the paper fully yet, so I’m not sure about the process.</p> ;;;; <p>This is a prinout of the training progress. Not sure which iteration you’re on, but this loss will probably not get much better.</p> ;;;; <p>When you run evaluation in the evaluation results you get the folder for the iteration and inside that folder a <code>csv</code> file with combined results and a folder for a specific trainset/shuffle. Inside that folder there should be 5 files, 2 <code>csv</code> and 3 <code>pickle</code>. Default run of <code>deeplabcut.evaluate_network(config)</code> should create those</p> ;;;; <p>Can you post a screenshot of the full traceback?</p> ;;;; <p>Thank you for the reply <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>In the meantime I also found another way to do it. I found out that I could simply just create an annotation over an area with a high fungal load and export my ROI to ImageJ, in which there is a built-in angle-tool. I know, that the measurements are then not done in QuPath per se, but I works very smooth and easy for this particular problem. <img src="https://emoji.discourse-cdn.com/twitter/smiley.png?v=12" title=":smiley:" class="emoji" alt=":smiley:" loading="lazy" width="20" height="20"></p>
<ul>
<li>Maybe others who have run into a similar problem could also benefit from this simplified and manual version of a solution <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20">
</li>
</ul> ;;;; <p>When you watch the labeled video, are all the detections correct and nothing is missing?</p> ;;;; <p>You should save the point layer only (<code>CollectedData</code>), shouldn’t crash then.</p>
<p>A little weird that the <code>machinelabels</code> files would casue those issues, it should just load to point layers in that case. Deleting the files is fine. Especially if you already corrected them and merged into the dataset.</p> ;;;; <p>Ok, maybe this helps?</p><aside class="quote quote-modified" data-post="2" data-topic="6234">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/etadobson/40/17_2.png" class="avatar">
    <a href="https://forum.image.sc/t/fiji-macro-does-not-wait-a-command-finished-before-running-the-next-command/6234/2">Fiji macro does not wait a command finished before running the next command</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    <a class="mention" href="/u/tryphon">@Tryphon</a> 
I have never had to run such a thing in a macro… however, there are a ton of people on the Forum who have - of course! 
Just to get you started in testing a few things… there should be a few <a href="https://imagej.net/ij/developer/macro/functions.html">Built-In Macro Functions</a> (a great place to search for applicable functions) that could help, such as: 
wait(n) 
Delays (sleeps) for n milliseconds. 
waitForUser(string) 
Halts the macro and displays string in a dialog box. The macro proceeds when the user clicks “OK”. Unlike showMessage, the dialog…
  </blockquote>
</aside>
 ;;;; <p>Could you show <code>pyqt</code> and <code>pyside</code> versions installed in your environment?</p> ;;;; <p><a class="mention" href="/u/rodrigo_rb">@Rodrigo_RB</a> sorry for the slow reply<br>
Are you importing data using the Desktop client OMERO.insight?<br>
We are currently looking at a problem in the client not closing resources properly on the server.<br>
This could be the source of the problem see <a href="https://github.com/ome/omero-insight/pull/354" class="inline-onebox">Close all HandleTie servants created during import by sbesson · Pull Request #354 · ome/omero-insight · GitHub</a></p>
<p>Cheers</p>
<p>Jean-Marie</p> ;;;; <p>Hi <a class="mention" href="/u/swa">@Swa</a></p>
<p>You can use the FFT and inverse FFT in Fiji but it is more manual than running a bandpass filter:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bdf395400d164ae4c033953b9b8684c755b56451.png" alt="FFT-to-remove-regular-pattern" data-base62-sha1="r6o7ciOzpW4U1iW0l15E2F7wU6J" width="490" height="176"></p>
<p>Original:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93.jpeg" data-download-href="/uploads/short-url/quJBqrQTanEm85r5Ts6Kw4xAI5Z.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_499x499.jpeg" alt="image" data-base62-sha1="quJBqrQTanEm85r5Ts6Kw4xAI5Z" width="499" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_499x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_748x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_998x998.jpeg 2x" data-dominant-color="848484"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1437×1437 593 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
“Cleaned up” image:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51.jpeg" data-download-href="/uploads/short-url/6LqufhdHc0JpX3AHGw9A1QB5GQF.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_499x499.jpeg" alt="image" data-base62-sha1="6LqufhdHc0JpX3AHGw9A1QB5GQF" width="499" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_499x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_748x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_998x998.jpeg 2x" data-dominant-color="767676"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1437×1437 371 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Does this make sense?</p>
<p>Best wishes,<br>
Marie</p> ;;;; <p>Hi all,</p>
<p>We’ve written a Python module for GPU-accelerated drift correction / image registration, that we call pyGPUreg. It is more than 40x faster than StackReg (the Python port of TurboReg) in our tests (using a Quadro P2200), and does not require a CUDA compatible GPU.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449.png" data-download-href="/uploads/short-url/h3vtifcuWEyoA4q23Cbx4pJwZFT.png?dl=1" title="pyPGUreg_vs_StackReg" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449_2_690x274.png" alt="pyPGUreg_vs_StackReg" data-base62-sha1="h3vtifcuWEyoA4q23Cbx4pJwZFT" width="690" height="274" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449_2_690x274.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449_2_1035x411.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449.png 2x" data-dominant-color="F6F6F6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">pyPGUreg_vs_StackReg</span><span class="informations">1048×417 51.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>We wrote it for our own use, but are wondering whether it may be useful for others as well; maybe it is worth integrating it as an ImageJ plugin or into CLIJ?</p>
<p>Registration with pyGPUreg is based on phase correlation. Input images are uploaded to the GPU, and forward FFTs, the phase correlation, and a reverse FFT are computed on the GPU as well, after which the resulting image is transferred to the CPU where the image shift is detected. This shift is used to launch a second GPU process, which re-samples on of the the input images and returns the registered image to the CPU.</p>
<p>It is available via pip (pip install pyGPUreg) and the code is available here: <a href="https://github.com/bionanopatterning/pyGPUreg" class="inline-onebox" rel="noopener nofollow ugc">GitHub - bionanopatterning/pyGPUreg</a><br>
It is still somewhat of a work in progress, so it comes with no guarantees that it really works. But it does work for us.</p>
<p>We’d be happy to be contacted by anyone who is interested!</p> ;;;; <p>Hi,</p>
<p>I am using a windows. I have tried to set up a conda environment for stardist 0.83 where in this case I have a program with just a printing of “Hello World”.</p>
<p>from stardist.models import StarDist2D<br>
from stardist.data import test_image_nuclei_2d</p>
<p>print(“Hello World!!”)</p>
<p>However the executable  always fails. I have tried many different sets of environments with different versions of Tensorflow 2 (2.4, 2.10) and corresponding set of Cuda. I tried also some different versions of Python. Have anyone succeeded in creating a working executable with a contained stardist and which type of environment was then used (exact versions and their installation order would be appreciated) ?</p>
<p>Kind regards<br>
Fredrik Olsson</p> ;;;; <p>We are currently generating some of our TIF files in the 24 bit RGB format.<br>
As part of the upgrade of our app to use the OME.TIF format, we are struggling with those as the Pixel.Type enumeration does not seem to have anything for 24rbg tifs.</p>
<p>see <a href="https://www.openmicroscopy.org/Schemas/Documentation/Generated/OME-2016-06/ome_xsd.html#Pixels" class="inline-onebox" rel="noopener nofollow ugc">Schema documentation for ome.xsd</a></p>
<p>Is there a way to support 24bit rgb tifs as a single TiffData in an OME.TIF file or do we need to separate the 8 bit channels and store them as separate TiffDatas?<br>
Or is there a better way?</p>
<p>Thanks in advance</p>
<p>Eddy</p> ;;;; <p>Hi <a class="mention" href="/u/kda9">@kda9</a> ,<br>
the plugin works without problems for me (on ubuntu).<br>
Do you get the error before or after the gui comes up?<br>
Best,<br>
Volker</p> ;;;; <p><a class="mention" href="/u/tibuch">@tibuch</a> Ah - OK, I see. I was being a bit selective on what I read: “we decided to create another zarr group (<em>projections</em>) in each well group” - but yes, I see you’re doing the same as labels.</p>
<p><a class="mention" href="/u/imagejan">@imagejan</a> I think that the labels under an Image must have the same dimensions, so you couldn’t have a projected 2D labels under a 3D image.</p> ;;;; <p>Hi <a class="mention" href="/u/yannick_blum">@Yannick_Blum</a>,</p>
<p>Are you looking to add this as part of a plugin or to contribute to Napari? In any case I would like you to point to a PR that exists regarding adding draw polygon functionality to Napari <a href="https://github.com/napari/napari/pull/5555" rel="noopener nofollow ugc">PR #5555</a>. If you would like to go through the code sometime, please let me know. Also feel free to try out and give feedback on your user experience:)</p>
<p>Wouter-Michiel</p> ;;;; <p>Hi Edward,</p>
<p>I’ve had to do quite some troubleshooting with a Linkam CMS196 in Python, so I think I may be able to help out - feel free to contact me. Some code can be found here (but mind that there are some hard-coded bits in there):</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py">
  <header class="source">

      <a href="https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py" target="_blank" rel="noopener nofollow ugc">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py" target="_blank" rel="noopener nofollow ugc">bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py</a></h4>


      <pre><code class="lang-py">from ctypes import *
import Parameters as prm
lsdk = None
stageHandle = c_int(0)
linkamProcessMessageCommon = None
stageLims = [(100.0, 10000.0), (100.0, 2600.0)]
connection = False
simulated = False
simulatedPosition = [0.0, 0.0]
from Utility import *
from os import system

class CommsInfo(Structure):
    _fields_ = [
        ('info', c_char*124),
    ]

class USBCommsInfo(Structure):
    _fields_ = [
        ('vendorID', c_uint16),
</code></pre>



  This file has been truncated. <a href="https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py" target="_blank" rel="noopener nofollow ugc">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <aside class="quote no-group" data-username="wmv1992" data-post="15" data-topic="77570">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/wmv1992/40/55089_2.png" class="avatar"> Wouter-Michiel Vierdag:</div>
<blockquote>
<p>Yes, the recordings will be made available very soon. <a class="mention" href="/u/joshmoore">@joshmoore</a> will put them here:</p>
</blockquote>
</aside>
<p>Files are now uploaded to <a href="https://downloads.openmicroscopy.org/presentations/2023/NGFF-community-call-2023-03-15/" class="inline-onebox">Index of /presentations/2023/NGFF-community-call-2023-03-15</a></p>
<aside class="quote no-group" data-username="aaxx" data-post="17" data-topic="77570">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/aaxx/40/59505_2.png" class="avatar"> Aliaksei Chareshneu:</div>
<blockquote>
<p>Could you tell me please if migration scripts from v0.4 or earlier versions will be provided?</p>
</blockquote>
</aside>
<p>The intention, Aliaksei, is very much that migration scripts will always be provided between released versions.</p>
<p>~J.</p> ;;;; <aside class="quote no-group" data-username="tibuch" data-post="4" data-topic="78868">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tibuch/40/30_2.png" class="avatar"> Tim-Oliver Buchholz:</div>
<blockquote>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── projections    
    │   │   │   │   └── 0         # zeroth resolution level projection
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
    │   │   │   │   ├── projections 
    │   │   │   │   │   └── 0     # labels corresponding to A/1/0/projections/0
</code></pre>
</blockquote>
</aside>
<p>Instead of that layout, we also thought of nesting the <code>labels</code> inside <code>projections</code>:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── labels   
    │   │   │   │   └── 0        # labels corresponding to A/1/0/0
    │   │   │   ├── projections    
    │   │   │   │   ├── 0        # zeroth resolution level projection
    │   │   │   │   └── labels
    │   │   │   │       └── 0    # labels corresponding to A/1/0/projections/0
</code></pre>
<p>(Would either layout match the requirements by the <a href="https://ngff.openmicroscopy.org/latest/#labels-md"><code>labels</code> spec</a>?)</p> ;;;; <p>How should I test it?</p>
<p>I downloaded the release, then I update? I’m getting a bunch of “locally modified files”, so I skipped the updating and just copied the plugin jar files over.</p>
<p>I don’t notice any changes in my plugin, and everything seems to be working.</p>
<p>When I go to open a file the file dialogue looks much worse for some reason. It seems to be that way for all of my Fiji versions though. When I just use <code>java -jar ij-1.54c.jar</code> the open file dialog looks nicer.</p> ;;;; <p>Hi <a class="mention" href="/u/yannick_blum">@Yannick_Blum</a>,</p>
<p>I tested your script and it seems to work without problem for me. The only thing you have to fix is to add a condition to add the most recent shape. It should only be added if the shape has more than one vertex. This is why you have to click twice: it generates an error when you try to add a shape with a single vertex the first time you click in the viewer. When you click again, you have two vertices and it works. This fixes it:</p>
<pre><code class="lang-python">if len(contour) &gt; 1:
    shapes.append(np.stack(contour))
</code></pre>
<p>I attach a short video of what I get. Is this what you expect? Maybe your napari is old? It worked for 0.4.15 and 0.4.17 for me.<br>
</p><div class="video-container">
    <video width="100%" height="100%" preload="metadata" controls="">
      <source src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e0d945507cdbcb933d4db21ac390d5a629c009.mp4">
      <a href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e0d945507cdbcb933d4db21ac390d5a629c009.mp4" rel="noopener nofollow ugc">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e0d945507cdbcb933d4db21ac390d5a629c009.mp4</a>
    </source></video>
  </div><p></p>
<p>Guillaume</p> ;;;; <p>After pip install --upgrade tensorflow-io-gcs-filesystem==0.27.0 the same error Dll load failed!</p>
<p>I don’t know what to do!</p>
<p>I really appreciate help!</p> ;;;; <p>Thank you Konard for the quick response ! I can’t seem to find the file you’re describing. Maybe there’s parameters i can change to make deeplabcut generate the file you’re talking about ?<br>
Thank you in advance.</p> ;;;; <aside class="quote no-group" data-username="dsudar" data-post="3" data-topic="78868">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/dsudar/40/13417_2.png" class="avatar"> Damir Sudar:</div>
<blockquote>
<p>Maybe your efforts to define acquisition_metadata can make use of the NBO(Q) approach and definitions explained here: <a href="https://www.nature.com/articles/s41592-021-01327-9">Towards community-driven metadata standards for light microscopy: tiered specifications extending the OME model | Nature Methods </a> ?</p>
</blockquote>
</aside>
<p>Our current metadata is clearly lacking. I think it makes sense for us to look more into this effort and see how we can best make use of it. Is it correct, that one would use the MicroMetaApp to create the json file and then store it together with the zarr file? Is there somewhere an example which uses MicroMetaApp and ome-zarr? Would be very curious to try this out!</p> ;;;; <p>Thanks for the feedback!</p>
<aside class="quote no-group" data-username="will-moore" data-post="2" data-topic="78868">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/will-moore/40/17250_2.png" class="avatar"> Will Moore:</div>
<blockquote>
<p>I don’t understand why you have 4-channel images for raw and projections, but in each case 1 channel is all zeros?</p>
</blockquote>
</aside>
<p>This dummy dataset might not be the best visualization, since all channels show the same information. In practice the four channels would correspond to four different types of structure. Where for the first two channels all z-planes are required, for channel 3 a z-stack is acquired but the microscope computes the maximum projection on the fly and discards the z-planes and for channel 4 only a single plane is necessary.<br>
In the case where we have zeros we would configure zarr to not write empty chunks.</p>
<aside class="quote no-group" data-username="will-moore" data-post="2" data-topic="78868">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/will-moore/40/17250_2.png" class="avatar"> Will Moore:</div>
<blockquote>
<p>I guess you’re storing projections under Wells (instead of under each Image) to make it easier to view the whole plate as projection? (rather than View a single Image and turn on/off projection view)?</p>
</blockquote>
</aside>
<p>We wanted to do it as it would be done for labels, but we might have made a mistake here. My understanding of the labels structure is the following:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                     # row
    │   ├── 1                 # column
    │   │   ├── 0             # zeroth field
    │   │   │   ├── 0         # zeroth resolution level
    │   │   │   └── labels    
</code></pre>
<p>With projections it is currently looking like this:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── projections    
    │   │   │   │   └──0         # zeroth resolution level projection
</code></pre>
<p>And then with labels it would look like this:</p>
<pre><code class="lang-auto">plate.zarr               
    ├── A                        # row
    │   ├── 1                    # column
    │   │   ├── 0                # zeroth field
    │   │   │   ├── 0            # zeroth resolution level
    │   │   │   ├── projections    
    │   │   │   │   └── 0         # zeroth resolution level projection
    │   │   │   ├── labels   
    │   │   │   │   ├── 0         # labels corresponding to A/1/0/0
    │   │   │   │   ├── projections 
    │   │   │   │   │   └── 0     # labels corresponding to A/1/0/projections/0
</code></pre> ;;;; <p>Yes, this is the issue. The next command in the actual workflow was to split the channels which doesn’t work as the input image is single channel.</p>
<p>I think the strangest thing is that if I run it NOT in batch mode and NOT in headless mode, but do call the macro from the command line, the same behaviour happens.</p> ;;;; <p>hi there, we didnt manage to test it very soon. But we have now installed it and used in on all our microscopes and it’s super handy for our application!<br>
So far, we havent found any glitches <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"><br>
Thank you so much <a class="mention" href="/u/nicost">@nicost</a> for implementing this!!</p> ;;;; <p>At least for me the problem was, that in batch mode, the image would not open at the right point in time (in the macro) but only at the end - and this of course screws with the flow. Headless was the same. So the error is subtle - you assume that a new image popped up - probably rename it, but you are actually doing it to the last image that was open, not to the output from the plugin.</p> ;;;; <p>This error occur when I open the project after I saved it. I cannot open the image. May I know how can I fix this.</p>
<p>Thanks.</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/668d766d2c753e867d8a30bcf18937686c0c9431.png" alt="QP error" data-base62-sha1="eDdMCwyDP1vgBvtdq9ADrAVgW65" width="527" height="258"></p> ;;;; <aside class="quote no-group" data-username="DrLachie" data-post="1" data-topic="78856">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/drlachie/40/17454_2.png" class="avatar"> DrLachie:</div>
<blockquote>
<p>the same thing happens and the error message pops up</p>
</blockquote>
</aside>
<p>Hi, which error message is it?</p>
<p>And, sorry I did not fully understand yet, is there any scenario where it <em>does</em> work?</p>
<p>General comment: I would recommend using tools like NextFlow for creating image analysis workflows including ilastik.</p> ;;;; <p>Hi,<br>
I am trying to create a lasso tool so I can create polygons in the shape layer by click and dragging the mouse. Here is the code I have so far:</p>
<pre><code class="lang-auto">import napari

viewer = napari.Viewer()
#label_layer=viewer.add_labels(remap)
#shapes = None
shape = viewer.add_shapes()#data=shapes,shape_type='polygon',edge_width=4,edge_color='coral', face_color='royalblue')
shape.blending = 'additive'

contour=[]

@shape.mouse_drag_callbacks.append
def update_selection(layer, event):
    
    if len(contour)&gt;1:
        contour.clear()
    
    print(shape.data)
    shapes_len = len(shape.data)
    yield
    
    while 'Alt'in event.modifiers and event.button == 1: #'Alt'in event.modifiers and
        
        data_coordinates = layer.world_to_data(event.position)
        cords = np.round(data_coordinates).astype(int)
        contour.append(cords)
    
        #time.sleep(0.01)
        shapes=shape.data
        
        if len(shapes)&gt;shapes_len:   
            shapes.pop(-1)
        
        shapes.append(np.stack(contour))
        shape.data=shapes
        
        yield 
    return
</code></pre>
<p>This works to create one polygon by click and dragging but for some reason when I want to create a second and I am triggering the mouse_drag_callback function my previous shape gets deleted (why I put the print statement there to figure out). I don’t understand why, but then I guess I don’t fully understand what the mouse decorator does.</p>
<p>Also there seems to be a lag to get in the while loop to draw the polygon. I need to click twice to initiate it. Any idea why this is?</p>
<p>Thanks in advance<br>
Yannick</p> ;;;; <p>Dear all,</p>
<p>Could you tell me please if migration scripts from v0.4 or earlier versions will be provided?</p>
<p>Thank you,<br>
Aliaksei</p> ;;;; <p>The likelihood scores are 1 for all frames and all bodyparts. I also notice that the conf score in the dist_20000 CSV file in the evaluation-results folder is 1 for all frames as well.</p>
<p>Not sure if this is contributing to the likelihood scores in the CSV files in the videos folder.</p>
<p>Thanks</p> ;;;; <p>I apologize deeply for my delayed response!</p>
<p>I encountered an error message even when using the dropdown menu too.<br>
Initially, I thought the error was random, but later discovered that it was caused by two files in the folder named “machinelables.csv” and “machinelables-iter0.h5”. After deleting these two files, I was able to drag the folder into the labeling GUI without any issues.</p>
<p>May I ask if deleting these two files will have any impact on the entire project?<br>
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>
Regarding the issue of DLC GUI crashing, the entire GUI crashes when I click on “Save All Layers”, and my labels are not saved.<br>
There are only two layers, “images” and “CollectedData” in the labeling GUI.</p> ;;;; <aside class="quote no-group" data-username="Maro" data-post="1" data-topic="78882">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/96bed5/40.png" class="avatar"> Maro:</div>
<blockquote>
<p>RoiManager.setGroup</p>
</blockquote>
</aside>
<p>Quick search of the code make it look like setGroup takes an int that is hardcoded to be limited to less than 255</p><aside class="onebox githubblob" data-onebox-src="https://github.com/imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868">
  <header class="source">

      <a href="https://github.com/imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868" target="_blank" rel="noopener">imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868</a></h4>



    <pre class="onebox"><code class="lang-java">
      <ol class="start lines" start="1858" style="counter-reset: li-counter 1857 ;">
          <li>		return groupNamesString;</li>
          <li>	}</li>
          <li>
          </li>
<li>	/** Sets the group names from a comma-delimeted string. */</li>
          <li>	public static void setGroupNames(String names) {</li>
          <li>		groupNamesString = names;</li>
          <li>		groupNames = null;</li>
          <li>	}</li>
          <li>
          </li>
<li>	/** Sets the group of this Roi, and updates stroke color accordingly. */</li>
          <li class="selected">	public void setGroup(int group) {</li>
          <li>		if (group&lt;0 || group&gt;255)</li>
          <li>			throw new IllegalArgumentException("Invalid group: "+group);</li>
          <li>		if (group&gt;0)</li>
          <li>			setStrokeColor(getGroupColor(group));</li>
          <li>		if (group==0 &amp;&amp; this.group&gt;0)</li>
          <li>			setStrokeColor(null);			</li>
          <li>		this.group = group;</li>
          <li>		if (imp!=null) // Update Roi Color in the GUI</li>
          <li>			imp.draw();</li>
          <li>	}</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
So there may not be a way around this without editing the source code. <a class="mention" href="/u/wayne">@Wayne</a> ?</p> ;;;; <aside class="quote no-group quote-modified" data-username="ctrueden" data-post="1" data-topic="78852">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png" class="avatar"> Curtis Rueden:</div>
<blockquote>
<p>|@mdoube|BoneJ2</p>
</blockquote>
</aside>
<p>Looks OK to me - checked through menu commands and nothing untoward happened.</p> ;;;; <p>Hi <a class="mention" href="/u/tibuch">@tibuch</a> ,</p>
<p>Looks like a nice approach to make use of the OME-NGFF HCS structure and add some solid functionality. I have one comment on the Extra Metadata area and specifically the acquisition_metadata: as part of QUAREP (<a href="https://quarep.org/working-groups/wg-7-metadata/" rel="noopener nofollow ugc">https://quarep.org/working-groups/wg-7-metadata/</a>), there is significant ongoing work on defining comprehensive microscopy metadata (currently known as NBO(Q)) starting with the OME model. The goal is to make that compatible with OME-NGFF and thus implement it initially as JSON and as JSON-LD in the (near) future. Maybe your efforts to define acquisition_metadata can make use of the NBO(Q) approach and definitions explained here: <a href="https://www.nature.com/articles/s41592-021-01327-9" class="inline-onebox" rel="noopener nofollow ugc">Towards community-driven metadata standards for light microscopy: tiered specifications extending the OME model | Nature Methods</a>  ?<br>
A few example JSON files that were generated with the MicroMetaApp (an application that works with the NBO(Q) model) can be found here: <a href="https://github.com/WU-BIMAC/MicroMetaApp.github.io" class="inline-onebox" rel="noopener nofollow ugc">GitHub - WU-BIMAC/MicroMetaApp.github.io: Microscopy Metadata for the real world!</a></p>
<p>Cheers,<br>
Damir</p> ;;;; <p>Hi,</p>
<p>I am confused about the Local root path. I want to run parallel jobs on Linux cluster where my data is stored. Why it needs the local root path when it not linked to the local computer? Thanks</p> ;;;; <p>Hi, as the subject says. We trained white rats (social behavior) on sufficient number of frames, but we have another experiment with black rats on the white floor. Shall I add such novel frames to the existing training set (and retrain with more iterations) or, should that be a separate model? Can’t find info on that… Thanks | P</p> ;;;; <p>Found this related post for KymographBuilder:</p><aside class="onebox githubissue" data-onebox-src="https://github.com/fiji/KymographBuilder/issues/13#issuecomment-399985738">
  <header class="source">

      <a href="https://github.com/fiji/KymographBuilder/issues/13#issuecomment-399985738" target="_blank" rel="noopener nofollow ugc">github.com/fiji/KymographBuilder</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/fiji/KymographBuilder/issues/13#issuecomment-399985738" target="_blank" rel="noopener nofollow ugc">Doesn't work at all with new version of Fij</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2018-05-14" data-time="14:48:59" data-timezone="UTC">02:48PM - 14 May 18 UTC</span>
      </div>

        <div class="date">
          closed <span class="discourse-local-date" data-format="ll" data-date="2018-06-25" data-time="15:36:27" data-timezone="UTC">03:36PM - 25 Jun 18 UTC</span>
        </div>

      <div class="user">
        <a href="https://github.com/jochenkrattenmacher" target="_blank" rel="noopener nofollow ugc">
          <img alt="jochenkrattenmacher" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d20e95943d4cec95fcc4ae5bf1dd50249ab4c18f.png" class="onebox-avatar-inline" width="20" height="20">
          jochenkrattenmacher
        </a>
      </div>
    </div>

    <div class="labels">
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">There is a drop-down menu popping up asking me to select "ImageDisplay", and whe<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">n I press OK it says "'imagedisplay' is required but unset."</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
(It seems this warning shouldn’t affect the function.)<br>
I also tested it on an image and did get a different output when changing the line width.</p> ;;;; <p><a class="mention" href="/u/gabriel">@gabriel</a> What I meant was the length of the touching borders.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/c/6cc8cd19c1f8af5b55f7a71d39f663b3e20cb221.png" data-download-href="/uploads/short-url/fwlMrtgjR07fHHrgEu8lc37lwIh.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/c/6cc8cd19c1f8af5b55f7a71d39f663b3e20cb221.png" alt="image" data-base62-sha1="fwlMrtgjR07fHHrgEu8lc37lwIh" width="531" height="500" data-dominant-color="D7C9C8"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">566×532 3.75 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Thank you! I will check it.</p> ;;;; <p>I reinstalled pyqt and pyside, but still get the same error. Do you know what this error means?</p> ;;;; <p>Thanks for the quick response! I’ll give the rosetta version a shot, sounds promising</p> ;;;; <p>Hi Peter,</p>
<p>I read the paper, and I’ve looked at the code again and I don’t think I understand how the cast_ray function is calculating the fiber diameter. Please do you understand how it does it or is there any other link you can send that helps explain this. I tried googling other ray casting functions, but I couldn’t find any that does something similar to what I think the cast_ray function in quanfima does.</p> ;;;; <p>Sorry, Konrad - can you please clarify what you mean with: is the prediction always perfect?</p>
<p>Do you mean watching and checking the videos? Or looking at different coordinate values in the CSV files, especially the filtered CSV file compared to the analyze_video CSV file?</p>
<p>Thanks</p> ;;;; <p>I see! Thank you so much!<br>
After that message shows up, my terminal’s filled with lines that say<br>
iteration: __000 loss: 0.0015 scmap loss: 00.0014 locref loss: 0.0000 limb loss: 0.0000 lr: 5e-05<br>
I think at least 20 of these lines showed up in the 2 days I left it running. Previously, my computer crashed and after a day the terminal shut itself off, but it’s been running for the past 3 days. Does this message mean my computer’s learning? Or should I do something else?</p> ;;;; <p>Hi,</p>
<p>Is there an implementation of the FFT Bandpass filter (or similar) that allows suppressing stripes of custom orientations (e.g. not vertical or horizontal)?</p>
<p>Thanks a lot</p> ;;;; <aside class="quote no-group" data-username="svkremer" data-post="1" data-topic="78866">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/s/5fc32e/40.png" class="avatar"> Sarah:</div>
<blockquote>
<p>My end goal is to analyze individual channels in CellProfiler.</p>
</blockquote>
</aside>
<p>Not sure about the channel handling, that might have changed recently, <a class="mention" href="/u/petebankhead">@petebankhead</a> or <a class="mention" href="/u/finglis">@finglis</a> might know more, but you shouldn’t need to split the channels for CellProfiler, it should handle multichannel files. <a href="https://carpenter-singh-lab.broadinstitute.org/blog/input-modules-tutorial" class="inline-onebox">Input Modules Tutorial | Carpenter-Singh Lab</a><br>
The existence of the other channels doesn’t prevent you from ignoring them.</p> ;;;; <p>Hi,</p>
<p>The link provided in the job description is not working for me.</p>
<p>Would you please guide me, how to apply for this position?</p>
<p>Thank you!</p> ;;;; <p>An interesting problem…<br>
Apart from the size, what do you mean <em>"the way it touches the neighboring segment</em>"? Are you thinking along the lines of perimeter length or the geometry of the contact?</p> ;;;; <p>Receiving error for line widths &gt;1 when creating kymographs. Looking to fix this issue for wider image and longer movement record.</p>
<p>Adding a new straight line to a .tif file and updating the line width to 30. Attempted plug-ins KymographBuilder and KymoResliceWide with both showing no more than 1 width errors. Attempted on multiple computers with separate Fiji downloads and have the same issue. How do I solve this?</p> ;;;; <p>Hi, bumping this thread since I’m having the same issue. I’ve read the various posts on here about this, and have tried increasing the max memory for java but without any change.</p>
<p>For context, in my case, I’m trying to make a pipeline for ~500 mb *.czi images. (2D, 4 color tile scans) I’m on a workstation with 192 gb of RAM and have allocated a max of 100 gb to java. However, even with only 1 image, as soon as I go into test mode, I get the above error. Any advice? (This workstation is running Windows 7 if that makes a difference!)</p> ;;;; <p>The color is supposed to be gold, and the background is supposed to be black.</p> ;;;; <p>I previously ran into this issue while I was trying to run multiple objects to be exported to database. However, I only have one ‘object’ selected currently, and I am still receiving an error when I begin to analyze images. Is there a way to avoid this? Thank you</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f3e3eb25ac1e60484e878ef344cce703fe4fda4.png" alt="error1" data-base62-sha1="6JVNC1i5C4eqxFcbPKdTlbZmOyw" width="649" height="301"><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3.png" data-download-href="/uploads/short-url/czWvj2dMfaF18mQ4lDLnUEBAQOD.png?dl=1" title="error2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_690x449.png" alt="error2" data-base62-sha1="czWvj2dMfaF18mQ4lDLnUEBAQOD" width="690" height="449" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_690x449.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_1035x673.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_1380x898.png 2x" data-dominant-color="EAEBED"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">error2</span><span class="informations">1768×1153 239 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/c/1cfcfd631da48ca8b93087b4d21b2ec36f34ccbc.png" alt="pipeline" data-base62-sha1="48roDf3xq264fau72sJV5iufcHW" width="432" height="409"></p> ;;;; <p>My ImageJ macro produces a cell count file for each channel which I then run through my R macro and get a nice spreadsheet.</p>
<p>I am sure it is doable it would just require me overhauling my entire pipeline.</p> ;;;; <p>what are the quirks in the image?</p> ;;;; <p>Hi <a class="mention" href="/u/bofallon">@bofallon</a> the Apple Silicon build for QuPath is marked as experimental. One of the main reasons is that it has limited support for some files, and doesn’t include OpenSlide by default.</p>
<p>There’s a bit more info at <a href="https://qupath.readthedocs.io/en/0.4/docs/intro/installation.html#apple-silicon" class="inline-onebox">Installation — QuPath 0.4.3 documentation</a></p>
<p>You can add OpenSlide support following those instructions (it’s awkward, but unfortunately necessary because I could never figure out <a href="https://github.com/qupath/qupath/issues/629">how to make a portable OpenSlide distribution properly on macOS</a>) and then perhaps the file will open.</p>
<p>Otherwise, QuPath is likely falling back to use Bio-Formats instead. However Bio-Formats fails to read some Hamamatsu files on M1 Macs for different reasons: <a href="https://github.com/ome/bioformats/issues/3756" class="inline-onebox">Libjpeg-turbo exception when converting/opening .ndpi file. · Issue #3756 · ome/bioformats · GitHub</a></p>
<p>Alternatively, you can use the macOS QuPath build for Intel processing and it should just work under Rosetta2 without any extra effort. If you try that and it still fails, then it’s probably some other issue… but since you say the files can be handled using OpenSlide in Python, then I’d expect them to work in QuPath as well.</p> ;;;; <p>Hi there, I’m fairly new to QuPath so perhaps I’m doing something incorrect, but I’m trying to load .ndpi formatted files from a Hamamatsu Nanozoomer using QuPath 0.4.3. Some ndpi files load just fine and I can view the no problem. However others cause a modal to pop up with the title “Load ImageData”: "Cannot invoke “java.awt.image.BufferedImage.getWidth() because imgInput is null”. The .ndpi can be loaded in python via openslide and manipulated in python there without issue, so I don’t think they’re corrupted or anything.<br>
Any ideas for troubleshooting?  Thanks in advance!</p>
<p>Hardware is an M1 Macbook pro running MacOS 12.2.1 Monterey</p> ;;;; <p>HEllo <a class="mention" href="/u/liam">@liam</a> , could you please a short pytorch script to load and run this torchscript model in colab on a random tensor?</p> ;;;; <p>Here is the full code</p>
<pre><code class="lang-auto">run("Bio-Formats Macro Extensions");
GetTime();
setBatchMode(true); 
input = getDirectory("Input directory, folder where your .lif file is and nothing else");
output = getDirectory("Output directory, where you'd like your adjusted .tiff files to go");

run("Input/Output...", "jpeg=100 gif=-1 file=.xls use_file copy_row save_column save_row");
Dialog.create("File type");
Dialog.addString("File suffix: ", ".tif", 5);
Dialog.addNumber("Ch1:", 0);
Dialog.addNumber("Ch1:", 65535);
Dialog.addNumber("Ch2:", 0);
Dialog.addNumber("Ch2:", 65535);
Dialog.addNumber("Ch3:", 0);
Dialog.addNumber("Ch3:", 65535);
Dialog.addNumber("Ch4:", 0);
Dialog.addNumber("Ch4:", 65535);
Dialog.addNumber("Ch5:", 0);
Dialog.addNumber("Ch5:", 65535);
Dialog.show();
suffix = Dialog.getString();
Range1 = Dialog.getNumber();
Range2 = Dialog.getNumber();
Range3 = Dialog.getNumber();
Range4 = Dialog.getNumber();
Range5 = Dialog.getNumber();
Range6 = Dialog.getNumber();
Range7 = Dialog.getNumber();
Range8 = Dialog.getNumber();
Range9 = Dialog.getNumber();
Range10 = Dialog.getNumber();

BC_range = newArray(Range1,Range2,Range3,Range4,Range5,Range6,Range7,Range8,Range9,Range10);

print("Brightness and contrast range is: ");
Array.print(BC_range);
print("Blue","Green","Red","Grays","Yellow");

suffix = ".lif";

processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
}

function processFile(input, output, file) {
	Ext.setId(input + file);
	Ext.getSeriesCount(count);
	print("Processing: " + input + file+ " - "+d2s(count,0)+" images");

	for (f=0;f&lt;count;f++) {
		openLif(input+file,f);
		print("Saving to: " + output);
		listImages();
	}
}

function openLif(input,f){
	Ext.setSeries(f);
	run("Bio-Formats Importer", "open=[" +input +"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_" + d2s(f+1,0));
}

//Create string "image_1 image_2 image_3 image_4...."
function seriesN(num){
	str = "";
	for (i=0; i&lt;num; i++){
		ii = i+1;
        str = str+"image_"+ii+" ";
     }
     return str;
}

//Save
function listImages(){
	imageList = getList("image.titles");
	if (imageList.length==0)
	     print("No image windows are open");
	else {
    	print("Image windows:");
     	for (i=0; i&lt;imageList.length; i++){
        	print("   "+imageList[i]);
        	// TODO: Add condition for saving unadjusted files
                saveTiff(imageList[i]);
                
                // Process the files as per the second script
                processTiff(output, output, imageList[i]);
     	}
	}
	print("");
}


function saveTiff(winName){
	selectWindow(winName); 
	saveAs("Tiff", output + winName);
	close();
}

function processTiff(input, output, file) {

	print("Processing: " + input + file);

	open(input+file+".tif");
	renderColor(file);
	brightnessNcontrast(file);
	deleteSlices(file);

	saveAs("Tiff", output + file + "_BCadj");
	rename(file);
	
	RGBmerge(file);
	scaleBar();
	makeMontage(file);

	//selectWindow("Montage1to5");
	//saveAs("Jpeg", output + file + "_montage1to5");
	selectWindow("MontageHorizontal");
	saveAs("Jpeg", output + file + "_BCmont");

	print("Saving to: " + output);
	run("Close All");
}



// Give colors for each slice
function renderColor(file){
	color = newArray("Blue","Green","Red","Grays","Yellow");
	//color = newArray("Blue","Green","Red");

	run("Make Composite", "display=Color");
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		run(color[i]);
	}
}
// Change brightness and contrast
function brightnessNcontrast(file){
	//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		setMinAndMax(BC_range[2*i],BC_range[2*i+1]);
	}
	
}

// Delete slices. You want to delete slice a, b
function deleteSlices(file){
	deleteA = 0;
	deleteB = 0;
	//deleteB &gt; deleteA. deleteB should be larger than deleteA

	print("deleting the slice "+deleteA+" and "+deleteB);
	selectWindow(file);
	if(deleteB&gt;0){
		setSlice(deleteB);
		run("Delete Slice", "delete=channel");
		}
	if(deleteA&gt;0){
		setSlice(deleteA);
		run("Delete Slice", "delete=channel");
	}	
}

// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.
function RGBmerge(file){
	run("Duplicate...", "title=4channels duplicate");
	run("RGB Color");
		
	selectWindow(file);
	run("Duplicate...", "title=Merge duplicate");
	Stack.setDisplayMode("composite");
	run("RGB Color");
	run("Copy");

	selectWindow("4channels (RGB)");
	setSlice(nSlices);
	run("Add Slice"); 
	run("Paste"); 

	close("4channels");
	close("Merge");
	close("Merge (RGB)");
	
	// "4channels (RGB)" is made.
}

// Add scale bar
function scaleBar(){
	setSlice(nSlices);
	run("Set Scale...", "distance=311.0016 known=100 pixel=1 unit=µm");
	//run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] hide");
	run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] bold");
}

function makeMontage(input){
	//selectWindow("4channels (RGB)");
	//run("Make Montage...", "columns=1 rows=5 scale=0.25 border=2");
	//rename("Montage1to5");
	selectWindow("4channels (RGB)");
	run("Make Montage...", "columns="+nSlices+" rows=1 scale=0.5 border=2");
	rename("MontageHorizontal");
}
function GetTime(){
     MonthNames = newArray("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec");
     DayNames = newArray("Sun", "Mon","Tue","Wed","Thu","Fri","Sat");
     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);
     TimeString ="Date: "+DayNames[dayOfWeek]+" ";
     if (dayOfMonth&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+dayOfMonth+"-"+MonthNames[month]+"-"+year+"\nTime: ";
     if (hour&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+hour+":";
     if (minute&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+minute+":";
     if (second&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+second;
     print(TimeString);
}
</code></pre>
<p>The error goes away when I comment out <code>selectWindow(file)</code> in</p>
<pre><code class="lang-auto">// Delete slices. You want to delete slice a, b
function deleteSlices(file){
	deleteA = 0;
	deleteB = 0;
	//deleteB &gt; deleteA. deleteB should be larger than deleteA

	print("deleting the slice "+deleteA+" and "+deleteB);
	//selectWindow(file);
	if(deleteB&gt;0){
		setSlice(deleteB);
		run("Delete Slice", "delete=channel");
		}
	if(deleteA&gt;0){
		setSlice(deleteA);
		run("Delete Slice", "delete=channel");
	}	
}
</code></pre>
<p>But I don’t think this is desirable as when I test this out, the adjusted .tiff seems to be identical to the raw .tiff, but the montage seems to be (correctly) affected by the brightness adjustments.</p>
<p>As to your questions, the name in the macro error corresponds to the name of the first image, which is what should be processed first as expected, and matches the name in the “Processing” line above it. There isn’t anything else popping up or open besides that in the screenshot.</p> ;;;; <p>Well, as long as it works, I don’t see why the high likelihood would be the problem. Maybe you just have an amazing dataset xD Is the prediction always perfect?</p> ;;;; <p>There is a pickle in evaluation results with <code>_full</code> suffix that contains all the information from evaluation with prediction, groundtruth keyed by the tuple of video name and frame index and metada.</p> ;;;; <p>Thanks very much, I’ll happily move to the bug tracker!<br>
I should have access to the stage soon to make the test you suggest.</p> ;;;; <p>What you could do on your side would be using <a href="https://www.ilastik.org/documentation/fiji_export/plugin">our fiji import/export plugin</a>, load your data as usual in fiji, but export it to hdf5 using our plugin.</p> ;;;; <p>That’s very strange. Do you have a system wide CUDA installed? What if you try <code>pip install --upgrade tensorflow-io-gcs-filesystem==0.27.0</code></p> ;;;; <p>Hi Hannah,</p>
<p>Thank you so much for the detailed response! My next question was going to be about the “distance to center biofilm” parameter but you answered it for me!</p> ;;;; <p>Hi everyone,<br>
I recently purchased a second hand Zeiss Axioimager 200M with a hamamatsu camera ORCA-ER, a led source Lumencore light engine AURA 5 and an Apotome 1 from Zeiss.<br>
I need to purchase a computer to run the system and I’m wondering how powerfull the computer needs to be for such equipment (quite outdated compared to most recent computers).<br>
This is for some prototyping applications.<br>
Thank you for your help</p> ;;;; <p>Hi Konstantin,<br>
I do have the same problem using CP4.2.5 (and earlier versions) with images acquired from spheroids through Opera Phenix and uploaded as *.tiff in CellProfiler.<br>
1 - Trying to update the NamesAndTypes module, CellProfiler is giving me an identical error message (see Screenshot#1).<br>
2 - Then when trying to visualize the acquired images through CellProfiler, it giving a second error message claiming that it failed to load the images (see Screenshot#2).<br>
3 - Also tried to re-import the acquired images but the error message persisted!<br>
<a>Uploading: Screenshot#1.png…</a> i<br>
<a>Uploading: Screenshot#2.png…</a>s<br>
Any help would be appreciated<br>
Thank you</p> ;;;; <p>Hi Sara and Mike,<br>
Thanks.  Looks like that script should do the trick!<br>
Mike</p> ;;;; <p>Hi, <a class="mention" href="/u/k-dominik">@k-dominik</a></p>
<p>You’re probably right about file misinterpreting. It could be a problem with NIS-Elements (Nikon), used for image acquisition. Original files are read as time-lapse series by many soft except NIS-Elements. Conversion via NIS helps read channels as channels. I’ve uploaded the original file (“sample_tyx”) and the file after conversion (“sample_cyx”) to the cloud.</p>
<p>Best,<br>
Natalia</p> ;;;; <p>Hello all,</p>
<p>Please find the summary of the meetings below. In case anything was missed or unclear, please let me know!</p>
<h1>
<a name="summary-1" class="anchor" href="#summary-1"></a>Summary</h1>
<h3>
<a name="brief-ome-ngff-status-2" class="anchor" href="#brief-ome-ngff-status-2"></a>Brief OME-NGFF status</h3>
<p>The specifications scheduled for v0.5 are near final call for comments and are already used by several communities. Outstanding comments on the PRs exist<br>
and should be revisited after testing in the wild and after initial merge.</p>
<p>The timeline for conversion to Zarr v3 is unclear but would be accelerated by help from the community. Norman R agreed to help with conversion. Likely there are no significant changes with respect to metadata in NGFF.</p>
<h3>
<a name="metadata-in-ngff-3" class="anchor" href="#metadata-in-ngff-3"></a>Metadata in NGFF</h3>
<p>The primary purpose of the meetings was to discuss how NGFF might deal with metadata.</p>
<p>General<br>
There are different ideas of what NGFF should / could do with metadata in general. In short these range from having full freedom regarding metadata to having more constraints:</p>
<ol>
<li>NGFF should only define where and how metadata is stored (e.g., “load file A/B/C which is in JSON”.) For the rest, users should have full freedom.</li>
<li>NGFF comes up with a minimal model of metadata, however, what is minimal to one is not minimal to someone else. Getting consensus on this could prove challenging.</li>
<li>NGFF supports a single, unified metadata framework like <a href="https://linkml.io" rel="noopener nofollow ugc">LinkML</a>, but there was some discussion that this could be a parallel but related effort.
<ul>
<li><em><strong>Potential Details</strong>: Standards that are maintained by communities can choose from fields present in these models. If a field is not present this could be added by opening a PR. Fields have a IRI (international resource identifier) as key. Valid values are determined by a given metadata standard itself. Metatadata standards have a name and can be registered in a registry such as linkml registry. Parent, child system of metadata models was discussed in case people partially would use a metadata specification.</em></li>
</ul>
</li>
</ol>
<p>Some of these options could live next to one another, too. First priority for NGFF would be to have format and location. Point 3 could be interoperable with that and could possibly encourage consensus building.</p>
<p>Format<br>
In general the community is in favor of moving away from xml to another format. There were no objections to linkml. People have volunteered for experimenting with the usage of linkml. Linkml could be used for schema authoring, provides loaders and dumpers and validation in several RDF related formats and formats often used by biologists (csv). Several people will be experimenting with this in <a href="https://github.com/ome/linkml-sandbox" class="inline-onebox" rel="noopener nofollow ugc">GitHub - ome/linkml-sandbox: Experimental repository for exploring a common framework for bioimaging metadata models</a></p>
<p>Location<br>
Opinions differ as to what the storage location of metadata should be. Multiple options have been discussed which are not neccesarily mutually exclusive:</p>
<ol>
<li>Metadata in one file stored at a given location within Zarr.</li>
<li>Metadata stored at multiple locations within Zarr. Location depends on what kind of metadata. A possible issue was raised regarding the time it would take to parse metadata from all locations to find what you want. Concept of chunked metadata raised by Josh Moore in response.</li>
<li>Links to external metadata. An example here would be donor metadata in case imaging and sequencing was performed. Without links this would lead to metadata duplication.</li>
</ol>
<p>A need for the possibility of consolidation of metadata at the top level of the Zarr store was raised. At the same time the metadata specification should allow for subsetting data without metadata loss.</p>
<p>Viewing configs<br>
Omero metadata is transitional, but only <a href="https://github.com/ome/ngff/issues/78" rel="noopener nofollow ugc">issue 78</a> was closest to where someone has gotten for a specification. For location it was mentioned that it should not be near image data as this would be more difficult when having multiple volumes with different rendering settings. An alternative would be storing with image data, but with allowing multiple viewconfigs.</p>
<h3>
<a name="other-matters-of-business-4" class="anchor" href="#other-matters-of-business-4"></a>Other matters of business</h3>
<p>Future meetings<br>
A need for more frequent meetings was raised to discuss current ongoing activities. Possibilities include more community meetings, spontaneous meeting in case of hot topic on github / image sc, or hybrid form of these.</p> ;;;; <p>Hi everyone,</p>
<p>I want to combine a large number of ROIs into groups corresponding to individual slices in a movie. The goal is to have only one grouped ROI per slice. The following macro works well when the total number of frames/slices is under 255:</p>
<p>for (i=0; i&lt;roiManager(“count”); i++) {<br>
roiManager(“Select”, i);<br>
Roi.getPosition(channel, slice, frame)<br>
RoiManager.setGroup(slice);<br>
}<br>
for (i=1; i&lt;=nSlices; i++) {<br>
RoiManager.selectGroup(i);<br>
roiManager(“Combine”);<br>
roiManager(“Add”);<br>
roiManager(“delete”);<br>
}<br>
Roi.remove;<br>
roiManager(“Show All with labels”);</p>
<p>If the movie is longer than 255 frames, I get the following error:</p>
<p>Error:		Group out of range in line 8:</p>
<pre><code>	RoiManager . setGroup ( slice &lt;)&gt; ; 
</code></pre>
<p>Does anyone know how to overcome this limitation?</p>
<p>Best</p> ;;;; <p>Just came across this now, but most programs will read the pixel values you assigned, which are 1-13, and will be a grayscale image. The exporter does not generate a color image, but is intended for use as labels.</p>
<p>In general, you end up with small pixel values in a single channel image, unless the program can somehow read what the lookup tables are.</p> ;;;; <p>Seems all sensible to me. I don’t understand why you have 4-channel images for raw and projections, but in each case 1 channel is all zeros?</p>
<p>I guess you’re storing projections under Wells (instead of under each Image) to make it easier to view the whole plate as projection? (rather than View a single Image and turn on/off projection view)?</p>
<p>In the case of labels, they are stored under each Image, so that if you open an individual image in a viewer, you don’t have to traverse the hierarchy to find the labels. But this makes it harder to view a plate of labels.</p> ;;;; <p>Hey,</p>
<p>we would like to use a camera synchronized with other hardware and I thought pycromanager will be a good idea to use. However, I still have some issue to understand the Acquistion function.</p>
<ol>
<li>We would like to use a DAQ card to send triggers to a stage, some hardware and maybe the camera (+ lasers). I have a function (<strong>fun</strong>) sending the correct, synchronized, sequence of signals (for one z-stack) and I would like to use micromanager to deal with the saving of the images. So, where do I put <strong>fun</strong>? Can I put it in the post_camera_hook_fn (after putting the camera into external triggering mode), as shown in the code below?</li>
<li>Is the post_camera_hook_fun only called once before the Acquisition if any loaded device is in sequencing mode (such as the DemoStage) or camera in external trigger mode?</li>
<li>Currently, the DAQ sequence only contains the signal for one z-stack, but we would like to acquire a time series of z-stacks. Can micromanager deal with the time sequencing using the order argument in the Aqcuisition “order=‘tz’” vs “order=‘tz’”? Is there a hook function which is called after each time point?</li>
</ol>
<p>Best wishes<br>
Fred</p>
<pre><code class="lang-auto">
def hookFn(event):
    # function sending a sequence of triggers and AO via a DAQ card to other hardware (including cam and stage)
    fun 
    return event


ev = multi_d_acquisition_events(z_start=0, z_end=0.9, z_step=0.1, num_time_points=5, time_interval_s=0, order='tz')
with Acquisition(directory="C:\\temp", name="scan", show_display=False, post_camera_hook_fn=hookFn) as acq:
    acq.acquire(ev)
    
</code></pre> ;;;; <aside class="quote" data-post="5" data-topic="75129">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar">
    <a href="https://forum.image.sc/t/qupath-detecting-positive-cells-based-on-subcellular-area/75129/5">QuPath detecting positive cells based on subcellular area</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Have you tried the existing script? It might need updating for 0.4.x <a href="https://forum.image.sc/t/need-help-quantifying-nuclear-and-cytoplasmic-rnascope-foci/51077/2" class="inline-onebox">Need help quantifying nuclear and cytoplasmic RNAscope foci - #2 by Research_Associate</a>
  </blockquote>
</aside>

<p>Followed by something like <a href="https://forum.image.sc/t/clearing-selected-objects-without-clearing-my-previously-annotated-cell-populations/78497" class="inline-onebox">Clearing 'selected objects' without clearing my previously annotated cell populations?</a></p> ;;;; <p>Weird, totally missed this one, but I’d recommend using <code>resetSelection()</code> before selecting new objects.</p> ;;;; <p>Great, thanks for letting me know <a class="mention" href="/u/henrypinkard">@henrypinkard</a>. If I don’t plan on using the Explore feature from Magellan and just want an xy grid (that’s reasonably in focus), would it make more sense to set the focus automatically/manually at a landmark and then just run an XYTiled? I’d like for it to be as automatic as possible, but going to a corner and focusing shouldn’t be too difficult.</p> ;;;; <p>Not that I know of, but you could remove the ones in the nucleus after classifying them using a script. You would then need to recaclulate the estimated spot count through.</p> ;;;; <p>I would recommend looking through some of the object types used in QuPath here <a href="https://qupath.readthedocs.io/en/0.4/docs/concepts/objects.html" class="inline-onebox">Objects — QuPath 0.4.3 documentation</a></p>
<p>You can copy the <code>classifiers</code> folder from the project, or individual classifiers (.json files) from one project’s classifier folder to another. Then, if you need to update the classifier, you go back to the original project, add more annotations, rebuild the classifier, and then move it over (and overwrite) the classifier in the active project.</p> ;;;; <p>Hey,<br>
I’m trying to evaluate my deeplabcut model using a custom metric. I’m trying to use the predictions in the snapshot h5 file in “evaluation-results” but from where i understand the data is shuffled. I tried to use the pickle file in “training-datasets” but with no success (I dont understand the variables and only contains information about the training data). Can someone tell me how to unshuffle the predictions or point to me where i can find the information necessary to do it ?<br>
Thank you so much in advance !</p> ;;;; <p>Is it possible to only detect spots in the cytoplasm with the function of subcellular detection in QuPath</p> ;;;; <p>I’m not sure what you mean by an annotation?  By annotation do you mean the name of each class? Also, how do you train in one project and then use the training in a second project? Thanks.</p> ;;;; <p>Hi Erick, thank you very much for your reply. Recover all annotations, ROIs would be very helpful for us. I will have a look into it!</p> ;;;; <p>Have you tried the existing script? It might need updating for 0.4.x <a href="https://forum.image.sc/t/need-help-quantifying-nuclear-and-cytoplasmic-rnascope-foci/51077/2" class="inline-onebox">Need help quantifying nuclear and cytoplasmic RNAscope foci - #2 by Research_Associate</a></p> ;;;; <p>Hi! <a class="mention" href="/u/talley">@talley</a>, <a class="mention" href="/u/sofroniewn">@sofroniewn</a> or <a class="mention" href="/u/kevinyamauchi">@kevinyamauchi</a></p>
<p>I am writing a plugin in napari and this thread really helped me to customise displayed controls. Thank you!!! However, when I am using a reference to the QT viewer:</p>
<pre><code class="lang-auto">qctrl = viewer.window.qt_viewer.controls.widgets[layer]
</code></pre>
<p>in the current version of napari it results in a following warning:</p>
<p>“Public access to Window.qt_viewer is deprecated and will be removed in v0.5.0. It is considered an “implementation detail” (…)”</p>
<p>Can you please indicate the updated approach to disabling layer buttons in napari that won’t become obsolete when the version will change??</p> ;;;; <p>It sounds like <a href="https://github.com/ome/omero-cli-transfer" rel="noopener nofollow ugc">omero-cli-transfer</a> was made for you <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> You’ll still need to transfer a file, but you should be able to recover all annotations, ROIs and so on on the destination server.</p> ;;;; <p>Hi <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a></p>
<p>I created a new environment and inside the environment i call: <code>conda install -c conda-forge cudnn</code> and then <code>conda list tensorflow</code><br>
output tensorflow 2.10.0<br>
tensorflow-estimator 2.10.0<br>
tensorflor-io-gcs-filesystem 0.31.0</p>
<p>when i call deeplabcut (python -m deeplabcut) the same error : dll load failed</p>
<p>I try to run pip install --upgrade tensorflow==2.10.0 but of course the same happen!</p>
<p>Julia</p> ;;;; <p>I belatedly discovered that a full range inversion of a density calibrated 16-bit image does not work as expected.</p>
<p>This macro reproduces the problem:</p>
<pre><code class="lang-auto">newImage("Untitled", "16-bit ramp", 500, 500, 1);
run("Divide...", "value=16");
resetMinAndMax();
run("Calibrate...", "function=[Straight Line] unit=[Gray Value] text1=[0 4096] text2=[0 1]");
run("Invert");
run("Select All");
run("Plot Profile");
</code></pre>
<p>As a result, I decided to make full range 16-bit inversions optional. To enable, check “Full range 16-bit inversions” in theEdit&gt;Options&gt;Conversions dialog. To make full range inversions the default, add</p>
<p><code>  setOption("FullRangeInversions");</code></p>
<p>to the Edit&gt;Options&gt;Startup dialog.</p>
<p>In plugins and scripts,  enable and disable full range 16-bit inversions using</p>
<p><code>Prefs.fullRange16bitInversions = fullRange</code></p> ;;;; <p>Hi everyone,<br>
I’m writing a plugin that contains some ‘print’ statements mostly for debugging. Is there a way of choosing where these are printed? When I run Napari through the console they are printed in the console directly which is fine. However, when I run it from a jupyter notebook they are printed in the last executed cell which makes the analysis a bit messy once I run a couple of cells. So, is it possible to print directly in Napari’s console or you recommend another way of doing this?</p>
<p>Thanks!</p> ;;;; <p>Ok I found it in ImageJ but not FIJI. Good enough…</p> ;;;; <p>Hello <a class="mention" href="/u/drlachie">@DrLachie</a>,</p>
<p>I have to admit that I ran into this problem myself (particularly the <code>--headless</code> part). To make it short - I didn’t find a solution and gave up.</p>
<p>Maybe <a class="mention" href="/u/christian_tischer">@Christian_Tischer</a> has an idea what to do in this situation? Does this have something to do with the virtual dataset? I remember discussing something related to that with you.</p>
<p>Cheers<br>
D</p> ;;;; <p>That looks to be coming from the <code>selectWindow(file);</code> command in deleteSlices. Is there a window open at the time of failure with the same name? Or is the name in the macro error incorrect for the open windows?</p> ;;;; <p>Hello <a class="mention" href="/u/ujjwal">@ujjwal</a>,</p>
<aside class="quote no-group" data-username="ujjwal" data-post="3" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>(extra note: Apparently Fiji does not like to read these h5 files using the native hdf5 plugin and so I use the ilastikj plugin again for that, do you know why that might be? It seems that it is also a little slower as well…?)</p>
</blockquote>
</aside>
<p>I’ve tried the plugin with some random result from ilastik and the normal reader works for me with the “individual hyperstacks” option (with correct dataset layout). The build-in plugin does appear to be faster. With the ilastik plugin you get some added convenience, that axistags are recognized correclty.</p>
<aside class="quote no-group" data-username="ujjwal" data-post="3" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>I did create a config file and during processing it does use 100% CPU but yeah…compute time is still relatively same when compared to a machine with 8 cores and 128GB memory.<br>
Any idea how to probably optimize it?</p>
</blockquote>
</aside>
<p>From your initial post I take it you already <a href="https://www.ilastik.org/documentation/basics/installation#controlling-cpu-and-ram-resources">set the number of threads for ilastik</a> via the config file… Even if you did, the speedup <a href="https://github.com/ilastik/ilastik/issues/1458">you get beyond 8 threads is not linear, last time I checked</a>, but with your available amount of memory definitely worth a try.</p>
<p>Another way to speed up the computation would be to start two instances of ilastik (say, with 7 cores each) to work on different images (if having multiple images available is a situation that occurs for you).</p>
<p>Going beyond that, <a href="https://www.ilastik.org/documentation/basics/headless#running-distributed-ilastik-via-mpi-potentially-trough-slurm">there is the option to run ilastik distributed on a cluster using MPI</a>.</p>
<aside class="quote no-group" data-username="ujjwal" data-post="3" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>Is it possible to have a single self-contained .ilp file, because the raw data for training is smaller than the stacks. In that case I can keep a single file for each trained model and not the data files separately.</p>
</blockquote>
</aside>
<p>You can <a href="https://www.ilastik.org/documentation/basics/dataselection#properties">copy the training data to your project file in the data selection applet</a>, by changing the <em>storage</em> property to <em>Copied to project file</em>. After that you can move your project file to different machines/operating systems and can always open/modify/work with it.</p>
<aside class="quote no-group" data-username="ujjwal" data-post="3" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>If the probability map is renormalized to 0…255 does the threshold of 0.5(or other value) still remain valid?</p>
</blockquote>
</aside>
<p>Threshold would be <code>128</code> then.</p>
<aside class="quote no-group" data-username="ujjwal" data-post="3" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>Perhaps a question, does the amount of annotations affect the computation time? or is it just the amount of features selected?</p>
</blockquote>
</aside>
<p>The number of annotated pixels has an influence on training time. If you annotate a lot, training tends to slow down quite a bit. Prediction time is not affected by it. This indeed is affected by the number of features you select.</p>
<aside class="quote no-group" data-username="ujjwal" data-post="3" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>Currently for my approach, Separating from the background it required only 4 features in stage 1 and then for stage 2 it was 17-19 features. If I select do the way you describe, does it then mean 17-19 features in each stage? The ram isn’t the biggest issue, it’s the time taken to process the 1500 image stack (currently about 5hrs) Is this around the time that is normally expected?</p>
</blockquote>
</aside>
<p>I am assuming that your image has only a single channel here. So when you select 4 features in the first stage, you have to compute (let’s oversimplify here and assume that all the features produce only a single channel) 4 additional channels. In the second stage, the predictions from the first round are stacked on top of the image. So if you had 2 classes in the first round, you’ll have three channels going into the second round (1 channel raw data, 2 channels probabilities).  The features you select in the second round, are computed for each of those channels. So if you select 20 features, there will be 60 additional channels computed. So choosing many in the second round comes at quite the computational cost.<br>
Not having seen your data you could potentially speed up the computation quite a bit if you use 2D features instead of 3D ones - this can make sense if the data resolution is very anisotropic.<br>
ilastik is giving quite some output during batch processing. You might gain some insight inspecting the log file (<em>Settings → Open Log Folder</em>).</p>
<p>Hope this helps in getting your processing a  little faster…</p>
<p>Cheers<br>
Dominik</p> ;;;; <p>Thanks <a class="mention" href="/u/icala">@ICala</a>, if you update to the latest Bio-Formats 6.12.0 do you still see the issue?</p>
<p>When updating if you select manage update sites and de-select the Bio-Formats site and instead have the Java-8 site selected. It is a little confusing but the Java-8 has the official release versions while the Bio-Formats site has regular updates of pre-release development versions for testing.</p> ;;;; <p>This is a bug I recently introduced. Hopefully will have a fix today</p> ;;;; <p>Thank you <a class="mention" href="/u/adamltyson">@adamltyson</a> , will check it out!</p>
<p>Ved</p> ;;;; <p>Has this problem been solved or disappeared? I wrote a plugin with very similar code and now I have exactly the same problem.</p>
<p>My plugin acquires z-size * t-count images and assigns coordinates. No matter if I save via Java code or “Save” button: When I reopen the created file I get the correct total number of images, but flat.</p>
<p>I can use ImageJ’s “Image → Hyperstacks → Stack to Hyperstack…” to reintroduce proper coordinates. Oddly, when doing so I have to provide the order of coordinates before saving, and I cannot even see an equivalent method in Datastore. How is that even supposed to work? Or isn’t it?</p> ;;;; <p>Thank you for taking time to read this question.</p>
<p>My question is. Is it possible to share/collaborate between two OMERO instances? I am using one host by my university, but our co-work is from another university, they have OMERO instance too.</p>
<p>I know the hard drive is one option for data transfer, but as both universities have OMERO instances, I felt it might be worthwhile to ask how to share/send/view data from one instance to another.</p>
<p>Does anyone have any experience or ideas?</p>
<p>Many thanks</p> ;;;; <p>Hi everyone,</p>
<p><a class="mention" href="/u/imagejan">@imagejan</a> and myself worked on a Python package <a href="https://github.com/fmi-faim/faim-hcs">faim-hcs</a> which is the start of a collection of functions we use to handle high-content screening (HCS) data at FMI Basel.</p>
<h1>
<a name="functionality-1" class="anchor" href="#functionality-1"></a>Functionality</h1>
<p>So far we have two functionalities:</p>
<ul>
<li>Convert Molecular Devices ImageXpress acquisitions into NGFF ome-zarr</li>
<li>Create <a href="https://mobie.github.io/">MoBIE</a> projects for the created ome-zarr files.</li>
</ul>
<h2>
<a name="molecular-devices-imagexpress-to-ome-zarr-2" class="anchor" href="#molecular-devices-imagexpress-to-ome-zarr-2"></a>Molecular Devices ImageXpress to OME-Zarr</h2>
<p>Molecular Devices allows to acquire single planes, projections and z-stacks. It is possible to mix these acquisition modes arbitrarily. For example we can acquire:</p>
<ul>
<li>Z-stacks were only acquired for for channels 1 and 2.
<ul>
<li>For channels 1 and 2, the maximum and best-focus projections were saved by the microscope as well.</li>
</ul>
</li>
<li>For channel 3, only a maximum projection was saved.</li>
<li>For channel 4, only a single z-plane was acquired and saved.</li>
</ul>
<h3>
<a name="ome-zarr-structure-3" class="anchor" href="#ome-zarr-structure-3"></a>OME-Zarr Structure</h3>
<p>We are trying to follow the NGFF spec as closely as possible, but with the mix of z-stacks, projections and single-planes we had to improvise a bit. Our goal is to have all acquired data in a single zarr container.</p>
<p>For the example data described above, we would create an OME-Zarr container where the zeroth field of each well is a multiscale <code>CZYX</code> image with four channels. For channels 1 and 2 we have the full z-stacks, channel 3 will be zeros (the zarr fill-value) and channel 4 will be zeros except for the single acquired plane.</p>
<p>Data in <code>plate[E/7/0/0]</code> (well E07, field 0, resolution-level 0):<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dcd1c72f57eb8ca827c056053fce318ec452e6d4.png" data-download-href="/uploads/short-url/vvsur90tl768Sn26ic72NbeQvSA.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dcd1c72f57eb8ca827c056053fce318ec452e6d4.png" alt="image" data-base62-sha1="vvsur90tl768Sn26ic72NbeQvSA" width="690" height="470" data-dominant-color="93789F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">813×554 21.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>For the projection data, we decided to create another zarr group (<em>projections</em>) in each well group (similar to the <em>labels</em> group). Inside this projections group we have again all the fields for the well in which the corresponding projection images are stored. In our case, these are z-projections and we store a <code>CYX</code> image with four channels. Channel 4 is only zeros since no projection data is available.</p>
<p>Data in <code>plate[E/7/0/projections/0</code> (well E07, field 0, projections, resolution-level 0):<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06f74a6e17be6912cc53c65d0fc9890757090386.png" data-download-href="/uploads/short-url/ZCGkF6Hg9zsYE281kBGw4hYmHk.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06f74a6e17be6912cc53c65d0fc9890757090386_2_690x449.png" alt="image" data-base62-sha1="ZCGkF6Hg9zsYE281kBGw4hYmHk" width="690" height="449" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06f74a6e17be6912cc53c65d0fc9890757090386_2_690x449.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06f74a6e17be6912cc53c65d0fc9890757090386.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06f74a6e17be6912cc53c65d0fc9890757090386.png 2x" data-dominant-color="916D9A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">841×548 43.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h3>
<a name="extra-metadata-4" class="anchor" href="#extra-metadata-4"></a>Extra Metadata</h3>
<p><code>{plate_name}/.zattrs</code>:</p>
<ul>
<li>
<code>barcode</code>: The barcode of the imaged plate</li>
<li>
<code>order_name</code>: Name of the plate order</li>
</ul>
<p><code>{plate_name}/{row}/{col}/0/.zattrs</code>:</p>
<ul>
<li>
<code>acquisition_metadata</code>: A dictionary with key channels.
<ul>
<li>
<code>channels</code>: A list of dicitionaries for each acquired channel, with the following keys:
<ul>
<li>
<code>channel-name</code>: Name of the channel during acquisition</li>
<li>
<code>display-color</code>: RGB hex-code of the display color</li>
<li><code>exposure-time</code></li>
<li><code>exposure-time-unit</code></li>
<li>
<code>objective</code>: Objective description</li>
<li><code>objective-numerical-aperture</code></li>
<li>
<code>power</code>: Illumination power used for this channel</li>
<li>
<code>shading-correction</code>: Set to <code>On</code> if a shading correction was applied automatically.</li>
<li>
<code>wavelength</code>: Name of the wavelength as provided by the microscope.</li>
<li>
<code>z-projection-method</code>: Optional and only present in the projection <code>.zattrs</code>
</li>
</ul>
</li>
<li>
<code>histograms</code>: A list of relative paths to the histograms of each channel.</li>
</ul>
</li>
</ul>
<h3>
<a name="histograms-5" class="anchor" href="#histograms-5"></a>Histograms</h3>
<p>We use a custom <a href="https://github.com/fmi-faim/faim-hcs/blob/e58611404f6677f9a16e574cd15cd16239a322f3/src/faim_hcs/UIntHistogram.py"><code>UIntHistogram</code></a> to aggregate and save histograms of the individual fields. These histograms are saved as <code>.npz</code> files in each field for each channel. The implementation allows to aggregate multiple histograms and compute <code>mean()</code>, <code>std()</code>, <code>quantile()</code>, <code>min()</code> and <code>max()</code> over the whole plate or any subset of wells.</p>
<h2>
<a name="mobie-projects-6" class="anchor" href="#mobie-projects-6"></a>MoBIE Projects</h2>
<p>We have functionality to build MoBIE projects for the zarr files we create. We are heavily relying on <a href="https://github.com/mobie/mobie-utils-python">mobie-utils-python</a> to create the plate and projection overviews. We also use the aggregated histograms to compute the display intensity ranges for each channel.</p>
<h1>
<a name="examples-7" class="anchor" href="#examples-7"></a>Examples</h1>
<ul>
<li><a href="https://github.com/fmi-faim/faim-hcs/blob/main/examples/Create%20ome-zarr%20from%20Single-Plane%20Multi-Field%20Acquisition.ipynb">Create ome-zarr from Single-Plane Multi-Field Acquisition</a></li>
<li><a href="https://github.com/fmi-faim/faim-hcs/blob/main/examples/Create%20ome-zarr%20from%20Z-Stack%20Multi-Field%20Acquisition.ipynb">Create ome-zarr from Z-Stack Multi-Field Acquisition</a></li>
<li><a href="https://github.com/fmi-faim/faim-hcs/blob/main/examples/Create%20MoBIE%20Project.ipynb">Create MoBIE Project</a></li>
<li><a href="https://github.com/fmi-faim/faim-hcs/blob/main/examples/Inspect%20Zarr.ipynb">Inspect Zarr</a></li>
</ul>
<hr>
<p>This is our pragmatic approach to store and inspect our HCS data. We would be glad to get feedback on the decisions we have made (e.g. extra metadata fields, projections-group, histograms). The more of these things we can get covered by an official spec or implementation in another package, the happier we will be!</p> ;;;; <p>Hello <a class="mention" href="/u/nat.ovs">@nat.ovs</a>,</p>
<p>first of all, welcome to the image.sc community <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12" title=":star_struck:" class="emoji" alt=":star_struck:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/tada.png?v=12" title=":tada:" class="emoji" alt=":tada:" loading="lazy" width="20" height="20"></p>
<p>and thank you for taking the time to report this problem with ilastik. This particular error message is unfortunately not very helpful in determining the root cause. I tried reproducing this on my side, without success. Maybe ilastik is misinterpreting the particular image. Do you think you could make it available (or something similar but small - it might be in the metadata). (I’ll send you a DM with an upload link).</p>
<p>Hope we can resolve this quickly.</p>
<p>Cheers<br>
D</p> ;;;; <p>Hi everyone,</p>
<p>I am new to image analysis, so apologies if this is a basic question! I’ve been trying to figure it out for a few days now.</p>
<p>I have .vsi images that I can open and visualize very nicely in QuPath with my four fluorescent channels (DAPI plus 3 specific stains). My end goal is to analyze individual channels in CellProfiler. Currently, I need to separate the fluorescent channels (without downsampling) so that I can export individual channels using the tile_exporter.</p>
<p>I know you can split channels in ImageJ, but going from QuPath to ImageJ requires significant downsampling. I would like to avoid downsampling and instead use tile_exporter so that I can analyze in CellProfiler. Is it possible to split fluorescent channels in QuPath and export them individually?</p>
<p>Thanks!</p>
<p>Best,<br>
Sarah</p> ;;;; <p>Hi <a class="mention" href="/u/hiroalchem">@hiroalchem</a> ,</p>
<p><a href="https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification">napari-apoc</a> is a <a href="https://napari.org">napari</a>-plugin that allows to merge labels by training a random forest clasfifier.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19.jpeg" data-download-href="/uploads/short-url/zpmNPdafIHzNirAIjv1Ww7iz47L.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_690x367.jpeg" alt="image" data-base62-sha1="zpmNPdafIHzNirAIjv1Ww7iz47L" width="690" height="367" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_690x367.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_1035x550.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_1380x734.jpeg 2x" data-dominant-color="6B6A69"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1909×1016 133 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The classifier can be trained on parameters such as:</p>
<ul>
<li>
<code>touch_portion</code>: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of <code>1/6</code> to each other.</li>
<li>
<code>mean_touch_intensity</code>: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way.</li>
<li>
<code>standard_deviation_intensity_difference</code>: The absolute difference between the standard deviation of the two objects. This measurement allows to differentiate [in]homogeneous objects and [not] merge them.</li>
<li>
<code>area_difference</code>: The difference in area/volume/pixel-count allows differentiating small and large objects and [not] merging them.</li>
</ul>
<p>Note: pixel-count-based features are recommended to be used in isotropic images only.</p>
<p>                    <a href="https://raw.githubusercontent.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/main/images/merge_objects.gif" target="_blank" rel="noopener" class="onebox">
            <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/4/d486f5c3b3b2f4923400d133636aaed44490ae07.gif" width="690" height="348">
          </a>

</p>
<p>After training the <code>LabelMerger</code> interactively, you can call it from a Python code as demonstrated here:<br>
<a href="https://github.com/haesleinhuepf/apoc/blob/main/demo/merge_objects.ipynb">https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/merge_objects.ipynb</a></p>
<p>Let me know if this works for you! The LabelMerger is still quite new and I’m eager for feedback in order to improve it.</p>
<p>Best,<br>
Robert</p> ;;;; <aside class="quote no-group" data-username="Grant" data-post="40" data-topic="78123">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/g/a5b964/40.png" class="avatar"> Grant:</div>
<blockquote>
<p>The first point of difference from previous attempts, I labeled the bodyparts the other way around today</p>
</blockquote>
</aside>
<p>Hi Konrad, I meant to say in this sentence that I labeled the individuals the other way around - sorry for the confusion.</p>
<p>Thanks</p> ;;;; <p>Hi Konrad,</p>
<p>I had another go today creating a new video and including 8 bodyparts on 2 individuals, with identity = True, and training the network with 20k iterations.</p>
<p>Please find all the coding I performed today attached.</p>
<p><a class="attachment" href="/uploads/short-url/zFZhCrW0FU5LgbW4VtCD6WP3ZAo.txt">multi script.txt</a> (68.7 KB)</p>
<p>The first point of difference from previous attempts, I labeled the bodyparts the other way around today just to see if an individual swap occurred in this instance. However, when I activated the refine_tracklets GUI the tracklets were not swapped, meaning I didn’t need to try to create a ‘flag’ window and lasso the bodyparts to the other individual. Therefore, I just left it the way it was because the tracklets looked fine (for the purpose of just trying to get it to work anyway).</p>
<p>Does the legend in the refine_tracklets just reflect the alphabetical order of the individual names?</p>
<p>I also performed create_labeled_video and everything looks fine. Thus, everything looks like it worked well.</p>
<p>However, the CSV files and the plot-poses (likelihood values) reported likelihood scores of ‘1’ again…?</p>
<p><a class="attachment" href="/uploads/short-url/qyOMoi8BgBm3PbSH923OeNUj6ws.csv">multipartsDLC_resnet50_multiparts1Mar21shuffle1_20000_el.csv</a> (1.4 MB)<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/847a2b4924af689ad062004f3d78a1b07e795d46.png" data-download-href="/uploads/short-url/iTWLSatOqrnfFgbdfnChGDQYSwK.png?dl=1" title="plot-likelihood_filteredLeft" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/847a2b4924af689ad062004f3d78a1b07e795d46_2_690x252.png" alt="plot-likelihood_filteredLeft" data-base62-sha1="iTWLSatOqrnfFgbdfnChGDQYSwK" width="690" height="252" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/847a2b4924af689ad062004f3d78a1b07e795d46_2_690x252.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/847a2b4924af689ad062004f3d78a1b07e795d46.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/847a2b4924af689ad062004f3d78a1b07e795d46.png 2x" data-dominant-color="F9F8F8"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">plot-likelihood_filteredLeft</span><span class="informations">816×299 16.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I don’t think this new video is corrupted (or is there a way to check this?)</p>
<p>Any other suggestions why these scores are happening - maybe something I’m not setting up correctly earlier on…?</p>
<p>Thanks</p> ;;;; <p>I am now segmenting nuclei in 2D and 3D. I am using watershed, but it sometimes results in over segmentation.<br>
In the process, I would like to merge the blob with its neighboring label depending on the size of the blob itself and the way it touches the neighboring segment.<br>
I know it happens often, but I can’t find an easy way to implement such a thing.<br>
Could someone please give me some advice?</p> ;;;; <p>Hi Mario,</p>
<p>Thank you so much! I had to step away from this project for a while, but your solution works perfectly! Really appreciate your help and expertise (:</p>
<p>Best,<br>
Sarah</p> ;;;; <p><a class="mention" href="/u/liangy10">@liangy10</a> I just had the same issue as you.</p>
<p>I wanted to save an image stack from an arbitrary plane orientation in BigDataViewer. Found this thread, and the <a href="https://forum.image.sc/t/save-one-tiff-projection-from-a-current-view-in-bdv/33965/20">linked solution</a> that <a class="mention" href="/u/nicokiaru">@NicoKiaru</a> shared doesn’t seem to work if you do the following:</p>
<ol>
<li>Close everything, open your image stack of interest</li>
<li>‘Open Current Image’ in BigDataViewer Plugin menu to produce a BDV window (and navigate to your arbitrary plane orientation)</li>
<li>‘Current BDV View To ImagePlus (Basic)’ from BDV playground</li>
</ol>
<p>That loads a blank BDV window (even though one is already open) and doesn’t produce any data when you start the function.</p>
<p>Alternatively, if you follow <a href="https://forum.image.sc/t/save-one-tiff-projection-from-a-current-view-in-bdv/33965/36">the post slightly further down</a>:</p>
<ol>
<li>Close everything, open your TIFF stack of interest</li>
<li><strong>‘Open Image’ in ImagePlus BigDataViewer menu</strong></li>
<li>‘BDV - Show Sources (new Bdv window)’ in BDV playground (and navigate to your arbitrary plane orientation)</li>
<li>‘Current BDV View To ImagePlus (Basic)’ from BDV playground</li>
</ol>
<p>That doesn’t open a blank window, and it produces the intended result of an image stack in the new plane orientation.</p> ;;;; <p>I found the documentation. I had to enable the jupyter notebook extension!</p>
<p>First I installed: <code>vedo</code>, <code>notebook</code>, and <code>k3d</code> into a virtual environment.</p>
<p>Then I ran:</p>
<pre><code>./vedo-env/bin/jupyter nbextension install --py --sys-prefix k3d
./vedo-env/bin/jupyter nbextension enable --py --sys-prefix k3d
</code></pre>
<p>I’m using a virtual environment so I use <code>--sys-prefix</code></p>
<p>I found the instructions here, <a href="https://k3d-jupyter.org/user/install.html" class="inline-onebox" rel="noopener nofollow ugc">Installation — K3D-jupyter documentation</a> which seems pretty obvious. They didn’t show up on the github though.</p>
<p>Here is an example.</p>
<pre><code class="lang-auto">from vedo import dataurl, Mesh, Plotter, Volume, settings

settings.default_backend = 'k3d'

msh = Mesh(dataurl+"beethoven.ply").c('gold').subdivide()
plt = Plotter(bg='black')
plt.show(msh)

</code></pre>
<p>Here is the output, you can see there are some quirks.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8.jpeg" data-download-href="/uploads/short-url/q2TvTy9mBAS5miCiu3qwVzzXUo0.jpeg?dl=1" title="resulting 3D image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_690x438.jpeg" alt="resulting 3D image" data-base62-sha1="q2TvTy9mBAS5miCiu3qwVzzXUo0" width="690" height="438" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_690x438.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_1035x657.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_1380x876.jpeg 2x" data-dominant-color="C4C4E7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">resulting 3D image</span><span class="informations">1703×1082 103 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>How can you filter spots (from subcellular analysis) in cytoplasmic compartments ?</p> ;;;; <p>HI<br>
I am failing to  install deeplabcut on ubuntu 20.04 in a computer with gpu.<br>
Can you tell me where is a good tutorial for this case?</p>
<p>There are so much information on the internet , it is not clear which one is the right one.</p>
<p>thanks</p> ;;;; <p>This fix should be released soon. In the mean time if you want to test it, then please install the release candidate with <code>pip install brainreg-napari==0.1.2rc0</code>.</p>
<p>Adam</p> ;;;; <p>Hi Curtis,</p>
<p>Thanks for reaching out !</p>
<p>I get an error with both 2.10 and 2.11, for script with 2 <code>#@ ImagePlus</code> fields (a single one works though).<br>
The GUI shows up but the dropdowns for the image have duplicate entries.</p>
<p>The script (jython, but other languages also have the issue)</p>
<pre><code class="lang-auto">#@ ImagePlus image1
#@ ImagePlus image2

print image1.getTitle()
</code></pre>
<p>Here the error message</p>
<pre><code class="lang-auto">[ERROR] Failed to refresh widget: class org.scijava.ui.swing.widget.SwingObjectWidget on EDT
java.lang.reflect.InvocationTargetException
	at java.awt.EventQueue.invokeAndWait(EventQueue.java:1349)
	at java.awt.EventQueue.invokeAndWait(EventQueue.java:1324)
	at org.scijava.thread.DefaultThreadService.invoke(DefaultThreadService.java:115)
	at org.scijava.ui.AbstractUIInputWidget.refreshWidget(AbstractUIInputWidget.java:80)
	at org.scijava.ui.swing.widget.SwingObjectWidget.set(SwingObjectWidget.java:93)
	at org.scijava.ui.swing.widget.SwingObjectWidget.set(SwingObjectWidget.java:54)
	at org.scijava.plugin.WrapperService.create(WrapperService.java:65)
	at org.scijava.widget.AbstractInputHarvester.addInput(AbstractInputHarvester.java:110)
	at org.scijava.widget.AbstractInputHarvester.buildPanel(AbstractInputHarvester.java:84)
	at org.scijava.widget.InputHarvester.harvest(InputHarvester.java:67)
	at org.scijava.ui.AbstractInputHarvesterPlugin.process(AbstractInputHarvesterPlugin.java:74)
	at org.scijava.module.ModuleRunner.preProcess(ModuleRunner.java:102)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:152)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.UnsupportedOperationException
	at java.util.AbstractList.set(AbstractList.java:132)
	at net.imglib2.img.planar.PlanarImg.setPlane(PlanarImg.java:256)
	at net.imglib2.img.planar.PlanarImg.setPlane(PlanarImg.java:66)
	at net.imglib2.img.display.imagej.PlanarImgToVirtualStack.setPlaneCastType(PlanarImgToVirtualStack.java:172)
	at net.imglib2.img.display.imagej.PlanarImgToVirtualStack.setPixelsZeroBasedIndex(PlanarImgToVirtualStack.java:167)
	at net.imglib2.img.display.imagej.AbstractVirtualStack.setPixels(AbstractVirtualStack.java:109)
	at ij.ImagePlus.setProcessor2(ImagePlus.java:778)
	at ij.ImagePlus.setStack(ImagePlus.java:841)
	at ij.ImagePlus.&lt;init&gt;(ImagePlus.java:162)
	at net.imglib2.img.display.imagej.PlanarImgToVirtualStack.wrap(PlanarImgToVirtualStack.java:119)
	at net.imagej.legacy.translate.ImagePlusCreator.createImagePlus(ImagePlusCreator.java:112)
	at net.imagej.legacy.translate.ImagePlusCreator.createLegacyImage(ImagePlusCreator.java:96)
	at net.imagej.legacy.translate.ImagePlusCreator.createLegacyImage(ImagePlusCreator.java:89)
	at net.imagej.legacy.translate.ImageTranslator.createLegacyImage(ImageTranslator.java:76)
	at net.imagej.legacy.LegacyImageMap.registerDataset(LegacyImageMap.java:234)
	at net.imagej.legacy.convert.DatasetToImagePlusConverter.convert(DatasetToImagePlusConverter.java:69)
	at org.scijava.convert.ConvertService.convert(ConvertService.java:68)
	at org.scijava.widget.DefaultWidgetModel.setValue(DefaultWidgetModel.java:158)
	at org.scijava.widget.DefaultWidgetModel.ensureValid(DefaultWidgetModel.java:323)
	at org.scijava.widget.DefaultWidgetModel.ensureValidObject(DefaultWidgetModel.java:306)
	at org.scijava.widget.DefaultWidgetModel.getValue(DefaultWidgetModel.java:140)
	at org.scijava.ui.swing.widget.SwingObjectWidget.doRefresh(SwingObjectWidget.java:110)
	at org.scijava.ui.AbstractUIInputWidget.lambda$refreshWidget$0(AbstractUIInputWidget.java:80)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$1(DefaultThreadService.java:211)
	at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:301)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758)
	at java.awt.EventQueue.access$500(EventQueue.java:97)
	at java.awt.EventQueue$3.run(EventQueue.java:709)
	at java.awt.EventQueue$3.run(EventQueue.java:703)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:728)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
[ERROR] Failed to refresh widget: class org.scijava.ui.swing.widget.SwingObjectWidget on EDT
java.lang.reflect.InvocationTargetException
	at java.awt.EventQueue.invokeAndWait(EventQueue.java:1349)
	at java.awt.EventQueue.invokeAndWait(EventQueue.java:1324)
	at org.scijava.thread.DefaultThreadService.invoke(DefaultThreadService.java:115)
	at org.scijava.ui.AbstractUIInputWidget.refreshWidget(AbstractUIInputWidget.java:80)
	at org.scijava.ui.swing.widget.SwingObjectWidget.set(SwingObjectWidget.java:93)
	at org.scijava.ui.swing.widget.SwingObjectWidget.set(SwingObjectWidget.java:54)
	at org.scijava.plugin.WrapperService.create(WrapperService.java:65)
	at org.scijava.widget.AbstractInputHarvester.addInput(AbstractInputHarvester.java:110)
	at org.scijava.widget.AbstractInputHarvester.buildPanel(AbstractInputHarvester.java:84)
	at org.scijava.widget.InputHarvester.harvest(InputHarvester.java:67)
	at org.scijava.ui.AbstractInputHarvesterPlugin.process(AbstractInputHarvesterPlugin.java:74)
	at org.scijava.module.ModuleRunner.preProcess(ModuleRunner.java:102)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:152)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.UnsupportedOperationException
	at java.util.AbstractList.set(AbstractList.java:132)
	at net.imglib2.img.planar.PlanarImg.setPlane(PlanarImg.java:256)
	at net.imglib2.img.planar.PlanarImg.setPlane(PlanarImg.java:66)
	at net.imglib2.img.display.imagej.PlanarImgToVirtualStack.setPlaneCastType(PlanarImgToVirtualStack.java:172)
	at net.imglib2.img.display.imagej.PlanarImgToVirtualStack.setPixelsZeroBasedIndex(PlanarImgToVirtualStack.java:167)
	at net.imglib2.img.display.imagej.AbstractVirtualStack.setPixels(AbstractVirtualStack.java:109)
	at ij.ImagePlus.setProcessor2(ImagePlus.java:778)
	at ij.ImagePlus.setStack(ImagePlus.java:841)
	at ij.ImagePlus.&lt;init&gt;(ImagePlus.java:162)
	at net.imglib2.img.display.imagej.PlanarImgToVirtualStack.wrap(PlanarImgToVirtualStack.java:119)
	at net.imagej.legacy.translate.ImagePlusCreator.createImagePlus(ImagePlusCreator.java:112)
	at net.imagej.legacy.translate.ImagePlusCreator.createLegacyImage(ImagePlusCreator.java:96)
	at net.imagej.legacy.translate.ImagePlusCreator.createLegacyImage(ImagePlusCreator.java:89)
	at net.imagej.legacy.translate.ImageTranslator.createLegacyImage(ImageTranslator.java:76)
	at net.imagej.legacy.LegacyImageMap.registerDataset(LegacyImageMap.java:234)
	at net.imagej.legacy.convert.DatasetToImagePlusConverter.convert(DatasetToImagePlusConverter.java:69)
	at org.scijava.convert.ConvertService.convert(ConvertService.java:68)
	at org.scijava.widget.DefaultWidgetModel.setValue(DefaultWidgetModel.java:158)
	at org.scijava.widget.DefaultWidgetModel.ensureValid(DefaultWidgetModel.java:323)
	at org.scijava.widget.DefaultWidgetModel.ensureValidObject(DefaultWidgetModel.java:306)
	at org.scijava.widget.DefaultWidgetModel.getValue(DefaultWidgetModel.java:140)
	at org.scijava.ui.swing.widget.SwingObjectWidget.doRefresh(SwingObjectWidget.java:110)
	at org.scijava.ui.AbstractUIInputWidget.lambda$refreshWidget$0(AbstractUIInputWidget.java:80)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$1(DefaultThreadService.java:211)
	at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:301)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758)
	at java.awt.EventQueue.access$500(EventQueue.java:97)
	at java.awt.EventQueue$3.run(EventQueue.java:709)
	at java.awt.EventQueue$3.run(EventQueue.java:703)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:728)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
</code></pre> ;;;; <p>Ok I will try this !</p>
<p>Thank’s a lot !</p>
<p>Bastien</p> ;;;; <p>I would like to create the 3D display in a jupyter notebook using the k3d backend but when I run my example nothing shows up.</p>
<p>I installed <code>vedo</code>, <code>notebook</code>, and <code>k3d</code> but then none of the examples create an interactive window. I suspect that I am missing a requirement, but it isn’t clear.</p>
<p>Thank you</p> ;;;; <p>Hello.</p>
<p>I have an error when I close an image I used with TrackMate. Apparently it is linked to wrapping an ImagePlus as a Dataset, through the imagej-legacy layer:</p>
<pre><code class="lang-java">(Fiji Is Just) ImageJ 2.11.0/1.54c; Java 1.8.0_322 [64-bit]; Mac OS X 10.16; 1458MB of 25081MB (5%)
 
java.lang.AbstractMethodError: Method net/imagej/legacy/display/LegacyImageDisplayService.getActiveDataset(Lnet/imagej/display/ImageDisplay;)Lnet/imagej/Dataset; is abstract
	at net.imagej.legacy.display.LegacyImageDisplayService.getActiveDataset(LegacyImageDisplayService.java)
	at net.imagej.legacy.LegacyImageMap.synchronizeAttachmentsToDataset(LegacyImageMap.java:583)
	at net.imagej.legacy.LegacyImageMap.lookupDisplay(LegacyImageMap.java:197)
	at net.imagej.legacy.DefaultLegacyHooks.unregisterImage(DefaultLegacyHooks.java:196)
	at ij.gui.ImageWindow.close(ImageWindow.java)
	at ij.gui.StackWindow.close(StackWindow.java:204)
	at ij.ImagePlus.close(ImagePlus.java:469)
</code></pre> ;;;; <p>… and scijava-parent-pom lists both:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5581-L5585">
  <header class="source">

      <a href="https://github.com/scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5581-L5585" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5581-L5585" target="_blank" rel="noopener">scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5581-L5585</a></h4>



    <pre class="onebox"><code class="lang-xml">
      <ol class="start lines" start="5581" style="counter-reset: li-counter 5580 ;">
          <li>			&lt;dependency&gt;</li>
          <li>				&lt;groupId&gt;org.jocl&lt;/groupId&gt;</li>
          <li>				&lt;artifactId&gt;jocl&lt;/artifactId&gt;</li>
          <li>				&lt;version&gt;${org.jocl.jocl.version}&lt;/version&gt;</li>
          <li>			&lt;/dependency&gt;</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<aside class="onebox githubblob" data-onebox-src="https://github.com/scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5604-L5608">
  <header class="source">

      <a href="https://github.com/scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5604-L5608" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5604-L5608" target="_blank" rel="noopener">scijava/pom-scijava/blob/1de596513b2bef856a5c99542f769aba8eeded5a/pom.xml#L5604-L5608</a></h4>



    <pre class="onebox"><code class="lang-xml">
      <ol class="start lines" start="5604" style="counter-reset: li-counter 5603 ;">
          <li>			&lt;dependency&gt;</li>
          <li>				&lt;groupId&gt;org.jogamp.jocl&lt;/groupId&gt;</li>
          <li>				&lt;artifactId&gt;jocl&lt;/artifactId&gt;</li>
          <li>				&lt;version&gt;${org.jogamp.jocl.jocl.version}&lt;/version&gt;</li>
          <li>			&lt;/dependency&gt;</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Any ideas why this worked in older versions of Fiji and now broke <a class="mention" href="/u/ctrueden">@ctrueden</a> ?</p> ;;;; <aside class="quote no-group" data-username="Bastien" data-post="10" data-topic="78743">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/b/51bf81/40.png" class="avatar"> Bastien:</div>
<blockquote>
<p>In order to obtain these images I do not use a microscope but a classic camera.</p>
</blockquote>
</aside>
<p>You can still use a dark background and more than one strong side illumination sources. Most likely that will solve the problem.</p> ;;;; <aside class="quote no-group" data-username="petebankhead" data-post="7" data-topic="78814">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<p>Have you tried? It looks like you can add preprocessing ops <em>and</em> global normalization, but the preprocessing will occur afterwards:</p>
</blockquote>
</aside>
<p>I have just tried using the Gaussian filter in a separate preprocess(), and it technically works, i.e. there are noticable differences with and without it in the script. But as you mentioned, it does not necessarily improve detection with the StarDist model.</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
def stains = imageData.getColorDeconvolutionStains()
def server = imageData.getServer()
def deconvServer = new TransformedServerBuilder(server).deconvolveStains(stains, 2).build() // 1 = haematoxylin, 2 = DAB, 3 = residual
def deconvImageData = new ImageData&lt;&gt;(deconvServer, imageData.getHierarchy(), imageData.getImageType())

def stardist = StarDist2D.builder(pathModel)
    .preprocess(
        StarDist2D.imageNormalizationBuilder()
            .maxDimension(4096)
            .percentiles(0, 99.8) // Increase min percentile to detect lower intensity objects, decrease max percentile to clip too intense objects
            .build()
    )
    .preprocess(
        ImageOps.Filters.gaussianBlur(1)
    )
    .threshold(0.5)              // Probability (detection) threshold
    .pixelSize(pixelSize)
    .cellExpansion(cellExpansionPixels)          // Approximate cells based upon nucleus expansion
    .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size
    .measureShape()              // Add shape measurements
    .measureIntensity()          // Add cell measurements (in all compartments)
    .includeProbability(true)    // Add probability as a measurement (enables later filtering)
    .build()

// Get annotations to run StarDist
def stardistParentAnno = getAnnotationObjects().findAll{it.getPathClass() == getPathClass("Tissue")}

stardist.detectObjects(deconvImageData, stardistParentAnno)
stardist.close()
</code></pre>
<p>Without Gaussian:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c.jpeg" data-download-href="/uploads/short-url/rAKa657ukRt4ZDVS7lBISPo5rmA.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_501x499.jpeg" alt="image" data-base62-sha1="rAKa657ukRt4ZDVS7lBISPo5rmA" width="501" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_501x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_751x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_1002x998.jpeg 2x" data-dominant-color="CFCCD9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1698×1692 331 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>With Gaussian:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/07249347383fd201059326cfda784b736b8cb620.jpeg" data-download-href="/uploads/short-url/11bHGbJgZFZkhTfNREvAvQuC1G0.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_501x499.jpeg" alt="image" data-base62-sha1="11bHGbJgZFZkhTfNREvAvQuC1G0" width="501" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_501x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_751x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_1002x998.jpeg 2x" data-dominant-color="CFCCD9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1698×1692 329 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I think the issue relates to the two JOCL.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/gpu/JOCL">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/gpu/JOCL" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/0616893a9313c77d901edb21cd0333f17908d58c_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/0616893a9313c77d901edb21cd0333f17908d58c_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/0616893a9313c77d901edb21cd0333f17908d58c_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/0616893a9313c77d901edb21cd0333f17908d58c.png 2x" data-dominant-color="F6F4F4"></div>

<h3><a href="https://github.com/gpu/JOCL" target="_blank" rel="noopener nofollow ugc">GitHub - gpu/JOCL: Java bindings for OpenCL</a></h3>

  <p>Java bindings for OpenCL. Contribute to gpu/JOCL development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
which is used by CLIJ</p>
<p>and jogamp jocl<br>
<a href="https://jogamp.org/jocl/www/" class="onebox" target="_blank" rel="noopener nofollow ugc">https://jogamp.org/jocl/www/</a></p> ;;;; <p>If I remove the if statement, I have some print for only the first subcellular cluster of each cell so I think I need to get all child object with a list, I try to change the [1] by other but do not work …<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26.jpeg" data-download-href="/uploads/short-url/aHPAQ4BpNw8iETF9rtrAJxRhDDM.jpeg?dl=1" title="Clipboard" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_690x346.jpeg" alt="Clipboard" data-base62-sha1="aHPAQ4BpNw8iETF9rtrAJxRhDDM" width="690" height="346" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_690x346.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_1035x519.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_1380x692.jpeg 2x" data-dominant-color="403F46"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Clipboard</span><span class="informations">1524×765 134 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi All,</p>
<p>I think since 0.4.0 there is a way to change the appearance of the scale bar in Preferences, it often is sufficient for my needs, but going to ImageJ gives you more options.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e96402fd2d7aac30d82e52c11767cc8a2ef0887.png" data-download-href="/uploads/short-url/fMiqOlHSXouJarvDbHhwQZ4x33F.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e96402fd2d7aac30d82e52c11767cc8a2ef0887.png" alt="image" data-base62-sha1="fMiqOlHSXouJarvDbHhwQZ4x33F" width="690" height="303" data-dominant-color="98A59A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">740×325 25.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Pable, this a bit outdated video on  <a href="https://www.youtube.com/watch?v=VW4kAbaSYMU" class="inline-onebox" rel="noopener nofollow ugc">Data visualization in QuPath - YouTube</a> might be helpful if you are looking for ways to customize the channels.</p>
<p>Best, Z.</p> ;;;; <p>Thanks for pinging me <a class="mention" href="/u/biovoxxel">@biovoxxel</a> . Quick question for <a class="mention" href="/u/ctrueden">@ctrueden</a> : Do you know the origin of jocl 2.4.0 and/or why it’s necessary? The jocl GitHub repo lists 2.0.x as recent version: <a href="https://github.com/gpu/JOCL/blob/fefa66d49dfefddde0f7f99ced4e3329e376cba8/pom.xml#L13" class="inline-onebox">JOCL/pom.xml at fefa66d49dfefddde0f7f99ced4e3329e376cba8 · gpu/JOCL · GitHub</a></p>
<p>The <a href="http://jocl.org">jocl.org</a> website recommends 2.0.2:<br>
<a href="http://www.jocl.org/">http://www.jocl.org/</a></p>
<p>And that’s what CLIJ is built on:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/clij/clij-parent-pom/blob/976fad3f80aab0cb263ad31e72525d6f9317d4fc/pom.xml#L121">
  <header class="source">

      <a href="https://github.com/clij/clij-parent-pom/blob/976fad3f80aab0cb263ad31e72525d6f9317d4fc/pom.xml#L121" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/clij/clij-parent-pom/blob/976fad3f80aab0cb263ad31e72525d6f9317d4fc/pom.xml#L121" target="_blank" rel="noopener">clij/clij-parent-pom/blob/976fad3f80aab0cb263ad31e72525d6f9317d4fc/pom.xml#L121</a></h4>



    <pre class="onebox"><code class="lang-xml">
      <ol class="start lines" start="111" style="counter-reset: li-counter 110 ;">
          <li>    &lt;clijx_assistant_imagej2_extensions.version&gt;0.6.0.1&lt;/clijx_assistant_imagej2_extensions.version&gt;</li>
          <li>    &lt;clijx_assistant_imglib2_extensions.version&gt;0.6.0.1&lt;/clijx_assistant_imglib2_extensions.version&gt;</li>
          <li>    &lt;clijx_assistant_simpleitk_extensions.version&gt;0.6.0.1&lt;/clijx_assistant_simpleitk_extensions.version&gt;</li>
          <li>
          <li>    &lt;clatlab.version&gt;2.5.1.4&lt;/clatlab.version&gt;</li>
          <li>    &lt;clatlabx.version&gt;0.32.1.1&lt;/clatlabx.version&gt;</li>
          <li>    &lt;clicy.version&gt;2.5.1.4&lt;/clicy.version&gt;</li>
          <li>    &lt;clij2imagej1.version&gt;2.5.1.4&lt;/clij2imagej1.version&gt;</li>
          <li>    &lt;clupath.version&gt;0.5.1.4&lt;/clupath.version&gt;</li>
          <li>
          <li class="selected">    &lt;jocl.version&gt;2.0.2&lt;/jocl.version&gt;</li>
          <li>    &lt;bridj.version&gt;0.7.0&lt;/bridj.version&gt;</li>
          <li>&lt;/properties&gt;</li>
          <li>
          <li>&lt;dependencyManagement&gt;</li>
          <li>    &lt;dependencies&gt;</li>
          <li>        &lt;dependency&gt;</li>
          <li>            &lt;groupId&gt;net.haesleinhuepf&lt;/groupId&gt;</li>
          <li>            &lt;artifactId&gt;clij-coremem&lt;/artifactId&gt;</li>
          <li>            &lt;version&gt;${clij.coremem.version}&lt;/version&gt;</li>
          <li>        &lt;/dependency&gt;				</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <blockquote>
<p>–pretrained_model <strong>tn3</strong></p>
</blockquote>
<p>Can you try using capital T and capital N?<br>
Here’s the relevant cellpose code I think:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20">
  <header class="source">

      <a href="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20" target="_blank" rel="noopener nofollow ugc">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20" target="_blank" rel="noopener nofollow ugc">MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20</a></h4>



    <pre class="onebox"><code class="lang-py">
      <ol class="start lines" start="19" style="counter-reset: li-counter 18 ;">
          <li>MODEL_NAMES = ['cyto','nuclei','tissuenet','livecell', 'cyto2', 'general',</li>
          <li>                'CP', 'CPx', 'TN1', 'TN2', 'TN3', 'LC1', 'LC2', 'LC3', 'LC4']</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/pab">@PAB</a> this definitely sounds like the bounds issue <a class="mention" href="/u/research_associate">@Research_Associate</a> mentions.</p>
<p>See here for some more info:</p><aside class="quote quote-modified" data-post="1" data-topic="44475">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/i/df788c/40.png" class="avatar">
    <a href="https://forum.image.sc/t/roi-annotation-to-openslide-coordinates/44475">Roi Annotation to Openslide Coordinates</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hello, 
I’d like to extract a rectangle annotation to get its corresponding coordinates in the original slide using openslide python. 
To do this, I’m using following script: 
def server = getCurrentServer()
print server.getMetadata()

def path = server.getPath()
def project = getProject()
def roi = getSelectedROI()
print roi

This gives me a roi of: INFO: Rectangle (19750, 19683, 1017, 723) . When I try exporting an image using this ROI directly using ImageJ, this works perfectly, but I’d like …
  </blockquote>
</aside>

<p>I’m not sure if you want to translate your annotations in QuPath, Python or elsewhere – but if it’s QuPath then a Groovy script like this should work:</p>
<pre><code class="lang-groovy">def server = getCurrentServer()
translateAllObjects(-server.boundsX, -server.boundsY)
</code></pre>
<p>(there still isn’t a public method to access the bounds info, but this should probably be added to QuPath… for now, accessing the internal boundsX and boundsY values should still work)</p>
<p>If you want to avoid the transform at all, then whenever you import images to a project you can provide a <code>--no-crop</code> as an optional argument – but that will cause lots of empty padding to be added.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84.png" data-download-href="/uploads/short-url/s5DFzdUjbwL6sHGS1T7oPOD4PEU.png?dl=1" title="Screenshot 2023-03-21 at 08.50.23"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84_2_690x406.png" alt="Screenshot 2023-03-21 at 08.50.23" data-base62-sha1="s5DFzdUjbwL6sHGS1T7oPOD4PEU" width="690" height="406" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84_2_690x406.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84_2_1035x609.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84.png 2x" data-dominant-color="C9CACB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-21 at 08.50.23</span><span class="informations">1326×782 46.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/ctrueden">@ctrueden</a>,</p>
<p>I tested some BioVoxxel 3D Box functions and ran into an error related to CLIJ. Therefore pinging <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a> here.</p>
<p>Trying to run a simple clij function (while it is true for others as well), such as measuring the mean intensity of all pixels in an image (<code> Plugins › ImageJ on GPU (CLIJ2) › Measure › Mean of all pixels on GPU</code>) brings up the following error.<br>
I just installed the BioVoxxel update sites, all clij update sites and MorphoLibJ (IJPB-plugins).</p>
<pre><code class="lang-auto">(Fiji Is Just) ImageJ 2.11.0/1.54c; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 89MB of 24256MB (&lt;1%)
 
java.lang.NoClassDefFoundError: org/jocl/NativePointerObject
	at net.haesleinhuepf.clij.clearcl.backend.ClearCLBackends.getBestBackend(ClearCLBackends.java:117)
	at net.haesleinhuepf.clij.CLIJ.getAvailableDeviceNames(CLIJ.java:212)
	at net.haesleinhuepf.clij.macro.AbstractCLIJPlugin.run(AbstractCLIJPlugin.java:263)
	at ij.plugin.filter.PlugInFilterRunner.processOneImage(PlugInFilterRunner.java:266)
	at ij.plugin.filter.PlugInFilterRunner.&lt;init&gt;(PlugInFilterRunner.java:114)
	at ij.IJ.runUserPlugIn(IJ.java:246)
	at ij.IJ.runPlugIn(IJ.java:210)
	at ij.Executer.runCommand(Executer.java:152)
	at ij.Executer.run(Executer.java:70)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.jocl.NativePointerObject
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 10 more
</code></pre>
<p>I screened the new <code>jocl-2.4.0.jar</code> file and it really seems to not contain the culprit function from the error message.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13c3f61a4f539439bef92e92eb2d99ba29e8ffc5.png" data-download-href="/uploads/short-url/2OQURBvZXUGPU8ECYhpPjeNu3pb.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13c3f61a4f539439bef92e92eb2d99ba29e8ffc5_2_690x70.png" alt="image" data-base62-sha1="2OQURBvZXUGPU8ECYhpPjeNu3pb" width="690" height="70" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13c3f61a4f539439bef92e92eb2d99ba29e8ffc5_2_690x70.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13c3f61a4f539439bef92e92eb2d99ba29e8ffc5_2_1035x105.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13c3f61a4f539439bef92e92eb2d99ba29e8ffc5.png 2x" data-dominant-color="F6F0EF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1376×141 16.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Furthermore, it seems the complete <code>org</code> subfolder is absent and the file is reorganized completely, which I guess will cause more issues than only the one I saw.</p> ;;;; <p>Hello Everyone.</p>
<p>Thank you for your good advice for my last article.<br>
Finally, I fixed that problem and succeeded in separating individual images from the “*_seg.npy” file.<br>
Now I am implementing the cell segmentation by using the following command.</p>
<blockquote>
<p>python -m cellpose --image_path “/app/mainApi/app/static/6414f306d4e67a92627f2b78/image3/3-Channel Cells_Z010_458.75 nm.ome.tiff” --pretrained_model <strong>tn3</strong> --chan 0 --chan2 0 --diameter 50 --stitch_threshold 0.0 --flow_threshold 0.4 --cellprob_threshold 0.7 --fast_mode  --save_png --save_outlines</p>
</blockquote>
<p>Whenever I use this command, the following error occurs.</p>
<blockquote>
<p><code>pretrained model has incorrect path</code></p>
</blockquote>
<p>Because of this error, the cellpose uses the “cyto” model automatically.</p>
<p>I think that the cellpose 2.2 provides some standard pretrained models(like cyto or tissuenet) which can be used without specifying the path.<br>
According to the cellpose 2.2 document, we can use some pretrained models and I think that “TN3” is one of them.<br>
Why do you think this error occurred?<br>
How can I fix this error?<br>
Which models can I use without specifying the path?</p>
<p>Any help would be appreciated.</p> ;;;; <p>Hello,<br>
There are one or two mistakes but your macro that you provided me with gives me a good basis to explore my problem!</p>
<p>Thanks a lot !</p> ;;;; <p><a class="mention" href="/u/research_associate">@Research_Associate</a> has already listed all the options.</p>
<p>QuPath doesn’t provide a menu because it already has</p>
<ul>
<li><em>View → Show command list</em></li>
<li><em>View → Show recent commands</em></li>
</ul>
<p>These provide searchable windows of commands, which can remain open on screen – so I personally think they are more convenient than a menu (i.e. you don’t need to move the mouse or click so often). They also provide access to help tooltip text and show any corresponding shortcuts.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/b/6be51b878d800aaad798939e7214c0140a982416.png" data-download-href="/uploads/short-url/fotWTH2UVSuBRXkGVJuV5fJbcTY.png?dl=1" title="Screenshot 2023-03-21 at 08.29.08"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6be51b878d800aaad798939e7214c0140a982416_2_689x491.png" alt="Screenshot 2023-03-21 at 08.29.08" data-base62-sha1="fotWTH2UVSuBRXkGVJuV5fJbcTY" width="689" height="491" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6be51b878d800aaad798939e7214c0140a982416_2_689x491.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6be51b878d800aaad798939e7214c0140a982416_2_1033x736.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/b/6be51b878d800aaad798939e7214c0140a982416.png 2x" data-dominant-color="F6F7F7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-21 at 08.29.08</span><span class="informations">1196×852 38.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello,</p>
<p>Thank you I will try this method, please excuse my time of answer I had some problems of connection to the forum these last times after posting my topic.</p>
<p>Do you think it is possible to apply this method on a stack of images?</p>
<p>Bastien</p> ;;;; <p>Hello Gabriel,</p>
<p>In order to obtain these images I do not use a microscope but a classic camera.</p>
<p>Bastien</p> ;;;; <p>Thank you for your answer.</p>
<p>I’ll give it a try!</p>
<p>Bastien</p> ;;;; <p>Could you open the file to show the data structure of sleap labelling?</p> ;;;; <p>Could you change the dtype of CollectedData to float and train for a short amount of time just to see if it runs?</p> ;;;; <p>Yep, makes total sense.   Thanks again so much for the detailed insights.  I’m going to dive more deeply into testing examples over the next week, and will definitely come back if I have additional questions <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Thanks for your reply. What I mean by “measure” is hopefully some macro that marks each “activated neuron” in the picture (thereby allowing me to check the results afterwards) and produces an excel for each picture/field of view with the number of activated neurons. The goal is to get a general picture if there is significantly different activation levels by different treatments.</p> ;;;; <p>Depending on the command, it might show up in the workflow tab, along with the settings used.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458.png" data-download-href="/uploads/short-url/hkoQ0dy0lFuOli7CqRUlf0iI4lO.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_438x375.png" alt="image" data-base62-sha1="hkoQ0dy0lFuOli7CqRUlf0iI4lO" width="438" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_438x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_657x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_876x750.png 2x" data-dominant-color="3B3E41"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1044×893 59.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>There is also <strong>CTRL+P</strong> for recent commands, as seen in the View menu.</p>
<p>Other commands you might try using the command search bar for, using <strong>CTRL+L</strong></p>
<p>Otherwise, you could always create a feature request on GitHub, though Pete should see it here too.</p> ;;;; <p>You don’t need the ROI, the pixel classifier trains on the pixels within the labeled annotations. You can apply the results in any annotation you select, it doesn’t matter where you put the annotation. Putting a * at the end of an annotation’s class name, like Ignore* or Region* will prevent it from being used during training.</p>
<p>It’s best to train classifiers across several images and keep the training data in a separate project so that it isn’t overwritten by scripts run across the project.</p> ;;;; <p>Is it possible to add a recent commands module in the menu bar so that the users can call some functions used recently.</p> ;;;; <p>Dear all, how can I make ROI and train pixel classifier in the certain ROI？Thanks so much！<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31.jpeg" data-download-href="/uploads/short-url/Y2jVEQ4EkFYwWKVER3fqT3TcCR.jpeg?dl=1" title="Snipaste_2023-03-21_11-07-46" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31_2_588x500.jpeg" alt="Snipaste_2023-03-21_11-07-46" data-base62-sha1="Y2jVEQ4EkFYwWKVER3fqT3TcCR" width="588" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31_2_588x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31.jpeg 2x" data-dominant-color="B973B7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Snipaste_2023-03-21_11-07-46</span><span class="informations">812×690 150 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I’d generally recommend using a completely separate project for training your pixel classifiers. If not, duplicate any images you want to use for training and rename them, but you still run the risk of accidentally running a script for “all images” and overwriting your annotations that way. Better to keep the training completely separate so that you can improve it when you see a problem.</p> ;;;; <aside class="quote no-group" data-username="MJCampbell" data-post="1" data-topic="78853">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/ba9def/40.png" class="avatar"> Michael Campbell:</div>
<blockquote>
<p>But not sure how to link a given csv file to its associated image in QuPath.</p>
</blockquote>
</aside>
<p>I hope the script helps - one thing to keep in mind is that the name association between the CSV files and the project entries, which is usually how you want to target the correct import location, needs to be maintained exactly.</p> ;;;; <p>Hi All,</p>
<p>I’m trying to use the ilastik pixel classifier as part of a larger FIJI macro workflow, but when running the macro using either Fiji’s --headless option, or with batchMode(true) then the pixel classifier runs, but doesn’t display a window so the pipeline won’t continue because it’s looking for a multiple channel image (the output from the classification). This is perhaps not surprising - although I’d like to find a workaround if I could - but what has surprised me is that if I run the macrom from the command line but omit the --headless option, the same thing happens and the error message pops up. Once I click “ok” the output to the classification does pop up.</p>
<p>A code snippet to reproduce this issue:</p>
<pre><code class="lang-auto">setBatchMode(false);
run("Gel");
fname = getTitle();
run("Run Pixel Classification Prediction", "projectfilename=MyProject.ilp inputimage="+fname+" pixelclassificationtype=Probabilities");
getDimensions(width, height, channels, slices, frames);
print(width,height,channels,slices,frames);
</code></pre>
<p>I have a MyProject.ilp that creates with three classes so the output to the log should be:<br>
“276 467 3 1 1”</p>
<p>but if I set batch mode to true it becomes “276 467 1 1 1”</p>
<p>futhermore if I set batch mode to False, but run from command line with (note the lack of headless flag, I was thinking of wrapping in xvfb-run if I couldn’t get headless working):</p>
<p>./ImageJ-linux64 -macro classify.ijm</p>
<p>Then I get “276 467 1 1 1” again, and the classified image shows up <em>after</em> the print command has executed and the macro completed.</p>
<p>I hope this is clear, other options I’ve tried with no success:</p>
<ul>
<li>calling “classify.ijm” from within another macro in the hope that the classified image would turn up before the wrapper macro continued</li>
<li>putting wait commands or running other processes after the ilastik command in case it was some kind of race condition</li>
<li>Considered moving to Jython, but the ilastik plugin doesn’t return a reference to the output ImagePlus as far as I can tell so I can’t work out how to select the output for processing.</li>
</ul>
<p>Any help would be greatly appreciated.</p>
<p>Thanks,<br>
-Lachie</p> ;;;; <p>Try this: <a href="https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/5" class="inline-onebox">There and back again, QuPath&lt;==&gt;CytoMAP cluster analysis - #5 by Mike_Nelson</a></p>
<p>“Variant script to import into QuPath a folder of CSV files that have been exported from CytoMAP”</p> ;;;; <p>I had not seen that before but after some debugging I think I’ve found the problem. I’ve reported it on <a href="https://github.com/python-microscope/microscope/issues/273" class="inline-onebox">LinkamCMS segfaults in Linux · Issue #273 · python-microscope/microscope · GitHub</a> Basically, we’re doing something that is not supported by Python and while it does work in Windows (or some of its architectures) it doesn’t work in Linux.</p>
<p>I no longer have access to a Linkam CMS and neither do the most active project developers. All the places I know using this code use Windows. If you’re comfortable hacking the code, I can provide some hints but this is now moving into bug fixing and maybe it should moved from this forum to the bug tracker.</p> ;;;; <p>I am getting the same error when trying to evaluate the model and analyze new videos.</p> ;;;; <p>I’ve recently started to work with quPath after Definiens software is not supported anymore but even with all the YT tutorials i’ve been struggling with simple IHC cell detection - so what I want to do is a simple IHC immune cell density analysis (pos cells/mm2) in tumor ROI followed by density maps.</p>
<p>What I do is …</p>
<ul>
<li>Image upload (works as a batch of 5-10 images)</li>
<li>Tissue detection (done by Classify &gt; Train images &gt; Create regional annotation (run for project) with “below treshold” annotation *Ignore</li>
<li>Manual correction for hair/dirt/bubbles/wrong positive areas with annotation for *Ignore (first unlock, then draw all annotations, merge them, lock)</li>
<li>Create region annotation in all slides and create training images</li>
<li>Train pixel classifier</li>
</ul>
<p> → Unfortunately all annotations of manually excluded bubbles and so on are gone after the pixel classifier - does anyone know how to deal with this problem?</p>
<p>Thanks and greetings,<br>
Markus</p> ;;;; <p>Does this model run normally if you try to analyze a video? I wonder if it might be due to the integers in your labeled data</p> ;;;; <p>Hi,<br>
Need some help scripting in QuPath.  I’m analyzing multiplex IF images off a Polaris imaging platform.<br>
Our current workflow is to import images into QuPath, run cell segmentation, then save the resulting detection measurements txt file with an appended ID column (unique integer for each cell).</p>
<p>Thresholding/gating/phenotyping is then run using R scripts, yielding a csv file with the cell IDs and their assigned phenotypes.</p>
<p>We then run a script to bring these phenotypes back into QuPath for visualization (see attached script).</p>
<p>This script works for an individual image… user is prompted to open the phenotype.csv file associated with the image currently open in QuPath and phenotype classes are assigned appropriately.</p>
<p>What I’d like to be able to do is run this batchwise for all images in a project.  All of the associated phenotype.csv files would be located in one folder and the user would be prompted for that folder, rather than the individual csv file.</p>
<p>Figured I could loop through the csv files with something like:</p>
<p>def folder = Dialogs.promptForDirectory(null)<br>
folder.listFiles().each{file-&gt;<br>
def csvReader = new BufferedReader(new FileReader(file));</p>
<p>But not sure how to link a given csv file to its associated image in QuPath.</p>
<p>Thanks for you help,<br>
Mike</p>
<pre><code class="lang-auto">
// assign phenotype class to each cell
// from imported csv file


cells = getCellObjects()
nCells = cells.size()

// Get location of csv
def file = Dialogs.promptForFile(null)

// Create BufferedReader
def csvReader = new BufferedReader(new FileReader(file));

// first row (header)
row = csvReader.readLine()
def columnNames = [:]; // Empty map
def cols = row.split(",");
for (i = 0; i &lt; cols.size(); i++){
    columnNames[cols[i]] = i;
}

print(columnNames)

def map = [:]; // Empty map
while ((row = csvReader.readLine()) != null) {
    def rowContent = row.split(",");
    int id = rowContent[columnNames["ID"]] as int;         // !!ID Col!!
    phenotype = rowContent[columnNames["Cell.Type1"]];    // Phenotype Col
    map[id] = [phenotype];
}

print "nCells: " + nCells;
print "Map size: " + map.size();
print "_______________";


for (i = 0; i &lt; nCells; i++){
    def measurementList = cells[i].getMeasurementList();
    int ID = measurementList.getMeasurementValue("ID") as int;
    print "ID: " + ID;
    Phenotype  = map[ID];
    print "Assigned to: " + Phenotype;
    cells[i].setPathClass(getPathClass(Phenotype[0]));
    print "_______________";
}

print "Done!"


</code></pre> ;;;; <p>After you create a new environment you can simply run in that new env <code>conda install -c conda-forge cudnn</code> and if your tensorflow isn’t <code>2.10.0</code> just run <code>pip install --upgrade tensorflow==2.10.0</code></p> ;;;; <p>Recently, I released Fiji 2.10.0 to the SciJava Maven repository, followed by Fiji 2.11.0 a few days later. However, I have not yet uploaded the changes to the core update site, because as always, we should do some testing first! <img src="https://emoji.discourse-cdn.com/twitter/person_climbing.png?v=12" title=":person_climbing:" class="emoji" alt=":person_climbing:" loading="lazy" width="20" height="20"></p>
<p>I have now made ZIP bundles for these releases, which you can find at:</p>
<ul>
<li><a href="https://downloads.imagej.net/fiji/releases/2.10.0/" class="inline-onebox">Index of /fiji/releases/2.10.0</a></li>
<li><a href="https://downloads.imagej.net/fiji/releases/2.11.0/" class="inline-onebox">Index of /fiji/releases/2.11.0</a></li>
</ul>
<p>The main updates from 2.10.0 to 2.11.0 are:</p>
<ul>
<li>TrackMate 7.9.2 → 7.10.2 (<img src="https://emoji.discourse-cdn.com/twitter/warning.png?v=12" title=":warning:" class="emoji" alt=":warning:" loading="lazy" width="20" height="20"> a backwards-incompatible API update!)</li>
<li>ij 1.54b → 1.54c</li>
<li>guava 27.1-jre → 31.1-jre</li>
<li>minor updates to various other third-party dependencies</li>
<li>minor improvements to some core ImageJ2 libraries</li>
</ul>
<p>The changes from Fiji 2.9.0 to 2.10.0/2.11.0 are more extensive, but still mostly under the hood. In particular, there is a JogAmp/JOGL update from 2.4.0-rc-20110111 to the 2.4.0 final release, and a couple of other major third-party dependency version updates such as asm 7.1 → 9.4, json 20090211 → 20230227, and kryo 4.0.2 → 5.4.0.</p>
<details><summary>Full list of differences from Fiji 2.9.0 to 2.11.0</summary>
<h3>
<a name="update-plugins-and-dependencies-1" class="anchor" href="#update-plugins-and-dependencies-1"></a>Update plugins and dependencies</h3>
<ul>
<li>FilamentDetector: 2.0.0 → 2.0.1</li>
<li>ST4: 4.0.8 → 4.3.4</li>
<li>T2-NIT: 1.1.3 → 1.1.4</li>
<li>T2-TreelineGraph: 1.1.3 → 1.1.4</li>
<li>TrackMate: 7.7.2 → 7.10.2</li>
<li>VectorString: 2.0.2 → 2.0.3</li>
<li>aircompressor: 0.18 → 0.21</li>
<li>animal-sniffer-annotations: 1.17 → 1.22</li>
<li>ant: 1.9.7 → 1.10.13</li>
<li>ant-launcher: 1.9.7 → 1.10.13</li>
<li>antlr: 3.5.2 → 3.5.3</li>
<li>antlr-runtime: 3.5.2 → 3.5.3</li>
<li>api-common: 1.8.1 → 2.6.2</li>
<li>asm: 7.1 → 9.4</li>
<li>asm-analysis: 7.1 → 9.4</li>
<li>asm-commons: 7.1 → 9.4</li>
<li>asm-tree: 7.1 → 9.4</li>
<li>asm-util: 7.1 → 9.4</li>
<li>auto-value-annotations: 1.7 → 1.10.1</li>
<li>autocomplete: 3.1.5 → 3.3.1</li>
<li>aws-java-sdk-core: 1.11.796 → 1.12.423</li>
<li>aws-java-sdk-kms: 1.11.796 → 1.12.423</li>
<li>aws-java-sdk-s3: 1.11.796 → 1.12.423</li>
<li>batik: 1.14 → 1.16</li>
<li>bigdataviewer-core: 10.4.3 → 10.4.5</li>
<li>bigdataviewer-vistools: 1.0.0-beta-30 → 1.0.0-beta-31</li>
<li>bio-formats/formats-api: 6.10.1 → 6.12.0</li>
<li>bio-formats/formats-bsd: 6.10.1 → 6.12.0</li>
<li>bio-formats/formats-gpl: 6.10.1 → 6.12.0</li>
<li>bio-formats/metakit: 5.3.4 → 5.3.5</li>
<li>bio-formats/ome-codecs: 0.3.2 → 0.4.4</li>
<li>bio-formats/ome-common: 6.0.13 → 6.0.14</li>
<li>bio-formats/ome-xml: 6.3.1 → 6.3.2</li>
<li>bio-formats/specification: 6.3.1 → 6.3.2</li>
<li>bio-formats/turbojpeg: 6.10.1 → 6.12.0</li>
<li>checker-qual: 2.5.2 → 3.32.0</li>
<li>commons-codec: 1.14 → 1.15</li>
<li>commons-compress: 1.21 → 1.22</li>
<li>commons-io: 2.7 → 2.11.0</li>
<li>commons-text: 1.9 → 1.10.0</li>
<li>error_prone_annotations: 2.2.0 → 2.18.0</li>
<li>ffmpeg: 4.2.1-1.5.2 → 5.1.2-1.5.8</li>
<li>fiji: 2.9.0 → 2.11.0</li>
<li>flatlaf: 2.4 → 2.6</li>
<li>fontchooser: 2.4 → 2.5.2</li>
<li>gax: 1.56.0 → 2.23.2</li>
<li>gax-httpjson: 0.73.0 → 0.108.2</li>
<li>google-api-client: 1.30.9 → 2.2.0</li>
<li>google-api-services-cloudresourcemanager: v1-rev20200210-1.30.9 → v1-rev20230129-2.0.0</li>
<li>google-api-services-storage: v1-rev20200326-1.30.9 → v1-rev20220705-2.0.0</li>
<li>google-auth-library-credentials: 0.20.0 → 1.16.0</li>
<li>google-auth-library-oauth2-http: 0.20.0 → 1.16.0</li>
<li>google-cloud-core: 1.93.2 → 2.12.0</li>
<li>google-cloud-core-http: 1.93.2 → 2.12.0</li>
<li>google-cloud-resourcemanager-0.117.2: alpha → 1.14.0</li>
<li>google-cloud-storage: 1.108.0 → 2.20.1</li>
<li>google-http-client: 1.40.0 → 1.43.0</li>
<li>google-http-client-appengine: 1.40.0 → 1.43.0</li>
<li>google-http-client-jackson2: 1.40.0 → 1.43.0</li>
<li>google-http-client-xml: 1.40.0 → 1.43.0</li>
<li>google-oauth-client: 1.30.5 → 1.34.1</li>
<li>grpc: 1.22.1 → 1.53.0</li>
<li>gson: 2.9.0 → 2.10.1</li>
<li>guava: 27.1-jre → 31.1-jre</li>
<li>httpclient: 4.5.13 → 4.5.14</li>
<li>httpcore: 4.4.14 → 4.4.16</li>
<li>httpmime: 4.5.9 → 4.5.14</li>
<li>ij: 1.53t → 1.54c</li>
<li>ij1-patcher: 1.2.2 → 1.2.5</li>
<li>imagej: 2.9.0 → 2.11.0</li>
<li>imagej-common: 0.35.0 → 2.0.3</li>
<li>imagej-legacy: 0.39.2 → 0.39.5</li>
<li>imagej-ops: 0.48.0 → 1.0.0</li>
<li>imagej-plugins-uploader-webdav: 0.3.2 → 0.3.3</li>
<li>imagej-scripting: 0.8.3 → 0.8.4</li>
<li>imagej-ui-swing: 0.23.3 → 1.0.0</li>
<li>imagej-updater: 0.11.0 → 1.0.0</li>
<li>imglib2-algorithm: 0.12.1 → 0.13.0</li>
<li>imglib2-roi: 0.12.1 → 0.13.0</li>
<li>istack-commons-runtime: 3.0.11 → 3.0.12</li>
<li>itextpdf: 5.5.13.1 → 5.5.13.3</li>
<li>j2objc-annotations: 1.1 → 2.8</li>
<li>jackrabbit-webdav: 2.21.7 → 2.21.15</li>
<li>jackson: 2.12.5 → 2.14.2</li>
<li>javacpp: 1.5.2 → 1.5.8</li>
<li>javassist: 3.28.0-GA → 3.29.2-GA</li>
<li>jaxb-runtime: 2.3.3 → 2.3.5</li>
<li>jcodings: 1.0.55 → 1.0.58</li>
<li>jdom2: 2.0.6 → 2.0.6.1</li>
<li>jffi: 1.2.19 → 1.3.10</li>
<li>jfreechart: 1.5.3 → 1.5.4</li>
<li>jfreesvg: 3.4.1 → 3.4.3</li>
<li>jgraphx: 4.1.0 → 4.2.2</li>
<li>jheaps: 0.11 → 0.14</li>
<li>jline: 2.14.5 → 2.14.6</li>
<li>jmespath-java: 1.11.796 → 1.12.423</li>
<li>jna: 5.12.1 → 5.13.0</li>
<li>jnr-constants: 0.9.12 → 0.10.4</li>
<li>jnr-ffi: 2.1.10 → 2.2.13</li>
<li>jnr-posix: 3.0.50 → 3.1.16</li>
<li>jogamp: 2.4.0-rc-20210111 → 2.4.0</li>
<li>joda-time: 2.10.6 → 2.12.2</li>
<li>joml: 1.10.2 → 1.10.5</li>
<li>joni: 2.1.29 → 2.1.48</li>
<li>jply: 0.2.0 → 0.2.1</li>
<li>json: 20090211 → 20230227</li>
<li>kotlin: 1.4.21 → 1.7.20</li>
<li>kryo: 4.0.2 → 5.4.0</li>
<li>languagesupport: 3.1.4 → 3.3.0</li>
<li>legacy-imglib1: 1.1.9 → 1.1.10</li>
<li>logback: 1.2.3 → 1.2.11</li>
<li>miglayout: 5.2 → 5.3</li>
<li>minlog: 1.3.0 → 1.3.1</li>
<li>mpicbg: 1.4.2 → 1.5.0</li>
<li>n5-ij: 3.2.2 → 3.2.3</li>
<li>n5-imglib2: 4.3.0 → 4.4.0</li>
<li>n5-zarr: 0.0.7 → 0.0.8</li>
<li>objenesis: 2.5.1 → 3.3</li>
<li>okhttp: 4.7.2 → 4.10.0</li>
<li>okio: 2.6.0 → 3.3.0</li>
<li>opencensus-api: 0.24.0 → 0.31.1</li>
<li>opencensus-contrib-http-util: 0.24.0 → 0.31.1</li>
<li>opencsv: 5.2 → 5.7.1</li>
<li>parsington: 3.0.0 → 3.1.0</li>
<li>picocli: 4.3.2 → 4.7.1</li>
<li>plexus-utils: 1.5.6 → 3.5.1</li>
<li>postgresql: 42.2.24 → 42.5.4</li>
<li>proto-google-common-protos: 1.17.0 → 2.14.2</li>
<li>proto-google-iam-v1: 0.13.0 → 1.9.2</li>
<li>protobuf-java: 3.11.4 → 3.22.0</li>
<li>protobuf-java-util: 3.11.4 → 3.22.0</li>
<li>re2j: 1.3 → 1.7</li>
<li>reflectasm: 1.11.3 → 1.11.9</li>
<li>reload4j: 1.2.18.3 → 1.2.24</li>
<li>rhino: 1.7.6 → 1.7.14</li>
<li>rsyntaxtextarea: 3.1.6 → 3.3.2</li>
<li>scifio: 0.43.2 → 0.44.0</li>
<li>scifio-bf-compat: 4.1.0 → 4.1.1</li>
<li>scijava-common: 2.89.0 → 2.91.0</li>
<li>scijava-config: 2.0.2 → 2.0.3</li>
<li>scijava-plugins-commands: 0.2.3 → 0.2.4</li>
<li>scijava-plugins-text-plain: 0.1.3 → 0.1.4</li>
<li>scijava-search: 1.0.0 → 2.0.1</li>
<li>scijava-table: 0.7.0 → 1.0.2</li>
<li>scijava-ui-swing: 0.17.1 → 1.0.0</li>
<li>script-editor: 0.7.8 → 1.0.0</li>
<li>script-editor-jython: 0.1.3 → 1.0.0</li>
<li>scripting-javascript: 0.5.0 → 1.0.0</li>
<li>scripting-jython: 1.0.0 → 1.0.1</li>
<li>slf4j-api: 1.7.32 → 1.7.36</li>
<li>snakeyaml: 1.29 → 1.33</li>
<li>threetenbp: 1.4.1 → 1.6.5</li>
<li>txw2: 2.3.3 → 2.3.5</li>
<li>weka-dev: 3.9.5 → 3.9.6</li>
<li>xmlgraphics-commons: 2.6 → 2.8</li>
<li>3D_Blob_Segmentation: 3.0.1 → 3.0.2</li>
<li>3D_Viewer: 4.0.3 → 4.0.5</li>
<li>Auto_Threshold: 1.17.2 → 1.18.0</li>
<li>CPU_Meter: 2.0.1 → 2.0.2</li>
<li>CorrectBleach_: 2.0.3 → 2.1.0</li>
<li>Descriptor_based_registration: 2.1.7 → 2.1.8</li>
<li>FS_Align_TrakEM2: 2.0.3 → 2.0.4</li>
<li>Feature_Detection: 2.0.2 → 2.0.3</li>
<li>Fiji_Developer: 2.0.7 → 2.0.8</li>
<li>H5J_Loader_Plugin: 1.1.4 → 1.1.5</li>
<li>IO_: 4.2.0 → 4.2.1</li>
<li>KymographBuilder: 2.1.1 → 3.0.0</li>
<li>Reconstruct_Reader: 2.0.4 → 2.0.5</li>
<li>SPIM_Registration: 5.0.23 → 5.0.24</li>
<li>Stitching_: 3.1.8 → 3.1.9</li>
<li>Trainable_Segmentation: 3.3.2 → 3.3.4</li>
<li>TrakEM2_Archipelago: 2.0.3 → 2.0.4</li>
<li>VIB_: 3.0.3 → 3.0.4</li>
<li>View5D_: 2.4.0 → 2.5.0</li>
<li>Volume_Calculator: 2.0.2 → 2.0.3</li>
<li>bigdataviewer_fiji: 6.2.2 → 6.2.3</li>
<li>bigwarp_fiji: 7.0.6 → 7.0.7</li>
<li>bio-formats_plugins: 6.10.1 → 6.12.0</li>
<li>blockmatching_: 2.1.3 → 2.1.4</li>
<li>mpicbg_: 1.4.2 → 1.5.0</li>
<li>n5-viewer_fiji: 4.5.0 → 5.3.0</li>
<li>register_virtual_stack_slices: 3.0.7 → 3.0.8</li>
</ul>
<h3>
<a name="removed-dependencies-2" class="anchor" href="#removed-dependencies-2"></a>Removed dependencies</h3>
<ul>
<li>jcommon (not used anymore)</li>
<li>scijava-plugins-io-table (obsolete)</li>
</ul>
<h3>
<a name="new-dependencies-3" class="anchor" href="#new-dependencies-3"></a>New dependencies</h3>
<ul>
<li>auto-value: 1.10.1</li>
<li>conscrypt-openjdk-uber: 2.5.2</li>
<li>gapic-google-cloud-storage-v2: 2.20.1-alpha</li>
<li>gax-grpc: 2.23.2</li>
<li>google-cloud-core-grpc: 2.12.0</li>
<li>google-http-client-apache-v2: 1.43.0</li>
<li>google-http-client-gson: 1.43.0</li>
<li>grpc-google-cloud-storage-v2: 2.20.1-alpha</li>
<li>okio-jvm: 3.0.0</li>
<li>opencensus-proto: 0.2.0</li>
<li>perfmark-api: 0.26.0</li>
<li>proto-google-cloud-resourcemanager-v3: 1.14.0</li>
<li>proto-google-cloud-storage-v2: 2.20.1-alpha</li>
</ul>
</details>
<p>As <a href="https://forum.image.sc/t/timeline-for-the-next-fiji-update/69640">we did with the previous Fiji release</a>, it would be awesome if maintainers of Fiji plugins could do some testing, to validate that their plugins still work as intended with Fiji 2.11.0. I will be out of the office from tomorrow through the weekend, back on Monday, March 27, at which point I will work to address any problems reported.</p>
<p>Incomplete table of maintainer mentions:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Who</th>
<th>What</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a class="mention" href="/u/albertcardona">@albertcardona</a> <a class="mention" href="/u/axtimwalde">@axtimwalde</a>
</td>
<td>TrakEM2</td>
</tr>
<tr>
<td><a class="mention" href="/u/axtimwalde">@axtimwalde</a></td>
<td>your registration plugins, and N5 including blosc support</td>
</tr>
<tr>
<td><a class="mention" href="/u/bene.schmid">@bene.schmid</a></td>
<td>3D Viewer and 3Dscript</td>
</tr>
<tr>
<td><a class="mention" href="/u/bogovicj">@bogovicj</a></td>
<td>BigWarp</td>
</tr>
<tr>
<td><a class="mention" href="/u/christian_tischer">@Christian_Tischer</a></td>
<td>MoBIE</td>
</tr>
<tr>
<td><a class="mention" href="/u/gselzer">@gselzer</a></td>
<td>ImageJ Ops</td>
</tr>
<tr>
<td><a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a></td>
<td>CLIJ and other fun things</td>
</tr>
<tr>
<td><a class="mention" href="/u/hinerm">@hinerm</a></td>
<td>SCIFIO: File &gt; Import &gt; Image… etc.; ImageJ-OMERO</td>
</tr>
<tr>
<td><a class="mention" href="/u/imagejan">@imagejan</a></td>
<td>batch-processor</td>
</tr>
<tr>
<td><a class="mention" href="/u/markkitt">@markkitt</a></td>
<td>JHDF5-related functionality</td>
</tr>
<tr>
<td><a class="mention" href="/u/mdoube">@mdoube</a></td>
<td>BoneJ2</td>
</tr>
<tr>
<td><a class="mention" href="/u/nicokiaru">@NicoKiaru</a></td>
<td>ABBA</td>
</tr>
<tr>
<td>
<a class="mention" href="/u/skalarproduktraum">@skalarproduktraum</a> <a class="mention" href="/u/kephale">@kephale</a>
</td>
<td>sciview</td>
</tr>
<tr>
<td><a class="mention" href="/u/stephanpreibisch">@StephanPreibisch</a></td>
<td>BigStitcher, Grid/Collection Stitching, etc.</td>
</tr>
<tr>
<td><a class="mention" href="/u/tferr">@tferr</a></td>
<td>SNT, as well as the new FlatLaf code</td>
</tr>
<tr>
<td><a class="mention" href="/u/tinevez">@tinevez</a></td>
<td>TrackMate</td>
</tr>
<tr>
<td><a class="mention" href="/u/tpietzsch">@tpietzsch</a></td>
<td>BigDataViewer and related things</td>
</tr>
<tr>
<td><a class="mention" href="/u/wayne">@Wayne</a></td>
<td>Anything you want related to core ImageJ</td>
</tr>
<tr>
<td>Everyone</td>
<td>Anything and everything you care about!</td>
</tr>
</tbody>
</table>
</div><p>Looking forward to hearing how everything is broken now, once I get back! <img src="https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12" title=":sweat_smile:" class="emoji" alt=":sweat_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>There is probably something similar you can do in Fiji, but in QuPath, you can perform thresholding on the three channels and then keep intersections between the channels that are over a given threshold.</p>
<p>I believe Fiji has options in the ROI manager for Union and Intersect, which probably provide similar results.</p>
<p>Colocalization experiments are tricky (colocalized doesn’t really mean anything, or it means everything - <a href="https://youtu.be/P2JvFe0hB_M?t=209" class="inline-onebox">Deconstructing co-localisation workflows: A journey into the black boxes [NEUBIAS Academy@Home] - YouTube</a>), and even more so since you haven’t stated what you want to measure. “Quantify” means very different things depending on whether you are looking at percentages of cells with a certain characteristic, percentage of area within given cell populations, Manders coefficients or Pearson’s coefficients.</p>
<p>Even worse if you have a 2D image of a 3D sample, which most are, since even the thinnest samples are actually 3D, and you are likely to have Z axis overlap without channels actually overlapping in Z. There is no fix for this, just something to be aware of when interpreting the results.</p> ;;;; <p>Could it be that they belong to the same track? That they come from a cell that divided?</p> ;;;; <p>Hi, I am trying to export my MPII trained DLC model to use in DLC-Live, but I am getting an error. I would appreciate some assistance with this. Thank you.<br>
This is the entire error message:<br>
2023-03-20 17:38:00.654692: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:227 : OUT_OF_RANGE: Read less bytes than requested<br>
Traceback (most recent call last):<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1377, in _do_call<br>
return fn(*args)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1360, in _run_fn<br>
return self._call_tf_sessionrun(options, feed_dict, fetch_list,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1453, in _call_tf_sessionrun<br>
return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,<br>
tensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested<br>
[[{{node save/RestoreV2}}]]</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):<br>
File “”, line 1, in <br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 309, in export_model<br>
sess, input, output, dlc_cfg = load_model(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 179, in load_model<br>
sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py”, line 238, in setup_GPUpose_prediction<br>
restorer.restore(sess, cfg[“init_weights”])<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 1417, in restore<br>
sess.run(self.saver_def.restore_op_name,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 967, in run<br>
result = self._run(None, fetches, feed_dict, options_ptr,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1190, in _run<br>
results = self._do_run(handle, final_targets, final_fetches,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1370, in _do_run<br>
return self._do_call(_run_fn, feeds, fetches, targets, options,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1396, in _do_call<br>
raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter<br>
tensorflow.python.framework.errors_impl.OutOfRangeError: Graph execution error:</p>
<p>Detected at node ‘save/RestoreV2’ defined at (most recent call last):<br>
File “”, line 1, in <br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 309, in export_model<br>
sess, input, output, dlc_cfg = load_model(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 179, in load_model<br>
sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py”, line 225, in setup_GPUpose_prediction<br>
restorer = tf.compat.v1.train.Saver()<br>
Node: ‘save/RestoreV2’<br>
Read less bytes than requested<br>
[[{{node save/RestoreV2}}]]</p>
<p>Original stack trace for ‘save/RestoreV2’:<br>
File “”, line 1, in <br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 309, in export_model<br>
sess, input, output, dlc_cfg = load_model(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 179, in load_model<br>
sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py”, line 225, in setup_GPUpose_prediction<br>
restorer = tf.compat.v1.train.Saver()<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 933, in <strong>init</strong><br>
self.build()<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 945, in build<br>
self._build(self._filename, build_save=True, build_restore=True)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 973, in _build<br>
self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 543, in _build_internal<br>
restore_op = self._AddRestoreOps(filename_tensor, saveables,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 363, in _AddRestoreOps<br>
all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 611, in bulk_restore<br>
return io_ops.restore_v2(filename_tensor, names, slices, dtypes)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py”, line 1501, in restore_v2<br>
_, _, _op, _outputs = _op_def_library._apply_op_helper(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py”, line 797, in _apply_op_helper<br>
op = g._create_op_internal(op_type_name, inputs, dtypes=None,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py”, line 3754, in _create_op_internal<br>
ret = Operation(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py”, line 2133, in <strong>init</strong><br>
self._traceback = tf_stack.extract_stack_for_node(self._c_op)</p>
<blockquote>
<blockquote>
<blockquote>
<p>deeplabcut.export_model(‘/home/user/Nishat/MPII_model-nishat-2023-02-27/config.yaml’, iteration=0, shuffle=1, trainingsetindex=0, snapshotindex=None, TFGPUinference=True, overwrite=False, make_tar=True)<br>
2023-03-20 17:40:05.589750: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:227 : OUT_OF_RANGE: Read less bytes than requested<br>
Traceback (most recent call last):<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1377, in _do_call<br>
return fn(*args)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1360, in _run_fn<br>
return self._call_tf_sessionrun(options, feed_dict, fetch_list,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1453, in _call_tf_sessionrun<br>
return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,<br>
tensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested<br>
[[{{node save/RestoreV2}}]]</p>
</blockquote>
</blockquote>
</blockquote>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):<br>
File “”, line 1, in <br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 309, in export_model<br>
sess, input, output, dlc_cfg = load_model(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 179, in load_model<br>
sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py”, line 238, in setup_GPUpose_prediction<br>
restorer.restore(sess, cfg[“init_weights”])<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 1417, in restore<br>
sess.run(self.saver_def.restore_op_name,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 967, in run<br>
result = self._run(None, fetches, feed_dict, options_ptr,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1190, in _run<br>
results = self._do_run(handle, final_targets, final_fetches,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1370, in _do_run<br>
return self._do_call(_run_fn, feeds, fetches, targets, options,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/client/session.py”, line 1396, in _do_call<br>
raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter<br>
tensorflow.python.framework.errors_impl.OutOfRangeError: Graph execution error:</p>
<p>Detected at node ‘save/RestoreV2’ defined at (most recent call last):<br>
File “”, line 1, in <br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 309, in export_model<br>
sess, input, output, dlc_cfg = load_model(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 179, in load_model<br>
sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py”, line 225, in setup_GPUpose_prediction<br>
restorer = tf.compat.v1.train.Saver()<br>
Node: ‘save/RestoreV2’<br>
Read less bytes than requested<br>
[[{{node save/RestoreV2}}]]</p>
<p>Original stack trace for ‘save/RestoreV2’:<br>
File “”, line 1, in <br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 309, in export_model<br>
sess, input, output, dlc_cfg = load_model(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/export.py”, line 179, in load_model<br>
sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py”, line 225, in setup_GPUpose_prediction<br>
restorer = tf.compat.v1.train.Saver()<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 933, in <strong>init</strong><br>
self.build()<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 945, in build<br>
self._build(self._filename, build_save=True, build_restore=True)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 973, in _build<br>
self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 543, in _build_internal<br>
restore_op = self._AddRestoreOps(filename_tensor, saveables,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 363, in _AddRestoreOps<br>
all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/training/saver.py”, line 611, in bulk_restore<br>
return io_ops.restore_v2(filename_tensor, names, slices, dtypes)<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py”, line 1501, in restore_v2<br>
_, _, _op, _outputs = _op_def_library._apply_op_helper(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py”, line 797, in _apply_op_helper<br>
op = g._create_op_internal(op_type_name, inputs, dtypes=None,<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py”, line 3754, in _create_op_internal<br>
ret = Operation(<br>
File “/home/user/.pyenv/versions/dlc-env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py”, line 2133, in <strong>init</strong><br>
self._traceback = tf_stack.extract_stack_for_node(self._c_op)</p> ;;;; <p>Another thing you can do to see if Java is configured correctly open up command prompt and type “java”. Do other java programs run correctly? Check your JAVA_HOME environmental variable. <a href="https://mkyong.com/java/how-to-set-java_home-on-windows-10/" rel="noopener nofollow ugc">https://mkyong.com/java/how-to-set-java_home-on-windows-10/</a></p> ;;;; <p>Hi everyone,</p>
<p>First time poster and new to Imagej, so I apologize for any mistakes in question format.<br>
Anyhow, I am currently working on measuring the difference in neuronal activation via whole mount staining with cfos, dapi and tuj. The goal is the compare the cfos localization with the dapi and finally see if they are “in/part of” the tuj/neuronal body, thereby technically verifying that this is indeed neuronal activation. However, I haven’t been able to find a good macro/technique to go about efficiently quantifying this. The problem with the few macros I have found and tried so far is that they don’t automatically quantify for me the cfos co-localization with dapi and not in relation to the presence of tuj. I was wondering if you guys have any ideas or recommendations for how to better quantify this?</p>
<p>Attached is a sample picture of what the stain looks like (it looks better when opened in imagej or lasx program).<br>
<a class="attachment" href="/uploads/short-url/ratzsmN1wOhiu4ZwTcPpDEsSxfL.tif">sample wm stain.tif</a> (3.0 MB)</p>
<p>Thanks!</p> ;;;; <p>Yup, that was the issue there! What an odd thing to have happened.</p>
<p>When I run the code now, it successfully prompts me for everything it should be (input/output directories, and brightness parameters) but then I get this Macro error</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7.jpeg" data-download-href="/uploads/short-url/pCOKQukZuSIdXXcjSRk1lBidzX9.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_690x294.jpeg" alt="image" data-base62-sha1="pCOKQukZuSIdXXcjSRk1lBidzX9" width="690" height="294" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_690x294.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_1035x441.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_1380x588.jpeg 2x" data-dominant-color="DDDBDB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">2936×1252 622 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi Erik. I tried extracting it in C drive but the problem is not getting resolved.<br>
Yes, in my previous laptop that was Windows 7 Fiji used to work absolutely fine.<br>
I followed the environmental variable steps, but I don’t know what value I should put. Can you please help me with it?</p> ;;;; <aside class="quote no-group" data-username="jonje219" data-post="5" data-topic="78830">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/j/e95f7d/40.png" class="avatar"> Jon Jerlström Hultqvist:</div>
<blockquote>
<p>Turning down the patch size and turning off the augmentations made it possible to start the training.</p>
</blockquote>
</aside>
<p>Does it work with augmentations?</p>
<blockquote>
<p>I thought the input patch size should be the size (or almost) the size of the input image.</p>
</blockquote>
<p>Perhaps in a perfect world, in reality you need to find a balance between available GPU memory and the input patch size. The idea to have the input patch size big enough to fit the context of the object, so that the network will be able to see it.</p>
<blockquote>
<p>I have several classes I would like to predict. Would it be advisable to run these separately or together?</p>
</blockquote>
<p>It depends, there is no direct answer. For example, it may depend on density of classes, if you have some very rare classes it may be better to crop those areas out and start training.<br>
If I am not completely mistaken, here is a video how to extract patches:</p><div class="onebox lazyYT lazyYT-container" data-youtube-id="QrKHgP76_R0" data-youtube-title="MIB Brief: Generation of patches for deep learning segmentation" data-parameters="feature=oembed&amp;wmode=opaque">
  <a href="https://www.youtube.com/watch?v=QrKHgP76_R0" target="_blank" rel="noopener">
    <img class="ytp-thumbnail-image" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99141162c80ce25eb5c591260ac25ecf11316772.jpeg" title="MIB Brief: Generation of patches for deep learning segmentation" width="480" height="360">
  </a>
</div>

<blockquote>
<p>And is masking the outside of the cell / ROIs also advisable?</p>
</blockquote>
<p>If you have not much experience, you should start with simple cases without masking (masking requires preprocessing). I typically tend to prepare the ground truth for training such that the mask is not needed, as it makes training faster. You can download sample networks from Menu-&gt;File-&gt;Example datasets. There are synthetic datasets with spots, they are very quick to do training.</p>
<blockquote>
<p>I’m very new to this</p>
</blockquote>
<p>it is a rewarding process, but start with something simple that will give you quicker the results and would allow to play with parameters and options.</p>
<p>One more thing, whenever possible try to use square input patches as it allows to use 90-degree rotations as one of augmentations.</p> ;;;; <p>In RankFilters.java:</p>
<pre><code class="lang-java">if (filterType == DESPECKLE) {
			filterType = MEDIAN;
			radius = 1.0;
		}
</code></pre> ;;;; <p>No experience with omero, but adding the omero tags in case someone more familiar might know how things could change. I recall some issues with multichannel images and the representations of the images being changed, what omero shows vs what the original data is. But not sure if/how that relates to the current issue.</p>
<p>If you could host/share a small project that might help, but not sure if your images are sharable. Again, not an issue that comes up normally that I can recall, but I haven’t tried since 0.4.2 or on a Mac.</p> ;;;; <p>Might be relevant <a href="https://forum.image.sc/t/actual-resolution-of-mirax-wsis-in-qupath/47968/2" class="inline-onebox">Actual resolution of Mirax WSIs in QuPath - #2 by petebankhead</a><br>
Since the number of X pixels is about the same its unlikely to be a downsampling/resizing. More likely an XY shift.</p>
<p>Alternatively, are the XY dimensions of pixels non-square?</p> ;;;; <p>Dear MicroscopyRA,</p>
<p>Thanks very much for your input.</p>
<p>I didn’t try BioFormats, but I should point out that in order to work efficiently I had downloaded slide images in .ndpi format from our local OMERO server. These were used for annotation, pixel classification and subsequent testing that revealed the issue of different results on original versus training images.</p>
<p>By contrast, when instead I created a project in which the slide was linked to the OMERO server by its URL and used that slide to create a training slide for pixel classification, the resulting classifier gave a much more similar annotation for both the training and original images. As a control, using the local .ndpi file to train the classifier again yielded training and original annotations that differed greatly.</p>
<p>So it’s looking to me like the culprit may be the use of the local .ndpi file downloaded from OMERO.  I’ll let you know as I continue testing whether this pans out, but to me the solution (albeit potentially less efficient) is to use OMERO-served images rather than local files for training. Maybe something was altered in the .ndpi  download from OMERO (metadata?) but I really have no idea.</p>
<p>Cheers,<br>
Mark</p> ;;;; <p>Dear Konrad, thank you for answering - I will try to obtain those versions. It’s not clear to me, on windows, which if those three items I need to download and install independently, and which will be installed automatically when I create the conda env from your yaml? In the past, I believe I only downloaded the GPU driver and the CUDA toolkit as standalone installs. Your version syntax looks python-y, so I’m wondering if you mean I should install some or all of those via some commands / package manager within python?</p> ;;;; <p>Hi <a class="mention" href="/u/dsudar">@dsudar</a>,</p>
<p>Yes, the recordings will be made available very soon. <a class="mention" href="/u/joshmoore">@joshmoore</a> will put them here:<br>
<a href="https://downloads.openmicroscopy.org/presentations/2023/NGFF-community-call-2023-03-15/" class="inline-onebox" rel="noopener nofollow ugc">Index of /presentations/2023/NGFF-community-call-2023-03-15</a>.</p>
<p>Here is the presentation for now already: <a href="https://docs.google.com/presentation/d/1QOW8Py_BCpQiysE0Kmyvdui3ghkjB6rD/edit?usp=sharing&amp;ouid=103031079254476210617&amp;rtpof=true&amp;sd=true" class="inline-onebox" rel="noopener nofollow ugc">Loading Google Slides</a></p>
<p>A summary of the meeting will soon be made available as well.</p> ;;;; <p>Here are two images! I am hoping to segment the plaques from channels 00 and 01.<br>
<a class="attachment" href="/uploads/short-url/bDndgZLusikoW0DSqdQZzZkPcvS.tif">cell profiler test_2021111 CC_AD_089 PBB5 Iba1 a _z06_ch00.tif</a> (92.9 KB)<br>
<a class="attachment" href="/uploads/short-url/gLWUJq0BbfGEaWqkKnugmDlxtwP.tif">cell profiler test_2021111 CC_AD_089 PBB5 Iba1 a _z06_ch01.tif</a> (81.2 KB)</p> ;;;; <p>Hi <a class="mention" href="/u/wmv1992">@wmv1992</a> ,<br>
Thanks much for organizing these sessions last week. That was extremely useful. Besides the HackMD record, would you be able to share your presentation? And was either (or both) of the sessions recorded?<br>
Thanks,<br>
Damir</p> ;;;; <p>I’d like to capture a grid with Micro-Magellan either at a specific Z location or with some autofocusing. I can create and capture the grid with the code below:</p>
<pre><code class="lang-auto">from pycromanager import Bridge,Acquisition

bridge = Bridge()
magellan = bridge.get_magellan()
magellan.create_grid('my_grid', 3, 3, 0.0, 0.0)
acq_settings = magellan.get_acquisition_settings(0)
acq_settings.set_acquisition_name('experiment_1')
acq_settings.set_saving_dir('D:\test')
acq_settings.set_xy_position_source('my_grid')
acq = Acquisition(magellan_acq_index=0)
</code></pre>
<p>However, I seem unable to get Magellan to start at a specific Z height, neither by manually setting the microscope to a particular Z nor w/ MM Core <code>mmcore.set_position(z)</code>. Magellan always starts the grid at z = 0. Is there a way to set this? In the Explore controls, I can set the Z limits, but if I do this from Pycromanager, I get an error “<code>java.lang.RuntimeException: Expected surface but didn't find one. Check acquisition settings</code>” One other thing I noted is that in the Explore GUI, the Z Device for “my_grid” is N/A.</p>
<p>Maybe this would be easier with an XYTiledAcquision, but 0.13.2 doesn’t have that enabled yet.</p>
<p>This is with Micro-Manager 2.0.0 and Pycromanager 0.13.2.</p> ;;;; <p>Hello, I am trying to switch over from using SLEAP to DLC and I have a bunch of old .slp labels. I was wondering if DLC lets me import SLEAP labels?</p> ;;;; <p>Hello everyone,</p>
<p>I have a Qupath project with multiple images and annotations on top of them. I successfully exported my annotations but I just realised that the level dimensions are off between the original whole slide images and the images in the Qupath project.</p>
<p>For instance, one image .mrxs is of size (118216, 264186) at level 0, but when opening the Qupath project it is of size (110768, 146800). I was not able to find in the documentation a part mentioning any resizing made implicitly by QuPath.</p>
<p>Do you know why this occurs and how I could translate my annotations in the original coordinate system of my slide?</p>
<p>Many thanks in advance</p>
<p>PAB</p> ;;;; <p>That is very odd. This is hard to troubleshoot since we both have the same operating system. Have you tried extracting it to another folder not the desktop? I tested with the extracted folder being C:/test and it worked fine. Have you managed to get Fiji running on another machine? I think some environmental value could be the issue. Check your java path environment variable <a href="https://www.java.com/en/download/help/path.html" rel="noopener nofollow ugc">https://www.java.com/en/download/help/path.html</a></p> ;;;; <aside class="quote no-group" data-username="ym.lim" data-post="6" data-topic="78814">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png" class="avatar"> Yau Mun Lim:</div>
<blockquote>
<p>I’m happy to report it technically works!</p>
</blockquote>
</aside>
<p>Great!</p>
<aside class="quote no-group" data-username="ym.lim" data-post="6" data-topic="78814">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png" class="avatar"> Yau Mun Lim:</div>
<blockquote>
<p>Although slightly sad that I can’t use the gaussian blur ImageOps filter as my objects of interest are grainy.</p>
</blockquote>
</aside>
<p>Have you tried? It looks like you can add preprocessing ops <em>and</em> global normalization, but the preprocessing will occur afterwards:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906">
  <header class="source">

      <a href="https://github.com/qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906" target="_blank" rel="noopener">qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906</a></h4>



    <pre class="onebox"><code class="lang-java">
      <ol class="start lines" start="896" style="counter-reset: li-counter 895 ;">
          <li>				.stream()</li>
          <li>				.filter(t -&gt; mask == null || mask.intersects(GeometryTools.createRectangle(t.getImageX(), t.getImageY(), t.getImageWidth(), t.getImageHeight())))</li>
          <li>				.collect(Collectors.toList());</li>
          <li>		</li>
          <li>		// Detect all potential nuclei</li>
          <li>		var server = imageData.getServer();</li>
          <li>		var cal = server.getPixelCalibration();</li>
          <li>		double expansion = cellExpansion / cal.getAveragedPixelSize().doubleValue();</li>
          <li>		var plane = request.getImagePlane();</li>
          <li>		</li>
          <li class="selected">		// Compute op with preprocessing</li>
          <li>		var fullPreprocess = new ArrayList&lt;ImageOp&gt;();</li>
          <li>		fullPreprocess.add(ImageOps.Core.ensureType(PixelType.FLOAT32));</li>
          <li>		</li>
          <li>		// Do global preprocessing calculations, if required</li>
          <li>		if (globalPreprocess != null) {</li>
          <li>			try {</li>
          <li>				var normalizeOps = globalPreprocess.createOps(op, imageData, roi, request.getImagePlane());</li>
          <li>				fullPreprocess.addAll(normalizeOps);</li>
          <li>			} catch (IOException e) {</li>
          <li>				throw new RuntimeException("Exception computing global normalization", e);</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>That being said, I’m not sure how much it will help – since there are convolutions in the StarDist model and I’m not sure if pre-convolving the image with a Gaussian filter would help or hurt.</p> ;;;; <p>Hello,</p>
<p>I have very recently installed Micro-Manager for the first time (in Windows 10), to use it with a few generic USB microscopes via OpenCVGrabber (fancier hardware may happen in the future). I am hitting an issue and I have not been able to get around it.</p>
<p>I have two USB microscopes connected to the computer (different views), and I can only get images from one of them (which is the least useful one, as Murphy’s laws indicate). The only way I found to be able to get images from the other microscope is to physically unplug the first one. That is not really useful…</p>
<p>I have looked at all the properties in the OpenCVgrabber device, and I do not see anything I can change that lets me change the USB camera. I tried adding a second OpenCVgrabber device, but it seems to be also stuck on the same one. I tried to edit the “Camera” property in the Hardware ConfigurationWizard (which starts as “undefined”) to both a number and the USB camera name, and nothing seems to work.</p>
<p>I do not need to acquire from both cameras at the same time (although that could be useful), but I would need to be able to switch between them, or at least to see them both as usable.</p>
<p>Any help here would be truly appreciated.</p>
<p>Thank you very much!</p> ;;;; <p>I’m happy to report it technically works! Although slightly sad that I can’t use the gaussian blur ImageOps filter as my objects of interest are grainy.</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
def stains = imageData.getColorDeconvolutionStains()
def server = imageData.getServer()
def deconvServer = new TransformedServerBuilder(server).deconvolveStains(stains, 1).build() // 1 = haematoxylin, 2 = DAB
def deconvImageData = new ImageData&lt;&gt;(deconvServer, imageData.getHierarchy(), imageData.getImageType())

def stardist = StarDist2D.builder(pathModel)
   .preprocess(
        StarDist2D.imageNormalizationBuilder()
            .maxDimension(4096)
            .percentiles(0, 99.8)
            .build()
   )
    .threshold(0.5)              // Probability (detection) threshold
    .pixelSize(pixelSize)
    .cellExpansion(cellExpansionPixels)          // Approximate cells based upon nucleus expansion
    .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size
    .measureShape()              // Add shape measurements
    .measureIntensity()          // Add cell measurements (in all compartments)
    .includeProbability(true)    // Add probability as a measurement (enables later filtering)
    .build()

// Get annotations to run StarDist
def stardistParentAnno = getAnnotationObjects().findAll{it.getPathClass() == getPathClass("Tissue")}

stardist.detectObjects(deconvImageData, stardistParentAnno)
stardist.close()
</code></pre> ;;;; <p>Possible entry…</p>
<pre><code class="lang-auto">//-------------------------
run("Duplicate...", "title=1");
run("Duplicate...", "title=2");
run("Invert");
imageCalculator("Subtract create", "2","1");
selectWindow("Result of 2");
run("Unsharp Mask...", "radius=30 mask=0.60");
setAutoThreshold("Default dark");
setThreshold(0, 94);
//run("Threshold...");
setOption("BlackBackground", true);
run("Convert to Mask");
run("Median", "radius=3");
run("Erode");
run("Erode");
run("Fill Holes");
doWand(1220, 104, 0.0, "Legacy");
setBackgroundColor(0,0,0);
run("Clear Outside");
selectWindow("1");
run("Restore Selection");
//-------------------------
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4.jpeg" data-download-href="/uploads/short-url/6h7IMr4AVEKGc5plz2hl3wj79yY.jpeg?dl=1" title="obtenir" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_139x250.jpeg" alt="obtenir" data-base62-sha1="6h7IMr4AVEKGc5plz2hl3wj79yY" width="139" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_139x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_208x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_278x500.jpeg 2x" data-dominant-color="4A4A4A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">obtenir</span><span class="informations">1254×2244 161 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Does the print statement show the correct results?<br>
Also, the script as written will only work for the first child object, and only if it is a cluster, due to the [0]</p> ;;;; <p>Turning down the patch size and turning off the augmentations made it possible to start the training. I thought the input patch size should be the size (or almost) the size of the input image.</p>
<p>I have several classes I would like to predict. Would it be advisable to run these separately or together? And is masking the outside of the cell / ROIs also advisable?</p>
<p>I’m very new to this. Thanks for the help.</p> ;;;; <p>Hi! VERY beginner programmer here, I’m trying to figure out if I can access the sholl profiles calculated from the normal semi-automated tracing functions in a python script to do further calculations without leaving SNT!</p>
<p>I’m hoping to calculate the Branching Index, an alternative to the Ramification index that biases more heavily towards distal branching, according to the paper cited below.  I have written a code that works if I use an exported csv file in a python compiler, but I was wondering if it would be possible to use with SNT for ease in the future!</p>
<p>Luis Miguel Garcia-Segura, Julio Perez-Marquez.  A new mathematical function to evaluate neuronal morphology using the Sholl analysis, Journal of Neuroscience Methods, Volume 226, 2014, Pages 103-109, ISSN 0165-0270, <a href="https://doi.org/10.1016/j.jneumeth.2014.01.016" rel="noopener nofollow ugc">https://doi.org/10.1016/j.jneumeth.2014.01.016</a>. (<a href="https://www.sciencedirect.com/science/article/pii/S0165027014000272" class="inline-onebox" rel="noopener nofollow ugc">A new mathematical function to evaluate neuronal morphology using the Sholl analysis - ScienceDirect</a>)</p>
<p>Here is my code so far, it uses the pandas package on python.</p>
<pre><code class="lang-python"># reading CSV file
data = read_csdata = read_csv("/Users/gabriellafricklas1/Downloads/testSholl.csv")

# converting column data to list
intersections = data['Inters.'].tolist()
radius = data['Radius'].tolist()
vector = list()
for i in range(1, len(radius)):
    newVal = (intersections[i] - intersections[i - 1]) *(i+1)
    if newVal &lt; 0:
        vector.append(0)

    else:
        vector.append(newVal)
BI = sum(vector)

print('BI', BI)
</code></pre> ;;;; <p>Yes, but you’ll need to switch</p>
<pre><code class="lang-auto">.extractChannels(0, 1)
</code></pre>
<p>to be</p>
<pre><code class="lang-auto">.deconvolveStains(stains, 0)
</code></pre>
<p>or something similar.</p> ;;;; <p>ok not easy to script, I try something that is not working yet (no error but no measure also !) :</p>
<p>def imageData = getCurrentImageData()<br>
def hierarchy = imageData.getHierarchy()<br>
def annotations = hierarchy.getAnnotationObjects()<br>
def server = imageData.getServer()</p>
<p>Area = 0<br>
String objectType = “subcell”</p>
<p>if(objectType == “cytoplasm”){<br>
getCellObjects().each{<br>
Area = it.getChildObjects()[0].getMeasurementList().getMeasurementValue(“Subcellular Cluster: Channel 3: Area”)<br>
println(Area)<br>
it.getMeasurementList().putMeasurement(“Area_phagocytes_inside_cyto”, Area)<br>
}<br>
}</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816.jpeg" data-download-href="/uploads/short-url/4nFRE3g5s6EQ1D4jjv5SMexdNmm.jpeg?dl=1" title="Clipboard" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816_2_689x360.jpeg" alt="Clipboard" data-base62-sha1="4nFRE3g5s6EQ1D4jjv5SMexdNmm" width="689" height="360" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816_2_689x360.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816_2_1033x540.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816.jpeg 2x" data-dominant-color="4E2E55"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Clipboard</span><span class="informations">1279×669 132 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <aside class="quote no-group" data-username="petebankhead" data-post="3" data-topic="78814">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<p>A possible workaround would be to use a trick like the one described here: <a href="https://forum.image.sc/t/running-superpixels-plugin-on-a-single-channel/78640/4">Running superpixels plugin on a single channel - #4 by petebankhead</a></p>
<p><a href="https://qupath.github.io/javadoc/docs/qupath/lib/images/servers/TransformedServerBuilder.html#deconvolveStains(qupath.lib.color.ColorDeconvolutionStains,int...)" rel="noopener nofollow ugc"><code>deconvolveStains</code></a> is one of the options when creating a <code>TransformedImageServer</code>.</p>
</blockquote>
</aside>
<p>If I am understanding you correctly, you mean to create a deconvolved imageData separately, and then run the StarDist builder with the global normalisation preprocess on the deconvolved imageData? e.g.:</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
def server = imageData.getServer()
def otherServer = new TransformedServerBuilder(server).extractChannels(0, 1).build()
def sneakyImageData = new ImageData&lt;&gt;(otherServer, imageData.getHierarchy(), imageData.getImageType())

def stardist = StarDist2D.builder(pathModel)
    .preprocess(
            StarDist2D.imageNormalizationBuilder()
                .perChannel(true)
                .maxDimension(4096)
                .percentiles(0, 99.8)
                .build()
    )
    .threshold(0.5)              // Probability (detection) threshold
    .pixelSize(pixelSize)
    .cellExpansion(cellExpansionPixels)          // Approximate cells based upon nucleus expansion
    .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size
    .measureShape()              // Add shape measurements
    .measureIntensity()          // Add cell measurements (in all compartments)
    .includeProbability(true)    // Add probability as a measurement (enables later filtering)
    .build()

// Get annotations to run StarDist
def stardistParentAnno = getAnnotationObjects().findAll{it.getPathClass() == getPathClass("Tissue")}

stardist.detectObjects(sneakyImageData, stardistParentAnno)
stardist.close()
</code></pre> ;;;; <p>Few quick comments:</p>
<ul>
<li>
<p>do you images have 1 color channel?</p>
</li>
<li>
<p>your input patch size seems to be quite large, you most likely will run out of GPU memory -  make it square and start with something as ‘512 512 1 1’. Also if I press Check network it gives an error indicating a wrong patch size:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff0a94ea01375a8b5a4026b23d84b0613fddcc4c.png" alt="image" data-base62-sha1="Aocqq5LILoqQc1YaadyEP1EafWQ" width="459" height="184">. so make it square and smaller.</p>
</li>
<li>
<p>after that check whether switching off augmentations will allow to start the training.</p>
</li>
</ul> ;;;; <p>Here is the training tab for Resnet18.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03.png" data-download-href="/uploads/short-url/c0H0lOLqHxYXHRlHzqyDUF2hZS3.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_690x223.png" alt="image" data-base62-sha1="c0H0lOLqHxYXHRlHzqyDUF2hZS3" width="690" height="223" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_690x223.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_1035x334.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_1380x446.png 2x" data-dominant-color="D9E5F2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1910×620 38.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Here is the training tab for U-net. I did adjust the input patch size to get the training to initiate:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58.png" data-download-href="/uploads/short-url/Ava3MjI6zsHL1CwtxOFqq56vf7y.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_690x223.png" alt="image" data-base62-sha1="Ava3MjI6zsHL1CwtxOFqq56vf7y" width="690" height="223" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_690x223.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_1035x334.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_1380x446.png 2x" data-dominant-color="D9E5F2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1911×619 37.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/ym.lim">@ym.lim</a> I think I just didn’t write code to handle that scenario. You can add preprocessing steps <em>after</em> the global normalization, but not before.</p>
<p>A possible workaround would be to use a trick like the one described here: <a href="https://forum.image.sc/t/running-superpixels-plugin-on-a-single-channel/78640/4" class="inline-onebox">Running superpixels plugin on a single channel - #4 by petebankhead</a></p>
<p><a href="https://qupath.github.io/javadoc/docs/qupath/lib/images/servers/TransformedServerBuilder.html#deconvolveStains(qupath.lib.color.ColorDeconvolutionStains,int...)"><code>deconvolveStains</code></a> is one of the options when creating a <code>TransformedImageServer</code>.</p> ;;;; <p>Thanks or much for the fast reply, that’s great!</p>
<p>We’re not quite up and running yet, but hopefully not too far away.</p>
<p>Using microscope 0.6.0 from pip, on Ubuntu 20.04, with the stage plugged in over USB.<br>
We’re running everything as root, and we confirm that the SimpleC programme that came with the SDK compiles and runs OK with:<br>
<code>SimpleC -c USB -vi 16da -pi 0007</code></p>
<p>Using microscope we do not succeed in completing <code>init_sdk()</code> – it manages to load the DLL (although we hard to write the path in hard)</p>
<pre><code class="lang-auto">__class__._lib = ctypes.CDLL("libLinkamSDK.so")
</code></pre>
<p>We’re then able to read the license correctly, however when we get to:</p>
<pre><code class="lang-auto">        cfunc = ctypes.CFUNCTYPE(_uint32_t, _CommsHandle, _ControllerStatus)(
            __class__._on_new_value
        )
</code></pre>
<p>We get this output:</p>
<blockquote>
<p>*** stack smashing detected ***: terminated<br>
Aborted</p>
</blockquote>
<p>Have you seen this before?<br>
We’ve tried this both with the “latest” 3.0.16 <code>libLinkamSDK.so</code> (~20.7MB) and the one that came on the official USB stick with the license (17.9MB) whose version it is not easy to understand (apparently &gt;3.0).</p>
<p>We’d really appreciate some tips, we’re working towards a projects that will be open sourced and given back to the community!</p>
<p>Edward</p> ;;;; <p>Yes Erik. I did that previously and even now I downloaded it again. Moved the downloaded folder to desktop, extracted them and tried to run the application but the logo appeared for 1 second and then it goes away.</p> ;;;; <aside class="quote no-group" data-username="Research_Associate" data-post="7" data-topic="78787">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p><a class="mention" href="/u/petebankhead">@petebankhead</a> offhand thought, is there any chance the re-centering of OpenSlide coordinates (from way back in the day, something about the boundaries) somehow mistranslates into the training image, so that the training annotations end up access pixels that are not expected?</p>
</blockquote>
</aside>
<p>I wouldn’t expect so – and I don’t think it generally affects .ndpi.</p>
<aside class="quote no-group" data-username="mpkrebs" data-post="2" data-topic="78787">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/46a35a/40.png" class="avatar"> Mark Krebs:</div>
<blockquote>
<p>I found the problem most pronounced when weighted deviation was included in the classifier. However, fine details differed even when the simpler settings for the pixel classifier were used.</p>
</blockquote>
</aside>
<p>That makes me think <a class="mention" href="/u/research_associate">@Research_Associate</a>’s assumption about edge artifacts could well be correct. All the filters are applied in image tiles. The tiles overlap to try to reduce/eliminate boundary issues, however padding needs to be used at the edge of the image (since QuPath doesn’t know what is beyond the image boundary).</p>
<p>See here for more info: <a href="https://bioimagebook.github.io/chapters/2-processing/4-filters/filters.html#filtering-at-image-boundaries" class="inline-onebox">Filters — Introduction to Bioimage Analysis</a></p>
<p>When you create the training image, the features you get at the edge of a training region will be slightly different from the features that you get in the same region of the original image: in the training image, QuPath will need to pad the image, because it doesn’t know what the pixels ‘outside’ the cropped region should be.</p>
<p>The effect of this is much greater when you use large filters. Some features (e.g. weighted deviation) are also likely to be more affected.</p>
<p>To mitigate this, you can create training images that include quite large regions – and only annotate the central parts.</p> ;;;; <p>Hi Jon,<br>
could you post a snapshot from the training tab?<br>
Also, do not do preprocessing, you can skip it for most of the tasks in this version.</p>
<p>Best regards,<br>
Ilya</p> ;;;; <aside class="quote no-group" data-username="ym.lim" data-post="1" data-topic="78814">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png" class="avatar"> Yau Mun Lim:</div>
<blockquote>
<pre><code class="lang-auto">ERROR: No signature of method: qupath.ext.stardist.StarDist2D$Builder.preprocess() is applicable for argument types: (qupath.ext.stardist.OpCreators$PercentileTileOpCreator, qupath.opencv.ops.ImageOps$Channels$ColorDeconvolutionOp...) values: [qupath.ext.stardist.OpCreators$PercentileTileOpCreator@59ee801b, ...]
Possible solutions: preprocess([Lqupath.opencv.ops.ImageOp;), preprocess(qupath.ext.stardist.OpCreators$TileOpCreator) in QuPathScript at line number 23
</code></pre>
</blockquote>
</aside>
<p>This seems to suggest that <code>preprocess()</code> only accepts either <code>ImageOps</code> or <code>imageNormalizationBuilder</code> separately but not together in one <code>preprocess()</code>.</p>
<p>As such, I have tried to put them in separate <code>preprocess()</code>:</p>
<pre><code class="lang-auto">def stardist = StarDist2D.builder(pathModel)
    .preprocess(
        StarDist2D.imageNormalizationBuilder()
            .perChannel(true)
            .maxDimension(4096)
            .percentiles(0.2, 99.8)
            .build()
    )
    .preprocess(
        ImageOps.Channels.deconvolve(stains),
        ImageOps.Channels.extract(1), // 0 = haematoxylin, 1 = DAB; can be summed for pseudo optical density sum (0,1)
        ImageOps.Filters.gaussianBlur(1)
    )
    ...
    .build()
</code></pre>
<p>This ran without errors but no detections were made.</p> ;;;; <p>Afraid I don’t know. Have used NDPI files for this, so generally it shouldn’t be an issue, though usually not with OpenSlide.<br>
<a class="mention" href="/u/petebankhead">@petebankhead</a> offhand thought, is there any chance the re-centering of OpenSlide coordinates (from way back in the day, something about the boundaries) somehow mistranslates into the training image, so that the training annotations end up access pixels that are not expected?</p>
<p>Alternatively, <a class="mention" href="/u/mpkrebs">@mpkrebs</a>, have you tried using BioFormats when Importing the files, needs to be done in a new project, I think, unfortunately.</p> ;;;; <aside class="quote no-group" data-username="Robert_Schweickart" data-post="8" data-topic="78774">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/robert_schweickart/40/67178_2.png" class="avatar"> Robert Schweickart:</div>
<blockquote>
<p>DAPI mask to overlay across the other channels for cell counting</p>
</blockquote>
</aside>
<p>If cellpose has already counted the cells, do you need that?</p>
<p>Alternatively, there are disucssions about combining StarDist for nuclear detection and CellPose for cell boundary detection for combined cell objects.</p> ;;;; <p>Hello,</p>
<p>I have question regarding the ROIs deletion in OMERO.figure.</p>
<p>To delete ROIs on a specific image, we usually go on <code>ROIS-&gt;edit</code> and select one or more ROIs to delete. But we cannot select individual ROIs that are included in a larger one (and therefore cannot delete them).</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339.jpeg" data-download-href="/uploads/short-url/7gRM0LIYZl13kknBEFXht1yKWVP.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339_2_517x251.jpeg" alt="image" data-base62-sha1="7gRM0LIYZl13kknBEFXht1yKWVP" width="517" height="251" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339_2_517x251.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339_2_775x376.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339.jpeg 2x" data-dominant-color="DBD5D6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">797×387 59 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And sometime, for very small annotations, it is difficult to select them. Would it be possible to have a list of all ROIs so that we can individually select and delete them ?</p>
<p>Thanks,<br>
Best,</p>
<p>Rémy.</p> ;;;; <p>Hi,<br>
Thanks for an excellent tool.</p>
<p>Unfortunately, I’ve run into issues running DeepMIB training (MIB v.2.84/Matlab R2021b). I’m able to do preprocessing using my data when I try to follow your 2D U-net example from YouTube I get the following error.</p>
<p>MATLAB:datastoreio:transformeddatastore:badTransformDef</p>
<p>Using Resnet18 I get the following error:</p>
<p>MATLAB:catenate:dimensionMismatch</p>
<p>I’m not sure how to fix this? Could it have something to do with the uneven dimensions of my dataset (1329:2388) which is different in size from my prediction dataset (1568:2040)? There are no error reported when I look at “Check network”. Maybe I’ve chosen an inappropriate number of patches (64)?</p>
<p>FYI, I was able to initiate trainings and do prediction of the mitochondrial test dataset so I guess the issues lies in my dataset.</p>
<p>Cheers<br>
Jon</p> ;;;; <p>Hi <a class="mention" href="/u/k-dominik">@k-dominik</a>, Thank you for the welcome and taking time to answer the questions!</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<ul>
<li>just to clarify: when I read through your post it seems that you converted your input data to h5, is this correct? (all input data) Did you use our fiji plugin?</li>
</ul>
</blockquote>
</aside>
<p>Indeed, initially I used the tiff stacks but it was quite some work to move the files back and forth from the network drive and reading through the suggestions I chose h5.<br>
Since Avizo is the most used software in our group, I have the most experience with using that, so the preprocessed files are converted using a macro in Fiji using the ilastikj plugin to h5.</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>to what format are you exporting?</p>
</blockquote>
</aside>
<p>The output from ilastik till now was the 8-bit simple segmentation stage 2 in h5 but changing the axis order to ‘zxyc’. The reverse conversion (this time to tiff sequence) is again done using a macro in Fiji.</p>
<p>(extra note: Apparently Fiji does not like to read these h5 files using the native hdf5 plugin and so I use the ilastikj plugin again for that, do you know why that might be? It seems that it is also a little slower as well…?)</p>
<p>I briefly tried to use the probability map but it creates 4 tiff stacks,one for each class. So I still need to figure that out.</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>what kind of machine are you using for your processing (how many cores, how much ram?)</p>
</blockquote>
</aside>
<p>It’s a workstation class machine, so 14 core, 256-384GB ram. There is a particularly powerful GPU as well (A5000) but ilastik does not benefit from it <img src="https://emoji.discourse-cdn.com/twitter/upside_down_face.png?v=12" title=":upside_down_face:" class="emoji" alt=":upside_down_face:" loading="lazy" width="20" height="20">.<br>
I did create a config file and during processing it does use 100% CPU but yeah…compute time is still relatively same when compared to a machine with 8 cores and 128GB memory.<br>
Any idea how to probably optimize it?</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>What version of ilastik are you using. When the classifier is saved to the project file, the raw data should not be needed anymore in headless processing (which I assume you’re doing). ilastik would only try to access it to train the classifer. I verified with ilastik <code>1.4.0</code> that this works in principle.</p>
</blockquote>
</aside>
<p>It is 1.4.0 initially it was 1.4.0rc8 but I updated it last month. I am currently working form the GUI but I plan to use it in headless mode once I am happy with the segmentation result. Right now if I move the project/raw files I receive the error that the files have been moved/ it is too late to change.</p>
<p>Is it possible to have a single self-contained .ilp file, because the raw data for training is smaller than the stacks. In that case I can keep a single file for each trained model and not the data files separately.</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>The probabilities can safely be exported to 8-bits (in that case you have to renormalize from <code>0..1</code> to <code>0..255</code>.<br>
For the thresholds - of course you could use lower values, or just the largest value (if documented properly). You are right of course, that this means that some regions might not get a segmentation. But I would think about it differently - by enforcing at least 0.5 (or any value you see fit) you will get some pixels that are not part of any class - and you will know which ones those are - isn’t that good for any downstream analysis, not to include those (after all they might be too different)?</p>
</blockquote>
</aside>
<p>If the probability map is renormalized to 0…255 does the threshold of 0.5(or other value) still remain valid?<br>
I was able to export it to 8-bit map but the I might need to write a macro to possibly combine the 4 channels into a single tiff stack(by setting the appropriate threshold value eg.0.5). Because right now when trying to export it just creates 4 stacks for each of the class with some garbage value.<br>
The reasoning does indeed make sense here, and based on the phenomenon observed, chances of one class over other is more which can be classified properly while post-processing</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>I don’t really dare to ask, but you’ve tried “just” pixel classification, and it was not good enough, yes?</p>
</blockquote>
</aside>
<p>I did <img src="https://emoji.discourse-cdn.com/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"> but yeah, Auto-context was definitely better.<br>
Perhaps a question, does the amount of annotations affect the computation time? or is it just the amount of features selected?</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>So if you are interested in 4 classes, then you should have 4 classes in the second stage. For the first stage you’d probably also want to have at least 4 classes.</p>
</blockquote>
</aside>
<p>Currently for my approach, Separating from the background it required only 4 features in stage 1 and then for stage 2 it was 17-19 features. If I select do the way you describe, does it then mean 17-19 features in each stage? The ram isn’t the biggest issue, it’s the time taken to process the 1500 image stack (currently about 5hrs) Is this around the time that is normally expected?</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>Gold standard to assess the quality of your segmentation would be to annotate at least a part of the volume densely (all pixels) and then compare that to the final segmentation result (after thresholding and all).<br>
The OOB is only an indication - data points are never tested against the full classifier, only against those trees in the forest that didn’t include them as training data.</p>
</blockquote>
</aside>
<p>That’s probably not possible in this case as the contrast is not that good, in certain regions the grey values change too much for the same phase, However, everything is more or less correctly classified apart from very small region(which I can better classify from the probability map).</p>
<aside class="quote no-group" data-username="k-dominik" data-post="2" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png" class="avatar"> k-dominik:</div>
<blockquote>
<p>But keep in mind that this <em>only</em> includes the annotated training data.</p>
</blockquote>
</aside>
<p>This does clear some of my concern and would help me better argue the observed results.</p>
<p>Apologies if my discussion is becoming too wordy. But the results obtained are much better that other approaches and I really want to get a clear understanding so that I can include it in a prospective article.</p>
<p>Best,<br>
Ujjwal</p> ;;;; <p>Hello,</p>
<p>We experience a weird behavior of OMERO.figure when we add manual labels.<br>
If we add the same label as an existing one (same font size, same position, same color), then the label is added on the figure but not in the list of labels.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074.png" data-download-href="/uploads/short-url/ki3kZzU1jnGu0dRipVYnW615GHW.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074_2_690x435.png" alt="image" data-base62-sha1="ki3kZzU1jnGu0dRipVYnW615GHW" width="690" height="435" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074_2_690x435.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074.png 2x" data-dominant-color="EBEBED"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">883×557 64.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>This is for me an issue because when I want to sort tags and manual labels, I need to manually rename each label to have them in the right order and when OMERO detects the two extact same labels, then it removes one from the list but not from the figure. My suggestion here is to keep the two extact labels in the list so that we can remove each one separetly.</p>
<p>And maybe having options to :</p>
<ol>
<li>Select only tag we want to show (and not every tags by default)</li>
<li>import all key-values at once (like the by-default for tags)</li>
</ol>
<p>would also be great if there are implemented.</p>
<p>Regarding the label names, if the label contains many underscores <code>_</code>, then the label is not displayed correctly.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d.jpeg" data-download-href="/uploads/short-url/ntSS0GFAlIp42Pla6qLdvmwGCrX.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d_2_554x500.jpeg" alt="image" data-base62-sha1="ntSS0GFAlIp42Pla6qLdvmwGCrX" width="554" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d_2_554x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d.jpeg 2x" data-dominant-color="CECDD0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">806×727 96.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thanks for having a look on that,<br>
Rémy.</p> ;;;; <p>Did you download the x64 version from this site. <a href="https://fiji.sc/" rel="noopener nofollow ugc">https://fiji.sc/</a> Then extract the folder and run ImageJ-win64.exe<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/4/e40215f98f37431cc785d2fd8314cb3904783611.png" alt="image" data-base62-sha1="wx3kANcb4ESYqey85tWbDvGNWI9" width="654" height="331"></p> ;;;; <p>Hi Alex,</p>
<p>given the structure of your biofilms as shown in the image it seems to me that it does indeed make sense to analyze the entire image and extract its parameters as if it were a single biofilm. Whether this makes sense in relation to your research question or not depends strongly on the question as well as the properties you wish to quantify. In my experience, images such as yours can be perfectly suitable to address a variety of research questions. Single cell resolution is not always needed.</p>
<p>In the current version of BiofilmQ, all properties are suitable to be calculated based on cubes, which means that they are useful to characterize low-resolution images. You don´t need to focus on only global biofilm properties, but can take advantage the full breadth of available properties.</p>
<p>One thing that you should be aware of in your context is that the spatial properties calculated by the “distance to center biofilm” parameter in the parameter calculation tab will in your case likely not be very useful, because they will mainly represent the distance to the center of the image. When using spatial properties, e.g. for visualizing localization of fluorescence etc. I would recommend to either use distanceToSubstrate or distanceToSurface.</p>
<p>Let me know if you would like more explanation on one of the things mentioned above or have any other questions.</p>
<p>Best,</p>
<p>Hannah</p> ;;;; <p>Hi Erik. Thanks for helping.<br>
My laptop is also x64-based PC.<br>
Please let me know how to proceed with the installation.</p> ;;;; <p>Skeleton is used just for visualisation, you can leave it as is</p> ;;;; <p>Hello,</p>
<p>I have a question regarding teh scalebar of OMERO.figure. Is it possible to set the scalebar width ? For some figure, it appears a bit thick and we would like to thin it a bit, with a drop-down menu (like for label font size) or simply manually.</p>
<p>Thanks,<br>
Rémy.</p> ;;;; <p>Dear MicroscopyRA,</p>
<p>Regarding edge effects, I think I am using the classifier in a slightly different way than in your image. The tissue I study (mouse retina) is highly structured with natural boundaries in staining color and intensity. Thus, although each section selected for the training set is at the center of a larger rectangle, and therefore free from edge effects, there are prominent edges within the tissue layer at all scales and channels.</p>
<p>But why would these be differentially detected in a training image and its corresponding original?</p>
<p>Cheers,<br>
Mark</p> ;;;; <p>OK, so I think <code>bioformats2raw</code> is doing the right thing here - creating a plate with 12 rows and 8 columns but it contains only 2 wells. That should be read by <code>napari-ome-zarr/ome-zarr-py</code> without errors.</p>
<p>The <code>DTScan_ID4.zarr</code> is not a <code>plate</code> format: it is a <code>bioformats2raw</code> image/collection, which is not yet supported by napari:</p>
<p>You can open the underlying image(s) with an extra <code>/0</code> to reach the nested image:</p>
<p><code>$ napari --plugin napari-ome-zarr /DTScan_ID4.zarr/0/</code></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/60089c8e51ee3a0ef84b062704469e4e3c0a412c.jpeg" data-download-href="/uploads/short-url/dHyhOrBJVggGjSU3j9m2A79yci0.jpeg?dl=1" title="Screenshot 2023-03-20 at 14.29.01"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/60089c8e51ee3a0ef84b062704469e4e3c0a412c.jpeg" alt="Screenshot 2023-03-20 at 14.29.01" data-base62-sha1="dHyhOrBJVggGjSU3j9m2A79yci0" width="690" height="404" data-dominant-color="1F2C31"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-20 at 14.29.01</span><span class="informations">976×572 45.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Sorry I am a bit of a newb. I wanted to look at the source code for the Despeckle filter that comes with ImageJ. I have FIJI 1.53t<br>
I found this: <a href="https://github.com/fiji/VIB/blob/794c4fa9cf89cc3505996ffba97dbebcc3269646/src/main/java/Despeckle_.java" class="inline-onebox" rel="noopener nofollow ugc">VIB/Despeckle_.java at 794c4fa9cf89cc3505996ffba97dbebcc3269646 · fiji/VIB · GitHub</a><br>
but I don’t think that’s the same “Despeckle” because it opens a dialog, and the one I am using doesn’t. Where can I find this code? It seems to be part of “filters-2.0.235” but where does that come from?</p> ;;;; <p>One last thing:</p>
<p>If I add more body parts, should I be inputting a specified skeleton as well?</p>
<p>At the moment, I’ve been leaving the skeleton as the default in the config file</p>
<p>Thanks</p> ;;;; <p>Dear MicroscopyRA,</p>
<p>I have been using RGB as inputs to the pixel classifier. The stain vectors are identical in both images, although I infer from your question the stain vectors don’t apply to RGB values, which makes sense.</p>
<p>Cheers,<br>
Mark</p> ;;;; <p>I’ll reply myself here, in case this helps someone else. I found another (unrelated ?) post that seems to have asked to delete certain entries on the ROI Manager based on name.</p>
<p>It seems the “JP” entries added by the Ridge Detection plugin pertain to Junction Points, which can be useful, but in my case they break my analysis flow.</p>
<p>This post shows a way to cycle through the ROI Manager and delete any that contain specific characters - in this case “JP”.</p>
<aside class="quote quote-modified" data-post="2" data-topic="76747">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/lmurphy/40/43611_2.png" class="avatar">
    <a href="https://forum.image.sc/t/delete-roi-in-the-roimanager-based-on-names/76747/2">Delete Roi in the RoiManager based on names</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    HI <a class="mention" href="/u/christelle_gobet">@christelle_gobet</a> 
The below should work, it goes backwards through the ROI manager, selecting one at a time and uses a regular expression with the matches function to find if “JP” appears somewhere in the ROI name. The for loop goes backwards because if it went forwards the index would get deleted and the next increase in the for loop would skip an ROI. 
Hope that makes sense! 
for (i = roiManager("Count")-1; i &gt;= 0; i--){ 
	roiManager("Select", i);
	name = Roi.getName;
	
	if(matches(name, "…
  </blockquote>
</aside>

<p>Thanks Community !</p> ;;;; <p>Only by thresholding first, as far as I know. So threshold each cell, then calculate features, move measurement to cell, delete object used for thresholding.</p> ;;;; <p>CellPose is fantastic at differentiating the cells. If only I could implement that into my ImageJ macro and have CellPose create the DAPI mask to overlay across the other channels for cell counting…it’d be perfect.</p> ;;;; <p>Great, any solution to have the mean intensity above a threshold like for ImageJ ?</p> ;;;; <p>I’ve been following your guide already - thank you for posting it <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>OK - I’ll create another video and I’ll do everything again, but making sure to set identity = true and label more bodyparts before training.</p>
<p>I’ll let you know how I go over the next couple of days.</p>
<p>Kindest regards,</p>
<p>GR</p> ;;;; <p>Below the additional statement. Does that make any sens to you?</p>
<p><em>Object allocated at (most recent call last):<br>
File “D:\WorkStuff\labA3Marc\marcPy\venv\Lib\site-packages\ndtiff\nd_tiff_current.py”, lineno 56<br>
self.file = open(tiff_path, “rb”)</em></p>
<p>Could I save it as something different than an NDTIFF (how would I do this)?</p> ;;;; <aside class="quote no-group" data-username="mpkrebs" data-post="2" data-topic="78787">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/46a35a/40.png" class="avatar"> Mark Krebs:</div>
<blockquote>
<p>OpenSlide and Sparse image servers might differ in image parameters</p>
</blockquote>
</aside>
<p>Also, did you use staining or RGB values as inputs to the pixel classifier, and if stains were used, are the stain vectors the same in both images before you started the pixel classifier?</p> ;;;; <p>Sometimes when I use the Ridge Detection plugin I get point selections, instead of lines.</p>
<p>As I have a macro that loops through several files, whenever this happens, it halts the process (doing Plot Profiles for the ROIs).</p>
<p>For most of my images, all works well, but some others it selects an ROI which is a Point, instead of the lines, as it should. I have Minimal Length set to 100.00 so I don’t understand why this happens. Also, it seems to always be at the end of the list and preceeded by “JP” in the ROI Manager.</p>
<p>I attach a tiff at the bottom with an image where this happens and present two screen grabs with the issue below.</p>
<p>Thanks already for helping out!<br>
Nuno</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7.jpeg" data-download-href="/uploads/short-url/yz3EvH5mEgHAZYjUjJN7oxdnwZ9.jpeg?dl=1" title="Screen Shot 2023-03-20 at 14.45.34" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_690x406.jpeg" alt="Screen Shot 2023-03-20 at 14.45.34" data-base62-sha1="yz3EvH5mEgHAZYjUjJN7oxdnwZ9" width="690" height="406" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_690x406.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_1035x609.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_1380x812.jpeg 2x" data-dominant-color="917D90"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-20 at 14.45.34</span><span class="informations">1920×1130 170 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1.jpeg" data-download-href="/uploads/short-url/xVzHCsybOU6dpWgjhyhtwvOXMS5.jpeg?dl=1" title="Screen Shot 2023-03-20 at 14.45.58" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_690x406.jpeg" alt="Screen Shot 2023-03-20 at 14.45.58" data-base62-sha1="xVzHCsybOU6dpWgjhyhtwvOXMS5" width="690" height="406" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_690x406.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_1035x609.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_1380x812.jpeg 2x" data-dominant-color="846780"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-20 at 14.45.58</span><span class="informations">1920×1131 184 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><a class="attachment" href="/uploads/short-url/8eAdNttWIYoNiqoHBUWLoMXcMg3.tif">ridge_detection_example.tif</a> (1.1 MB)</p> ;;;; <p>Hi,</p>
<p>I recently had the problem that Trackmate has assigned the same TrackID to 2 nuclei close to each other.<br>
The procedure that I am using is</p>
<ol>
<li>to segment nuclei with Stardist (in order to perform a cell tesselation)</li>
<li>use the ROIs  detected by Stardist to create SpotCollection for Trackmate<br>
And as you can see in the result table, the same TrackId is used for 2 nuclei in the same frame:</li>
</ol>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297.png" data-download-href="/uploads/short-url/6yW9QIugZFG98VpmOf9bKZKKEBN.png?dl=1" title="Screen Shot 2023-03-20 at 14.04.45" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297_2_566x499.png" alt="Screen Shot 2023-03-20 at 14.04.45" data-base62-sha1="6yW9QIugZFG98VpmOf9bKZKKEBN" width="566" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297_2_566x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297_2_849x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297.png 2x" data-dominant-color="E9E9E9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-20 at 14.04.45</span><span class="informations">950×838 112 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>the columns correspond to:</p>
<ol>
<li>TrackID</li>
<li>SpotID</li>
<li>POSITION_X</li>
<li>POSITION_Y</li>
<li>POSITION_T</li>
<li>FRAME</li>
<li>QUALITY</li>
</ol>
<p>And as you can see in the screeshot, for the TrackID 44 there are 2 different positions for each frame. The 2 nuclei are on the following image close to each other<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28acb0a92939afba5155ca1ea940db16c0d716f3.png" data-download-href="/uploads/short-url/5NP5j5olvxNF2BMqoBy9eFOs4YX.png?dl=1" title="Screen Shot 2023-03-20 at 14.06.48" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28acb0a92939afba5155ca1ea940db16c0d716f3.png" alt="Screen Shot 2023-03-20 at 14.06.48" data-base62-sha1="5NP5j5olvxNF2BMqoBy9eFOs4YX" width="473" height="500" data-dominant-color="0C0C0B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-20 at 14.06.48</span><span class="informations">968×1022 25.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Do you have any idea how to fix this problem ?</p>
<p>thanks by advance</p> ;;;; <p>You can try out my guide on video preprocessing and re-encoding: <a href="https://deeplabcut.github.io/DeepLabCut/docs/recipes/io.html#tips-on-video-re-encoding-and-preprocessing" class="inline-onebox" rel="noopener nofollow ugc">Input/output manipulations with DeepLabCut — DeepLabCut</a></p>
<p>Cropping can be added as a <code>-filter:v "crop=out_width:out_height:x_start:y_start"</code> instead of the scale mentioned in the tutorial, or with if you want to apply both (just add a comma between them). As long as you can consistently say which hand is which you can set the <code>identity</code> in the config to <code>true</code></p> ;;;; <aside class="quote no-group" data-username="mpkrebs" data-post="2" data-topic="78787">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/46a35a/40.png" class="avatar"> Mark Krebs:</div>
<blockquote>
<p>I wonder how the OpenSlide and Sparse image servers might differ in image parameters.</p>
</blockquote>
</aside>
<p>Have you looked at the visualization for the channels/features in the sparse image areas and made sure there are not any edge effects for the largest size filters used? Or did you stick to areas far enough from the edges of each patch to make sure there’d not be any edge effects.</p>
<p>Somewhat terrible example showing edge of image effects for a poorly chosen set of image regions<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426.jpeg" data-download-href="/uploads/short-url/84JOhBjmk8T7BwqRA3OtYzbvxfE.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_690x436.jpeg" alt="image" data-base62-sha1="84JOhBjmk8T7BwqRA3OtYzbvxfE" width="690" height="436" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_690x436.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_1035x654.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_1380x872.jpeg 2x" data-dominant-color="4D4B4C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1567×991 84.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi Konrad,</p>
<p>No need to apologise <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Totally understand <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"></p>
<p>Firstly, I wasn’t sure if I should have identity learned…?</p>
<p>Because I’m only tracking 2 individuals (in my case: 2 hands) and they don’t really come in close proximity to each other (not like the multi-animal projects other researchers conduct), I thought having identity learned would be fine.</p>
<p>Do you recommend I should have identity learned but increase the number of bodyparts? I’ve only included 3 bodyparts in my current project and this was mainly due to some hand areas becoming occluded during the task.</p>
<p>Maybe I could improve our camera angle to observe more relevant bodyparts during the task…?</p>
<p>I’m away from my desk but, from memory, the created videos looked fine. I’ll double-check the labeled video tomorrow morning when I’m back at work</p>
<p>With the flagging error, I’ll create a new video (with more bodyparts as suggested) and retry the refine_tracklets GUI (after doing all other actions).</p>
<p>What’s the best way to ensure a video file isn’t corrupted before uploading it to a project?</p>
<p>I’ve been compressing and cropping the .mp4 file created with a GoPro camera on my laptop using ffpmeg in the DLC environment and then transferring it to our work computer.</p>
<p>Does that sound OK?</p>
<p>Thanks, again</p> ;;;; <p>Hi,<br>
I generated a classifier model using CPA 3.0.4 based on features from the actin cytoskeleton of cells. When I plug this model into the ClassifyObjects module in CP 4.2.4 and run the pipeline on new images, I get an error message indicating that some measurements don’t exist, but the appropriate modules are present and running. I would appreciate any help or insight I can get for this issue. I am attaching here all the required information for others to look into (error message, CP pipeline, CPA model, sample images).<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png" data-download-href="/uploads/short-url/sAwr8M64Nvc1FyOISchV1Hie36Y.png?dl=1" title="errorCellProfiler" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png" alt="errorCellProfiler" data-base62-sha1="sAwr8M64Nvc1FyOISchV1Hie36Y" width="421" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 2x" data-dominant-color="CBD2DA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">errorCellProfiler</span><span class="informations">430×510 73.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<a class="attachment" href="/uploads/short-url/5v48dYmZdYzze8TvbWMxMGMPWX.zip">CPAmodel_CPpipeline.zip</a> (9.8 MB)</p>
<p>Thank you!</p>
<p>Victoria</p> ;;;; <p>Hi All,</p>
<p>As an update, I reproduced the issue described in my earlier post by creating a new project from a single slide, annotating a single rectangle surrounding a section on that slide with class set to Region*, generating a training set from this annotation, and using the training set to create various pixel classifiers. The classifiers were created at Very high resolution, scales 1, 2, 4, and 8, and 1–3 of the following features: gaussian, weighted deviation, and structure tensor coherence.</p>
<p>I found the problem most pronounced when weighted deviation was included in the classifier. However, fine details differed even when the simpler settings for the pixel classifier were used. For example, a pixel classifier created at Moderate resolution, a single scale of 1, and a gaussian as the only feature showed many subtle differences between the training image and the original.</p>
<p>I wonder how the OpenSlide and Sparse image servers might differ in image parameters. My images are brightfield images of hematoxylin and eosin slides scanned at 20x using a Hamamatsu NanoZoomer 2.0 (.ndpi format). Are .ndpi slides known to be problematic when analyzed by this approach (that is, by using a training set to train a pixel classifier in QuPath)?</p>
<p>Cheers,<br>
Mark</p> ;;;; <p>Hi <a class="mention" href="/u/will-moore">@will-moore</a> and <a class="mention" href="/u/s.besson">@s.besson</a></p>
<pre><code>However, I’m not sure why `bioformats2raw` is creating a 96-Well plate here instead of a 2-Well plate? 
</code></pre>
<p>This is exactly the interesting question. I suspect that bioformats2raw is parsing the experiment metadata inside the CZI metadata instead of sticking to what the actual image metadata show. My reasing for this is:</p>
<ul>
<li>96well.czi is acquired and the full well is used (1 Position per well - to keep it simple)
<ul>
<li>the experiment section will indicate 96 well</li>
<li>the image metadata will show 96 scenes</li>
</ul>
</li>
<li>in ZEN a subset is created → 96well_A1+A2.czi
<ul>
<li>the experiment metadata will not be changed of course</li>
<li>the image metadata will now show 2 scenes only</li>
</ul>
</li>
</ul>
<p>I am not sure if this is what happens, but at least this could be checked because this would explain why “somebody” still tries to find all the missing images from those wells.</p>
<p>PS: I will also add this to the mentioned Github issue.</p> ;;;; <p>Could you create the env from scratch and then within the new env run <code>conda install -c conda-forge cudnn</code> then <code>conda list tensorflow</code> and if it’s not <code>2.10.0</code> run <code>pip install --upgrade tensorflow==2.10.0</code>?</p> ;;;; <p>It’s always hard to say without knowing the data but quality of labelling is really important, so remember to be precise and consistent in the way you label the bodypart you want to track.</p>
<p>As for the refinement - if you analyze a video and extract outliers from it, it will be added to the project and extracted frames will be added to <code>labeled-data</code> folder.</p>
<p>If you bump the iteration in the config and create a new training dataset, evaluation will be done for the new iteration in a separate folder - within the same training set iteration but different snapshots, evaluation results are appended to the csv file with evaluation results.</p>
<p>You can also consider modyfing augmentation parameters for your specific use case - hard to tell exactly how without seeing the data.</p> ;;;; <p>Open up system information by searching for it in the start menu and look at the System Type variable.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f6efe6ea0c24c8b1c35f83bb763595ecb0d7a250.png" data-download-href="/uploads/short-url/zevrCazRWCBo4sbFcWRkvwv5RJe.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f6efe6ea0c24c8b1c35f83bb763595ecb0d7a250.png" alt="image" data-base62-sha1="zevrCazRWCBo4sbFcWRkvwv5RJe" width="690" height="221" data-dominant-color="F5F5F6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1047×336 14.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Can you install <code>cudnn==8.4.1.50</code> and <code>cudatoolkit==11.8</code> and <code>tensorflow==2.10.0</code>?</p>
<p>To create a new environment just open the yaml file used for env creation and change the name to whatever you want the env be named</p> ;;;; <p>Sorry, I’ve been super busy lately.</p>
<p>If the identity isn’t learned by the model it keeps identity of the animal within one video - if you are training with identity then using more bodyparts helps a lot (we’ve had great results using around 13 bps - where the model is basically flawless after only 60 frames labeled). Likelihood=1 everywhere seems a little weird - I’ve never seen a perfect dataset with no occlusions at all. If you create a labeled video does it look good?</p>
<p>About the error with flagging, I cannot reproduce it even when flagging just a single frame. Maybe there is something wrong with the videos or tracking data is corrupted somehow?</p> ;;;; <p>Can you reinstall <code>pyqt</code> and <code>pyside</code> and see if it helps?</p> ;;;; <p>This is a warning about an experimental feature that still isn’t stable in tensorflow so it’s all fine.</p> ;;;; <p>Hello, Matthias Arzt.</p>
<p>I’d like to ask you a question.</p>
<p>I wonder what the meaning of “time series” in the images into a time series you mentioned above is in the data type part. (Number 3. / 4.)</p>
<p>For example, if the input image is a grayscale image, overlap the input image with multiple raster images.</p>
<p>To sum up, I wonder what the “time series image” data looks like.</p> ;;;; <p>Make sure the objects are selected before running the <em>Analyze-Calculate Features-Add Intensity Features</em></p><aside class="quote quote-modified" data-post="36" data-topic="27906">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar">
    <a href="https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/36">QuPath Intro: Choose your own analysis(adventure)</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Adding Features
The Calculate features menu has been split into the Calculate features and Spatial analysis menus in 0.2.0. 
<details><summary>Add Intensity features</summary>The largest and most flexible feature generator summarizes intensity data in or around a given detection. You can select a single object (cell) and run this, or everything, or an annotation. A simple use would be to create a full image annotation and collect the mean value of all channels/deconvolutions for all of your images, and take a look at the r…</details>
  </blockquote>
</aside>

<p>There is no background subtraction, you would need to do that yourself.</p> ;;;; <p>Hi there,<br>
I’m trying to combine Live/Dead images for analysis. Files are named like<br>
Run-10_23d802-01_B3_Treat5_Eth_1.tif</p>
<p>and the metadata regex<br>
^(?P.<em>)_(?P.</em>)<em>(?P[A-P][0-9])</em>(?P.<em>)_(?P.</em>)_(?P[0-9]{1}).tif<br>
creates the expected table of metadata correctly.</p>
<p>When I then try to assign Live and Dead channels using the metadata, there is the dreaded “Sorry, … no valid image sets…” error. Images shall be matched by Plate, Well and Nr (=repeat). I cut it down to two images to no avail. I also tried to assign channels based on the filename (“contains: Eth”) but that didn’t help - no sets detected (it used to work with a few files with the “order” detection but we need the metadata).</p>
<p>Can anyone point my mistake out? The path contains a space, but I used CP before like that without issues.</p>
<p>Any help is greatly appreciated,<br>
Konstantin</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/76903dda9a68a70886058df3e19e86986b91bca1.png" alt="cp-1" data-base62-sha1="gURnq4QzDYn09T8Y9bpxMZ3LV8R" width="234" height="33"><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f306d25890600738d74f5876d98029528d83c78.png" data-download-href="/uploads/short-url/i9aohOGZbTv93ZvGs1y2YfJdxe8.png?dl=1" title="cp-2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f306d25890600738d74f5876d98029528d83c78.png" alt="cp-2" data-base62-sha1="i9aohOGZbTv93ZvGs1y2YfJdxe8" width="690" height="255" data-dominant-color="E9E9E9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">cp-2</span><span class="informations">851×315 8.42 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1a515ef432a735be6f8134893dfc21fc6bc8ddf.png" data-download-href="/uploads/short-url/n3Yzag3TGnm8C1zV2AlKsmOBWft.png?dl=1" title="cp-3" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1a515ef432a735be6f8134893dfc21fc6bc8ddf.png" alt="cp-3" data-base62-sha1="n3Yzag3TGnm8C1zV2AlKsmOBWft" width="690" height="458" data-dominant-color="EAEAEA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">cp-3</span><span class="informations">902×600 20.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>That also looks to be due to the function calls in processTiff all being lowercase for some reason, I believe in the original they had the correct cases so not sure what has happened there. The correct version should be as below:</p>
<pre><code class="lang-auto">function processTiff(input, output, file) {
	print("Processing: " + input + file);

	open(input+file);
	renderColor(file);
	brightnessNcontrast(file);
	deleteSlices(file);

	saveAs("Tiff", output + file + "_BCadjusted");
	rename(file);
	
	RGBmerge(file);
	scaleBar();
	makeMontage(file);

	//selectWindow("Montage1to5");
	//saveAs("Jpeg", output + file + "_montage1to5");
	selectWindow("MontageHorizontal");
	saveAs("Jpeg", output + file + "_BCmontage");

	print("Saving to: " + output);
	run("Close All");
}
</code></pre> ;;;; <h2>
<a name="version-23030-1" class="anchor" href="#version-23030-1"></a>Version 23.03.0</h2>
<p><strong>New context menu for meshes</strong></p>
<ul>
<li>When hovering on a mesh in the 3D viewport, you can now hide the mesh, reload it, remove it, and get the segment ID. You can also choose a position on the 3D mesh and jump to this position in the other viewports. <a href="https://github.com/scalableminds/webknossos/pull/6813" rel="noopener nofollow ugc">#6813</a>
</li>
</ul>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73154166ae8356e18378256dc63a28cd5e2980ba.gif" alt="custom color mesh" data-base62-sha1="gq4rOXVJH8nkZcGQlZGAuHJ1mz0" width="690" height="366" class="animated"></p>
<p><strong>Other</strong></p>
<ul>
<li>
<p>Remote datasets can now also be streamed from Google Cloud Storage URIs (<code>gs://</code>). <a href="https://github.com/scalableminds/webknossos/pull/6775" rel="noopener nofollow ugc">#6775</a></p>
</li>
<li>
<p>Remote volume datasets in the neuroglancer precomputed format can now be viewed in WEBKNOSSOS. <a href="https://github.com/scalableminds/webknossos/pull/6716" rel="noopener nofollow ugc">#6716</a></p>
</li>
<li>
<p>We have implemented an automatic locking mechanism for situations where an annotation that others are allowed to edit is opened. <a href="https://github.com/scalableminds/webknossos/pull/6819" rel="noopener nofollow ugc">#6819</a></p>
</li>
</ul>
<p><a href="https://github.com/scalableminds/webknossos/releases/tag/23.02.0" rel="noopener nofollow ugc">Read the full changelog</a>: <a href="https://github.com/scalableminds/webknossos/releases" rel="noopener nofollow ugc">Releases · scalableminds/webknossos · GitHub</a></p> ;;;; <p>Hi Marie,</p>
<p>Could you upload one image, please?  It’s difficult to help you without.</p>
<p>M.</p> ;;;; <p>Hi Erik. Thank you for helping.<br>
Yes I am still having problems running the application on my laptop. My processor is I5.<br>
Regarding your last question, how can I know whether my laptop is Arm or x64?</p> ;;;; <p>Hello everyone!</p>
<p>I am working on segmenting and classifying cell nuclei on a TIF File with 3 channels. I ran into an Assertion error in the “Feature Selection” step for Ilastik 1.4.0. Interesting that I don’t get with it in a previous build or while using Pixel Classification and Object Classification separately on the latest one.</p>
<blockquote>
<p>File “D:\ilastik-1.4.0-gpu\lib\site-packages\lazyflow\slot.py”, line 1323, in maybe_call_within_transaction<br>
self.graph.maybe_call_within_transaction(fn)<br>
File “D:\ilastik-1.4.0-gpu\lib\site-packages\lazyflow\graph.py”, line 143, in maybe_call_within_transaction<br>
fn()<br>
File “D:\ilastik-1.4.0-gpu\lib\site-packages\lazyflow\operator.py”, line 500, in _setupOutputs<br>
self.setupOutputs()<br>
File “D:\ilastik-1.4.0-gpu\lib\site-packages\lazyflow\operators\opFeatureMatrixCache.py”, line 99, in setupOutputs<br>
assert self.FeatureImage.meta.getAxisKeys()[-1] == “c”<br>
AssertionError</p>
</blockquote> ;;;; <p>You will likely struggle to segment the root via intensity contrast, because, as you said, there isn’t any.</p>
<p>If you only have a few of those images, it might be quicker to create a selection (polygon tool) and create a mask by hand.</p>
<p>You could als try to enhance the contrast (maybe with the local contrast CLAHE filter) and try an edge detection filter first.</p> ;;;; <p>Same, 1.53t.</p>
<p>I can’t reproduce the tiling error now, so maybe that is fixed? I still have trouble with the rotation pivot though. Maybe that is just me being stupid, but I’m sure it used to be in the centre, or at least you could set it to the centre.</p> ;;;; <p>Hi Dominik,</p>
<p>Thanks for your reply. I was using a correct hostname, but it was the hostname of the web server. Just tried with the back end hostname and that worked. I had assumed logins, imports and such like had to be done through the web server.</p>
<p>Thanks,</p>
<p>Tim</p> ;;;; <p>Dear <a class="mention-group notify" href="/groups/ome">@ome</a>,<br>
I am experiencing some memory issues caused by data imports. As shown in the image below, every time a new dataset is uploaded the memory committed keeps increasing over time with slight decreases between uploads but never entirely returning to the baseline level. The memory drop seen on the 15th of March is due to a server restart.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/653fd86b780489ae513fd142443955d699ee251f.png" data-download-href="/uploads/short-url/erH0MyC9o24hY3Csld7F846ljJR.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653fd86b780489ae513fd142443955d699ee251f_2_680x499.png" alt="image" data-base62-sha1="erH0MyC9o24hY3Csld7F846ljJR" width="680" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653fd86b780489ae513fd142443955d699ee251f_2_680x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653fd86b780489ae513fd142443955d699ee251f_2_1020x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/653fd86b780489ae513fd142443955d699ee251f.png 2x" data-dominant-color="A6C69F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1137×835 279 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>We do not see anything out of the ordinary in the logs, but we have noticed the following process getting forked many times and remaining alive.</p>
<pre><code class="lang-auto">java -Xmx5005m -XX:MaxPermSize=1g -XX:+IgnoreUnrecognizedVMOptions -Djava.awt.headless=true -Dlogback.configurationFile=etc/logback.xml -Domero.logfile=var/log/${omero.name}.log -Domero.name=Blitz-0 ome.services.blitz.Entry --Ice.Config=/var/log/${omero.name}.log -Domero.name=Blitz-0 ome.services.blitz.Entry --Ice.Config=/opt/omero/server/OMERO.server-5.6.6-ice36/var/master/servers/Blitz-0/config/config
</code></pre>
<p>Bests,<br>
Rodrigo</p> ;;;; <p>Hi <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a></p>
<p>I did what you suggested: <code>pip uninstall tensorflow-gpu</code> and <code>pip install tensorflow==2.10</code>! but now when i launch Deeplabcut:</p>
<p>Error no module named ´tensorflow´.</p>
<p>then i uninstall tensorflow (pip uninstall tensorflow)and i install again:  <code>pip install tensorflow==2.10</code><br>
the same error than before DLL load failed</p>
<p>Thanks for your help</p> ;;;; <p>I think it’s a bit more complicated than that <a class="mention" href="/u/brisvag">@brisvag</a><br>
See my post in the other thread about reading cellpose .npy:</p><aside class="quote quote-modified" data-post="8" data-topic="76319">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png" class="avatar">
    <a href="https://forum.image.sc/t/importing-seg-npy-files-into-napari/76319/8">Importing seg.npy files into napari?</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Looking more closely at the cellpose save code: 

The .npy file contains actually a dictionary with more than just the labels, which is what you’d visualize with napari and further process with region props, etc. There’s no way I don’t think for napari builtin reader to a priori know that it needs to do: 
data_to_load = np.load('_seg.npy', allow_pickle=True).item()
labels = data_to_load['masks']

Seems like this requires a cellpose-specific npy reader plugin. 
Now cellpose offers a png output, w…
  </blockquote>
</aside>

<p>So what you need to do to load them is to mimic what cellpose does. For example to get the masks:</p>
<pre><code class="lang-auto">data_to_load = np.load('_seg.npy', allow_pickle=True).item()
labels = data_to_load['masks']
</code></pre> ;;;; <p>Hi <a class="mention" href="/u/petebankhead">@petebankhead</a> ,</p>
<p>I’m just trying to use the new <a href="https://qupath.readthedocs.io/en/0.4/docs/deep/stardist.html#improving-input-normalization" rel="noopener nofollow ugc">global normalisation feature</a> introduced in the StarDist extension v0.4.0 with deconvolved channels but I was unsuccessful due to an error.</p>
<p><code>StarDist2D.imageNormalizationBuilder()</code> worked with RGB channels:</p>
<pre><code class="lang-auto">def stardist = StarDist2D.builder(pathModel)
    .preprocess(
        StarDist2D.imageNormalizationBuilder()
            .maxDimension(4096)
            .percentiles(0.2, 99.8)
            .build()
    )
    ...
    .build()
</code></pre>
<p>but not when I try to do deconvolution:</p>
<pre><code class="lang-auto">def stardist = StarDist2D.builder(pathModel)
    .preprocess(
        ImageOps.Channels.deconvolve(stains),
        ImageOps.Channels.extract(1), // 0 = haematoxylin, 1 = DAB; can be summed for pseudo optical density sum (0,1)
        ImageOps.Filters.gaussianBlur(1),
        StarDist2D.imageNormalizationBuilder()
            .maxDimension(4096)
            .percentiles(0.2, 99.8)
            .build()
    )
    ...
    .build()
</code></pre>
<p>This is the error that was spat out:</p>
<pre><code class="lang-auto">ERROR: It looks like you've tried to access a method that doesn't exist.


ERROR: No signature of method: qupath.ext.stardist.StarDist2D$Builder.preprocess() is applicable for argument types: (qupath.ext.stardist.OpCreators$PercentileTileOpCreator, qupath.opencv.ops.ImageOps$Channels$ColorDeconvolutionOp...) values: [qupath.ext.stardist.OpCreators$PercentileTileOpCreator@59ee801b, ...]
Possible solutions: preprocess([Lqupath.opencv.ops.ImageOp;), preprocess(qupath.ext.stardist.OpCreators$TileOpCreator) in QuPathScript at line number 23

ERROR: org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:72)
    org.codehaus.groovy.vmplugin.v8.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:161)
    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
    QuPathScript.run(QuPathScript:23)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)
    qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)
    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)
    qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)
    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    java.base/java.lang.Thread.run(Unknown Source)

ERROR: 
For help interpreting this error, please search the forum at https://forum.image.sc/tag/qupath
You can also start a new discussion there, including both your script &amp; the messages in this log.
</code></pre>
<p>Deconvolution works normally when I exclude <code>StarDist2D.imageNormalizationBuilder()</code> and use <code>.normalizePercentiles()</code> instead.</p>
<p>Am I missing anything to get the global normalisation working?</p> ;;;; <p>Dear community</p>
<p>Is there any publicly available DL models to segment the cerebellar dentate nucleus on MRI images? There are a few papers on it but no link to the actual models to try.</p>
<p>Thank you for your help.</p>
<p>Kai</p> ;;;; <p>Hi. Did you ever get anywhere with this?</p> ;;;; <p>Hi <a class="mention" href="/u/jameswang80428">@JamesWang80428</a>! I’m not familiar with cellpose, but it sounds like youre trying to split out a multichannel image.</p>
<p>If that’s a simple numpy save file (as it appears), yous hould be able to do the following:</p>
<pre data-code-wrap="pyhton"><code class="lang-plaintext">import numpy as np

data = np.load(image_filepath)
</code></pre>
<p><code>data</code> should now have a specific <code>shape</code> where one of the dimensions is the channel axis. This is usually the first dimension:</p>
<pre><code class="lang-python">data.shape  # something like (4, X, Y)

first_channel = data[0]
second_channel = data[1]
</code></pre>
<p>I recomment reading the <a href="https://numpy.org/doc/stable/user/absolute_beginners.html" rel="noopener nofollow ugc">introducting docs of numpy</a>, you need very little to get a long way <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi Eric,</p>
<p>I have the same problem as Mirla and since she didn’t answer, I don’t know how to fix it.</p>
<p>To answer your questions:</p>
<ul>
<li>yes I can use the Show z-stack and navigate through the different images in the preview panel.</li>
<li>I use the cubes during segmentation on both channels before merging the second channel on the firs one.</li>
<li>My aim is to do something similar as the 4D-XYZC-scatter plot presented on your website (in Data visualization), with one graph per frame (<a href="https://drescherlab.org/data/biofilmQ/docs/usage/visualization.html" class="inline-onebox" rel="noopener nofollow ugc">Data Visualization — BiofilmQ @ Drescher lab</a>). However, when I enter the same properties along the same axis, I end up with only one graph for the frame “0”, as if the whole stack was considered as being a single frame.</li>
</ul>
<p>My images are .czi files before importation, each of them being a z-stack of 30 z-planes for both channel.</p>
<p>I hope this makes the problem clearer. Thank you in advance for your answer!</p>
<p>Yazid</p> ;;;; <p>Dear David thanks for your reply and apologize me for the delay.<br>
My Bio-format version is the 6.10.1-20220731.012559</p> ;;;; <p>Since your microscope is Zeiss I would recommend using the Zeiss MTB API incase you can’t get micromanager to work and know C#. I have a github project which uses the API which may be helpful. <a href="https://github.com/BiologyTools/BioImager" class="inline-onebox" rel="noopener nofollow ugc">GitHub - BiologyTools/BioImager: A .NET microscopy imaging application based on Bio library. Supports various microscopes by using imported libraries &amp; GUI automation. Supports XInput game controllers to move stage, take images, run ImageJ macros on images or Bio C# scripts.</a></p> ;;;; <p>Since your device is Zeiss I would rather use the MTB API provided by Zeiss especially if you know C#. I have created a project on Github which uses the MTB API <a href="https://github.com/BiologyTools/BioImager" class="inline-onebox" rel="noopener nofollow ugc">GitHub - BiologyTools/BioImager: A .NET microscopy imaging application based on Bio library. Supports various microscopes by using imported libraries &amp; GUI automation. Supports XInput game controllers to move stage, take images, run ImageJ macros on images or Bio C# scripts.</a></p> ;;;; <p>Dear Qupath,  I try to find a way to measure Mean Fluo intensity (MFI)  in the different channels of my annotations, by default I have only area and perimeter. How can I extract these measurents ? It will be great also to have the possibility to put a threshold to measure above a theshold ? The same if I do detections, how to obtain these informations (if by default) these measurements was not checked in Cellpose script or others. Thanks so much, Qupath is a great software.</p> ;;;; <p>Yes, I agree that <code>(napari) ome-zarr</code> shouldn’t complain so much about missing Wells, since they are not present in the  <code>plate</code> metadata.</p>
<p>I created an issue at <a href="https://github.com/ome/ome-zarr-py/issues/263" class="inline-onebox">Sparse plate, with "missing" Wells shouldn't throw exceptions · Issue #263 · ome/ome-zarr-py · GitHub</a></p>
<p>However, I’m not sure why <code>bioformats2raw</code> is creating a 96-Well plate here instead of a 2-Well plate?</p> ;;;; <p>Are you still having trouble? What is the processor architecture of your system? If your laptop is Arm instead of x64 you may need to perform additional steps. <a href="https://imagej.nih.gov/ij/download.html" class="inline-onebox" rel="noopener nofollow ugc">Download</a></p> ;;;; <p><a class="mention" href="/u/thomasboudier">@ThomasBoudier</a>,</p>
<p>ah ok, that worked. Thanks!</p> ;;;; <p>Good morning,</p>
<p>I work on the feeding kinematic of salamanders and I would like to use Deeplabcut to be more efficient for the tracking of jaw and hyoid movement of the individuals on the video I made. The thing is, I try to understand the software for 2 months now and there are still some details I don’t understand and I’m not sure I do everything right.</p>
<p>I do single animal project<br>
I work with Deeplabcut version 2.2.3<br>
My operating system is windows 11<br>
My computer has a NVIDIA GeForce RTX 3080 Ti Laptop GPU</p>
<p>I have created a project importing at first 13 videos with different individuals of the same species laterally filmed during suction feeding. I labeled 200 frames for each of these 13 videos and I trained the network with 500000 iterations. Then, when analyzing and creating labeled videos, I obtained satisfying results.</p>
<p>After that, I added 20 other videos of individuals of the same species in the folder “videos” of the project and then I restarted the analysis but the results for these 20 new videos were not as good as for the previous 13 ones.</p>
<p>So, I wanted to refine outliers but:<br>
-First, I can’t choose myself the videos I want to refine because, only some of them present a h5 file when using extract frame (whatever the refining method chosen).<br>
-Secondly, after refining videos proposed by the software, I feel the improvement of the tracking are really weak, even if I run 500000 iterations…</p>
<p>I don’t know if there is a way to force the software to let me refine specific videos. I’m also not sure if there is an ideal number of iterations.</p>
<p>I always remove the folders of “evaluation result” when retraining and I also remove the fields in the folder videos as the software doesn’t overwrite them. I wonder if I must also remove the folder “iteration” in “training-datasets” when merging the data and recreating training dataset…</p>
<p>I’m sorry if my questions are naive, but I would be very pleased if you have any advice to give me.</p> ;;;; <p>Hi <a class="mention" href="/u/biovoxxel">@biovoxxel</a>,</p>
<p>Thanks for testing the new version of 3D Manager, the procedure to display the objects  is, at the moment :</p>
<ul>
<li>Import Image</li>
<li>Compute Rois</li>
<li>Live Roi</li>
<li>Select all</li>
</ul>
<p>Note that if you select another image, you need to <em>refresh</em> the display by selecting/deselecting some objects.</p>
<p>Best,</p>
<p>Thomas</p> ;;;; <p>Hello Everybody,</p>
<p>First of all, thank you for giving me really good advice.<br>
Nowadays, I am trying to get 4 individual images from the “_seg.npy” file in cellpose 2.2.<br>
As you can see on the screen, the result shows individual 4 images but actually it is one image(I mean that those 4 images are inside one image).<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e.jpeg" data-download-href="/uploads/short-url/zQo3vr4uDifjbN1KadaLNNXMHN4.jpeg?dl=1" title="028_img.ome_cp_output" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_690x172.jpeg" alt="028_img.ome_cp_output" data-base62-sha1="zQo3vr4uDifjbN1KadaLNNXMHN4" width="690" height="172" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_1380x344.jpeg 2x" data-dominant-color="A1B0A0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">028_img.ome_cp_output</span><span class="informations">3600×900 207 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
But I need the individual 4 images for the next step of processing.<br>
If you know the python source code to separate those 4 images from the “*_seg.npy” file, please share with me.<br>
This task is really important for me but I am not an expert in this field.<br>
So I wish for your kind helpness.</p> ;;;; <p>Hi,</p>
<p>the problem is that CellProfiler requires Python3.8 but Ubuntu 22.04 comes with Python3.10. In my case it helped to install Python3.8 and make a local environment with it. I made some new instructions for it, integrating some of the instructions I found on the GitHUB pages. It works, hopefully it will work for others as well. You can find the instructions on our lab web page: <a href="http://www.pfistererlab.org/tools.html" class="inline-onebox" rel="noopener nofollow ugc">Pfisterer Laboratory</a></p>
<p><a href="https://www.pfistererlab.org/cubbli22_cellprofiler_4_installation.html" class="onebox" target="_blank" rel="noopener nofollow ugc">https://www.pfistererlab.org/cubbli22_cellprofiler_4_installation.html</a></p> ;;;; <p>Hi <a class="mention" href="/u/roympc">@roympc</a></p>
<p>It would be much easier for us to help you if you uploaded an image (or better: several).</p> ;;;; <p>Hi, I’m trying to get up and running with the latest DLC (use case is single subject) and am stuck on the GPU-related aspects. I recall from earlier installations that this is tricky business, and want to get it right and avoid a mess. I’ve carefully read the installation instructions and tips and followed links to external (stackoverflow) threads, but I feel the install docs are not consistent. In short, what are the current recommendations for the GPU install?</p>
<p>Here are some of the relevant texts from the install page (which I find confusing/inconsistent):</p>
<ul>
<li>Note, DeepLabCut is up to date with the latest CUDA and tensorflow versions!</li>
<li>Install CUDA (versions up to CUDA11 are supported, together with TF2.5)</li>
<li>All of the TensorFlow versions work with DeepLabCut</li>
<li>We recommend TF2.10 now<br>
<a href="https://deeplabcut.github.io/DeepLabCut/docs/installation.html" class="inline-onebox" rel="noopener nofollow ugc">How To Install DeepLabCut — DeepLabCut</a>
</li>
</ul>
<p>The latest driver for my GPU (RTX 2070 Super, Windows 10) is 528.49 (I currently have 516.94).<br>
The latest CUDA version is 12.1 (I currently have 11.7)<br>
The latest Tensorflow version seems to be 2.11 (but not Win10 compatible?)</p>
<p>At <a href="http://tensorflow.org" rel="noopener nofollow ugc">tensorflow.org</a>, I see the following (scary) message: “TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2, or install tensorflow-cpu and, optionally, try the TensorFlow-DirectML-Plugin”</p>
<p><a href="http://Tensorflow.org" rel="noopener nofollow ugc">Tensorflow.org</a>’s install guide for Windows suggests an older CUDA, it seems:<br>
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0</p>
<p>The “tested build configurations” (for building from source) don’t show any CUDA greater than 11.2, even for TF2.11:  <a href="https://www.tensorflow.org/install/source_windows" class="inline-onebox" rel="noopener nofollow ugc">Twórz ze źródła w systemie Windows  |  TensorFlow</a></p>
<p>Can I go ahead and install DLC or should I upgrade, or even downgrade? Recall I’m currently on CUDA 11.7 and GPU 516.94.</p>
<p>Finally: There seems to be an DLC install on this computer. Ideally I’d like to set up a new conda environment for the latest DLC now to start fresh, and not delete/remove any old envs. Can I still follow the normal install instructions (git clone, conda create env), but somehow ensure that the new env gets a unique name (i.e. not DEEPLABCUT)? Thanks!</p> ;;;; <p>Sorry for the late reply;</p>
<p>The example macro I posted was meant to work with the “Fire” LUT as used in the previously posted image; in which the L channel has a steady rise in value across the LUT. The image you posted has a different LUT, so you need a different type of color “decoding” to get it to work.</p>
<p>But I am not sure if it is worth the effort to try to analyze the image as is, as it is a JPEG image so the translation between colors and values are already lost in the compression. If you zoom in on the color bar there is no consistency in colors across the width of the bar, so any attempt on translating colors to physical values will not work well as you do not have any proper reference. Try to get hold of the original data instead.</p>
<p>Stein</p> ;;;; <aside class="quote no-group" data-username="mdoube" data-post="2" data-topic="78788">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mdoube/40/30454_2.png" class="avatar"> Michael Doube:</div>
<blockquote>
<p>When the sampling vector passes through an image boundary, I am not sure if it counts the ‘outside’ as foreground, background, or doesn’t count it at all when it is counting foreground-background interfaces: we would have to take a look at the code.</p>
</blockquote>
</aside>
<p>To confirm, each vector passes through only the image pixels and doesn’t sample outside the image.</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209">
  <header class="source">

      <a href="https://github.com/bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209" target="_blank" rel="noopener">bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209</a></h4>



    <pre class="onebox"><code class="lang-java">
      <ol class="start lines" start="199" style="counter-reset: li-counter 198 ;">
          <li>	 * Only affects the random offset 0 &amp;lt; o &amp;lt; increment added to the sampling points</li>
          <li>	 * (see {@link #sampleSegment(RandomAccessible, Segment, Vector3dc, double)})</li>
          <li>	 * &lt;/p&gt;</li>
          <li>	 * @param seed seed value</li>
          <li>	 */</li>
          <li>	public static void setSeed(final long seed) {</li>
          <li>		ParallelLineMIL.seed = seed;</li>
          <li>	}</li>
          <li>
          </li>
<li>	// region -- Helper methods --</li>
          <li class="selected">	private static &lt;B extends BooleanType&lt;B&gt;&gt; long countPhaseChanges(</li>
          <li>		final RandomAccessible&lt;B&gt; interval, final Vector3d start,</li>
          <li>		final Vector3dc gap, final long samples)</li>
          <li>
          </li>
<li>	{</li>
          <li>		final RandomAccess&lt;B&gt; access = interval.randomAccess();</li>
          <li>		boolean previous = false;</li>
          <li>		long phaseChanges = 0;</li>
          <li>		for (long i = 0; i &lt; samples; i++) {</li>
          <li>			final boolean current = getVoxel(access, start);</li>
          <li>			if (current != previous) {</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <aside class="quote no-group" data-username="jgonet" data-post="3" data-topic="78788">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/j/ea666f/40.png" class="avatar"> Jordan_Gônet:</div>
<blockquote>
<p>I am not particularly affected by this bias, since the vectors do not touch the sides of the image?</p>
</blockquote>
</aside>
<p>That’s correct. In BoneJ 1.4.3 the centres of the sampling spheres are randomly distributed inside the image stack, but never closer than their sphere’s radius to the sides of the image.</p>
<p><a href="https://doi.org/10.1111/j.1365-2818.2004.01277.x">Ketcham and Ryan wrote this</a>:</p>
<blockquote>
<p>the uneven sampling inherent in a cubic VOI makes features orientated near 45° to one or more axes more likely to be included and measured than near-orthogonal features, particularly with small specimens and/or when the VOI is small compared with the fabric elements</p>
</blockquote>
<p>So basically the problem is that you might have an interaction between the shape of the image (cuboid) and the orientation of the texture, because the diagonal is longer edge-to-edge and corner-to-corner, which would mean sampling in those directions is more frequent than sampling in the face-to-face direction, which might tend to over-represent some texture directions. This interaction would only become meaningful when the size of the cuboid ROI approaches the frequency of the texture, and I expect for most specimens is likely to be drowned out by uncertainty in the sampling and variation in the specimen. As they acknowledge:</p>
<blockquote>
<p>it is difficult to gauge whether these effects are an artefact or reflect true specimen features</p>
</blockquote>
<p>(in other words, you may sample a different range of textures and get a different DA when you change the ROI a bit, which is not unexpected given local variations in trabecular bone geometry)</p>
<p>Note that the ‘m’ in MIL stands for ‘mean’, such that textures in each direction are averaged over the length of line probes in that direction, so the increased amount of sampling in a particular direction is removed by dividing the intersection count by the amount of sampling in that direction. To be honest, I don’t think that this is a real sampling bias problem, and is more of a problem of specimen variation and sampling uncertainty.</p> ;;;; <p>If that information is not included in the metadata of the original file, there is no way for Fiji to know that information, unfortunately. Much like pixel size, some of that information needs to come from the file, or from you.<br>
If you opened the image through BioFormats, there should be a checkbox to look at the image metadata that you can select. Alternatively, you can check Image → Properties… or Image → Show info to see if the information is stored there.</p> ;;;; <p>Hi Pablo,</p>
<p>So QuPaths current approach to scale bars is an adaptable one in the lower left side of the screen that changes depending on the current view. If you’re unable to see it then check out the “View” dropdown menu and you will find “show scalebar”. From there you can take a screenshot of the viewers content with the scalebar via “File” → “Export snapshot …” → “current viewer content”.</p>
<p>This is however is a small and un-constomisable scale bar (currently) so to I’d advise sending the screenshot over to imageJ within QuPath and then create a scale bar there as the transfer handles setting the correct scale. How to do that can be found <a href="https://qupath.readthedocs.io/en/0.4/docs/advanced/imagej.html#sending-image-regions-to-imagej" rel="noopener nofollow ugc">here</a>.</p>
<p>Alternativley, to set the scale in ImageJ without the need for QuPath at all, go to “Analyze” → “Set Scale”. From here you can fill in the window with your scanners scale values and then you’re ready to create the bars.</p>
<p>Hope this helps!</p>
<p>Fiona</p> ;;;; <p>Thanks <a class="mention" href="/u/jomaydc">@jomaydc</a> I tried to do the way you mentioned. But in the results how can I check the time interval? in the measurement table its mentioning that the mean, min and max values. But I am not sure about the time interval e.g. 0 min, 5min or 15 min</p> ;;;; <p>Hey Marcos,<br>
Very clever.  Can you share your code?<br>
Cheers,<br>
JAmes</p> ;;;; <p>Thanks for the detailed response. I used BoneJ 1.4.3 for this paper. If I understand correctly, I am not particularly affected by this bias, since the vectors do not touch the sides of the image?</p> ;;;; <p>Hi Konrad,</p>
<p>I hope you are well <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"></p>
<p>I just wanted to check if you had any luck with the error we had been working on?</p>
<p>Thanks</p>
<p>GR</p> ;;;; <p>Hey <a class="mention" href="/u/abdubey123">@abdubey123</a> and welcome!</p>
<p>The issue is that a spacing of (25, 1, 1) is rather extreme anisotropy. Is the xy resolution 400nm? (10µm/25) If so, then I personally don’t know of a good way to display such data in 3D, other than perhaps as individual 2D slices in 3D space — though we don’t have a mode for that kind of display. Empirically, I think any sampling more extreme than 5:1:1 or 10:1:1 at most looks bad.</p>
<p>Does anyone else have suggestions for making it look better?</p>
<p>I don’t know the oif format but the exact spacing is probably in the file metadata somewhere…?</p> ;;;; <p>Hi,</p>
<p>I found this Daniel’s post and debugged a bit differences between marlin and sprinter… (just basic linux commands used… nothing fancy). I used an old arduino uno board and ”&gt;” sprinter and marlin responses to a file, just to check is there something different with the format. And the visible/printed format was quite the same, but when cat -e theese files it started to come clear why marlin does not work with this ”plug-in”</p>
<p>The main reason why (any)marlin version does not work is, because line ending on sprinter is in ”windows” format and on marlin it is in unix format. Windows format is lfcr (line feed carriage return) and in unix it is just lf (line feed)</p>
<p>that is why ramps ”plug-in” does not recognize ”ok” lines correctly. It try to search ”ok” with <em>lfcr</em> or such and fails with ”ok” and <em>lf</em></p>
<p>I found this out while playing with 3d printer with marlin firmware just trying to use it like xyz stage… reason just for fun and idea was to check out bugs and plants (leafs and flowers mostly) with my kids. I have cheap usb microscope unused. Bit overkill but…</p>
<p>I am not good at coding/writing programs… hopefully somebody could make new ”marlin compatible” version of this ”plug-in” with this information… but it is maybe so that nobody wants to play with this kind of a ”toys”…</p>
<p>Big thanks to op for doing great job trying out different versions (I tried just different marlin versions never tought about sprinter)</p> ;;;; <p>Absolutely, and you would probably have more powerful and useful tools than a text output of a confusion matrix. This script was only intended for people that wanted to check things within QuPath, since all too often classifiers are not well validated.</p>
<p>Edit<br>
The main reason I did it within QuPath was in order to associate the points with the objects. You would need some way to do that, likely by adding a numerical value for the class to the list of measurements for the cell object. If each annotation is an individual point and not part of a Points object, you might try adding the measurement (class number) for the cell into the point, or use it as the point name. In the latter case, you would end up with a point class assigned when the point was created, and a name that matches the class of the cell the point exists within the ROI of.</p> ;;;; <aside class="quote no-group" data-username="jgonet" data-post="1" data-topic="78788">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/j/ea666f/40.png" class="avatar"> Jordan_Gônet:</div>
<blockquote>
<p>BoneJ plugin in Fiji</p>
</blockquote>
</aside>
<p>Which version of BoneJ are you using? The old 1.x downloaded from <a href="http://bonej.org">bonej.org</a> or the newer 2.x from the ImageJ updater?</p>
<p>In the old Anisotropy, spherical ‘balls’ of sampling vectors were made in random positions around the image until the result stabilised, and the vectors did not touch the sides of the image. In the new one, sampling vectors pass through the entire image volume. When the sampling vector passes through an image boundary, I am not sure if it counts the ‘outside’ as foreground, background, or doesn’t count it at all when it is counting foreground-background interfaces: we would have to take a look at the code. I would imagine that this would be the important implementation detail that could contribute to the edge and corner bias you mention. If image edges counted as interfaces, texture at corners and edges would look like smaller features than they really are, because of the way they have been ‘cut’ out of the parent texture. However, a way to get around that is not to count artefactual ‘interfaces’ at the image sides in the calculation of MIL.</p>
<p>There is usually a lot of uncertainty in estimating anisotropy using mean intercept length, due to random sampling effects and variation within the sample, even if you come up with a perfect unbiased sampling scheme. There are some other interesting effects that result from things like the rectangular pixel grid interacting with texture scale and sampling frequency. So its possible that the magnitude of this edge-and-corner bias may be negligible in the face of the stochasticity and uncertainties inherent in the method.</p>
<p>Note that some implementations of mean intercept length use a single sampling sphere placed inside the image, with all sampling vectors radiating from a single point. This sampling approach means that points closer to the centre of the sampling sphere are relatively over sampled compared to points towards the ends of the sampling vectors, and only the point in the centre is sampled from different directions.</p>
<p>In BoneJ 1.x we used many sampling spheres randomly placed inside the image volume, so that each point in the texture could be sampled from many directions, but that means each sampling sphere centre is relatively oversampled compared to the sphere’s periphery. To overcome this in BoneJ2, Anisotropy does a ‘rotating bed of nails’ sampling strategy, so that all the pixels have an equal chance of being sampled from all directions.</p> ;;;; <p>Does anyone know why when I put python -m deeplabcut into the terminal, I get a series of errors that ultimately ends with:<br>
*** End stack trace ***<br>
Abort trap: 6</p>
<p>What should I do?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da.jpeg" data-download-href="/uploads/short-url/ySfAPmXd6MSpZ5FbJICAF7fivj4.jpeg?dl=1" title="Screen Shot 2023-03-18 at 7.43.32 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_690x491.jpeg" alt="Screen Shot 2023-03-18 at 7.43.32 PM" data-base62-sha1="ySfAPmXd6MSpZ5FbJICAF7fivj4" width="690" height="491" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_690x491.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_1035x736.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_1380x982.jpeg 2x" data-dominant-color="272A2B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-18 at 7.43.32 PM</span><span class="informations">1552×1106 156 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi, I am new to Cell Profiler and am hoping to analyze confocal zstacks.</p>
<p>I exported my LEICA image as individual .tifs, but when I double click any image, I get a pipeline error (see below). Tutorial module images work fine and I added imagecodecs to PATH, but this image is still showing up. A similar error happens if I try with jpgs.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c64cabf9ab92a718242452748c13bba8cbacafb2.png" data-download-href="/uploads/short-url/sieO4HWlSchcMrzmIvCD3Tlu9MK.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c64cabf9ab92a718242452748c13bba8cbacafb2.png" alt="image" data-base62-sha1="sieO4HWlSchcMrzmIvCD3Tlu9MK" width="690" height="362" data-dominant-color="DBDBDB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1332×700 69.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I was able to get the labeled frames into a training dataset, but when I try to train the network, I get a series of lines from the yaml file, and the final line says “I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc::354] MLIR V1 optimization pass is not enabled”.</p>
<p>I started the training through the GUI on the most up-to-date version of DLC. I’m also using a Windows 10. Is this supposed to happen or what should I do?</p> ;;;; <p>Thank you so much, that worked for me!</p> ;;;; <p>Hii</p>
<p>I have just started exploring napari for visualizing multiphoton microscopy data. I am having trouble getting the 3d view. The 3d view from side angles looks like hazy or probably more stretched out.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3.jpeg" data-download-href="/uploads/short-url/As0gr6AVUkz5a4lz4XFv0UqKiFd.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3_2_676x500.jpeg" alt="image" data-base62-sha1="As0gr6AVUkz5a4lz4XFv0UqKiFd" width="676" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3_2_676x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3.jpeg 2x" data-dominant-color="18131D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">855×632 79 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The image file format is .oif which I load using <a href="https://pypi.org/project/oiffile/" rel="noopener nofollow ugc">oiffile</a>. The shape of the image is (3,25,512,512). The second axis is probably the number of z-stacks. I use the following scaling factors while loading the image : (25,1,1). [example image above and the video below has more number of z-stacks somewhere close to 70!] Each stack during acquisition was 10um apart. I guess the cause of my issue here is scaling but I do not know how to get the correct scaling factors.</p>
<p>The video of the 3d view can be found <a href="https://drive.google.com/file/d/1OW3g_we1p8GvaxI7ntDDHGrIDS6hUPDT/view?usp=share_link" rel="noopener nofollow ugc">here</a>.</p> ;;;; <p>Yes, that did it!</p>
<p>Now the next line to error out is 80 <img src="https://emoji.discourse-cdn.com/twitter/upside_down_face.png?v=12" title=":upside_down_face:" class="emoji" alt=":upside_down_face:" loading="lazy" width="20" height="20"></p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/20161cca7abaf3b5471b37c4dba74826a9b1453b.png" alt="image" data-base62-sha1="4zQEB15G12QW80SnpW4ybyxMtvR" width="353" height="480"></p>
<p>I believe line 80 is</p>
<pre><code class="lang-auto">	rendercolor(file);
</code></pre>
<p>Which is inside <code>processTiff</code></p>
<p>Interesting there’s no other mention of <code>rendercolor</code> in the whole forum, or on Google…<br>
Edit: nevermind it’s not interesting, it’s just the name of the function named in the script <img src="https://emoji.discourse-cdn.com/twitter/person_facepalming.png?v=12" title=":person_facepalming:" class="emoji" alt=":person_facepalming:" loading="lazy" width="20" height="20"></p> ;;;; <p>I am out of town right now, but do you think that I could export the point information and classifier prediction to some sort of csv file and then do the analysis in matlab? Like could I get a cell identifier number, the class predicted by the classifier, and then the point information into a csv from QuPath and then compare them in matlab?</p>
<p>I can try to go line by line next week but I just thought of a work around since I can’t seem to get confusion matrix in QuPath working</p> ;;;; <p>I’m glad you found your solution, but I’d like to offer another approach.</p>
<p>I’ve tried layering these methods but ended up giving up due to having to optimize too many parameters at once (i.e. threshold, watershed, particle size). For my purposes, this finetuning is unacceptable due to the data volume we handle. I now use <a href="https://www.cellpose.org/" rel="noopener nofollow ugc">CellPose</a> to avoid the threshold/watershed/segment loop entirely.<br>
Specifically, we trained a custom model that has a performance we like and now deploy this model en-masse.</p>
<p>Here’s a relevant example from <a href="https://nbviewer.org/github/leogolds/MicroscopyPipeline/blob/main/walkthrough.ipynb" rel="noopener nofollow ugc">our repo</a>.</p> ;;;; <p>Hi <a class="mention" href="/u/iarganda">@iarganda</a>,</p>
<p>In the example above, when the orientation of the structure to the segment is known, wouldn’t it be possible to only use that orientation (<code>nAngles = 1</code> and somehow Gabor angle = 90)?</p>
<p>Thanks!</p> ;;;; <p>I am currently working on the corrections of an article in which I perform anisotropy measurements on cubic bone volumes extracted from femoral heads. To do so, I use the BoneJ plugin in Fiji. One of my reviewers mentioned the existence of an “edge and corner bias” with cubic VOIs. Indeed, it seems that higher component values in orientations towards the edges or corners are found compared to spherical volumes (Ketcham &amp; Ryan, 2004, Journal of Microscopy, 213). I was wondering if BoneJ takes this bias into account when measuring anisotropy with cubic volumes or not at all.</p> ;;;; <p>Hi <a class="mention" href="/u/petebankhead">@petebankhead</a> Thank you very much! This worked and has solved the issue.</p> ;;;; <p>Hi All,</p>
<p>I am using QuPath 0.4.2 on a Mac OSX 12.6.3 M1 Pro to annotate H&amp;E-stained sections with a pixel classifier. I made a training set of rectangular annotations from about 20 slides differing somewhat in stain intensity and created a pixel classifier from manual annotation of the tissue.</p>
<p>The classifier gives very different results for when applied to images on the training set compared to their originals from the same project. I can find no differences in the corresponding descriptions on the Image tab (pixel resolution and stain vectors are the same), except the Server type is OpenSlide for the original and Sparse image server for the training slide.</p>
<p>What might be wrong?</p>
<p>Thanks for your help.</p>
<p>Cheers,<br>
Mark</p> ;;;; <p>Hello, I have the same question on version 2.3.0<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f44e1382e160f37a745d9a64fe0374bc8b06ecee.png" data-download-href="/uploads/short-url/yRdMnvQ90rIvmA1hNzzVNpc0CwS.png?dl=1" title="1679141790682" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f44e1382e160f37a745d9a64fe0374bc8b06ecee.png" alt="1679141790682" data-base62-sha1="yRdMnvQ90rIvmA1hNzzVNpc0CwS" width="690" height="359" data-dominant-color="2C2C2C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">1679141790682</span><span class="informations">1488×776 70.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi,<br>
Create two mask images from the source images with the Threshold… or Color Threshold… commands. Then compute the intersection of these masks with Image Calulator… using the AND operator. Measure the intersecting area on the resulting image.</p>
<p>Jerome</p> ;;;; <p>Try downloading the java source file from <a href="https://imagej.nih.gov/ij/plugins/download/Concentric_Circles.java">https://imagej.nih.gov/ij/plugins/download/Concentric_Circles.java</a></p>
<p>Save it as Concentric_Circles.java inside your plugins folder, open it with ImageJ and se the Compile and Run… command from the editor File menu.</p>
<p>Jerome</p> ;;;; <p>it’s the uppercase T in the function name.</p>
<p>Jerome</p> ;;;; <p><em>View → Rotate</em> rotates the contents of the viewer. It doesn’t make any changes to any images (but will impact all images opened in that viewer).</p>
<p>If you want to rotate the image itself by an increment of 90 degrees, you can do that during import:<br>
<a href="https://qupath.readthedocs.io/en/0.4/docs/tutorials/projects.html#add-images" class="onebox" target="_blank" rel="noopener">https://qupath.readthedocs.io/en/0.4/docs/tutorials/projects.html#add-images</a></p> ;;;; <p>Something is wrong with your location. It repeats itself multiple times.</p> ;;;; <p>You can create <code>OverlayOptions</code> without creating a viewer. See <a href="https://forum.image.sc/t/export-qupath-images-and-overlays-without-a-viewer/64302" class="inline-onebox">Export QuPath images and overlays without a viewer</a></p> ;;;; <p>You will probably have to create a Viewer, though not sure if that comes with even more complications without a GUI.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://qupath.github.io/javadoc/docs/qupath/lib/gui/viewer/QuPathViewer.html#%3Cinit%3E(qupath.lib.images.ImageData,qupath.lib.gui.images.stores.DefaultImageRegionStore,qupath.lib.gui.viewer.OverlayOptions,qupath.lib.display.ImageDisplay)">
  <header class="source">

      <a href="https://qupath.github.io/javadoc/docs/qupath/lib/gui/viewer/QuPathViewer.html#%3Cinit%3E(qupath.lib.images.ImageData,qupath.lib.gui.images.stores.DefaultImageRegionStore,qupath.lib.gui.viewer.OverlayOptions,qupath.lib.display.ImageDisplay)" target="_blank" rel="noopener">qupath.github.io</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://qupath.github.io/javadoc/docs/qupath/lib/gui/viewer/QuPathViewer.html#%3Cinit%3E(qupath.lib.images.ImageData,qupath.lib.gui.images.stores.DefaultImageRegionStore,qupath.lib.gui.viewer.OverlayOptions,qupath.lib.display.ImageDisplay)" target="_blank" rel="noopener">QuPathViewer (QuPath 0.4.0)</a></h3>

  <p>declaration: package: qupath.lib.gui.viewer, class: QuPathViewer</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/roympc">@roympc</a> . You can convert your images into a stack. Draw your ROI on the first image of the stack. Add this to the ROI Manager. Assuming your object is at the same position in all your images, you can just go to ‘More’ on the ROI Manager, then select Multi-Measure.</p> ;;;; <h3>
<a name="sample-image-andor-code-1" class="anchor" href="#sample-image-andor-code-1"></a>Sample image and/or code</h3>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe.jpeg" data-download-href="/uploads/short-url/kv1pZgB7bkBKuG1Fb2omCEEymuO.jpeg?dl=1" title="09a" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_450x375.jpeg" alt="09a" data-base62-sha1="kv1pZgB7bkBKuG1Fb2omCEEymuO" width="450" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_450x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_675x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_900x750.jpeg 2x" data-dominant-color="879D89"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">09a</span><span class="informations">1060×881 323 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h3>
<a name="background-2" class="anchor" href="#background-2"></a>Background</h3>
<p>I am using QuPath through command line.  I can’t however figure out how to save the final image along with its annotation as a jpeg/png.</p>
<p>I have this script, but obviously the annotations aren’t saved:</p>
<p><em>def server = getCurrentServer()</em><br>
<em>// Downsample image</em><br>
<em>def srv = RegionRequest.createInstance(server, 2)</em><br>
<em>def img_dir = ‘D:/KI67_data/Annotated images’</em><br>
<em>mkdirs(img_dir)</em><br>
<em>def name = GeneralTools.getNameWithoutExtension(getProjectEntry().getImageName())</em><br>
<em>def path = buildFilePath(img_dir, name + ‘.jpg’)</em><br>
<em>writeImageRegion(server, srv, path)</em></p>
<h3>
<a name="challenges-3" class="anchor" href="#challenges-3"></a>Challenges</h3>
<p>getCurrentViewer(), getOverlayOptions() return errors (<em>javax.script.ScriptException: Cannot invoke method getOverlayOptions() on null object in KI_annotation.groovy at line number 106</em>) when running through command line.</p>
<p>Therefore, I cannot use this in my script (from <a href="https://qupath.readthedocs.io/en/0.3/docs/advanced/exporting_images.html" class="inline-onebox" rel="noopener nofollow ugc">Exporting images — QuPath 0.3.0 documentation</a>)</p>
<p><em>// Write the full image, displaying objects according to how they are currently shown in the viewer</em><br>
<em>def viewer = getCurrentViewer()</em><br>
<em>writeRenderedImage(viewer, ‘/path/to/export/rendered.png’)</em></p>
<p>Is it possible to write a script without using getCurrentViewer(), getOverlayOptions() and save the image along with its annotation?</p> ;;;; <p><a class="mention" href="/u/mmusy">@mmusy</a> hello,<br>
It actually worked! Thank you very much for your help and can I ask you questions if I had any, in regard of image processing?<img src="https://emoji.discourse-cdn.com/twitter/pray/3.png?v=12" title=":pray:t3:" class="emoji" alt=":pray:t3:" loading="lazy" width="20" height="20"><img src="https://emoji.discourse-cdn.com/twitter/pray/3.png?v=12" title=":pray:t3:" class="emoji" alt=":pray:t3:" loading="lazy" width="20" height="20"></p> ;;;; <p>Nice to see this is starting to roll <img src="https://emoji.discourse-cdn.com/twitter/sunglasses.png?v=12" title=":sunglasses:" class="emoji" alt=":sunglasses:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/grinning.png?v=12" title=":grinning:" class="emoji" alt=":grinning:" loading="lazy" width="20" height="20"><br>
Thank you Josh Moore!</p> ;;;; <p>Hi <a class="mention" href="/u/sharkweekshane">@sharkweekshane</a> ,</p>
<p>Did you set the scale before starting SNT or after? I can reproduce this if I start SNT with an uncalibrated image, then set the scale. If the scale is set before starting, it seems to work as expected.</p> ;;;; <p>Hi <a class="mention" href="/u/zahraa_sweidan">@Zahraa_Sweidan</a> ,</p>
<p>There are several ways you could do this, but if you want a 3D animation you could use the reconstruction viewer (<em>3D tab &gt; Open Reconstruction Viewer</em>). Double-clicking will toggle a rotation animation. In the utilities menu (wrench icon), there is a <em>Record rotation</em> option which will save the frames of one 360 degree rotation to the snapshot directory, which can be changed in the <em>Global preferences…</em> of the gear menu. See <a href="https://imagej.net/plugins/snt/reconstruction-viewer" class="inline-onebox" rel="noopener nofollow ugc">SNT › Reconstruction Viewer</a></p>
<p>The frames can be rendered as an .mp4 with ffmpeg, e.g.,<br>
<code>ffmpeg -framerate 30 -i %5d.png video.mp4</code></p>
<p></p><div class="video-container">
    <video width="100%" height="100%" preload="metadata" controls="">
      <source src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/540a9967b51f7a64c9cddde4ff20c3d5be6d47a6.mp4">
      <a href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/540a9967b51f7a64c9cddde4ff20c3d5be6d47a6.mp4" rel="noopener nofollow ugc">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/540a9967b51f7a64c9cddde4ff20c3d5be6d47a6.mp4</a>
    </source></video>
  </div><p></p>
<p>There are many options for customizing the rendering of trees and meshes, including morphometric color-mapping, thickness, transparency. The viewer is also completely scriptable, see <em>Scripts &gt; Demos &gt; Reconstruction Viewer Demo</em> for an example.</p>
<p>For a 2D animation, you could use the Fill Manager to reconstruct the fluorescent volume around the skeleton, then export the segmentation as a grayscale/binary mask and save the stack as an .avi or view it in the 3D viewer, as per <a class="mention" href="/u/jbehnsen">@jbehnsen</a> 's suggestion.</p> ;;;; <p>+1 to what Gabriel wrote.</p>
<p>A couple of other thoughts - change gamma to better visualize the stain</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18.jpeg" data-download-href="/uploads/short-url/pf6Q9U7tQt7SJMluZsH9acpHcKc.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_690x280.jpeg" alt="image" data-base62-sha1="pf6Q9U7tQt7SJMluZsH9acpHcKc" width="690" height="280" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_690x280.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_1035x420.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_1380x560.jpeg 2x" data-dominant-color="8F7B9C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×780 201 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Skipping hematoxylin can help to separate the stains, here is an example of Alcian Blue-PAS in the crypts of the small intestine differentially staining Goblet cells and Paneth cells.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2.jpeg" data-download-href="/uploads/short-url/xeCeAJt5D1wb3FFlMumvce67c2u.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_528x500.jpeg" alt="image" data-base62-sha1="xeCeAJt5D1wb3FFlMumvce67c2u" width="528" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_528x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_792x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_1056x1000.jpeg 2x" data-dominant-color="B4D2DF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1221×1155 93.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Adding a blue or blackish hematoxylin to it will make it a lot harder. Histology stains are weird, it’s not always additive, and the limitations of RGB imaging mean that less is often more.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813.jpeg" data-download-href="/uploads/short-url/iiy23xqzFPV6MgVYMLzXtEGrWj9.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_378x500.jpeg" alt="image" data-base62-sha1="iiy23xqzFPV6MgVYMLzXtEGrWj9" width="378" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_378x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_567x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_756x1000.jpeg 2x" data-dominant-color="6480BA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">902×1193 60.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>For collagen staining, I highly recommend Picrosirus red - Fast green, it’s much more straightforward to use with a simple thresholder or pixel classifier on color-deconvolved samples and in our hands, there are fewer artifacts like plasma being stained by components of trichrome <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4682672/" class="inline-onebox" rel="noopener nofollow ugc">Histochemical Detection of Collagen Fibers by Sirius Red/Fast Green Is More Sensitive than van Gieson or Sirius Red Alone in Normal and Inflamed Rat Colon - PMC</a></p>
<p>An example file to play with: <a href="https://drive.google.com/open?id=1TJBdmc6Mt8CgUBQ90bu39BUs_-s4bV_p&amp;authuser=zmikulski%40lji.org&amp;usp=drive_fs" class="inline-onebox" rel="noopener nofollow ugc">PSR-Fast Green.czi - Google Drive</a></p>
<p>Best, Z.</p> ;;;; <p>Thank you so much!</p> ;;;; <p>This script, worked out with <a href="https://forum.image.sc/t/combining-two-imagej-macros-matching-in-output-directories-bio-formats/78534/7">help from</a> <a class="mention" href="/u/dgault">@dgault</a> (thank you!), whose purpose is to convert images in a .lif file to .tifs and also adjust them (brightness/contrast).</p>
<pre><code class="lang-auto">run("Bio-Formats Macro Extensions");
setBatchMode(true); 
input = getDirectory("Input directory, folder where your .lif file is and nothing else");
output = getDirectory("Output directory, where you'd like your adjusted .tiff files to go");


suffix = ".lif";

processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
}

function processFile(input, output, file) {
	Ext.setId(input + file);
	Ext.getSeriesCount(count);
	print("Processing: " + input + file+ " - "+d2s(count,0)+" images");

	for (f=0;f&lt;count;f++) {
		openLif(input+file,f);
		print("Saving to: " + output);
		listImages();
	}
}

function openLif(input,f){
	Ext.setSeries(f);
	run("Bio-Formats Importer", "open=[" +input +"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_" + d2s(f+1,0));
}

//Create string "image_1 image_2 image_3 image_4...."
function seriesN(num){
	str = "";
	for (i=0; i&lt;num; i++){
		ii = i+1;
        str = str+"image_"+ii+" ";
     }
     return str;
}

//Save
function listImages(){
	imageList = getList("image.titles");
	if (imageList.length==0)
	     print("No image windows are open");
	else {
    	print("Image windows:");
     	for (i=0; i&lt;imageList.length; i++){
        	print("   "+imageList[i]);
        	// TODO: Add condition for saving unadjusted files
                saveTiff(imageList[i]);
                
                // Process the files as per the second script
                processTiff(output, output, imageList[i]);
     	}
	}
	print("");
}


function saveTiff(winName){
	selectWindow(winName); 
	saveAs("Tiff", output + winName);
	close();
}

function processtiff(input, output, file) {

	print("processing: " + input + file);
	
	
	open(input+file+".tif");
	rendercolor(file);
	brightnessncontrast(file);
	deleteslices(file);

	saveas("tiff", output + file + "_bcadjusted");
	rename(file);
	
	rgbmerge(file);
	scalebar();
	makemontage(file);

	selectwindow("montage1to5");
	saveas("jpeg", output + file + "_montage1to5");
	selectwindow("montagehorizontal");
	saveas("jpeg", output + file + "_bcmontage");

	print("saving to: " + output);
	run("close all");
}



// Give colors for each slice
function renderColor(file){
	color = newArray("Blue","Green","Red","Grays");
	//color = newArray("Blue","Green","Red");

	run("Make Composite", "display=Color");
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		run(color[i]);
	}
}
// Change brightness and contrast
function brightnessNcontrast(file){
	//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		setMinAndMax(BC_range[2*i],BC_range[2*i+1]);
	}
	
}

// Delete slices. You want to delete slice a, b
function deleteSlices(file){
	deleteA = 0;
	deleteB = 0;
	//deleteB &gt; deleteA. deleteB should be larger than deleteA

	print("deleting the slice "+deleteA+" and "+deleteB);
	selectWindow(file);
	if(deleteB&gt;0){
		setSlice(deleteB);
		run("Delete Slice", "delete=channel");
		}
	if(deleteA&gt;0){
		setSlice(deleteA);
		run("Delete Slice", "delete=channel");
	}	
}

// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.
function RGBmerge(file){
	run("Duplicate...", "title=4channels duplicate");
	run("RGB Color");
		
	selectWindow(file);
	run("Duplicate...", "title=Merge duplicate");
	Stack.setDisplayMode("composite");
	run("RGB Color");
	run("Copy");

	selectWindow("4channels (RGB)");
	setSlice(nSlices);
	run("Add Slice"); 
	run("Paste"); 

	close("4channels");
	close("Merge");
	close("Merge (RGB)");
	
	// "4channels (RGB)" is made.
}

// Add scale bar
function scaleBar(){
	setSlice(nSlices);
	run("Set Scale...", "distance=311.0016 known=100 pixel=1 unit=µm");
	//run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] hide");
	run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] bold");
}

function makeMontage(input){
	//selectWindow("4channels (RGB)");
	//run("Make Montage...", "columns=1 rows=5 scale=0.25 border=2");
	//rename("Montage1to5");
	selectWindow("4channels (RGB)");
	run("Make Montage...", "columns="+nSlices+" rows=1 scale=0.5 border=2");
	rename("MontageHorizontal");
}
function GetTime(){
     MonthNames = newArray("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec");
     DayNames = newArray("Sun", "Mon","Tue","Wed","Thu","Fri","Sat");
     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);
     TimeString ="Date: "+DayNames[dayOfWeek]+" ";
     if (dayOfMonth&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+dayOfMonth+"-"+MonthNames[month]+"-"+year+"\nTime: ";
     if (hour&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+hour+":";
     if (minute&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+minute+":";
     if (second&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+second;
     print(TimeString);
}
</code></pre>
<p>Unfortunately, I run into an error of:</p>
<pre><code class="lang-auto">Undefined identifier in line 61
(called from line 29)
(called from line 17)
(called from line 9)

&lt;processTiff&gt; (output,output,imageList[i]);
</code></pre>
<p>And I don’t know why? I saw <a href="https://forum.image.sc/t/unidentified-identifier-error/53920/4">this post</a> which has a similar problem and the solution was related to variables being global/not, but I don’t understand how that would apply here</p> ;;;; <p>Hi</p>
<p>The Linkam CMS196 stage was the first stage device we implemented in Python-Microscope before we had settled on a stage interface. As such, the interface is slightly different and does not implement <code>microscope.abc.Stage</code>. That’s the reason why it was not listed under supported devices. It’s used in production in a few places but we never went back to “retrofit” it.</p>
<p>To use the implementation you need the LinkamSDK which is sold separately by Linkam (the SDK supports Linux and Windows). Here’s a short code snippet on how to use it (untested - I no longer have access to the hardware).</p>
<pre><code class="lang-auto">from microscope.stages.linkam import LinkamCMS

stage - LinkamCMS(uid="")  # I think you can leave the UID empty if you only have one stage connected

# Returns dict with connection state, temperature, light, etc
print(stage.get_status())

assert stage.get_status().get("connected")

position = stage.get_position()

print("current position is", stage.get_position())  # dict with X and Y keys
stage.move_to((xpos, ypos))  # 2 element tuple of floats

print("x limit is", stage.get_value_limits("MotorSetpointX")
print("y limit is", stage.get_value_limits("MotorSetpointY")
</code></pre> ;;;; <aside class="quote no-group" data-username="Grambuld" data-post="1" data-topic="78764">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/g/278dde/40.png" class="avatar"> Alexander:</div>
<blockquote>
<p>PAS-staining</p>
</blockquote>
</aside>
<p>Hi, I strongly suspect that those two slides were stained with PAS  and they also had haematoxylin (?) to identify nuclei.<br>
PAS looks magenta colour, so in those two slides you can’t really tell what is the contribution of the PAS and which is haematoxylin.<br>
One idea would be to stain for PAS but not use any other nuclear stain.<br>
If you want to “quantify” glycogen (as in how much there is), please consider that PAS is not specific to glycogen as it also stains <em><strong>many</strong></em> other things like basement membrane components, fungi, cellulose, glycoproteins, etc.<br>
I would be a bit cautious about attempting to equate stain to quantities just by considering intensity without a proper calibration <strong>and</strong> being confident that the stain is stoichiometric (not sure it is). Be aware that if it is not stoichiometric, the intensity of the stain detected will not  be a reliable measure of the amount of glycogen present.<br>
Hope this helps.</p> ;;;; <p>I got it! Thanks SO much for the feedback. I hope to one day contribute to this forum.</p> ;;;; <aside class="quote no-group" data-username="Bastien" data-post="3" data-topic="78743">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/b/51bf81/40.png" class="avatar"> Bastien:</div>
<blockquote>
<p>No we don’t have this type of microscope.</p>
</blockquote>
</aside>
<p>If you have a bright field microscope, you do…<br>
I suggest doing a bit of reading.</p> ;;;; <p>Autofluorescence might be another option.</p> ;;;; <p>I may be missing an obvious way to do this (I hope), but in a project can I rotate an individual image without applying the same rotation to all images in the project? When I have an image opened and use View &gt; Rotate image on it, my rotation is applied to all images, which is not what I want.</p> ;;;; <p>I may have found a convoluted method to get what I want.</p>
<p>Run Find Edges followed my analyze particles to eliminate the small debris, use fill x2, run watershed, and then another analyze particles.</p>
<p>It seems to give me what I would be expecting but it is super roundabout.</p> ;;;; <p>That code looks to be a bit old <a href="https://imagej.nih.gov/ij/plugins/colocalization.html" class="inline-onebox">Colocalization</a></p>
<p>Have you tried <a href="https://imagej.net/plugins/coloc-2" class="inline-onebox">Coloc 2</a></p>
<p>If you wanted someone to test, we would need sample images.</p> ;;;; <p>hi James,<br>
I found out that in my case opening all series at once and then processing each series individually is orders of magnitude faster than opening each one of the series individually in sequence. It went from taking a couple of mins processing a single series to processing several series/second) The only limitation is that you need to run the macro in a system with enough free RAM, which doesn’t seems to be an issue in your system.</p> ;;;; <p>Yes, once you find edges, you have lines, which are now all connected (for touching cells), so that is the expected result.<br>
You would need to do something like shrink the ROI by one pixel or two to make sure the touching edges are not a single line. Unfortunately, standard erosion won’t work since that’s on the pixels and not the ROI.</p>
<aside class="quote no-group" data-username="Robert_Schweickart" data-post="4" data-topic="78774">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/robert_schweickart/40/67178_2.png" class="avatar"> Robert Schweickart:</div>
<blockquote>
<p>small debris when the size parameter within Analyze Particles can’t do it alone</p>
</blockquote>
</aside>
<p>Not sure what you gain from the outline then. You should be able to get a perimeter from the objects after analyze particles + Measure, or any of a variety of other measurements to filter the ROIs down more.<br>
Could be wrong, but I don’t think there is an easy way to find the shared edges once you have removed all of the information about shared edges by using Find Edges (it becomes an image with pixels, objects are gone).</p>
<p>If you wanted to, you could probably load the old ROIs into the Find Edges result… and maybe do some sort of ROI intersection? Not sure.</p> ;;;; <p>Ah, I was thinking pure collagen detection, which only requires a blue thresholder. If you want context, that will be much harder to impossible for the pixel classifier. Until there is a deep learning option, I don’t think it will work well unless there is a strong per pixel or pixel cluster difference between areas. Context (it’s red, but it’s nearby enough blue stuff) isn’t going to work.</p> ;;;; <p>Hi all!</p>
<p>I am unable to download the concentric circles plug in. I am unsure if it’s my Mac and I should just try on a pc or if there is another way to download this plugin if anyone else has encountered this error before. please help if able. Thank you very much, I hope to be able to contribute to this platform someday!<br>
When I go to my folder download my computer says I am unable to download this plugin because it can’t recognize if it’s malware. How can I get around that?<br>
Thanks!!</p> ;;;; <p>Now it works. Thank you very much for your guidance.</p> ;;;; <p>Find Edges seems great in that it helps eliminate the small debris when the size parameter within Analyze Particles can’t do it alone. But without watershedding, multiple cells are being counted as a single cell.</p>
<p>I hope this helps lol.</p> ;;;; <p>This is an image of watershedding. The cells are being being divided up.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/58f882e7943d46e2de1a0381142167645357176b.png" data-download-href="/uploads/short-url/cH4rVJtV4acH1BKBpsW41SjqIZl.png?dl=1" title="Screen Shot 2023-03-17 at 2.57.10 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_472x500.png" alt="Screen Shot 2023-03-17 at 2.57.10 PM" data-base62-sha1="cH4rVJtV4acH1BKBpsW41SjqIZl" width="472" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_472x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_708x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_944x1000.png 2x" data-dominant-color="252525"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-17 at 2.57.10 PM</span><span class="informations">1378×1458 187 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>This is the results of running analyze particles.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac.png" data-download-href="/uploads/short-url/mg7DmazvudExyOfJxx3RLHyvbyY.png?dl=1" title="Screen Shot 2023-03-17 at 10.52.29 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_469x500.png" alt="Screen Shot 2023-03-17 at 10.52.29 AM" data-base62-sha1="mg7DmazvudExyOfJxx3RLHyvbyY" width="469" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_469x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_703x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_938x1000.png 2x" data-dominant-color="22221D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-17 at 10.52.29 AM</span><span class="informations">1370×1458 64.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>This is watershedding + Find Edges<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9.png" data-download-href="/uploads/short-url/khHhAvjuo1VjfciqfAr10nFdYoN.png?dl=1" title="Screen Shot 2023-03-17 at 3.03.23 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_475x500.png" alt="Screen Shot 2023-03-17 at 3.03.23 PM" data-base62-sha1="khHhAvjuo1VjfciqfAr10nFdYoN" width="475" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_475x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_712x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_950x1000.png 2x" data-dominant-color="0F0F0F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-17 at 3.03.23 PM</span><span class="informations">1382×1452 235 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And this is analyze particles. What we see is that the number of cells actually decrease because the watershedding (even though the watershedding lines are visible) isn’t being considered.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254.png" data-download-href="/uploads/short-url/njDUKygKOWB9Mhpppmuvxb2nDc8.png?dl=1" title="Screen Shot 2023-03-17 at 3.03.46 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_492x500.png" alt="Screen Shot 2023-03-17 at 3.03.46 PM" data-base62-sha1="njDUKygKOWB9Mhpppmuvxb2nDc8" width="492" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_492x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_738x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_984x1000.png 2x" data-dominant-color="0C0C08"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-17 at 3.03.46 PM</span><span class="informations">1382×1404 77 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I have an update on this issue. The name of individuals in the config file was originally changed to “Rat1”,  “Rat2”, and “Rat3” from the default “ind1”, “ind2” and “ind3”. When Analyzing videos with 2 subjects in a network trained for 3 individuals, the names of individuals are changed to the default “ind#” labels. Therefore, individuals are reported in the CSV output file as “ind1” and “ind2”, instead of “Rat1” and “Rat2”. The video for checking labels is made, but not the Final video with the ID colors. When I change the cofig file by removing “Rat3”, then the output CVS file rightly labels the individuals as “Rat1” and “Rat2”. Evenmore, the final video is made.</p> ;;;; <p>I just came across this interesting (old) thread. As another alternative, here’s colour deconvolution in pyvips. It should be much faster than numpy, and will work on images of any size without using much memory.</p>
<aside class="onebox githubissue" data-onebox-src="https://github.com/libvips/pyvips/issues/289#issuecomment-994539912">
  <header class="source">

      <a href="https://github.com/libvips/pyvips/issues/289#issuecomment-994539912" target="_blank" rel="noopener nofollow ugc">github.com/libvips/pyvips</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/libvips/pyvips/issues/289#issuecomment-994539912" target="_blank" rel="noopener nofollow ugc">Save image to tif</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2021-12-09" data-time="11:29:45" data-timezone="UTC">11:29AM - 09 Dec 21 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/SikangSHU" target="_blank" rel="noopener nofollow ugc">
          <img alt="SikangSHU" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8533c1c50978d0086b67fccf80e94e3545e8e98.png" class="onebox-avatar-inline" width="20" height="20">
          SikangSHU
        </a>
      </div>
    </div>

    <div class="labels">
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">Dear Professor:
      I consider to save images to tif,  but there is an error <span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">occured as follows. I don't know how to save. I would greatly appreciate it if you could help me.
     This is my code:

```python
img = pyvips.Image.new_from_file('E:\\testpicture_jupyter\\ometif_def6.tif', access='sequential')
patch_size = 512
n_across = img.width // patch_size
n_down = img.height // patch_size
x_max = n_across - 1
y_max = n_down - 1
print(x_max, y_max)

for y in range(0, n_down):
    print("row {} ...".format(y))
    for x in range(0, n_across):
        patch = img.crop(x * patch_size, y * patch_size,
                         patch_size, patch_size)
        image = pyvips.Image.arrayjoin(patch, across=img.width)
image.tiffsave("huge.tif")
```

     And the error occurred is:

```
(wsi2.py:1468): GLib-GObject-WARNING **: 19:19:47.663: value "34102784" of type 'gint' is invalid or out of range for property 'width' of  type 'gint'
```</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Is there a line selection on the Image? (The image you sent had one.) If not, delete the first 2 lines of the macro.</p> ;;;; <p>You can do the colour deconvolution in pyvips, fwiw. It should be a lot quicker than numpy, and will work for any size image without running out of memory. You could run it on an entire slide image, for example.</p>
<p>Sample code here:</p>
<aside class="onebox githubissue" data-onebox-src="https://github.com/libvips/pyvips/issues/289#issuecomment-994539912">
  <header class="source">

      <a href="https://github.com/libvips/pyvips/issues/289#issuecomment-994539912" target="_blank" rel="noopener nofollow ugc">github.com/libvips/pyvips</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/libvips/pyvips/issues/289#issuecomment-994539912" target="_blank" rel="noopener nofollow ugc">Save image to tif</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2021-12-09" data-time="11:29:45" data-timezone="UTC">11:29AM - 09 Dec 21 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/SikangSHU" target="_blank" rel="noopener nofollow ugc">
          <img alt="SikangSHU" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8533c1c50978d0086b67fccf80e94e3545e8e98.png" class="onebox-avatar-inline" width="20" height="20">
          SikangSHU
        </a>
      </div>
    </div>

    <div class="labels">
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">Dear Professor:
      I consider to save images to tif,  but there is an error <span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">occured as follows. I don't know how to save. I would greatly appreciate it if you could help me.
     This is my code:

```python
img = pyvips.Image.new_from_file('E:\\testpicture_jupyter\\ometif_def6.tif', access='sequential')
patch_size = 512
n_across = img.width // patch_size
n_down = img.height // patch_size
x_max = n_across - 1
y_max = n_down - 1
print(x_max, y_max)

for y in range(0, n_down):
    print("row {} ...".format(y))
    for x in range(0, n_across):
        patch = img.crop(x * patch_size, y * patch_size,
                         patch_size, patch_size)
        image = pyvips.Image.arrayjoin(patch, across=img.width)
image.tiffsave("huge.tif")
```

     And the error occurred is:

```
(wsi2.py:1468): GLib-GObject-WARNING **: 19:19:47.663: value "34102784" of type 'gint' is invalid or out of range for property 'width' of  type 'gint'
```</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Here are some examples:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4.jpeg" data-download-href="/uploads/short-url/7LHc1HMc3DGBzmMTCbbmKdpSvUE.jpeg?dl=1" title="141 masson pre" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4_2_690x424.jpeg" alt="141 masson pre" data-base62-sha1="7LHc1HMc3DGBzmMTCbbmKdpSvUE" width="690" height="424" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4_2_690x424.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4_2_1035x636.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4.jpeg 2x" data-dominant-color="AB558F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">141 masson pre</span><span class="informations">1249×769 259 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e.jpeg" data-download-href="/uploads/short-url/9zwGgGRuLdTIPxmMGI1BOxIgSlg.jpeg?dl=1" title="whole case" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_690x305.jpeg" alt="whole case" data-base62-sha1="9zwGgGRuLdTIPxmMGI1BOxIgSlg" width="690" height="305" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_690x305.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_1035x457.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_1380x610.jpeg 2x" data-dominant-color="573E49"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">whole case</span><span class="informations">1920×851 106 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Here am trying to “teach” qupath what is fibrotic tissue:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade.jpeg" data-download-href="/uploads/short-url/zkkSg1Tb9bysMcbM3B2IBaG46hE.jpeg?dl=1" title="fibrosis teachin" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_690x305.jpeg" alt="fibrosis teachin" data-base62-sha1="zkkSg1Tb9bysMcbM3B2IBaG46hE" width="690" height="305" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_690x305.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_1035x457.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_1380x610.jpeg 2x" data-dominant-color="AE5892"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">fibrosis teachin</span><span class="informations">1920×851 429 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <aside class="quote no-group" data-username="Fabio_Tavora" data-post="4" data-topic="78764">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fabio_tavora/40/49123_2.png" class="avatar"> Fabio Tavora:</div>
<blockquote>
<p>I tried before to establish thresholds to differentiate colors in qupath</p>
</blockquote>
</aside>
<p>Hmm, Masson’s should be pretty straightforward since the blue and red are generally well separated (with color decon), but if you have <em>overlap</em> (nuclei are a problem), if the staining is too dark you will be unable to accurately separate the colors. In general, dark staining is a problem because black is all colors, and the closer you get to black, the less information you have.</p>
<p>Pixel classifier might be a better way to go about it, but last time I did Masson’s QuPath didn’t have a pixel classifier and relied on… dun dun dunnnnnn superpixels (<a class="mention" href="/u/smcardle">@smcardle</a> ).</p> ;;;; <p>Hi <a class="mention" href="/u/research_associate">@Research_Associate</a> , I agree the PAS stains seems very dark on the posted image. Polysaccharides such as glycogen, glycoproteins, and glycolipids stain bright magenta with PAS without diastase, while normal liver without glycogen, for example, would have only the hematoxylin counter stain. So one would have to design a QuPath experiment in distinguishing cells with the magenta pigment to the ones without it.</p>
<p>I tried before to establish thresholds to differentiate colors in qupath, but I was unsuccesful. I tried to measure fibrosis using Masons Trichrome in human cardiac tissue.</p> ;;;; <p>The same error (no selection in line 1) appeared by using ImageJ.<br>
Is it possible to add x-y values manually to draw the curve on the initial picture (with the<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062.jpeg" data-download-href="/uploads/short-url/9z5STzBpXYOD76Nj6psvidhtYJ4.jpeg?dl=1" title="a" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062_2_689x404.jpeg" alt="a" data-base62-sha1="9z5STzBpXYOD76Nj6psvidhtYJ4" width="689" height="404" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062_2_689x404.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062_2_1033x606.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062.jpeg 2x" data-dominant-color="CECECE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">a</span><span class="informations">1231×722 176 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
reference system)?</p> ;;;; <p>Thank you all</p> ;;;; <p>It would probably help to show some example pictures. Intuitively I would not expect find edges and watershedding to interact well together (lines versus areas), but not sure what you are looking at.</p> ;;;; <aside class="onebox githubblob" data-onebox-src="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb">
  <header class="source">

      <a href="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb" target="_blank" rel="noopener">MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb</a></h4>


      <pre><code class="lang-ipynb">{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
</code></pre>



  This file has been truncated. <a href="https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
From github repository, it looks like the flags are default set to false to output those images individually (see last cell).</p> ;;;; <p>Since QuPath is linked in this, if you run CellPose through QuPath, you will get the positions of all of the cells, along with measurements, as ROI/detection objects. I am not as sure about the outlines and cell poses though, maybe <a class="mention" href="/u/oburri">@oburri</a> knows more.  I kind of doubt it since that would be a huge amount of data by default if run across whole slide images.</p> ;;;; <p>Ah, sorry, 1.53t. I didn’t state since it was the same as yours.<br>
The built in sample image I used was the Organ of Corti.</p> ;;;; <p>Also want to throw out there that SimpleTissueDetection has been deprecated as of the development of the thresholder. <a href="https://qupath.readthedocs.io/en/0.4/docs/tutorials/thresholding.html" class="inline-onebox">Detecting tissue — QuPath 0.4.3 documentation</a></p>
<p>Any particular reason you are looking into it?</p> ;;;; <p>Crosslinking here in case anyone has any thoughts on the PAS staining shown. <a href="https://forum.image.sc/t/aid-in-analysis-of-pas-stained-slides-using-qupath/78764" class="inline-onebox">Aid in analysis of PAS-stained slides using Qupath</a></p> ;;;; <aside class="quote no-group" data-username="Research_Associate" data-post="2" data-topic="78764">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p>Might be worth checking your reagents, since that is much darker than I usually see for PAS.</p>
</blockquote>
</aside>
<p>Maybe <a class="mention" href="/u/smcardle">@smcardle</a> or <a class="mention" href="/u/zbigniew_mikulski">@Zbigniew_Mikulski</a> or the pathologists have more experience though. Will cross link once I find that.</p> ;;;; <aside class="quote no-group" data-username="Grambuld" data-post="1" data-topic="78764">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/g/278dde/40.png" class="avatar"> Alexander:</div>
<blockquote>
<p>Visually there is a tremendous difference, but I would like to obtain a quantitative measure for my observation to assess my results with statistics (Avoiding a self-made scoring system of whom I am not a fan). Do you have any ideas on a interesting measure (percentage area, positive cells etc.) and how to approach this image analysis?</p>
</blockquote>
</aside>
<p>Seems most appropriate to use a thresholder for a particular stain deconvolution if you want to assess disease severity by area. If distribution is important, or cell density, you may try cell segmentation with one of the deep learning algorithms, but I have my doubts how well that would work for the darker stain.<br>
Might be worth checking your reagents, since that is much darker than I usually see for PAS.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc.png" data-download-href="/uploads/short-url/4BOXVxydKQW47UcwmHkioOQvNVO.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_690x228.png" alt="image" data-base62-sha1="4BOXVxydKQW47UcwmHkioOQvNVO" width="690" height="228" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_690x228.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_1035x342.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_1380x456.png 2x" data-dominant-color="EFEFEF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1388×459 48.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
The intensity might be due to disease, but the color might be an issue as well, which could cause difficulties with quantification.</p>
<p>I don’t know that PAS staining is quantitative, and I kind of doubt you can use stain intensity to measure an amount of something in this case. Especially when it is so dark.</p> ;;;; <p>Hey all,</p>
<p>Is there a way to combine these two processes? Currently, ‘Find Edges’ is overriding watershedding? I say override, but in actuality what is happening is after running the ‘Find Edges’ tool, analyze particles is no longer picking up the watershedding that is taking place.</p>
<p>Any help is appreciated!</p> ;;;; <p>This looks like a Fiji problem. Try using ImageJ.</p> ;;;; <p>Dear community (esp. python-microscope),<br>
I see there is code online to drive a Linkam CMS196 online here:<br>
<a href="https://github.com/python-microscope/microscope/tree/master/microscope/stages" class="inline-onebox" rel="noopener nofollow ugc">microscope/microscope/stages at master · python-microscope/microscope · GitHub</a><br>
But there doesn’t seem to be any guide about how to implement it, could I have some hints?</p>
<p>Also this stage seems to be catered for explicitly, looking through the source code, but the stage doesn’t appear here:<br>
<a href="https://python-microscope.org/doc/architecture/supported-devices.html" class="inline-onebox" rel="noopener nofollow ugc">Supported Devices — Python Microscope</a></p>
<p>Given a few hints, we’d be happy to feedback a howto writeup,</p>
<p>Thanks,</p>
<p>Edward<br>
(EPFL Center for Imaging)</p> ;;;; <aside class="quote no-group" data-username="Alvin" data-post="1" data-topic="78484">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png" class="avatar"> Alvin Lam:</div>
<blockquote>
<p>However, I was unable to install AnalyzeSkeleton into the QuPath version of ImageJ. I am not sure is the plugin on the Fiji side utilizing ImageJ2</p>
</blockquote>
</aside>
<p>Just in case, stepping back, AnalyzeSkeleton does not show up in the Plugins menu. That doesn’t explain errors, but it is a part of Fiji <strong>and</strong> ImageJ, not a plugin (well, not in the plugins menu, it is default included for both).</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6.jpeg" data-download-href="/uploads/short-url/rrhu09BGEuto3T47Q9NcA9A6Vx4.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_690x365.jpeg" alt="image" data-base62-sha1="rrhu09BGEuto3T47Q9NcA9A6Vx4" width="690" height="365" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_690x365.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_1035x547.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_1380x730.jpeg 2x" data-dominant-color="A5AFAE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1759×931 252 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>ImageJ2 is not Fiji, it’s a coding framework that some parts of Fiji use, but no parts of ImageJ1 should use. Just because something is in Fiji does not mean it uses ImageJ2.</p>
<aside class="quote no-group" data-username="Alvin" data-post="8" data-topic="78484">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png" class="avatar"> Alvin Lam:</div>
<blockquote>
<p>I assume if the <a href="http://image.net">image.net</a> page has a dedicated link for the installation of the plugin for ImageJ1, the Fiji default version should be something that utilizes the counterpart ImageJ2.</p>
</blockquote>
</aside>
<p>Reasonable, but no. Just an older version that doesn’t seem to be maintained.</p>
<p>The script as <a class="mention" href="/u/ym.lim">@ym.lim</a> works for me using the Fiji plugins directory, or the default ImageJ. Not sure if you have changed your Fiji in some way for it not to work.</p> ;;;; <p>Here’s a helpful thread doing some of what you want: <a href="https://forum.image.sc/t/need-help-quantifying-nuclear-and-cytoplasmic-rnascope-foci/51077/5" class="inline-onebox">Need help quantifying nuclear and cytoplasmic RNAscope foci - #5 by Research_Associate</a></p> ;;;; <p>good morning. I find myself with the problem that every time I finish making a measurement the image closes and I would like to make multiple measurements on the same image. Is there a way to do it?</p> ;;;; <p><code>pip uninstall tensorflow-gpu</code></p>
<p><code>pip install tensorflow==2.10</code></p>
<p>Should fix this</p> ;;;; <p>Hello everyone,</p>
<p>First of all, really thank you for the cellpose developers and experts.<br>
I am a really fresh developer in this field and so curious about the powerful functions of cellpose 2.2.<br>
Now I just started the cell segment by using cellpose 2.2 and succeeded to get  “<em>_seg.npy" and "</em>_cp_output.jpg” files from origine images by using windows command.<br>
This is the result.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d.jpeg" data-download-href="/uploads/short-url/vPEVnpF0ctIpkD4pADsSgPXzX8V.jpeg?dl=1" title="029_img.ome_cp_output" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg" alt="029_img.ome_cp_output" data-base62-sha1="vPEVnpF0ctIpkD4pADsSgPXzX8V" width="690" height="172" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1380x344.jpeg 2x" data-dominant-color="A5A7A3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">029_img.ome_cp_output</span><span class="informations">3600×900 239 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And now I want to get more detailed images from the “<em>_seg.npy" file.<br>
I really want to do this but I am not sure how to do this because I just have some knowledge about Python and image processing.<br>
If possible, please share with me the python source code to get these individual images and correct position of ROIS from the "</em>_seg.npy” file.</p>
<p>Thank all of you again.</p> ;;;; <p>Hello everyone,</p>
<p>First of all, really thank you for all of your kind helpness.<br>
And also thank you for the cellpose developers and experts.<br>
I am a really fresh developer in this field and so curious about the powerful functions of cellpose 2.2.<br>
Now I just started the cell segment by using cellpose 2.2 and succeed (I am not sure how you think about this but it’s certainly big success for me.) to get  “<em>_seg.npy" and "</em>_cp_output.jpg” files from origine images by using windows command.<br>
This is the result.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d.jpeg" data-download-href="/uploads/short-url/vPEVnpF0ctIpkD4pADsSgPXzX8V.jpeg?dl=1" title="029_img.ome_cp_output" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg" alt="029_img.ome_cp_output" data-base62-sha1="vPEVnpF0ctIpkD4pADsSgPXzX8V" width="690" height="172" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1380x344.jpeg 2x" data-dominant-color="A5A7A3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">029_img.ome_cp_output</span><span class="informations">3600×900 239 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And now I want to get more detailed images from “<em>_seg.npy" file.<br>
I mean that I want to get these 4 individual images from the "</em>_seg.npy” file and also to get the correct position of ROIS.<br>
I really want to do this but I am not sure how to do this because I just have some knowledge about Python and image processing.<br>
If possible, please share with me the python source code to get these individual images and correct position of ROIS from the “*_seg.npy” file.</p>
<p>Thank all of you again.</p> ;;;; <p>I am trying to analyze the change of intensity in different time frames. I am selecting an irregular shaped particle by freehand and measuring the intensity. There are 5 pictures of the same sample to measure the change of intensity with time. How can I select the same particle in all 5 images?<br>
If anyone can help me in this regard, that would be great.<br>
Thanks in advance.</p> ;;;; <p>Hi,</p>
<p>How would I overlay these images and find the area of the red parts of the brain in the right image that is covered by the white parts of the brain in the left image.</p>
<p>Thanks<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8.jpeg" data-download-href="/uploads/short-url/gAlWGG7caBuCsUTj4vEbBePbDiE.jpeg?dl=1" title="Screen Shot 2023-03-17 at 1.09.30 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_690x194.jpeg" alt="Screen Shot 2023-03-17 at 1.09.30 PM" data-base62-sha1="gAlWGG7caBuCsUTj4vEbBePbDiE" width="690" height="194" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_690x194.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_1035x291.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_1380x388.jpeg 2x" data-dominant-color="475157"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-17 at 1.09.30 PM</span><span class="informations">1920×542 38.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>This is the version of the ImageJ I am using:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png" data-download-href="/uploads/short-url/wHHVEARNewpXphnJDKGeFnnveO6.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png" alt="image" data-base62-sha1="wHHVEARNewpXphnJDKGeFnnveO6" width="674" height="500" data-dominant-color="4D4A1D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">758×562 103 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I am operating on Windows 10.<br>
The problem is when I open a colorized stack composite in 3D viewer plugin, I go to view, and take snapshot, and nothing is in frame, and a black screen results:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238.jpeg" data-download-href="/uploads/short-url/g6LaNsmsUqFvLDHHVK3g9nw25As.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg" alt="image" data-base62-sha1="g6LaNsmsUqFvLDHHVK3g9nw25As" width="690" height="398" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1035x597.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1380x796.jpeg 2x" data-dominant-color="171714"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1467×847 28.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg" data-download-href="/uploads/short-url/bM3txLoZ9CSkkGWNbCPKzu7ylSh.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg" alt="image" data-base62-sha1="bM3txLoZ9CSkkGWNbCPKzu7ylSh" width="690" height="404" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_1035x606.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg 2x" data-dominant-color="100E0D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1350×791 54.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>It will take a snapshot of the bounding box, but the sample is missing from the snapshot. I tried on a mac laptop, and it worked fine, but I’m wondering if anyone else is having this problem in ImageJ Windows 10? Is there a solution to this?</p> ;;;; <p><a class="mention" href="/u/jbehnsen">@jbehnsen</a> What ImageJ version are you running?</p> ;;;; <p>What version of ImageJ are you using?</p> ;;;; <p>Hi <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a>,</p>
<p>I uninstall everything and i did what you recommended:<br>
cuda 11.8.89<br>
Nvidia drivers 530<br>
tensorflow-gpu==2.10<br>
cuDNN 8.4.1.50</p>
<p>the same error persist!<br>
Thanks for your help!<br>
Julia</p> ;;;; <p>Hi <a class="mention" href="/u/valentin">@Valentin</a> ,<br>
Because the runPlugin command expects a string, you’ll need to it inside a nested loop, where you can deal with string conversion. Also, I recommend not actually testing every single integer in all of those ranges, because as it stands right now, you’re testing 2e18 combinations.</p>
<pre><code class="lang-auto">double pixelSize=getCurrentImageData().getServer().getPixelCalibration().getAveragedPixelSize()

for (t=0; t&lt;226; t++){
    for (p=1; p&lt;20; p++){
        for (min=0; min&lt;10000000; min+=10000){
            for (max=0; max&lt;10000000; max+=10000){
                runPlugin('qupath.imagej.detect.tissue.SimpleTissueDetection2', '{"threshold":'+t+',"requestedPixelSizeMicrons":'+p*pixelSize+',"minAreaMicrons":'+min+',"maxHoleAreaMicrons":'+max+',"darkBackground":false,"smoothImage":true,"medianCleanup":true,"dilateBoundaries":false,"smoothCoordinates":true,"excludeOnBoundary":false,"singleAnnotation":true}')
//MEASUREMENTS GO HERE
            }   
         }
    }  
}
</code></pre>
<p>I’ve also changed the pixel size parameter to simply be a multiple of image pixel size, because I’m pretty sure QuPath uses the pre-defined pyramid levels, not arbitrary downsampling.</p> ;;;; <p>Thank you very much for your follow-up. Unfortunately, I am a beginner and probably incorrectly using the code. This is the error which is appeared.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8.jpeg" data-download-href="/uploads/short-url/xQUy4LarnVW4xvjB8KxNwcoZoEg.jpeg?dl=1" title="Screenshot 2023-03-17 174253" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_690x463.jpeg" alt="Screenshot 2023-03-17 174253" data-base62-sha1="xQUy4LarnVW4xvjB8KxNwcoZoEg" width="690" height="463" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_690x463.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_1035x694.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_1380x926.jpeg 2x" data-dominant-color="D4D4D4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-17 174253</span><span class="informations">1563×1050 243 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I have downloaded Fiji from several sources but unfortunately the logo appears for 1 second and vanishes without giving any error message. I have also installed Java, but still it doesn’t work. I have tried extracting the downloaded files on desktop, but the problem persists. Can anybody please help because I have tried all possible ways to fix it.</p> ;;;; <p>Hello everyone,</p>
<p>excuse my basic questions but I have so far used QuPath only on fluorescent images and just recently been trying to incorporate it also on Light-Microscopy.</p>
<p>My samples are mouse-livers that are stained via PAS-staining. My aim is to quantify the amount of glycogen in various degrees of septic disease on my slides. Visually there is a tremendous difference, but I would like to obtain a quantitative measure for my observation to assess my results with statistics (Avoiding a self-made scoring system of whom I am not a fan). Do you have any ideas on a interesting measure (percentage area, positive cells etc.) and how to approach this image analysis?</p>
<p>Thank you all in advance<br>
Best and have a nice weekend<br>
Alex<br>
(To avoid upcoming questions, there is no difference in staining intensity, both organs were stained on the same glass slide to avoid confusion. The difference is almost exclusively to be attributed to the disease severity)</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca.jpeg" data-download-href="/uploads/short-url/eunvW3UVQGrQRdPOmBCZe1OHzHI.jpeg?dl=1" title="Leber 90p 9.tif - Series 0 (1, x=15116, y=12487, w=3321, h=2421)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_685x500.jpeg" alt="Leber 90p 9.tif - Series 0 (1, x=15116, y=12487, w=3321, h=2421)" data-base62-sha1="eunvW3UVQGrQRdPOmBCZe1OHzHI" width="685" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_685x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_1027x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_1370x1000.jpeg 2x" data-dominant-color="776494"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Leber 90p 9.tif - Series 0 (1, x=15116, y=12487, w=3321, h=2421)</span><span class="informations">1920×1400 434 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33.jpeg" data-download-href="/uploads/short-url/nF6JpOr3SSIn5RCRWRo4CgmZLXR.jpeg?dl=1" title="Leber Base 1.tif - Series 0 (1, x=4292, y=11320, w=6138, h=5039)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_609x500.jpeg" alt="Leber Base 1.tif - Series 0 (1, x=4292, y=11320, w=6138, h=5039)" data-base62-sha1="nF6JpOr3SSIn5RCRWRo4CgmZLXR" width="609" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_609x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_913x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_1218x1000.jpeg 2x" data-dominant-color="452257"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Leber Base 1.tif - Series 0 (1, x=4292, y=11320, w=6138, h=5039)</span><span class="informations">1920×1576 503 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I try the subcellular detection  and it seems to work but can I classify a cell based on the subcellular component area inside ? I resume I want to classify cell in a  class interaction if there are subcelluar components of area &gt; 2 um² (at least one)</p> ;;;; <p>Hi <a class="mention" href="/u/martaurb">@MartaUrb</a>, I retested the fileset from the original post and the values in the lif seemed to be in the correct range, similar to those in your exported TIF.</p>
<p>Do you know which version of Bio-Formats you are currently using? You can find the version number in <code>Help &gt; About Plugins &gt; Bio-Formats Plugins</code></p>
<p>If that looks up to date (6.12.0 is the latest) then would you be able to share a sample file that I can test? If you need a suitable upload location we recommend using <a href="https://zenodo.org/">https://zenodo.org/</a></p> ;;;; <p>Hi Juan,<br>
Thank you for your response!<br>
I can drag-and-drop the .oir files into napari and the AICSImageIO plug-in could open it. But When I tried to import data from the notebook, I get this error message. I confirmed that bioformats_jar is installed.</p>
<pre><code class="lang-auto">from aicsimageio import AICSImage
img = AICSImage("/Users/Desktop/composite_0001.oir")
</code></pre>
<pre><code class="lang-auto">---------------------------------------------------------------------------
UnsupportedFileFormatError                Traceback (most recent call last)
Cell In[2], line 2
      1 # pip install bioformats_jar
----&gt; 2 img = AICSImage("/Users/Desktop/composite_0001.oir")

File /opt/anaconda3/envs/napari-env/lib/python3.9/site-packages/aicsimageio/aics_image.py:264, in AICSImage.__init__(self, image, reader, reconstruct_mosaic, fs_kwargs, **kwargs)
    254 def __init__(
    255     self,
    256     image: types.ImageLike,
   (...)
    260     **kwargs: Any,
    261 ):
    262     if reader is None:
    263         # Determine reader class and create dask delayed array
--&gt; 264         ReaderClass = self.determine_reader(image, fs_kwargs=fs_kwargs, **kwargs)
    265     else:
    266         # Init reader
    267         ReaderClass = reader

File /opt/anaconda3/envs/napari-env/lib/python3.9/site-packages/aicsimageio/aics_image.py:218, in AICSImage.determine_reader(image, fs_kwargs, **kwargs)
    216 if readers[0] in READER_TO_INSTALL:
    217     installer = READER_TO_INSTALL[readers[0]]
--&gt; 218     raise exceptions.UnsupportedFileFormatError(
    219         "AICSImage",
    220         path,
    221         msg_extra=(
    222             f"File extension suggests format: '{format_ext}'. "
    223             f"Install extra format dependency with: "
    224             f"`pip install {installer}`. "
    225             f"See all known format extensions and their "
    226             f"extra install name with "
    227             f"`aicsimageio.formats.FORMAT_IMPLEMENTATIONS`. "
    228             f"If the extra dependency is already installed this "
    229             f"error may have raised because the file is "
    230             f"corrupt or similar issue. For potentially more "
    231             f"information and to help debug, try loading the file "
    232             f"directly with the desired file format reader "
    233             f"instead of with the AICSImage object."
    234         ),
    235     )
    236 else:
    237     raise exceptions.UnsupportedFileFormatError(
    238         "AICSImage",
    239         path,
    240     )

UnsupportedFileFormatError: AICSImage does not support the image: '/Users/Desktop/composite_0001.oir'. File extension suggests format: 'oir'. Install extra format dependency with: `pip install bioformats_jar`. See all known format extensions and their extra install name with `aicsimageio.formats.FORMAT_IMPLEMENTATIONS`. If the extra dependency is already installed this error may have raised because the file is corrupt or similar issue. For potentially more information and to help debug, try loading the file directly with the desired file format reader instead of with the AICSImage object.
</code></pre>
<p>So I followed the instruction and found the bioformats reader. And that did the work.</p>
<pre><code class="lang-auto">img = aicsimageio.readers.bioformats_reader.BioformatsReader("/Users/Desktop/composite_0001.oir")
</code></pre> ;;;; <p>Hi all,<br>
I’m writing a small macro that would create a 3D heatmap from ROIs found in a stack.  At the end of the macro, I have a stack that contains a series of spheres of value=1 for each ROI.  Where the spheres intersect, the values are added.  The stack is visualised using 3D viewer.<br>
My issue is the last line of the macro: I’m trying to apply transparency to the 3D image using <code>call("ij3d.ImageJ3DViewer.setTransparency", "0.75");</code>  as recorded when called manually from the 3D viewer.  Unfortunately, the command is not implemented… What am I doing wrong?</p>
<p>M.</p>
<p>Here is the full macro:</p>
<pre><code class="lang-auto">
//setting minimum required measurements
run("Set Measurements...", "area centroid stack display redirect=None decimal=3");

//Getting file information
Title=getTitle();
getVoxelSize(width, height, depth, unit);
VoxelWidth=width;
VoxelHeight=height;
VoxelDepth=depth;
Voxelunit=unit;
RatioXYZ=VoxelDepth/VoxelWidth; //unless the image is isotropic, Z step is longer than XY, so the sphere is actually skewed like a rugby ball

getDimensions(width, height, channels, slices, frames);

//ask user for sphere radius, which will determine how far in the vicinity one needs to look for neighbours
SphereRadius=getNumber("What is the average radius of the sphere that represents a ROI, in pixels", 20);
SphereRadiusZ=SphereRadius/RatioXYZ;

//collecting ROIs coordinates -currently using regular 2D, will have to adapt to get from 3D suite
NumberOfRois=roiManager("count");
roiManager("Measure");

XRois=newArray();
YRois=newArray();
ZRois=newArray();

for (j = 0; j &lt; NumberOfRois; j++) {
	X=getResult("X", j);
	XRois=Array.concat(XRois,X);
	Y=getResult("Y", j);
	YRois=Array.concat(YRois,Y);
	Z=getResult("Slice", j);
	ZRois=Array.concat(ZRois,Z);
}

roiManager("reset");

setBatchMode(true);

/* Creating the spheres for 3D heat map, one sphere per ROI
 *  Each sphere is created as a stack of discs with increasing and decreasing radii.
 * Each sphere is created in a new stack and stacks are added one by one to the first stack
 * Each sphere is given the value of 1.  As they are added to the stack, regions of overlapping spheres have increased values
*/
for (i = 0; i &lt; NumberOfRois; i++) {
	newImage("Sphere_"+i, "8-bit color-mode", width, height, 1, slices, 1);
	topSlice=round(ZRois[i]-SphereRadiusZ+1);
	bottomSlice=round(ZRois[i]+SphereRadiusZ-1);
	NumberOfSlices=round(2*SphereRadiusZ);
	SphereSlice=1;
	selectWindow("Sphere_"+i);
	
	for (sl= topSlice; sl &lt; bottomSlice; sl++) { //i.e. from top of sphere to bottom
		
		H=sl-ZRois[i];  //this is the distance between the centre of the sphere and the centre of the disc being added to the stack
		
		//the following commands is necessary if a sphere is created by the edge of the stack, with coordinates outwith, which leads to error
		if(sl&lt;1){  //if Z coordinate is above the stack, then Z is automatically at the first slice
			Z=1;
		}
		else{
			if(sl&gt;slices){ //if Z coordinate is below the stack, then Z is automatically at the last slice.
				Z=slices;
			}
			else{
				Z=sl;
			}
		}
		Stack.setSlice(Z);
		CurrentRadius=sqrt(pow(SphereRadius,2)-pow(H,2)); //using Pythagoras to calculate the radius of each disc
		
		makeOval(XRois[i]-CurrentRadius, YRois[i]-CurrentRadius, CurrentRadius*2, CurrentRadius*2);
		roiManager("add");
		run("Set...", "value=1 slice");

		SphereSlice=SphereSlice+1;

		//roiManager("reset");
	}
	roiManager("reset");
	//adding the newly formed stack to the first stack
	if(i&gt;0){
	imageCalculator("Add stack", "Sphere_0","Sphere_"+i);
	close("Sphere_"+i);
	}
}	

//visualising result in 3D viewer

selectWindow("Sphere_0");
setBatchMode(false);
rename("3D_HeatMap");
run("glasbey_on_dark");
run("3D Viewer");
call("ij3d.ImageJ3DViewer.setCoordinateSystem", "false");
call("ij3d.ImageJ3DViewer.add", "3D_HeatMap", "None", "3D_HeatMap", "0", "true", "true", "true", "2", "0");
call("ij3d.ImageJ3DViewer.setTransparency", "0.75");

</code></pre>
<p>…and here is a simple macro to create a volume with random ROIs:</p>
<pre><code class="lang-auto">newImage("HyperStack", "16-bit grayscale-mode", 500, 500, 1, 500, 1);
getDimensions(width, height, channels, slices, frames);
for (i = 0; i &lt; 200; i++) {
	x=random*width;
	y=random*height;
	z=round(random*slices);
	setSlice(z);
	makePoint(x, y);
	roiManager("add");

}

</code></pre> ;;;; <p>Hello,<br>
I am acquiring images from Olimpus VS200 scanner. When I opened the images on imageJ I have the information in pixels and  I do not know how to calibrate the image to add a scale bar.</p>
<p>So, I started to use Qupath which shows me the metadata and I have the pixel width and pixel height<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ffdf58135fb4c22f484950ad57ba36c7497a746d.png" alt="image" data-base62-sha1="AvygzN2pmEmjRfgZ1UOIzXY2Qzj" width="378" height="123"></p>
<p>Unfortunately, Qupath cannot add a scale bar or change colors or dimensions.</p>
<p>Can someone help me to figure out how to add a scale bar from Qupath or in ImageJ.</p>
<p>I have a Mac and I cannot use the viewer from Olympus.</p>
<p>Thank you<br>
Pablo</p> ;;;; <p>Dear Qupath specialist,</p>
<p>I try to find a way to quantify cells interactions in Qupath.<br>
My first cell type are well detected with cells detection  or cellpose. The second type of cells are difficult/impossible  to segment so I just identify the labelling above a threshold with create thresholder that create a annoation (phagocyte). I would like to quantify the region overlap in my cytoplasm (area percentage or area size in um²) of each cell and add a measure for this. Any help will be great ! Thanks</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2.jpeg" data-download-href="/uploads/short-url/upRxrLGmwHu1u5x3EcbZrHHZxqW.jpeg?dl=1" title="image1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_690x388.jpeg" alt="image1" data-base62-sha1="upRxrLGmwHu1u5x3EcbZrHHZxqW" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_1380x776.jpeg 2x" data-dominant-color="685964"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image1</span><span class="informations">1914×1077 240 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi everyone, I am very new to CellProfiler but I have read a couple of recent papers using this software to calculate nuclear envelope invaginations in cells. I would like to do something similar, but am not sure where to start. I have attached the images I want to use this technique for. I have stained for DAPI, Lamin B1, and Actin. A lot of the papers specified they used the amount of Lamin B1 fluorescence in the nucleus to calculate this occurrence. However, I know I am asking a lot with little insight so I do understand if no one is willing to help.<br>
Thank you in advanced. <img src="https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12" title=":slightly_smiling_face:" class="emoji" alt=":slightly_smiling_face:" loading="lazy" width="20" height="20"></p>
<p><a class="attachment" href="/uploads/short-url/tIUmm4aZIFXYdOvyJotMnxYosGr.pdf">Screen Shot 2023-03-17 at 10.57.23 AM.pdf</a> (3.2 MB)</p> ;;;; <p>Hi Egor,<br>
I can’t find any Java analog with your Groovy code.<br>
Can you help me please?</p>
<p>Kind regards.</p> ;;;; <p>It sounds like you’re having some problem with a file pointer to the acquired data remaining open. Hard to say why this is without more information and/or the ability to reproduce. It could be related to the underlying hard drive somehow. It’s quite possible that no real problems will result from this.</p>
<p>You could try doing as suggested and put at the top of your code:</p>
<pre><code class="lang-auto">import tracemalloc
tracemalloc.start()
</code></pre> ;;;; <aside class="quote group-team" data-username="s.besson" data-post="19" data-topic="78482">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/s.besson/40/8046_2.png" class="avatar"> Sébastien Besson:</div>
<blockquote>
<p><code>-options=zeissczi.attachments=false</code></p>
</blockquote>
</aside>
<p>I should have know this … - sorry.</p>
<p>When using this option I can at least open the OME-ZARR in napari:</p>
<pre><code>napari "c:\Temp\zarr\w96_A1+A2_test_no_attachment.zarr" --plugin napari-ome-zarr
</code></pre>
<p>It open but the console is full of errors which look like the plugins is confused because " of missing wells"? This CZI has 2 wells (A1 + A2) but is was created by creating a subset from a 96 well. But the NGFF converter, BioFormats etc. all correctly recognize that there are only two wells. See also the OME.XML metadata inside the ZARR folder.</p>
<p>I am wondering why the plugin is trying to find wells with are not there and why it assumes they should be?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/b/3b3b58642716f2cd64a41031174a789945547ffb.png" data-download-href="/uploads/short-url/8rZjGc8zqfi5jGyMRljrhBP2l1F.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/b/3b3b58642716f2cd64a41031174a789945547ffb_2_675x500.png" alt="image" data-base62-sha1="8rZjGc8zqfi5jGyMRljrhBP2l1F" width="675" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/b/3b3b58642716f2cd64a41031174a789945547ffb_2_675x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/b/3b3b58642716f2cd64a41031174a789945547ffb_2_1012x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/b/3b3b58642716f2cd64a41031174a789945547ffb_2_1350x1000.png 2x" data-dominant-color="27292C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1909×1413 184 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I copies the error into a textfile to keep this post readable.</p>
<p><a class="attachment" href="/uploads/short-url/xVy4CgnssmCAUSsjbPj4j9GtD5K.txt">errors_napari.txt</a> (283.1 KB)</p>
<p>And besides this the NGFF converter still created files, were the plugin fails with</p>
<pre><code>ValueError: There is no registered plugin named 'napari-ome-zarr'.
Names of plugins offering readers are: {'bfio', 'ome-types'}
</code></pre>
<p>Download ZARR with two wells here: <a href="https://www.dropbox.com/s/hn3yzj101pj7i45/w96_A1%2BA2_test_no_attachment.zarr.zip?dl=0" class="inline-onebox" rel="noopener nofollow ugc">Dropbox - w96_A1+A2_test_no_attachment.zarr.zip - Simplify your life</a></p>
<p>Download ZARR (no wells) here:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.dropbox.com/s/2yrke4ktj3mv80q/DTScan_ID4.zarr.zip?dl=0">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f499463b34f261b47d577b42b412e4c639e0c212.png" class="site-icon" width="32" height="32">

      <a href="https://www.dropbox.com/s/2yrke4ktj3mv80q/DTScan_ID4.zarr.zip?dl=0" target="_blank" rel="noopener nofollow ugc">Dropbox</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f1f990cbac6b5571218c9046e9dead200fe6f1b.png" class="thumbnail onebox-avatar" width="160" height="160">

<h3><a href="https://www.dropbox.com/s/2yrke4ktj3mv80q/DTScan_ID4.zarr.zip?dl=0" target="_blank" rel="noopener nofollow ugc">DTScan_ID4.zarr.zip</a></h3>

  <p>Shared with Dropbox</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hello.<br>
I’m using DeepLabCut for the first time and everytime I try to creat a new project I receive the following error:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec.png" data-download-href="/uploads/short-url/9shjHyFWj7s5irX9MVhwkj9Pqc4.png?dl=1" title="errodeeplabcut" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_690x370.png" alt="errodeeplabcut" data-base62-sha1="9shjHyFWj7s5irX9MVhwkj9Pqc4" width="690" height="370" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_690x370.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_1035x555.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_1380x740.png 2x" data-dominant-color="28323C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">errodeeplabcut</span><span class="informations">1920×1032 51.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thank you for your time.</p> ;;;; <p>That worked to fix the file not found error. Now the script runs through entirely, but only doing the function of the first macro (converting .lif to .tiff) without actually giving any of the prompts to adjust the .tiff files to do the job of the second macro.</p>
<p>When I uncomment the</p>
<pre><code class="lang-auto">  //processTiff(output, output, imageList[i]);
</code></pre>
<p>line, I get this error<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/db5277d59146c8aa8cc41e0f2bd83ccace551407.png" alt="image" data-base62-sha1="vidfH0kCnAJfos7PPXEH9MdqYOX" width="518" height="430"></p>
<p>After successfully getting through the prompts for selecting the input/output directories, and after the log shows processing the first image file</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be.png" data-download-href="/uploads/short-url/aNOabuSHXybglUrqlQSMa0AWdn8.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_690x194.png" alt="image" data-base62-sha1="aNOabuSHXybglUrqlQSMa0AWdn8" width="690" height="194" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_690x194.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_1035x291.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_1380x388.png 2x" data-dominant-color="C7DADB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3224×910 113 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Edit: since this is a distinct issue with just this new script, I created a <a href="https://forum.image.sc/t/undefined-identifier-in-line-61-called-from-line/78781">new topic</a></p> ;;;; <p>Hey,</p>
<p>Like Leo said, these models were trained on different data.<br>
Here is a figure to explain them, in a paper: <a href="https://www.nature.com/articles/s41592-022-01663-4" class="inline-onebox" rel="noopener nofollow ugc">Cellpose 2.0: how to train your own model | Nature Methods</a><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8.jpeg" data-download-href="/uploads/short-url/u0R9Xgsm23jLDChQ2DdYMloKYY0.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_690x451.jpeg" alt="image" data-base62-sha1="u0R9Xgsm23jLDChQ2DdYMloKYY0" width="690" height="451" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_690x451.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_1035x676.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_1380x902.jpeg 2x" data-dominant-color="CCCDCD"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1751×1146 228 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Best<br>
Yuan</p> ;;;; <p>Here is a macro that changes the color of the line selection on the image to red, adds it to an overlay, gets the x-y points from the table, converts the points from scaled to unscaled and creates a new selection from them.</p>
<pre><code class="lang-auto">Roi.setStrokeColor("red");
run("Add Selection...");
xpoints = Table.getColumn("x_points");
ypoints = Table.getColumn("y_points"); 
toUnscaled(xpoints, ypoints);
makeSelection("freeline", xpoints, ypoints);
</code></pre>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/d/dda600b889d8aff051dab709e72ef6887e549280.png" alt="Screenshot" data-base62-sha1="vCNbbjhKgmu1QN2kgaEhfhhlafK" width="687" height="409"></p> ;;;; <p>Hello <a class="mention" href="/u/ujjwal">@ujjwal</a>,</p>
<p>first of all, welcome to the image.sc forum <img src="https://emoji.discourse-cdn.com/twitter/tada.png?v=12" title=":tada:" class="emoji" alt=":tada:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12" title=":star_struck:" class="emoji" alt=":star_struck:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"></p>
<p>second, thank you for taking the time to look at all the previous information that there is. I know it’s not always straight forward to find! This is some very proficient usage of ilastik!</p>
<p>Before I go on answering the questions, I have a few as well:</p>
<ul>
<li>just to clarify: when I read through your post it seems that you converted your input data to h5, is this correct? (all input data) Did you use our fiji plugin?</li>
<li>to what format are you exporting?</li>
<li>what kind of machine are you using for your processing (how many cores, how much ram?)</li>
</ul>
<p>Now to your questions?</p>
<aside class="quote no-group" data-username="ujjwal" data-post="1" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>I have to train a new workflow for each type of sample(considerable differences in structure), this is fine, however It seems that the project requires accompanying .h5 files to function, this balloons already large amount of data. But looking at the size of the .ilp file it looks like it already includes the raw data? It’s a bit redundant having to save the same data twice. <em>Is it possible to use the trained model without having the raw files with it?</em></p>
</blockquote>
</aside>
<p>What version of ilastik are you using. When the classifier is saved to the project file, the raw data should not be needed anymore in headless processing (which I assume you’re doing). ilastik would only try to access it to train the classifer. I verified with ilastik <code>1.4.0</code> that this works in principle.</p>
<aside class="quote no-group" data-username="ujjwal" data-post="1" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>There are 4 phases that I have to separate, reading through the forum it is recommended to export the probabilities and then threshold(using a value of 0.5) these to obtain the phases. I some scans where a specific region could correspond to one of two classes and they might have very similar probabilities, My question is what happens in regions where no class has the value 0.5. <em>Is it not possible to segment that region? Also could the probability export be in 8-bit?</em><br>
eg:( probabilities of the two phases, zoomed-in)</p>
</blockquote>
</aside>
<p>The probabilities can safely be exported to 8-bits (in that case you have to renormalize from <code>0..1</code> to <code>0..255</code>.<br>
For the thresholds - of course you could use lower values, or just the largest value (if documented properly). You are right of course, that this means that some regions might not get a segmentation. But I would think about it differently - by enforcing at least 0.5 (or any value you see fit) you will get some pixels that are not part of any class - and you will know which ones those are - isn’t that good for any downstream analysis, not to include those (after all they might be too different)?</p>
<aside class="quote no-group" data-username="ujjwal" data-post="1" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>Regarding the auto-context workflow, generally I’ve seen that I need to annotate the 4 classes in the 1st stage and then the foreground and the background in the 2nd stage. <em>But how do I get the 4 classes from the export of 2nd stage then?</em> The way I’m doing it right now is in reverse, where separating the foreground in step 1 which works great and then annotating the classes in stage 2. <em>Am I approaching this wrong?</em></p>
</blockquote>
</aside>
<p>The advice here is empirical: In general, what we suggest is to annotate everything that is in the image in the first stage (so this is usually many classes) and then only what you are interested in in the second stage. So if you are interested in 4 classes, then you should have 4 classes in the second stage. For the first stage you’d probably also want to have at least 4 classes. Compared to your current setup this means having twice as many computations overall, so it would slow you down even more. (And it would also require more RAM). But probably deliver the highest fidelity. I don’t really dare to ask, but you’ve tried “just” pixel classification, and it was not good enough, yes?</p>
<aside class="quote no-group" data-username="ujjwal" data-post="1" data-topic="78696">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png" class="avatar"> Ujjwal:</div>
<blockquote>
<p>*And finally regarding the <em>OOB error, what value is acceptable enough?</em> I know that it need to be as low as possible but what is an objective difference between 0.0001 and 0.001 when visually I cannot tell much difference between the two but the computational time is long. Some minor differences I do notice is for very few pixels and those are not really relevant in the context of the whole volume. Another issue is the multiple workflows for each volume, so might be good to know the maximum acceptable value to aim for each.</p>
</blockquote>
</aside>
<p>Gold standard to assess the quality of your segmentation would be to annotate at least a part of the volume densely (all pixels) and then compare that to the final segmentation result (after thresholding and all).<br>
The OOB is only an indication - data points are never tested against the full classifier, only against those trees in the forest that didn’t include them as training data. A value <code>0.001</code> would mean that only 1 in 1000 pixels (from the training data) gets classified incorrectly in this setup. <code>0.0001</code> would even be 1 in 10000. For me this would also be extremely hard to see. Both these values are extremely low. But keep in mind that this <em>only</em> includes the annotated training data.<br>
You seem have looked at the data and the result (the <em>most</em> important thing here) and found both equally good, which is the important (albeit less objective) measure. And again, to get a “real” indicator, you’d have to annotate some volume densely.</p>
<p>I hope I could clear up <em>some</em> of your concerns. Happy to go into any of the topics in more detail in case you have further questions!</p>
<p>Cheers<br>
Dominik</p> ;;;; <p>Chatgpt to the rescue:</p>
<blockquote>
<p>In Java and using org.apache.commons and commons-csv, could you write a function which takes a csv file with 3 columns, and rename the first column as ‘y’, the second as ‘x’, the third as ‘z’. It should also add a fourth column filled with 0, named ‘t’ ?</p>
</blockquote>
<pre><code class="lang-auto">import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;

import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVParser;
import org.apache.commons.csv.CSVPrinter;
import org.apache.commons.csv.CSVRecord;

public class CSVColumnRenamer {

    public static void renameColumns(String inputFilePath, String outputFilePath) throws IOException {
        // Define the CSV format and open the input file for parsing
        CSVFormat format = CSVFormat.DEFAULT.withHeader();
        FileReader reader = new FileReader(inputFilePath);
        CSVParser parser = new CSVParser(reader, format);

        // Define the output CSV format and open the output file for writing
        CSVFormat outputFormat = CSVFormat.DEFAULT.withHeader("y", "x", "z", "t");
        FileWriter writer = new FileWriter(outputFilePath);
        CSVPrinter printer = new CSVPrinter(writer, outputFormat);

        // Iterate over each record in the input file
        for (CSVRecord record : parser) {
            // Get the values of the first three columns
            String y = record.get(0);
            String x = record.get(1);
            String z = record.get(2);

            // Write a new record with the renamed columns and a new 't' column with value 0
            printer.printRecord(y, x, z, "0");
        }

        // Close the input and output files
        parser.close();
        printer.close();
    }

}
</code></pre> ;;;; <p>Unfortunately I can’t share the image, but basically:</p>
<ul>
<li>If I export in the .tiff format, ImageJ sees the different channels but they are all superimposed even when you switch between them (you always see the original images and the annotations)</li>
<li>If I export in the .ome.tif format, then ImageJ is able to split the images contained in each channel, but the original image is in black and white.</li>
</ul>
<p>That said, I am not an ImageJ user, so I might just be missing something!</p> ;;;; <p>It looks nice, unfortunately I do not know how to use it for my use case: I want to import the spots only and use another tracker. But:</p>
<pre><code class="lang-auto">        TrackMateImporter.builder()
                .imp(imp)
                .csvFilePath(spotsFile.getAbsolutePath())
                .xCol(2)
                .yCol(1)
                .frameCol(3)
                .radius(default_radius)
                .logger(Logger.IJ_LOGGER)
                .declareAllFeatures(true)
                .create().getModel(,,)
</code></pre>
<p>returns a <code>Model</code>, and I want a <code>Settings</code> in order to set my tracker.</p>
<p>and</p>
<pre><code class="lang-auto">        TrackMateImporter.builder()
                .imp(imp)
                .csvFilePath(spotsFile.getAbsolutePath())
                .xCol(2)
                .yCol(1)
                .frameCol(3)
                .radius(default_radius)
                .logger(Logger.IJ_LOGGER)
                .declareAllFeatures(true)
                .create().getSettings()
</code></pre>
<p>returns a settings where <code>settings.detectorFactory = new ManualDetectorFactory&lt;&gt;();</code> so basically no spot is detected during the tracking.</p>
<p>I know I’m not very clear, but I think I’ll find a way by copying parts of the <code>getModel</code> method from the builder.</p> ;;;; <aside class="onebox githubblob" data-onebox-src="https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py">
  <header class="source">

      <a href="https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py" target="_blank" rel="noopener nofollow ugc">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py" target="_blank" rel="noopener nofollow ugc">DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py</a></h4>


      <pre><code class="lang-py">#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. &amp; M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import warnings


class PoseNetFactory:
    _nets = dict()

    @classmethod
    def register(cls, type_):
        def wrapper(net):
            if type_ in cls._nets:
</code></pre>



  This file has been truncated. <a href="https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py" target="_blank" rel="noopener nofollow ugc">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>losses are an output of the <code>create</code> function of the <code>PoseNetFactory</code> class.</p>
<p>Usage is here: <a href="https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/core/train.py#L173" class="inline-onebox" rel="noopener nofollow ugc">DeepLabCut/train.py at 6c429271b338cdd010348f640b30f62cfc986edc · DeepLabCut/DeepLabCut · GitHub</a></p> ;;;; <p>Hello <a class="mention" href="/u/zoeyzhengg">@Zoeyzhengg</a>,</p>
<p>This is probably a bug in ilastik related to the special characters you have in your filename. Specifically all the square brackets <code>Deconvolved|TsfDAPI377,447]+Tsf{GFP 469,525]+Tsf[ TexasRed 586,647]]_82_1_001.png</code>. If you replaces them by something else, then it will work.</p>
<p>So this really is a problem on the ilastik side, sorry you ran into this.</p>
<p>Cheers<br>
Dominik</p>
<p>edit: corresponding issue in our issuetracker: <a href="https://github.com/ilastik/ilastik/issues/2391" class="inline-onebox">File names with square brackets cannot be opened · Issue #2391 · ilastik/ilastik · GitHub</a></p> ;;;; <p>Hi Oliver,</p>
<p>Now I am trying to use MitoSegNet with their pretrained model with 8 bit images of my mitochondria, and found your message about success with MitoSegNet.</p>
<p>I run prediction in the Basic mode with Post-segmentation filtering, but after prediction had been done, two windows opened - one with original raw image and another one - blank white image with no prediction. Have you ever had this issue and what can cause it?</p>
<p>I would be very grateful for any help, there are few publicly available examples of this software usage.</p> ;;;; <p>Since live multianimal isn’t fully ready yet and it is possible this might be a bug, consider making an issue of this with steps to reproduce on the DLC-live github</p> ;;;; <p>Copy of the answer I gave on gitter:<br>
Seems like a PyQT issue. But maybe let’s start with the basics. Are you turning on the terminal as administrator? What happens if you run without going into ipython with <code>python -m deeplabcut</code> ?</p> ;;;; <p>Generally I wouldn’t recommend labeling invisible bodyparts, this can skew the model into frequent prediction errors. The model has to try to label all the bodyparts but if they aren’t really there the detections should have very low likelihood and can be easily filtered out. For a more relative approach to determining bodypart position you can use multianimal DLC on one animal. The PAF graphs should be helpful here.</p> ;;;; <p>Oh thanks <a class="mention" href="/u/will-moore">@will-moore</a>, that’s a pretty neat solution.</p> ;;;; <p>You should save just the point data layer</p> ;;;; <p>Hello <a class="mention" href="/u/zoeyzhengg">@Zoeyzhengg</a>,</p>
<p>first of all, welcome to the image.sc community <img src="https://emoji.discourse-cdn.com/twitter/handshake.png?v=12" title=":handshake:" class="emoji" alt=":handshake:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/tada.png?v=12" title=":tada:" class="emoji" alt=":tada:" loading="lazy" width="20" height="20"></p>
<p>Since you want to reason about cells as entities, the default route here would be (more or less what we have in our <a href="https://youtu.be/F6KbJ487iiU">i2k  tutorial</a>:</p>
<p>1.) Pixel Classification to segment cell-pixels vs background.<br>
2.) Object classification with the probability maps exported from 1) to classify cells into the classes <em>red</em>, <em>red+green</em> and probably a third class where you’d put <em>incomplete, or bad segmentations</em>. In the end you’d get a classification table that you can then use (with Excel, Python, R,…) to get the ratio you’re looking for.</p>
<p>Things to know that might help you on the way:</p>
<ul>
<li>ilastik will in general work on all channels you give it - there is no way to subselect only a single channel <em>for computation</em>.</li>
<li>there are different ways of <em>viewing</em> multi-channel images:
<ul>
<li>per default (if you have 2-3 channels) they will render in color (with first channel being red, second green, third blue). Sometimes it is then difficult to see all the additional colors you get in ilastik (at least for me). In situations like this I often choose to <em>look</em> at the channels in grayscale (one channel at a time - again, this will not change anything for the computation, where always <em>all</em> channels are included). See <a href="https://www.ilastik.org/documentation/basics/dataselection#properties">dataset properties - display mode/channel display</a>. When switching to <em>grayscale</em> you can navigate between channels in the layerstack (bottom right) like this:</li>
</ul>
</li>
</ul>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bc912daf8631e256489a2278df593139a26ba1ef.gif" alt="switch-channels" data-base62-sha1="qU8O5pwRgxkBxemoZkcConfyZNt" width="690" height="485" class="animated"></p>
<p>Hope this helps! Please update this thread in case you have questions related to the same analysis <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Cheers<br>
Dominik</p> ;;;; <p>Hi,</p>
<p>There’s a URL that you can use to download original files from the webclient, based on ID:</p>
<p><code>webclient/download_original_file/ID/</code>.</p>
<p>If your script knows the URL where your webclient is deployed, you could generate that link and include that in the script response “message”.</p>
<pre><code class="lang-auto">from omero.rtypes import wrap

...
        orig_file_id = 123
        webclient = "https://my-server/webclient/"
        url = f"{webclient}download_original_file/{orig_file_id}/"
        client.setOutput("URL", wrap({"type": "URL", "href": url}))
        client.setOutput("Message", wrap("Click the button to download"))
</code></pre>
<p>This will generate a button to open the URL (and download the file) in a new window:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/c/fc3ef3fce60f815965f5e9299e82e4b83d69147d.png" alt="Screenshot 2023-03-17 at 12.32.30" data-base62-sha1="zZtckzrR6BQKMsuZK0Tgl7k0t1H" width="406" height="102"></p> ;;;; <p>Hello</p>
<p>I don’t know what you use, but programmatically it is nice to use the <code>Builder</code> in the <code>TrackMate Importer</code></p><aside class="onebox githubblob" data-onebox-src="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/src/main/java/fiji/plugin/trackmate/importer/csv/TrackMateImporter.java">
  <header class="source">

      <a href="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/src/main/java/fiji/plugin/trackmate/importer/csv/TrackMateImporter.java" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/src/main/java/fiji/plugin/trackmate/importer/csv/TrackMateImporter.java" target="_blank" rel="noopener">trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/src/main/java/fiji/plugin/trackmate/importer/csv/TrackMateImporter.java</a></h4>


      <pre><code class="lang-java">/*-
 * #%L
 * TrackMate: your buddy for everyday tracking.
 * %%
 * Copyright (C) 2017 - 2023 TrackMate developers.
 * %%
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as
 * published by the Free Software Foundation, either version 3 of the
 * License, or (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public
 * License along with this program.  If not, see
 * &lt;http://www.gnu.org/licenses/gpl-3.0.html&gt;.
 * #L%
</code></pre>



  This file has been truncated. <a href="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/src/main/java/fiji/plugin/trackmate/importer/csv/TrackMateImporter.java" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Look how the script uses it:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/scripts/CsvToTrackMate.py">
  <header class="source">

      <a href="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/scripts/CsvToTrackMate.py" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/scripts/CsvToTrackMate.py" target="_blank" rel="noopener">trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/scripts/CsvToTrackMate.py</a></h4>


      <pre><code class="lang-py">"""
A Jython script that parse arguments and uses them to configure a CSV to TrackMate 
importer. 

This script must be called from Fiji (to have everything on the class path), for instance
in headless mode. Here is an example of a call from the command line:

./ImageJ-macosx --headless   ../../../TrackMate-CSVImporter/scripts/CsvToTrackMate.py 
	--csvFilePath="../../../TrackMate-CSVImporter/samples/data.csv" 
	--imageFilePath="../../../TrackMate-CSVImporter/samples/171004-4mins-tracking.tif"
	 --xCol=1 
	 --radius=2 
	 --yCol=2 
	 --zCol=3 
	 --frameCol=0
	 --targetFilePath="../../../TrackMate-CSVImporter/samples/data.xml"
"""

from fiji.plugin.trackmate.importer.csv import TrackMateImporter
from java.io import File
</code></pre>



  This file has been truncated. <a href="https://github.com/trackmate-sc/TrackMate-CSVImporter/blob/5c8378d6b569750d8d4812c17cc6b0504bbc651a/scripts/CsvToTrackMate.py" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>I circumvent it by attaching it to a project now and returning the file annotation, that works fine.</p>
<p>But that doesn’t really answer my original question, and it requires the user providing an extra input (the project) giving more clutter to the screen =)</p>
<p>Regards, Torec</p> ;;;; <p>Searching for <code>libjpegturbo.turbojpeg.TJDecompressor</code> and bioformats found this issue: <a href="https://github.com/ome/bioformats/issues/3756" class="inline-onebox">Libjpeg-turbo exception when converting/opening .ndpi file. · Issue #3756 · ome/bioformats · GitHub</a> which looks like what you’re seeing.</p> ;;;; <p>Thank you Nick for raising the issue and providing sample data and investigation. I can confirm that this does look to be a bug in the Bio-Formats ND2Reader, unfortunately we do not an immediate fix for the problem. I have opened a Bio-Formats GitHub Issue for tracking this problem: <a href="https://github.com/ome/bioformats/issues/3964" class="inline-onebox">ND2: 8 bit RGB channels being read as single 8 bit channels · Issue #3964 · ome/bioformats · GitHub</a></p> ;;;; <p>The groovy syntax is largely the same but there will be some differences in the script. For the imports in groovy it should look like:</p>
<pre><code class="lang-auto">import loci.plugins.BF
import loci.common.DebugTools
import loci.formats.ImageReader
</code></pre>
<p>You can see an example of a Groovy script using Bio-Formats <a href="https://github.com/dgault/bio-formats-examples/blob/6cdb11e8c64566611b18f384b3a257dab5037e90/src/main/macros/groovy/OverlappedTiledPyramidConversion.groovy">here</a>. It isn’t using the DebugTools but it might help with the syntax.</p> ;;;; <p>I think it should then work if you change the open command in processTiff to the below:<br>
<code>open(input+file+".tiff");</code></p>
<p>The use of input as a parameter in processTiff can be updated in the future if you wanted. Currently when the function is called (as below) it is using the output folder from the initial conversion for both input and  output, so everything is just using the same directory for now.</p>
<p><code>processTiff(output, output, imageList[i]);</code></p> ;;;; <p>If anyone can advise how to get GPU working for stardist and cellpose it would be much appreciated.<br>
I cannot work it out</p> ;;;; <p>Well, it kind of is the full output. For our original script the full output lines are:</p>
<p><em>1. start of acq<br>
2. end of acq<br>
3. Acquisition finished!<br>
4. D:\WorkStuff\labA3Marc\marcPy\softwareHelpers\uManagerInt.py:98: ResourceWarning: unclosed file &lt;_io.BufferedReader name=‘C:\temp\scan_30\scan_NDTiffStack.tif’&gt;<br>
5. uManagerMDA(prog)<br>
6. ResourceWarning: Enable tracemalloc to get the object allocation traceback</em></p>
<p>Unfortunately, I could not reproduce the issue in a minimal example. It only appears in our original script. However the different output lines give a hint about the origin.<br>
<em>start of acq</em> and <em>end of acq</em> is around the acquisition statement (see origninal script below). <em>Acquisition finished!</em> is printed out when the thread, where the acquisition runs in, stops (I tired it without the thread and the warning still apears). Any idea what causes this or how I can approach the issue?</p>
<p>original script:</p>
<pre><code># some stuff before this
print("start of acq")
if mode == 'uManager' or notZstack():
    with Acquisition(directory=save_directory, name=save_name, pre_hardware_hook_fn=preHardwareFn,
                     show_display=False, saving_queue_size=5000) as acq:
        acq.acquire(events)
elif mode == 'uManagerAO':
    setAOTask(ni.setUpSingleChAO("Dev1/ao0"))  # initialise task (AO output)
    with Acquisition(directory=save_directory, name=save_name, pre_hardware_hook_fn=preHardwareFn,
                     post_camera_hook_fn=cameraHookFn, show_display=False, saving_queue_size=5000) as acq:
        acq.acquire(events)               # do acquisition
    ni.setSingleValueChAO(getAOTask(), 0)
    ni.closeTask(taskAO)
else:
    print("no such uManager moder registered")
print("end of acq")
# some stuff after this
</code></pre> ;;;; <p>I’m using the latest version of DLC with version 4.0.7 of Python on a Windows 10</p>
<p>I’m able to create a new project, extract frames, configure the yaml file, and label the frames, yet when I do Save All Layers, I get a series of error messages and endless lines of addresses. Afterward, it says it *** END STACK TRACE POINTER ***</p>
<p>Did I download something wrong? What can I do to fix this problem?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f.png" data-download-href="/uploads/short-url/p3q6T22xrEphqM2rXM3cXsBfGsn.png?dl=1" title="Screenshot 2023-03-17 040327" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_690x365.png" alt="Screenshot 2023-03-17 040327" data-base62-sha1="p3q6T22xrEphqM2rXM3cXsBfGsn" width="690" height="365" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_690x365.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_1035x547.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_1380x730.png 2x" data-dominant-color="1B1B1B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-17 040327</span><span class="informations">1920×1018 72.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>This area of the root is more transparent than the rest of the root. It is therefore normal to see a difference.</p>
<p>I tried to increase and improve the focus but it did not change anything since the transparency of the root is a reality.</p>
<p>No we don’t have this type of microscope.</p>
<p>It’s frustrating since only the root tip is good in segmentation…</p>
<p>Thanks</p> ;;;; <p>What about improving the focus?<br>
Did you tried dark field microscopy?</p> ;;;; <p>I have posted the .csv file and also the original image. Thank you very much for your help.<br>
<a class="attachment" href="/uploads/short-url/iapedURsvfQZM1zhwLnbaSyKnhH.csv">fitted_points15.csv</a> (47.7 KB)<br>
<a class="attachment" href="/uploads/short-url/AajR9p6SnqkPJAy5VwfBHF5qyR0.tif">final1.tif</a> (8.7 MB)</p> ;;;; <p>Hello everyone.</p>
<p>I’m asking for your help because I can’t make a root segmentation.</p>
<p>Indeed, the intensity of the pixels at the end of my root is similar to the intensity of pixels in the background of the image. (Cf Photo) Conclusion, the segmentation doesn’t work well …<br>
<a class="attachment" href="/uploads/short-url/vRbdVGkZDE1X3L1li9HyMOfcEqo.tif">DSC_215546-1.tif</a> (2.7 MB)</p>
<p>Do you have any pre-processing or methods to advise me so that I can finally have my root in full despite this difference in pixel intensity?</p>
<p>Thank you in advance,</p>
<p>Bastien</p> ;;;; <p>Hey, sorry about this. For some reason that part of the plugin has a mouse brain image hardcoded. I’ve raised an issue to track the fix <a href="https://github.com/brainglobe/brainreg-napari/issues/53">here</a>.</p>
<p>Thanks,<br>
Adam</p> ;;;; <p>Hello everyone!<br>
I’m new to QuPath so not sure if my problem is relevant at all. I’m trying to run a sensitivity analysis on SimpleTissueDetection2 parameters. I would need to make each value of the parameters to vary within a specific range and export the measurement for each combination. The goal is to analyses how parameters influence tissue detection outcome for a set of images and to define a range of reliability for each parameters for a specific dataset in a consistent way.</p>
<h3>
<a name="challenges-1" class="anchor" href="#challenges-1"></a>Challenges</h3>
<ul>
<li>
<p>What stops you from proceeding?<br>
I think I would need to use a for loop somewhere inside or outside the plugin but I’m not sure if the plugin can admit something different than a number as parameter.</p>
</li>
<li>
<p>What have you tried already?<br>
I have tried to define a function getNumbersInRange() to create a list of integer within a specific range and use this function for each quantitative parameter of SimpleTissueDetection2.</p>
</li>
<li>
<p>Have you found any related forum topics? If so, cross-link them.<br>
I haven’t found any related topic for QuPath and/or SimpleTissueDetection2 plugins specifically.</p>
</li>
<li>
<p>What software packages and/or plugins have you tried?<br>
I’m working with QuPath-0.4.3 and SimpleTissueDetection2 (<a href="https://qupath.github.io/javadoc/docs/qupath/imagej/detect/tissue/SimpleTissueDetection2.html" class="inline-onebox" rel="noopener nofollow ugc">SimpleTissueDetection2 (QuPath 0.4.0)</a>).</p>
</li>
</ul>
<h3>
<a name="sample-image-andor-code-2" class="anchor" href="#sample-image-andor-code-2"></a>Sample image and/or code</h3>
<p>Whole slide images in open-access: <a href="https://openslide.cs.cmu.edu/download/openslide-testdata/" class="inline-onebox" rel="noopener nofollow ugc">openslide-testdata</a></p>
<pre><code class="lang-auto">
public List&lt;Integer&gt; getNumbersInRange(int start, int end) {
    List&lt;Integer&gt; result = new ArrayList&lt;&gt;();
    for (int i = start; i &lt; end; i++) {
        result.add(i);
    }
    return result;
}

runPlugin('qupath.imagej.detect.tissue.SimpleTissueDetection2', ' {
    "threshold": getNumbersInRange(0, 255),  
    "requestedPixelSizeMicrons": getNumbersInRange(0, 100),  
    "minAreaMicrons": getNumbersInRange(0, 100000000),  
    "maxHoleAreaMicrons": getNumbersInRange(0, 100000000),
    "darkBackground": false,  
    "smoothImage": true,  
    "medianCleanup": true,  
    "dilateBoundaries": true,  
    "smoothCoordinates": true,  
    "excludeOnBoundary": true,  
    "singleAnnotation": true
    }');
</code></pre> ;;;; <p>Hello all,</p>
<p>I would also recommend <a href="https://imagej.net/plugins/bigstitcher/index" class="inline-onebox" rel="noopener nofollow ugc">BigStitcher</a>, even if you do not need stitching <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
In the new version it has Arrange Views → (De)Skew Images… which works really well (and probably you would need to scale one of the axis (or just provide proper voxel sizes).<br>
You can export result as tiff or BDV HDF5 (skipping stitching step).</p>
<p>Cheers,<br>
Eugene</p> ;;;; <p>Apologies, I was using an out-of-date version of NGFF converter above.<br>
With the latest I get valid NGFF - see <a href="https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1%2BA2_test.zarr" class="inline-onebox">OME-NGFF validator</a></p> ;;;; <p><a href="http://www.libtiff.org/man/tiffinfo.1.html">Tiffinfo</a> is part of the the <a href="http://www.libtiff.org/">libtiff</a> library. You can usually simply install it via your linux package manager. And there’s also a Windows binary for download on the website, but I’ve never tried that.<br>
Kind Regards,<br>
Dominik</p> ;;;; <aside class="quote no-group" data-username="EPet" data-post="4" data-topic="78691">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/epet/40/46601_2.png" class="avatar"> Ery Petropoulou:</div>
<blockquote>
<p><code>if (ROI: 0.50 μm per pixel: MHC-I: Mean &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet'))</code></p>
</blockquote>
</aside>
<p>You can just use <code>intensity</code> here as you have defined it as a variable earlier, i.e.:</p>
<pre><code class="lang-auto">if (intensity &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet'))
</code></pre> ;;;; <p>Hi <a class="mention" href="/u/alvin">@Alvin</a></p>
<p>If it helps, I’ve put the Analyze Skeleton and Skeletonize3D plugins I used with QuPath into a ZIP file for you to download.<br>
<a class="attachment" href="/uploads/short-url/fjPe2zp0zcVA8cooaiC7jRuiXXB.zip">AnalyzeSkeleton_-3.4.2.zip</a> (57.0 KB)</p> ;;;; <p>Hi <a class="mention" href="/u/nobel">@nobel</a>,</p>
<p>you still get the error</p>
<p><code>Macro Error unrecognized command “enhance local contrast (CLAHE)”, “blocksize=127 histogram=256 maximum=3 mask = *None fast_(less_accurate)” &lt;)&gt; ;</code><br>
?</p>
<p>Could you post the macro?</p>
<p>Best,<br>
Volker</p> ;;;; <p>Hi so I tried the following, and it gave me an error at line 10. I am also using version 0.3.2 so I kept the fireHierarchyUpdate. So i think the problem is with my ‘ROI: 0.50 μm per pixel: MHC-I: Mean’ value.</p>
<pre><code class="lang-auto">// Get all annotations in the current image
def annotations = getAnnotationObjects()

// Loop through each annotation
for (annotation in annotations) {
  // Get the mean intensity value of the annotation
  def intensity = annotation.measurements.get('ROI: 0.50 μm per pixel: MHC-I: Mean') // or annotation.measurements['Intensity']
  
  // Check if the intensity is greater than 750 and the class name is "Islet"
  if (ROI: 0.50 μm per pixel: MHC-I: Mean &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {
    // Change the class name to "H-islet"
    annotation.setPathClass(getPathClass('H-islets'))
  }
}
// Update the display
fireHierarchyUpdate()

This is the error:
RROR: MultipleCompilationErrorsException at line 10: startup failed:
Script14.groovy: 11: Unexpected input: '{\n  // Get the mean intensity value of the annotation\n  def intensity = annotation.measurements.get('ROI: 0.50 μm per pixel: MHC-I: Mean') // or annotation.measurements['Intensity']\n  \n  // Check if the intensity is greater than 750 and the class name is "Islet"\n  if (ROI:' @ line 11, column 10.
     if (ROI: 0.50 μm per pixel: MHC-I: Mean &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {</code></pre> ;;;; <p>Cross-linking to <a href="https://github.com/glencoesoftware/bioformats2raw/issues/194" class="inline-onebox">NGFF-Converter 1.1.4 fails to convert CZI file which opens normally in Fiji using BioFormats · Issue #194 · glencoesoftware/bioformats2raw · GitHub</a> where this issue has also been raised and discussed. And re-emphasizing <a class="mention" href="/u/sebi06">@sebi06</a>’s point, both the OME-NGFF specification and the converter currently do not support the conversion of preview images alongside HCS data.</p>
<p>With the current version of NGFF-Converter (1.1.4), it is possible to ignore these preview images during the conversion of CZI files by passing</p>
<pre><code class="lang-auto">--options=zeissczi.attachments=false
</code></pre>
<p>in the <code>Extra arguments</code> window. This is the command-line equivalent of the checkbox in the configuration window above and this will create a NGFF with two multiscales images (<code>A/1/0</code> and <code>A/2/0</code>) following the 0.4 version of the specification.</p> ;;;; <p>Hey <a class="mention" href="/u/sebi06">@sebi06</a><br>
Great to see you so active in the OME-Zarr community and I hope the other issues can also get figured out.</p>
<p>Regarding the PR that <a class="mention" href="/u/will-moore">@will-moore</a> linked to above (<a href="https://github.com/ome/ome-zarr-py/pull/241" rel="noopener nofollow ugc">that one here</a>), it will load plates where wells have irregular sizes (the spec allows that, so eventually the viewers should support that as well). It currently doesn’t scale great with plate sizes though (WIP), so more useful for testing and smaller datasets.</p> ;;;; <p>Hello everybody and in particular <a class="mention" href="/u/tinevez">@tinevez</a> ,</p>
<p>I’m trying the use the CSVImporter for trackmate programmatically, but I do not manage to get it to ignore the z column. How can I do that ?</p>
<p>So far I wrote:</p>
<pre><code class="lang-auto">        double default_radius = 0.11;
        File spotsFile = new File("blabla.csv");

        CSVImporterDetectorFactory detectFactory = new CSVImporterDetectorFactory();
        Map&lt;String, Object&gt; map = detectFactory.getDefaultSettings();
        map.put("FILE_PATH", spotsFile.getAbsolutePath());
        map.put("X_COLUMN", "x [um]");
        map.put("Y_COLUMN", "y [um]");
        map.put("Z_COLUMN", (Object) null); // I tried "", "-1"
        map.put("FRAME_COLUMN", "z");
        map.put("QUALITY_COLUMN", "");
        map.put("NAME_COLUMN", "");
        map.put("ID_COLUMN", "");
        map.put("RADIUS", default_radius);
        map.remove("RADIUS_COLUMN");
        //map.put("X_ORIGIN", DEFAULT_X_ORIGIN);
        //map.put("Y_ORIGIN", DEFAULT_Y_ORIGIN);
        //map.put("Z_ORIGIN", DEFAULT_Z_ORIGIN);
        //map.put("RADIUS_COLUMN", (Object)null);
        //map.put("RADIUS", DEFAULT_RADIUS);

        settings.detectorFactory = detectFactory;
        settings.detectorSettings = map;
</code></pre>
<p>But I get the message <code>Parameter Z_COLUMN could not be found in settings map, or is null.</code> and no spot is detected.</p> ;;;; <p>Hi <a class="mention" href="/u/will-moore">@will-moore</a> and <a class="mention" href="/u/s.besson">@s.besson</a> ,</p>
<p>ok, that are interesting finds.</p>
<p>Clearly one issue is that the NGFF converter cannot deal with the preview images, because the CZI contains 2 FoV from 2 wells and an overview of the sample (taken from a preview camera for automatic sample carrier detection).</p>
<p>Obviously this one has totally different dimensions and metadata compared to the actual image data and should be ignores when creating the OME-ZARR (or stored different).</p>
<p>When using BioFormats it works fine since it allows to ignore such preview images. Maybe this should become an option for the NGFF converter as well?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0.png" data-download-href="/uploads/short-url/hIKoAVoo7bZyGSTdjjbQCHQBKo0.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_435x500.png" alt="image" data-base62-sha1="hIKoAVoo7bZyGSTdjjbQCHQBKo0" width="435" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_435x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_652x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_870x1000.png 2x" data-dominant-color="E4E4E4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">900×1033 144 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>So the issue with the different sizes at least is now explainable, but the other issue with the not registered plugin is still unclear, correct? Because I have several OME-ZARR where this “unregistered plugin error” comes up, while the same plugin &amp; napari work fine for others.</p> ;;;; <p>Hello <a class="mention" href="/u/cgohlke">@cgohlke</a>,</p>
<p>Thanks for your quick answer !<br>
I’ll do that and probably contact Vilber support.</p>
<p>I didn’t know about this library for Tiff checking. How can I install it ?</p>
<p>Best,<br>
Rémy.</p> ;;;; <p>to be precise, this is the message from test 3</p>
<p>ImageJ 1.53v; Java 17.0.6 [64-bit]; Mac OS X 13.2.1; 76MB of 8192MB (&lt;1%)</p>
<p>java.awt.IllegalComponentStateException: The dialog is decorated<br>
at java.desktop/java.awt.Dialog.setBackground(Unknown Source)<br>
at ij.gui.GenericDialog.(GenericDialog.java:113)<br>
at ij.gui.GenericDialog.(GenericDialog.java:97)<br>
at sc.fiji.analyzeSkeleton.AnalyzeSkeleton_.createSettingsDialog(AnalyzeSkeleton_.java:3434)<br>
at sc.fiji.analyzeSkeleton.AnalyzeSkeleton_.run(AnalyzeSkeleton_.java:286)<br>
at ij.plugin.filter.PlugInFilterRunner.processOneImage(PlugInFilterRunner.java:266)<br>
at ij.plugin.filter.PlugInFilterRunner.(PlugInFilterRunner.java:114)<br>
at ij.IJ.runUserPlugIn(IJ.java:241)<br>
at ij.IJ.runPlugIn(IJ.java:205)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.base/java.lang.Thread.run(Unknown Source)</p>
<p>I am not in anyway a scripting language person.</p> ;;;; <p>Hi again Research_Associate,</p>
<p>I should have mentioned that I have set the plugin folder for QuPath-ImageJ extention same as the standalone Fiji directory. The Default ImageJ plugin list doesn’t seem to have analyse skeleton. It is because of that I have the question of plugin capability of ImageJ 1 vs ImageJ2/Fiji. I assume if the <a href="http://image.net" rel="noopener nofollow ugc">image.net</a> page has a dedicated link for the installation of the plugin for ImageJ1, the Fiji default version should be something that utilizes the counterpart ImageJ2.</p>
<p>I ran a few test and here are the results</p>
<ol>
<li>Copied analyse skeleton .jar file from Fiji directory to ImageJ1 standalone plugin directory &gt; works fine</li>
<li>Set Fiji plugin directory as QuPath-ImageJ plugin directory &gt; i. Manual operation, nothing popped up; ii. by script, error message</li>
<li>Set ImageJ1 standalone plugin directory (as of test 1) as QuPath-ImageJ plugin directory &gt; error message</li>
</ol> ;;;; <p>No Sorry Marcos.</p> ;;;; <p>How can I download this particular version of cellprofiler</p> ;;;; <p>Hi all,</p>
<p>I used to use CellProfiler years ago adn recently got excited when I saw the implementation of Stardist and Cellpose.  So I thought I’d give it a go…</p>
<p>I tried a few approaches to install CellProfiler from source.<br>
This one on the <a href="https://github.com/CellProfiler/CellProfiler/wiki/Source-installation-%28Windows%29" rel="noopener nofollow ugc">github</a> repo did work.<br>
I failed trying to do it using anaconda.</p>
<p>Having navigated through this I discovered you need to also clone the <a href="https://github.com/CellProfiler/CellProfiler-plugins" rel="noopener nofollow ugc">plugins</a>.</p>
<p>I followed the Use in the README.md.</p>
<p>when I got to step 2 (b).</p>
<pre><code class="lang-auto">pip install -r requirements-windows.txt
</code></pre>
<p>cellh5 and keras installed<br>
but cntk did not</p>
<pre><code class="lang-auto">ERROR: Could not find a version that satisfies the requirement cntk (from versions: none)
ERROR: No matching distribution found for cntk
</code></pre>
<p><strong>Is there a solution for this ?</strong></p>
<hr>
<p>I also then found the <a href="https://github.com/CellProfiler/CellProfiler-plugins/blob/0c2e7cab5c13d17fa376b2bd42ca6a7e5db04960/Instructions/Install_environment_instructions_windows.md" rel="noopener nofollow ugc">Beginner Guide</a><br>
ASIDE:This suggests using anaconda which is at odds with the cellprofiler github page that recommends PIP.</p>
<p>This works well.The resulting install has the runstardist and runcellpose modules.  Havent tested they work though. Only obvious issues</p>
<pre><code class="lang-auto">Could not load variancetransform
Traceback (most recent call last):
  File "C:\Users\Admin\anaconda3\envs\CP_plugins\lib\site-packages\cellprofiler_core\utilities\core\modules\__init__.py", line 71, in add_module
    m = __import__(mod, globals(), locals(), ["__all__"], 0)
  File "C:\Users\Admin\OneDrive - The University of Sydney (Students)\Documents\GitHub\CP2\CellProfiler-plugins\variancetransform.py", line 20, in &lt;module&gt;
    import pandas
ModuleNotFoundError: No module named 'pandas'
could not load these modules: variancetransform
</code></pre>
<p>NB. The anaconda based approach → CellProfiler 4.2.1<br>
The PIP approach → CellProfiler 5.0.0b1 but no runStarDist</p>
<p>Tried to  solve with</p>
<pre><code class="lang-auto">pip install stardist csbdeep --no-deps
pip install omnipose
</code></pre>
<p>did not work ;(</p>
<p>Cheers.</p>
<p>James</p> ;;;; <p>Thanks, I wanted to take the first frame from a stack and this worked</p> ;;;; <aside class="quote no-group" data-username="Jose_Cabot" data-post="5" data-topic="23445">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jose_cabot/40/57310_2.png" class="avatar"> Jose Cabot:</div>
<blockquote>
<pre><code class="lang-auto">[ERROR] Traceback (most recent call last):
  File "Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py", line 42, in &lt;module&gt;
ImportError: No module named Microscope_Calibrations_user_settings

	at org.python.core.Py.ImportError(Py.java:329)
	at org.python.core.imp.import_first(imp.java:1230)
	at org.python.core.imp.import_module_level(imp.java:1361)
	at org.python.core.imp.importName(imp.java:1528)
	at org.python.core.ImportFunction.__call__(__builtin__.java:1285)
	at org.python.core.PyObject.__call__(PyObject.java:433)
	at org.python.core.__builtin__.__import__(__builtin__.java:1232)
	at org.python.core.imp.importOneAs(imp.java:1564)
	at org.python.pycode._pyx0.f$0(Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py:212)
	at org.python.pycode._pyx0.call_function(Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py)
	at org.python.core.PyTableCode.call(PyTableCode.java:173)
	at org.python.core.PyCode.call(PyCode.java:18)
	at org.python.core.Py.runCode(Py.java:1687)
	at org.python.core.__builtin__.eval(__builtin__.java:497)
	at org.python.core.__builtin__.eval(__builtin__.java:501)
	at org.python.util.PythonInterpreter.eval(PythonInterpreter.java:255)
	at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:57)
	at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:31)
	at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)
	at org.scijava.script.ScriptModule.run(ScriptModule.java:157)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
</code></pre>
</blockquote>
</aside>
<p>Follow the steps here, and download the latest version: <a href="https://github.com/demisjohn/Microscope-Measurement-Tools" class="inline-onebox" rel="noopener nofollow ugc">GitHub - demisjohn/Microscope-Measurement-Tools: Microscope Measurement/Calibration plugin for FIJI</a></p>
<p>Place your unzipped Microscope Measurement Tools folder in the PLUGINS directory of your Fiji folder (NOT in the SCRIPT folder, as indicated in some other instructions), then follow instructions as usual.</p>
<p>Once installed in Fiji, you will find this tool in your Fiji software in the Plugins&gt;Analyze&gt;Microscope Measurement Tools and your calibrations will appear.</p>
<p>Hope this helps!</p> ;;;; <p>Within FIJI, an option is to use CLIJ.<br>
You can use the <a href="https://clij.github.io/clij2-docs/reference_affineTransform3D" rel="noopener nofollow ugc">AffineTransform3D</a> class for deskew, followed by a rotation for coverslip rotation.</p>
<p>Other options:</p>
<ul>
<li><a href="https://monash-merc.github.io/llsm/" class="inline-onebox" rel="noopener nofollow ugc">LLSM</a></li>
<li><a href="https://gist.github.com/jdmanton/722b8a5618365062a854c190a8bc366a" class="inline-onebox" rel="noopener nofollow ugc">ImageJ/Fiji script for deskewing stage-scan light sheet data (inc. OPM) · GitHub</a></li>
</ul> ;;;; <p>Hi,</p>
<p>I also keep getting the same message after trying the instructions. Were you able to find any solutions so far? I am using Fiji in MacOS.</p> ;;;; <aside class="quote no-group" data-username="Alvin" data-post="4" data-topic="78484">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png" class="avatar"> Alvin Lam:</div>
<blockquote>
<p>Manually, I cannot find nor install the plugin hence the script cannot run through.</p>
</blockquote>
</aside>
<p>You shouldn’t need to, it is included in Fiji (if not ImageJ as well), so following the instructions to point to that plugins directory should allow you to use it.</p> ;;;; <p>Yep, that whole domain does seem to be gone. It sounded like you meant that <a href="http://imagej.net">imagej.net</a> was down for you.</p> ;;;; <p>Alternatively, how about saving the masks directly to TIFF. Wouldn’t that work?<br>
Personal <a href="https://github.com/leogolds/MicroscopyPipeline/blob/3346facb359cfb448e01514a45aaa80fa58381f1/utils.py#L73" rel="noopener nofollow ugc">example</a> for saving in H5 and optionally to TIFF.</p> ;;;; <p>Thanks <a class="mention" href="/u/praveen">@Praveen</a>.  Hope things are working well for you.</p>
<p><a class="mention" href="/u/maria_traver">@Maria_Traver</a> , do you know the angle and direction of skew (Y or X).  It should work on other LLSM datasets, provided you know the angle and direction of skew (X or Y). We recently identified a “bug”, where I realized that the direction of stage-scan matters too! This will be fixed soon on pyclesperanto-prototype library upstream.</p>
<p>napari-lattice-lightsheet has been inspired a lot from <a class="mention" href="/u/talley">@talley</a> 's code.</p> ;;;; <p>Each contour is a closed loop around a contiguous bunch of “True” pixels. Does that make sense?</p> ;;;; <p>Hi Research_Associate,</p>
<p>This is the page I am referring to.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://imagej.net/plugins/analyze-skeleton/">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/17cd920cc3af5597be55618d6d6ea4a54809d2eb.png" class="site-icon" width="32" height="32">

      <a href="https://imagej.net/plugins/analyze-skeleton/" target="_blank" rel="noopener nofollow ugc">ImageJ Wiki</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d255a5176a70445402413f516b0a0776a37504cd.png" class="thumbnail onebox-avatar" width="256" height="256">

<h3><a href="https://imagej.net/plugins/analyze-skeleton/" target="_blank" rel="noopener nofollow ugc">AnalyzeSkeleton</a></h3>

  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
Right at the top where it says “For the ImageJ 1.x plugin, see [this page]”. I have troubles entering the dedicated link.</p> ;;;; <p>Hi Yau Mun,</p>
<p>Thank you very much for the suggestion. That was indeed one of the scripts that I tried. The issue seemed to be the "run(“Analyze Skeleton (2D/3D)” part in the ImageJ macro. Manually, I cannot find nor install the plugin hence the script cannot run through.</p> ;;;; <p>This is now a reported and known bug in Spyder &gt;5.4.1 and has to do with the updating of the variable explorer (see <a href="https://github.com/spyder-ide/spyder/issues/20635" rel="noopener nofollow ugc">Issue 20635</a>). A fix has been proposed with a goal for the release of Spyder 6.</p>
<p>The current workaround is to define the</p>
<blockquote>
<p>data</p>
</blockquote>
<p>variable as a private variable by changing the code to it</p>
<blockquote>
<p>_data = <a href="http://ij.io" rel="noopener nofollow ugc">ij.io</a>().open(/‘Test.tif’)</p>
</blockquote>
<p>After testing this workaround, it allows PyImageJ to work with images just fine in Spyder.</p> ;;;; <aside class="quote no-group" data-username="Fotos_Stylianou" data-post="1" data-topic="78719">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fotos_stylianou/40/69173_2.png" class="avatar"> Fotos Stylianou:</div>
<blockquote>
<p>I would like to sort the rois in the roimanager and chatgpt suggested the following command:</p>
<pre><code class="lang-auto">rm.runCommand('Sort', 'by=Size')
</code></pre>
</blockquote>
</aside>
<p>It’s amazing that ChatGPT knows about such an obscure ImageJ method. Unfortunately, it made up a non-existent option for that method. The only thing the rm.runCommand(‘Sort’) method does is sort the ROI names into alphanumeric order.</p> ;;;; <p>Hey!<br>
Regarding CellPose models, they were trained on different data and perform better/worse depending on your specific situation. We use the base models to train our own domain specific models to improve performance for our use case.</p>
<p>Regarding files created/missing, an example of your issues might be appropriate.<br>
In general, sharing something you tried and the issue you experienced is typically a better approach than soliciting for “general advice” <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Good luck!<br>
Leo</p> ;;;; <aside class="quote no-group" data-username="jni" data-post="4" data-topic="78544">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jni/40/5991_2.png" class="avatar"> Juan Nunez Iglesias:</div>
<blockquote>
<p>skimage.measure.find_contours</p>
</blockquote>
</aside>
<p><a class="mention" href="/u/jni">@jni</a>  Wow!  I think this fits my needs perfectly!  Thanks for the insights, and awesome explanations <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>One question on the contours, since I’m not completely clear on that part.<br>
So when I print out the contours variable, I get a long list where the following are first few arrays for it. How do I interpret each of the arrays?   Does each array correlate to a line segment?</p>
<p>[array([[285.5,  51. ],<br>
[285.5,  50. ],<br>
[285. ,  49.5],<br>
…,<br>
[284.5,  52. ],<br>
[285. ,  51.5],<br>
[285.5,  51. ]]), array([[466.5, 434. ],<br>
[466. , 433.5],<br>
[465.5, 433. ],<br>
…,<br>
[465.5, 435. ],<br>
[466. , 434.5],<br>
[466.5, 434. ]]), array([[215.5, 375. ],<br>
[215. , 374.5],<br>
[214.5, 374. ],<br>
…,<br>
[214.5, 376. ],<br>
[215. , 375.5],<br>
[215.5, 375. ]])</p> ;;;; <p>Yes, the trajectory of the middle particle is there for all the frames. When I used more frames, the trajectories overlapped, and I could see the middle particle was black, so it was not visible on the black background.</p>
<p>I have tried doing it multiple times. black color trajectory is always there; just the particle changes.</p> ;;;; <p>Hi</p>
<p>I am using the cellpose 2.2 in cell segmentation.<br>
This system was built in WEB platform and the cellpose was realized by using command.<br>
Firstly, while researching the cellpose, I found that cellpose provide 14 pretrained models.<br>
But I am not sure what the differences(and their utilizing examples) are between them because I am not an expert in this field.<br>
Please give me good advice about this problem.<br>
Secondly, I also found that the cellpose provide “*_output.png” file when we get cell segment result by using commands.<br>
But sometimes there are some cases that it’s not created(Because I used the same command, I think that it’s related with the kind of image file).<br>
Which case is it created or not in?</p>
<p>Any help would be appreciated.</p> ;;;; <aside class="quote no-group" data-username="Ali_Amininejad" data-post="5" data-topic="78461">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ali_amininejad/40/68919_2.png" class="avatar"> Ali :</div>
<blockquote>
<p>I saved x-y values in .csv. Then, opened it in ImageJ and used the two lines macro code you wrote, but it resulted in this: undefined variables xvalues=Table.getColumn(&lt;“x”&gt;)</p>
</blockquote>
</aside>
<p>Change the curly quotes to straight quotes. Post the .csv file and I will try to create an example macro that uses the x-y values in the .csv file to plot a curve on an image.</p> ;;;; <p>Hi <a class="mention" href="/u/sebi06">@sebi06</a></p>
<p>I found the same as <a class="mention" href="/u/s.besson">@s.besson</a> - and I think the same as you got this time. I see the same error when I try to open in napari.</p>
<p>The output from my conversion is public and can be browsed at:</p>
<p><a href="https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr" class="onebox" target="_blank" rel="noopener">https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr</a></p>
<p>I see a few issues:</p>
<p>The validator fails since wells have <code>row_index</code> and <code>column_index</code> instead of 'rowIndex<code>and</code>columnIndex`. This looks like a bug in the NGFF validator.</p>
<p>The <code>plate</code> is missing a <code>version</code> - so it’s being validated with the v0.4 schema.</p>
<p>Strangely the validator reports that some images in the plate are invalid, but when each Well and Image are opened alone, they seem to be valid.</p>
<p>E.g. first image: <a href="https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr/0/0/0/" class="inline-onebox">OME-NGFF validator</a><br>
and third image: <a href="https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr/0/0/2/" class="inline-onebox">OME-NGFF validator</a></p>
<p>The images themselves are <code>v0.2</code> and missing <code>omero</code> so e.g. Vizarr shows both channels on a single slider: <a href="https://hms-dbmi.github.io/vizarr/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr/0/0/0" class="inline-onebox">vizarr</a></p>
<p>The error that you’re seeing in <code>napari</code> is that it assumes ALL images are the same size, so it fails to stitch them together.<br>
There’s actually a PR at <a href="https://github.com/ome/ome-zarr-py/pull/241" class="inline-onebox">[WIP] Support plate loading for varying well sizes (ref #240) by jluethi · Pull Request #241 · ome/ome-zarr-py · GitHub</a> to address that issue.<br>
Using that branch of ome-zarr-py, I can open the plate in napari:</p>
<p><code>$ napari --plugin napari-ome-zarr w96_A1_A2_test.zarr/</code></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee.png" data-download-href="/uploads/short-url/npfW7SlMPut1fLVwfY9ungk4QGi.png?dl=1" title="Screenshot 2023-03-16 at 23.09.09"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_690x433.png" alt="Screenshot 2023-03-16 at 23.09.09" data-base62-sha1="npfW7SlMPut1fLVwfY9ungk4QGi" width="690" height="433" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_690x433.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_1035x649.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_1380x866.png 2x" data-dominant-color="1A1C1F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-16 at 23.09.09</span><span class="informations">1886×1186 217 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Thanks a lot! Hopefully it helps.</p> ;;;; <p>Thank you Mike, I will try to get another computer and see then.</p> ;;;; <aside class="quote no-group" data-username="BUMJUN_KIM" data-post="3" data-topic="78726">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bumjun_kim/40/17896_2.png" class="avatar"> Bumjun Kim:</div>
<blockquote>
<p>In this case “mean intensity” would be the integration of intensity obtained from each pixel in the object divided by # of pixels in that detection object? Is it the right way to interpret or can you elaborate on this specific example?</p>
</blockquote>
</aside>
<p>It could be, depends on what your actual pixel size is compared to the pixel size you chose when making the measurement. Either way the result should be pretty much the same, you are effectively pre-averaging some of the pixels together for larger pixel sizes is all.</p> ;;;; <p>Thanks for the quick response. I do not think I understood it correctly. Let me elaborate more to make sure that I am on the same page with you.</p>
<p>So let’s say I apply a pixel classifier to get an detection/annotation object. Then I will do the intensity measurement in this detection object. The QuPah will spit out the “Area(um2)” of detection object and “mean intensity” as discussed above. In this case “mean intensity” would be the integration of intensity obtained from each pixel in the object divided by # of pixels in that detection object? Is it the right way to interpret or can you elaborate on this specific example?</p>
<p>Thanks</p> ;;;; <p><a class="mention" href="/u/smcardle">@smcardle</a>’s answer would be much faster I suspect, though you’d probably want to draw the line based off that nearest coordinate rather than trying to tie it to the nearest XY coordinate cell.</p> ;;;; <p>You might want to look into <a href="https://www.napari-hub.org/plugins/napari-ndtiffs" rel="noopener nofollow ugc">napari-nd-tiff</a> developed by <a class="mention" href="/u/talley">@talley</a> and also <a href="https://www.napari-hub.org/plugins/napari-lattice" rel="noopener nofollow ugc">napari-lattice-lightsheet</a> developed by <a class="mention" href="/u/pr4deepr">@pr4deepr</a> . The latter works quiet well for lattice-lightsheet-data acquired with a Zeiss ligthsheet, I am not sure if this would work on other LLSM datasets.</p> ;;;; <p>Thanks for the suggestions!</p> ;;;; <p>Hey <a class="mention" href="/u/awalle">@AWallE</a> ,<br>
Here’s how I would approach it.</p>
<ol>
<li>Get the centroids of all ClassB cells. Turn them into a <a href="https://locationtech.github.io/jts/javadoc/org/locationtech/jts/geom/MultiPoint.html">multipoint geometry.</a>
</li>
<li>Run a loop over each ClassA cell. Turns it’s centroid into a single <a href="https://locationtech.github.io/jts/javadoc/org/locationtech/jts/geom/Point.html">point geometry</a>.</li>
<li>Use <a href="https://locationtech.github.io/jts/javadoc/org/locationtech/jts/operation/distance/DistanceOp.html#nearestPoints--">NearestPoints</a> to search for the closest centroid between CellA and the entire ClassB</li>
</ol> ;;;; <p>A modification of this <a href="https://forum.image.sc/t/qupath-script-nearest-neighbors/31824" class="inline-onebox">QuPath Script: Nearest Neighbors</a> might be closer to what you want, since you can minimize the loop to your class of interest, and also loop over the members of each TMA.</p> ;;;; <p>Yeah, unfortunately most of those functions use a distance map, which doesn’t help identify the particular cell you are interested in.</p> ;;;; <p>It’s simply the mean intensity, not area based. Or I suppose per pixel might be a way of looking at it. But the average intensity is just the average intensity based on the image acquisition parameters. It isn’t going to double because you double the estimated pixel size (the ROI 1.29um), which you can test by changing that value.</p>
<p>The measurement is <em>roughly</em> independent of the pixel size used for downsampling (which could sort-of also be thought of as “which layer of the pyramidal image am I using to calculate the mean intensity”). Though there would be differences based on edges, which will generally be small if the estimated pixel size is small relative to the objects being measured. If your pixel size is the whole image… well, it will be the average intensity across the entire image and might be way off.</p> ;;;; <p>Hello <a class="mention" href="/u/carlosuc3m">@carlosuc3m</a>, thank you for the quick reply. The torchscript file is too large to include in this post but I will provide a link that you should be able to use to download it.</p>
<p>Thanks,</p>
<p>Liam</p>
<p><a href="https://panthers-my.sharepoint.com/:u:/g/personal/ljemison_uwm_edu/EUbtQmE03ntAg9OT6-MCH2QBxwGEZxkod3jcpOOOb0zD5w?e=EK8CB6" class="onebox" target="_blank" rel="noopener nofollow ugc">https://panthers-my.sharepoint.com/:u:/g/personal/ljemison_uwm_edu/EUbtQmE03ntAg9OT6-MCH2QBxwGEZxkod3jcpOOOb0zD5w?e=EK8CB6</a></p> ;;;; <p>You can get some extra info about that package (<code>quanfima</code>) in their publication: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0215137" class="inline-onebox" rel="noopener nofollow ugc">Quanfima: An open source Python package for automated fiber analysis of biomaterials</a></p> ;;;; <aside class="quote no-group" data-username="f.goerlitz" data-post="1" data-topic="78654">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>When we take a multidimensional acquisition (MDA) using Acquisition.acquire(events) we get a resource warning from python (ResourceWarning: unclosed file &lt;_io.BufferedReader name=‘C:\temp\scan_5\scan_NDTiffStack.tif’&gt;). The acquisition works fine, however we are wondering if that is a big issue or if we can ignore the warning. Especially, if this slows down our code?!</p>
</blockquote>
</aside>
<p>Can you post the full output? I wouldn’t think there’s any reason that this is slowing things down. Do you see otherwise?</p>
<aside class="quote no-group" data-username="f.goerlitz" data-post="1" data-topic="78654">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png" class="avatar"> Fred:</div>
<blockquote>
<p>Gerneral question: If we initialise xyz-events for a z-scan, the events give us a the correct z position (eg. 0.25um). However this seems not to be stored in the metadata (eg. when opening in ImageJ), it always shows 1um as z-step. Do we have to write this separately to the metadata or are we doing something wrong with intialising the xyz-events. Similar issue, if we initialise t-events it is still saved as a z-stack with an 1um z-step.</p>
</blockquote>
</aside>
<p>ImageJ or FIJI? ImageJ has its own weird way of storing this metadata, which the default format of pycromanager does not write. The metadata is there–you can access it through python, for example</p> ;;;; <p>I just received a set of lightsheet data (.tif files) from a prototype instrument that someone wants to visualize.  The stacks are of course still skewed due to the angle of the lightsheet.  I’ve got little experience with lightsheet data, and about all I know is I need to shift each slice by z*cos (lightsheet angle).</p>
<p>Before I start reinventing the wheel, are there plugins or programs that already exist that deskew lightsheet data where you can input z and the angle?  (Note that I’m most familiar with FIJI/ImageJ, but I have a basic working knowledge of napari and Matlab.)</p> ;;;; <p>Hi,</p>
<p>I was trying brainreg plugin in Napari on a zebrafish image using the mpin_zfish_1um atlas. I wanted to check the orientation of my data first, so followed these <a href="https://docs.brainglobe.info/brainreg-napari/checking-orientation" rel="noopener nofollow ugc">instructions</a>, but for some reason it is generating views based on, what seems to me, a mouse brain atlas (top row), even though I selected the zebrafish atlas. Screenshot below:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240.png" data-download-href="/uploads/short-url/f7aZp4wG4qwKSxiC5XD9lw2mtLW.png?dl=1" title="Screenshot" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240_2_690x404.png" alt="Screenshot" data-base62-sha1="f7aZp4wG4qwKSxiC5XD9lw2mtLW" width="690" height="404" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240_2_690x404.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240_2_1035x606.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240.png 2x" data-dominant-color="35383C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot</span><span class="informations">1339×784 153 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Any idea why this might be happening?</p>
<p>ping <a class="mention" href="/u/adamltyson">@adamltyson</a></p>
<p>Thank you!</p> ;;;; <p>I am working with whole slide images which contains a TMA grid. Hence, the constraint is that I only want to draw lines between cells which are in the same TMA core. I don’t know if this would simplify the problem.</p>
<p>Since there is a function for finding the shortest distance to a detected cell of some specified class, I assumed that there might be a way to change the detectionToAnnotationDistances script to fit my needs. I have been reading the gitHub linked below:</p>
<p><a href="https://github.com/qupath/qupath/blob/main/qupath-core/src/main/java/qupath/lib/analysis/DistanceTools.java" rel="noopener nofollow ugc">https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-core/src/main/java/qupath/lib/analysis/DistanceTools.java</a></p> ;;;; <p>Hi folks,<br>
I’m working on using the convex hull analysis in SNT from FIJI to calculate the volume of an axon plexus. SNT doesn’t seem to be following the global set scale feature in FIJI and I’m wondering what the units are. Are they pixels or is it in um?</p>
<p>Do you folks have any tips for me? Here is a screenshot with the trace of an axonal plexus, the convexhull, and the corresponding size calculation with confusing units.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8.jpeg" data-download-href="/uploads/short-url/9pXcqjFwyqkHJ6JcixE0YmEAhF6.jpeg?dl=1" title="axon1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8_2_527x500.jpeg" alt="axon1" data-base62-sha1="9pXcqjFwyqkHJ6JcixE0YmEAhF6" width="527" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8_2_527x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8_2_790x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8.jpeg 2x" data-dominant-color="18191E"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">axon1</span><span class="informations">1046×991 94.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi community,</p>
<p>I have a dump question but I rather ask than assuming.</p>
<p>When exporting intensity measurement, it gives two value, Area and mean intensity as below.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/88b5ceedae8a22769a7f45fc98fa2ce03809b1c6.png" alt="image" data-base62-sha1="jvosqsSuFoI9vGGmyHxMCnW8dn0" width="349" height="21"><br>
Here does “1.29Aum per pixel: NPs: Mean” indicate mean intensity/pixel or mean intensity/um2.</p>
<p>I was thinking the former but just wanted to make sure if that is correct.<br>
Thanks!</p> ;;;; <p>Hmm, I can think of a few roundabout ways to approach it, like cycling through every class A cell, changing it to class B, and then performing Delaunay clustering and checking the connections for that cell for the shortest one, get the centroids, then changing the class back to A and repeat. Maybe <a class="mention" href="/u/petebankhead">@petebankhead</a> has something more efficient though.</p>
<p>If you have a constraint in terms of maximum distances that might open up some more options by limiting the search radius, but if you want it open ended that could be challenging.</p> ;;;; <p>Yes, I am able to get the distance, but I am unable to obtain the coordinates of the detected cell which is closest. My purpose is mostly for visualization.</p> ;;;; <aside class="quote no-group" data-username="AWallE" data-post="1" data-topic="78725">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/awalle/40/69183_2.png" class="avatar"> AWallE:</div>
<blockquote>
<p>I am able to use the LineROI function to draw a single line</p>
</blockquote>
</aside>
<p>Getting the distance to the nearest cell is quite easy, there is a built in function for that. Drawing the line is less so, but why is the line important? Is this for visualization or is there something about the angle or relative direction that is important?</p> ;;;; <p>The previous user solved it by using a different computer. It sounds like there may be security settings enabled which prevent you from writing to the specified directory, or which prevent CytoMAP from creating any new files. Aside from that, I do not know enough to make a recommendation.</p> ;;;; <p>I am currently working with IF images and have classified the cells into two classes( for simplicity call them Class A and B). My goal is to draw a line from the centroid of Class A cells to the centroid of the closest Class B cell. Furthermore, I would like to do this for every detected Class A cell. Since I will need to do this for potentially hundreds of thousands of cells, I would like to create a script to automate this process.</p>
<p>I am able to use the LineROI function to draw a single line, but I’m not sure how to first find the cell with closest distance and get the coordinates to draw a line between these two cells. Since there is a Shortest Distance to Annotation function, I imagine that it would be possible to write script for this. However, I am not sure where to start. I apologize if this is a simple task as I am fairly new to qupath.</p> ;;;; <p>Thanks for that suggestion! I got the same error after doing this. After completing this step a dialog box is supposed to pop up for the next step but this does not happen. I am pretty sure I have followed everything else as needed so I think this is a macro problem but not sure of any other ideas to fix it. Thanks!!</p> ;;;; <p>I would like to know if all the open source BIA software in the list below are self-contained and if, in Windows, there would be any issue in solely archiving/copying back this folder for redeployment to the same machine.</p>
<p>ImageJ, CellProfiler, QuPath, ilastik, Python Anaconda.</p> ;;;; <p>What si also confusing me is this layout:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e56ec0042c3bcef628622dba2a2755a26216b036.png" data-download-href="/uploads/short-url/wJECxCA7pEfQgA2ycmnr09b0foq.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e56ec0042c3bcef628622dba2a2755a26216b036.png" alt="image" data-base62-sha1="wJECxCA7pEfQgA2ycmnr09b0foq" width="690" height="99" data-dominant-color="F6F5F4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">730×105 3.69 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Shouldn’t there only be two folder “1” and “2” because the file has two wells (A1 &amp; A2)?</p> ;;;; <p>I have an image of a battery separator (i.e what separates the anode from the cathode in a battery).</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/000f35c2e25864dc13866ace5edd0ab035232d21.png" alt="background" data-base62-sha1="wApwx13k0z6tutzKFr2RCbbeF" width="187" height="199"></p>
<p>I am able to segment this image and find the pores (i.e the black holes) in the image effectively.</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/efc70ac8621c64fa0b8e4391484726b18eb2981b.png" alt="overlay_prototype" data-base62-sha1="ydayNiKqoOBapcZlK62ZXoKoC0z" width="187" height="199"></p>
<p>However, another important quantitative measurement I’ll like to obtain is the length/diameter of the fibers. I tried using some signal processing analysis using scikit-image and opencv (e.g morphological operations, canny, edge detection etc.) but nothing works.</p>
<p>Furthermore, I also found this package (<a href="https://github.com/rshkarin/quanfima/tree/master" class="inline-onebox" rel="noopener nofollow ugc">GitHub - rshkarin/quanfima: Quanfima (Quantitative Analysis of Fibrous Materials)</a>) that estimates fiber properties from this related forum topic (<a href="https://forum.image.sc/t/automatically-measuring-cellulose-fiber-diameter-and-length-from-a-ct/62294/2" class="inline-onebox">Automatically measuring cellulose fiber diameter and length from a µCT - #2 by psobolewskiPhD</a>), however, I’m unable to verify the veracity of the ‘estimate_fiber_measurements’ method. I tried to look at the documentation but I couldn’t find any on here (<a href="https://quanfima.readthedocs.io/en/latest/quanfima.html#quanfima-morphology-module" class="inline-onebox" rel="noopener nofollow ugc">API Reference — quanfima 0.1a1 documentation</a>), so I tried reading the source code and stepping through the code but I stumped.</p>
<p>Any help will be much appreciated. Thanks!!</p> ;;;; <p>As far as I remember i used NGFF 1.1.4 under Windows 10.</p>
<p>I just did run the NGFF converter again and dropped the ZARR into napari and this time I get a different error …</p>
<p>Log from conversion:</p>
<pre><code class="lang-plaintext">18:29:17 INFO  c.g.convert.App - Beginning file conversion...

18:29:17 INFO  c.g.c.ConverterTask - Executing bioformats2raw with args [C:\Temp\zarr\w96_A1+A2_test.czi, C:\Temp\zarr\w96_A1+A2_test.zarr, --log-level=INFO]
18:29:17 INFO  c.g.c.ConverterTask - Working on w96_A1+A2_test.czi
18:29:18 INFO  l.f.ImageReader - ZeissCZIReader initializing C:\Temp\zarr\w96_A1+A2_test.czi
18:29:20 INFO  c.g.b.Converter - Using 4 pyramid resolutions
18:29:20 INFO  c.g.b.Converter - Preparing to write pyramid sizeX 1960 (tileWidth: 1024) sizeY 1416 (tileWidth: 1024) sizeZ 1 (tileDepth: 1) imageCount 2
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 1024] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - chunk read complete 1/8
18:29:20 INFO  c.g.b.Converter - chunk read complete 2/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760413] time[37] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 4/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760413] time[46] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 0] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 1024, 0] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 1024] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 1024, 1024] to A/1/0/0
18:29:20 INFO  c.g.b.Converter - chunk read complete 5/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760589] time[33] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 6/8
18:29:20 INFO  c.g.b.Converter - chunk read complete 7/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760589] time[36] tag[getChunk]
18:29:20 INFO  o.p.TimingLogger - start[1678987760589] time[36] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 8/8
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Reducing active tileWidth to 980
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/1
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/1
18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[111] tag[getTileDownsampled]
18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[111] tag[getTileDownsampled]
18:29:20 INFO  c.g.b.Converter - chunk read complete 1/2
18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[112] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 2/2
18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[113] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=0 yy=0 zz=0 width=980 height=708 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=1 xx=0 yy=0 zz=0 width=980 height=708 depth=1
18:29:20 INFO  c.g.b.Converter - Reducing active tileWidth to 490
18:29:20 INFO  c.g.b.Converter - Reducing active tileHeight to 354
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/2
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/2
18:29:20 INFO  o.p.TimingLogger - start[1678987760835] time[10] tag[getTileDownsampled]
18:29:20 INFO  o.p.TimingLogger - start[1678987760835] time[10] tag[getTileDownsampled]
18:29:20 INFO  c.g.b.Converter - chunk read complete 1/2
18:29:20 INFO  o.p.TimingLogger - start[1678987760835] time[10] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 2/2
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=0 xx=0 yy=0 zz=0 width=490 height=354 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=1 xx=0 yy=0 zz=0 width=490 height=354 depth=1
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/3
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/3
18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getTileDownsampled]
18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getTileDownsampled]
18:29:20 INFO  c.g.b.Converter - chunk read complete 1/2
18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 2/2
18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=1 xx=0 yy=0 zz=0 width=245 height=177 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=0 xx=0 yy=0 zz=0 width=245 height=177 depth=1
18:29:20 INFO  c.g.b.Converter - Using 4 pyramid resolutions
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 1024] to A/2/0/0
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/2/0/0
18:29:20 INFO  c.g.b.Converter - chunk read complete 1/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760876] time[19] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 2/8
18:29:20 INFO  c.g.b.Converter - chunk read complete 3/8
18:29:20 INFO  c.g.b.Converter - chunk read complete 4/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760876] time[24] tag[getChunk]
18:29:20 INFO  o.p.TimingLogger - start[1678987760876] time[25] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1
18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 1024, 1024] to A/2/0/0
18:29:20 INFO  c.g.b.Converter - chunk read complete 5/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760948] time[11] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - chunk read complete 6/8
18:29:20 INFO  c.g.b.Converter - chunk read complete 8/8
18:29:20 INFO  o.p.TimingLogger - start[1678987760948] time[13] tag[getChunk]
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1
18:29:20 INFO  c.g.b.Converter - Reducing active tileWidth to 980
18:29:20 INFO  c.g.b.Converter - Reducing active tileHeight to 708
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/1
18:29:21 INFO  o.p.TimingLogger - start[1678987761003] time[49] tag[getTileDownsampled]
18:29:21 INFO  o.p.TimingLogger - start[1678987761003] time[49] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=0 yy=0 zz=0 width=980 height=708 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=1 xx=0 yy=0 zz=0 width=980 height=708 depth=1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/2
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/2
18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[10] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - chunk read complete 1/2
18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[10] tag[getChunk]
18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[11] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - chunk read complete 2/2
18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[12] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=1 xx=0 yy=0 zz=0 width=490 height=354 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=0 xx=0 yy=0 zz=0 width=490 height=354 depth=1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/3
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/3
18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getTileDownsampled]
18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - chunk read complete 1/2
18:29:21 INFO  c.g.b.Converter - chunk read complete 2/2
18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getChunk]
18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=1 xx=0 yy=0 zz=0 width=245 height=177 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=0 xx=0 yy=0 zz=0 width=245 height=177 depth=1
18:29:21 INFO  c.g.b.Converter - Using 5 pyramid resolutions
18:29:21 INFO  c.g.b.Converter - Preparing to write pyramid sizeX 3066 (tileWidth: 1024) sizeY 2030 (tileWidth: 1024) sizeZ 1 (tileDepth: 1) imageCount 1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 2048] to A/3/0/0
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/0
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/3/0/0
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 0] to A/3/0/0
18:29:21 INFO  c.g.b.Converter - chunk read complete 1/6
18:29:21 INFO  c.g.b.Converter - chunk read complete 2/6
18:29:21 INFO  c.g.b.Converter - chunk read complete 3/6
18:29:21 INFO  o.p.TimingLogger - start[1678987761108] time[9] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - chunk read complete 4/6
18:29:21 INFO  o.p.TimingLogger - start[1678987761108] time[9] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=0 zz=0 width=1024 height=1024 depth=1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 1024] to A/3/0/0
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 2048] to A/3/0/0
18:29:21 INFO  c.g.b.Converter - chunk read complete 5/6
18:29:21 INFO  c.g.b.Converter - chunk read complete 6/6
18:29:21 INFO  o.p.TimingLogger - start[1678987761129] time[10] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=1024 zz=0 width=1024 height=1006 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=2048 yy=0 zz=0 width=1018 height=1024 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=2048 yy=1024 zz=0 width=1018 height=1006 depth=1
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=1024 zz=0 width=1024 height=1006 depth=1
18:29:21 INFO  c.g.b.Converter - Reducing active tileHeight to 1015
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/3/0/1
18:29:21 INFO  o.p.TimingLogger - start[1678987761172] time[26] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=1024 yy=0 zz=0 width=509 height=1015 depth=1
18:29:21 INFO  o.p.TimingLogger - start[1678987761172] time[37] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=0 yy=0 zz=0 width=1024 height=1015 depth=1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/2
18:29:21 INFO  o.p.TimingLogger - start[1678987761222] time[14] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=0 xx=0 yy=0 zz=0 width=766 height=507 depth=1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/3
18:29:21 INFO  o.p.TimingLogger - start[1678987761248] time[2] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - chunk read complete 1/1
18:29:21 INFO  o.p.TimingLogger - start[1678987761248] time[3] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=0 xx=0 yy=0 zz=0 width=383 height=253 depth=1
18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/4
18:29:21 INFO  o.p.TimingLogger - start[1678987761296] time[1] tag[getTileDownsampled]
18:29:21 INFO  c.g.b.Converter - chunk read complete 1/1
18:29:21 INFO  o.p.TimingLogger - start[1678987761296] time[1] tag[getChunk]
18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=4 plane=0 xx=0 yy=0 zz=0 width=191 height=126 depth=1
18:29:21 INFO  c.g.c.ConverterTask - Successfully created: w96_A1+A2_test.zarr

18:29:21 INFO  c.g.c.ConverterTask - Completed conversion of 1 files.

</code></pre>
<p>The I open napari (up-to-date) and try to drag an drop the zarr folder</p>
<pre><code class="lang-plaintext">napari: 0.4.17
Platform: Windows-10-10.0.19045-SP0
Python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]
Qt: 5.15.6
PyQt5: 5.15.7
NumPy: 1.23.5
SciPy: 1.10.0
Dask: 2023.1.1
VisPy: 0.11.0
magicgui: 0.6.1
superqt: unknown
in-n-out: 0.1.6
app-model: 0.1.1
npe2: 0.6.2

OpenGL:
  - GL version:  4.6.0 NVIDIA 512.36
  - MAX_TEXTURE_SIZE: 32768

Screens:
  - screen 1: resolution 1920x1080, scale 1.0

Plugins:
  - bfio: 2.3.0 (4 contributions)
  - napari: 0.4.17 (77 contributions)
  - napari-accelerated-pixel-and-object-classification: 0.12.3 (36 contributions)
  - napari-aicsimageio: 0.7.2 (2 contributions)
  - napari-assistant: 0.4.4 (4 contributions)
  - napari-brightness-contrast: 0.1.8 (2 contributions)
  - napari-console: 0.0.7 (0 contributions)
  - napari-crop: 0.1.7 (2 contributions)
  - napari-cupy-image-processing: 0.3.1 (38 contributions)
  - napari-czann-segment: 0.0.16 (2 contributions)
  - napari-griottes: 0.3.9 (12 contributions)
  - napari-label-interpolator: 0.1.0 (2 contributions)
  - napari-layer-details-display: 0.1.5 (2 contributions)
  - napari-ome-zarr: 0.5.2 (2 contributions)
  - napari-plugin-search: 0.1.4 (2 contributions)
  - napari-pyclesperanto-assistant: 0.22.1 (51 contributions)
  - napari-simpleitk-image-processing: 0.4.5 (104 contributions)
  - napari-skimage-regionprops: 0.10.0 (2 contributions)
  - napari-svg: 0.1.6 (2 contributions)
  - napari-time-slicer: 0.4.9 (2 contributions)
  - napari-tools-menu: 0.1.19 (0 contributions)
  - ome-types: 0.3.3 (4 contributions)
</code></pre>
<p>and this is what I get:</p>
<p>ValueError: (‘Shapes do not align: %s’, [(177, 245), (177, 245), (253, 383), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245)])</p> ;;;; <p>You are using a conda env for your napari here right? So only what’s inside the env should matter…<br>
Are you making a fresh env using conda then installing using pip and still getting pyqt5 5.9.7? or you have something else in the env or it has full (old) anaconda in it?</p>
<p>Or try making a fresh env using conda-forge channel:<br>
<code>conda create -y -n napari-env -c conda-forge python=3.9</code><br>
then you can either try pip again or conda:<br>
<code>conda install -c conda-forge napari</code></p> ;;;; <p>I saved x-y values in .csv. Then, opened it in ImageJ and used the two lines macro code you wrote, but it resulted in this: undefined variables xvalues=Table.getColumn(&lt;“x”&gt;)<br>
Please explain to me with which macro and how exactly I can use the x-y values in the csv file to plot a curve on the initial image.</p> ;;;; <h1>
<a name="version-1790-1" class="anchor" href="#version-1790-1"></a>Version 1.79.0</h1>
<p>This version focuses on improving the 3D capabilities of JIPipe by including the following features:</p>
<ul>
<li>a 3D viewer for images</li>
<li>integration of the 3D ROI data type from the 3D ImageJ Suite</li>
<li>porting existing JIPipe ROI processing functionalities to 3D ROI</li>
<li>introducing a new filaments processing feature</li>
</ul>
<h2>
<a name="h-3d-viewer-and-processing-2" class="anchor" href="#h-3d-viewer-and-processing-2"></a>3D viewer and processing</h2>
<p>The JIPipe image viewer received major internal improvements that allowed us to include the basic functions of the <a href="https://imagej.net/plugins/3d-viewer/">ImageJ 3D Viewer</a> into the interface. Please note that the JIPipe 3D viewer is only designed for viewing 3D images and 3D ROI and does not have the full feature set of the ImageJ plugin.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d.png" data-download-href="/uploads/short-url/xWjyjVXe2gMk2HrWIuOTOeDGyT3.png?dl=1" title="version-1.79.0-3d-viewer"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_481x375.png" alt="version-1.79.0-3d-viewer" data-base62-sha1="xWjyjVXe2gMk2HrWIuOTOeDGyT3" width="481" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_481x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_721x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_962x750.png 2x" data-dominant-color="716582"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">version-1.79.0-3d-viewer</span><span class="informations">1154×898 536 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>To allow the processing of 3D data, the functions of the <a href="https://imagej.net/plugins/3d-imagej-suite/">3D ImageJ Suite</a> were included into JIPipe via a dedicated extension (Tools &gt; Manage JIPipe plugins).</p>
<p>Similar to how ImageJ ROI are handled, JIPipe introduces a “3D ROI List” data type that allows to process 3D ROI directly via dedicated nodes. Additionally, the new plugin wraps existing 3D ImageJ Suite functions as nodes.</p>
<p>* Please note that some 3D ImageJ Suite functions were split or slightly renamed to improve the user experience</p>
<h2>
<a name="filaments-3" class="anchor" href="#filaments-3"></a>Filaments</h2>
<p>The new JIPipe version comes with the first iteration of the “Filaments” plugin that focuses around the processing and measuring of filamentous structures represented by a graph.</p>
<p>Filaments are stored in a dedicated data type and are capable of representing 3D structures (X, Y, Z). Additionally, filament vertices can also store the channel and frame, metadata, radius, and value.</p>
<p>The generation of filament data begins with a binary skeleton (“Morphological skeletonize 2D”) that is converted into a filament via “Binary skeleton to 2D filaments”. Small loops and noise are removed via “Smooth filaments”.</p>
<p>Please note that the filaments library is still unfinished and will be expanded with additional functionality in the future.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d.png" data-download-href="/uploads/short-url/3TN3nbpB2OiqPvNfdrGpqdrnRtH.png?dl=1" title="version-1.79.0-filaments"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_481x375.png" alt="version-1.79.0-filaments" data-base62-sha1="3TN3nbpB2OiqPvNfdrGpqdrnRtH" width="481" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_481x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_721x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_962x750.png 2x" data-dominant-color="5B5B5A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">version-1.79.0-filaments</span><span class="informations">1154×898 66.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h2>
<a name="clij2-improvements-4" class="anchor" href="#clij2-improvements-4"></a>CLIJ2 improvements</h2>
<p>All CLIJ2 nodes now have an option “Avoid allocating GPU memory” that will only allocate GPU memory during the processing. Afterwards, the image data is automatically de-allocated.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168.png" data-download-href="/uploads/short-url/bAV0VJjprDxypsmzf3ZSrlYKpn2.png?dl=1" title="1.79.0-clij-nodes"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168_2_517x324.png" alt="1.79.0-clij-nodes" data-base62-sha1="bAV0VJjprDxypsmzf3ZSrlYKpn2" width="517" height="324" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168_2_517x324.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168_2_775x486.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168.png 2x" data-dominant-color="F0F0F0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">1.79.0-clij-nodes</span><span class="informations">784×491 39.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h2>
<a name="additional-improvements-5" class="anchor" href="#additional-improvements-5"></a>Additional improvements</h2>
<ul>
<li>
<img src="https://emoji.discourse-cdn.com/twitter/new.png?v=12" title=":new:" class="emoji" alt=":new:" loading="lazy" width="20" height="20"> Rewire graph tool (toolbar on the left side of the graph) to quickly rewire multiple connections from one input/output to another input/output</li>
<li>
<img src="https://emoji.discourse-cdn.com/twitter/new.png?v=12" title=":new:" class="emoji" alt=":new:" loading="lazy" width="20" height="20"> Copy-paste &amp; deletion of compartments will automatically generate convenient IO interface nodes that preserve the connections</li>
<li>
<img src="https://emoji.discourse-cdn.com/twitter/new.png?v=12" title=":new:" class="emoji" alt=":new:" loading="lazy" width="20" height="20"> Histogram plot: set bin axis limits</li>
<li>
<img src="https://emoji.discourse-cdn.com/twitter/new.png?v=12" title=":new:" class="emoji" alt=":new:" loading="lazy" width="20" height="20"> Node templates/Bookmarks are now shown even when selecting a node (in parameters panel)</li>
<li>
<img src="https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12" title=":white_check_mark:" class="emoji" alt=":white_check_mark:" loading="lazy" width="20" height="20"> Fixed bug that prevented JIPipe from being usable on X remote connections</li>
<li>
<img src="https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12" title=":white_check_mark:" class="emoji" alt=":white_check_mark:" loading="lazy" width="20" height="20"> Fixed various memory leaks</li>
</ul> ;;;; <p>Thanks.<br>
Qt version is the only available one in my school’s clusters, that’s why I used that. Since I was already in a conda environment, I did not create one more inside it, and also I am not sure if it is the right move. I’ll try this in my computer’s anaconda environment (in spider), where I am freer, and I’ll let you know. Thanks again.</p> ;;;; <p>Hi all,<br>
I am new here! I am writing a python fiji script with the help of chatgpt. I would like to sort the rois in the roimanager and chatgpt suggested the following command:</p>
<pre><code class="lang-auto">rm.runCommand('Sort', 'by=Size')
</code></pre>
<p>In the documentation for runCommand I cannot find a second argument for the Sort.<br>
The command works but I am trying to understand how chatgpt was able to suggest such a command when the documentation does not say anything about it.<br>
Can anyone point me to a documentation that has this information or the code?</p>
<p>Thanks in advance<br>
Fotos</p> ;;;; <p>Thanks <a class="mention" href="/u/sebi06">@sebi06</a> for providing the original file and the converted NGFF file.</p>
<p>I am unable to reproduce the issue with missing metadata using NGFF-Converter 1.1.4 and OS X as the top-level <code>.zattrs</code> contains all the expected elements</p>
<pre><code class="lang-auto">(base) sbesson@Sebastiens-MacBook-Pro-2 Downloads % head -n 5 w96_A1+A2_test.zarr/.zattrs 
{
  "bioformats2raw.layout" : 3,
  "plate" : {
    "columns" : [ {
      "name" : "1"
</code></pre>
<p>One difference I found is that the your NGFF seems to have been generated with Bio-Formats 6.12.0 while mine was generated with Bio-Formats 6.11.1:</p>
<pre><code class="lang-auto">(base) sbesson@Sebastiens-MacBook-Pro-2 Downloads % head -n 6 w96_A1+A2_test_zarr/A/1/0/.zattrs 
{
  "multiscales" : [ {
    "metadata" : {
      "method" : "loci.common.image.SimpleImageScaler",
      "version" : "Bio-Formats 6.12.0"
    },
(base) sbesson@Sebastiens-MacBook-Pro-2 Downloads % head -n 6 w96_A1+A2_test.zarr/A/1/0/.zattrs 
{
  "multiscales" : [ {
    "metadata" : {
      "method" : "loci.common.image.SimpleImageScaler",
      "version" : "Bio-Formats 6.11.1"
    },
</code></pre>
<p>Which version of NGFF-Converter was used? And under which operating system?</p> ;;;; <p>I am attempting to colocalize 3 fluorescent signals using the Colocalization plugin (Pierre Bourdoncle). I am donig this by first colocalizing two of the signals then using the 8-bit output from that colocalization to colocalize with the 3rd. For some reason when I do this in one order Ch0:Ch02 then Ch03, this works great…but when I do it in another order ch0:ch03 then ch02, the plugin will not recognize the colocalization. Any suggestions?</p> ;;;; <p>I’m glad this works for you!</p>
<p>Could you describe what you mean by “ImageJ is not really able to split the channels”. What did you see?</p> ;;;; <p>Here is the corresponding CZI:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.dropbox.com/s/ka6os6djp1iiuyk/w96_A1%2BA2_test.czi?dl=0">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b05b6a5aef581aef17d6d558f8dfa5ef3c697208.png" class="site-icon" width="32" height="32">

      <a href="https://www.dropbox.com/s/ka6os6djp1iiuyk/w96_A1%2BA2_test.czi?dl=0" target="_blank" rel="noopener nofollow ugc">Dropbox</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png" class="thumbnail onebox-avatar" width="160" height="160">

<h3><a href="https://www.dropbox.com/s/ka6os6djp1iiuyk/w96_A1%2BA2_test.czi?dl=0" target="_blank" rel="noopener nofollow ugc">w96_A1+A2_test.czi</a></h3>

  <p>Shared with Dropbox</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>I’m not a linux expert, but this <code>module load anaconda/3.21.05</code> seems like a very old version of anaconda. Also anaconda typically comes with a number of packages, which in this case are likely to be out-dated, which is probably why you end up with very old Qt that we don’t support.</p>
<p>Perhaps you can consider using miniforge or mambaforge?</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/conda-forge/miniforge">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/conda-forge/miniforge" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983.png 2x" data-dominant-color="F2F4F1"></div>

<h3><a href="https://github.com/conda-forge/miniforge" target="_blank" rel="noopener nofollow ugc">GitHub - conda-forge/miniforge: A conda-forge distribution.</a></h3>

  <p>A conda-forge distribution. Contribute to conda-forge/miniforge development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
This will give you a modern conda with the <code>conda-forge</code> channel by default, which is also most up-to-date.<br>
Then you can use</p>
<pre><code class="lang-auto">conda create -y -n napari-env -c conda-forge python=3.9
conda activate napari-env
python -m pip install "napari[all]"
</code></pre>
<p>if you want to use pip or just:</p>
<pre><code class="lang-auto">conda create -y -n napari-env -c conda-forge python=3.9 napari
</code></pre>
<p>If you want a pure conda solution.</p> ;;;; <p>If you right click on the class and select by class (as shown in your image), you can generate a line of code in the Workflow that should read something like<br>
<code>selectObjectsByClassification("Dermis");</code><br>
As long as you get the class name or class names right, it will select those objects at that point in the script. You can also resetSelection(), I think, to unselect previously selected objects.</p> ;;;; <p>Per the title, I am trying to control a ViALUX V-7000 digital micromirror device in Micro-Manager. Unfortunately, there does not appear to be a device driver for this particular DMD. However, since it is based on the same Texas Instruments DMD Discovery 4100 chipset used in just about every other DMD, I was hoping perhaps someone else has looked into this, and knows of some way to control this device in MM?</p>
<p>For reference, I have contacted ViALUX, but to their knowledge, no plugin currently exists. They do provide full API support for their controller suite, ALP-4 <em>basic</em>, even going as far as including an example visual studio project, so I guess it is not out of the question that someone already tried to write a device adapter for it.</p>
<p>Unfortunately, since this particular device does not show up as a monitor, I cannot use the GenericSLM adapter. The <a href="https://micro-manager.org/GenericSLM" rel="noopener nofollow ugc">GenericSLM</a> page does mention that a very similar product from Digital Light Innovations can be converted to DVI with an adapter, but as far as I can tell, ViALUX does not have a similar converter. If someone knows a way, that too would be most helpful!</p> ;;;; <p>I am a student and my project group wants to convert a very large OpenCV Matrix (about ten gigabytes) into 3D Texture, but the maximum resource capacity we have is 2 gigabytes for the 3D texture. we don’t know how to optimize this matrix to be able to create the texture. we are working with UNITY tool and it is for 3d visualization project</p> ;;;; <p>The easiest way to solve this issue: <a href="https://mac.eltima.com/how-to-open-jar-file-mac.html" class="inline-onebox" rel="noopener nofollow ugc">How to Open a JAR File on Mac Computer | Commander One</a></p> ;;;; <p>None of those images conforms to the TIFF specification, even the “working” one. See for example the warnings of libtiff’s <code>tiffinfo</code> tool. Check if there is a software update that fixes the issue or contact Vilber support.</p>
<pre><code class="lang-auto">&gt; tiffinfo imageNotWorking.Tif
TIFFReadDirectoryCheckOrder: Warning, Invalid TIFF directory; tags are not sorted in ascending order.
TIFFReadDirectory: Warning, Unknown field with tag 1807 (0x70f) encountered.
TIFFReadDirectory: Warning, Unknown field with tag 1808 (0x710) encountered.
TIFFFetchNormalTag: Warning, ASCII value for tag "ImageDescription" does not end in null byte.
TIFFFetchNormalTag: Warning, IO error during reading of "Tag 1807"; tag ignored.
TIFFFetchNormalTag: Warning, IO error during reading of "Tag 1808"; tag ignored.
=== TIFF directory 0 ===
TIFF Directory at offset 0x8006be (8390334)
  Subfile Type: (0 = 0x0)
  Image Width: 2048 Image Length: 2048
  Resolution: 300, 300 (unitless)
  Bits/Sample: 16
  Compression Scheme: None
  Photometric Interpretation: min-is-black
  Samples/Pixel: 1
  Rows/Strip: 2048
  Planar Configuration: single image plane
  ImageDescription:
</code></pre> ;;;; <p>Hi,<br>
I’m attempting to use MIB for segementaiton of FIB-SEM data. I’ve created a model from a subsection of the a datastack and I’m trying to used DeepMIB to train a 2D U-net model to detect 4 materials.</p>
<p>I’ve followed your 2D U-net tutorial and it works well until I get to the training step. I use the following input file and parameters:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be1d7062de5936bf6d8226c2feb09dc2d45f477a.png" data-download-href="/uploads/short-url/r7PN76nOH9qyTictOXyOyd83aFs.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be1d7062de5936bf6d8226c2feb09dc2d45f477a.png" alt="image" data-base62-sha1="r7PN76nOH9qyTictOXyOyd83aFs" width="373" height="500" data-dominant-color="F2F2F3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">398×533 8.77 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4b715201fe72f80cc70c65720701ca36b0d3d0a.png" data-download-href="/uploads/short-url/pMGcdW4xre64apbFAjd0HShCNyG.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4b715201fe72f80cc70c65720701ca36b0d3d0a.png" alt="image" data-base62-sha1="pMGcdW4xre64apbFAjd0HShCNyG" width="690" height="429" data-dominant-color="CFE4F1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">986×614 30.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>When I start training rapidly I get the following error. I’m not sure how to proceed.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/2/e216e51f2d8d0ffcdd812f671adee749a9453ca8.png" alt="image" data-base62-sha1="wg4XyJfDfvfDiJexqxJSlgEPEso" width="437" height="213"></p>
<p>I use this version of MIB.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b1217fe9793b501560ab2f99aae1483fc2afc73.jpeg" alt="image" data-base62-sha1="cZEdmaG0KUv8xGTpjPN2w9fc3F9" width="668" height="441"></p>
<p>Cheers<br>
Jon</p> ;;;; <p>Hi <a class="mention" href="/u/nobel">@nobel</a>,<br>
replace the line with</p>
<pre><code class="lang-auto">run("Enhance Local Contrast (CLAHE)", "blocksize=127 histogram=256 maximum=3 mask=*None* fast_(less_accurate)");
</code></pre>
<p>You can activate the macro recorder and run the command from the menu <code>(Process › Enhance Local Contrast (CLAHE)</code>, to check the syntax.</p>
<p>Best,<br>
Volker</p> ;;;; <p>Hi <a class="mention" href="/u/mzugravu">@mzugravu</a> ,<br>
happy that it works for you.</p>
<p>You don’t actually need to display it, but it has to become the active image, which you can also achieve when displaying it in batch mode.</p>
<p>The <code>IJ.open</code> does not return anything (input_image will be None) and the SIFT plugin does not take an image as parameter. It works because the <code>open</code>opens and displays the image which becomes the active image that is used by the SIFT plugin.<br>
So to avoid confusion, you should better write:</p>
<pre><code class="lang-auto">def align_stack(source_file, output_file):
    IJ.open(source_file)
    IJ.run("Linear Stack Alignment with SIFT", "initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate")
    image = IJ.getImage()
    IJ.save(image, output_file)
</code></pre>
<p>Best,<br>
Volker</p> ;;;; <p>Hi,</p>
<p>I am analysing Picroserius red stained skin sections viewed under polarised light. I want select that red, green and yellow areas. To do this I use 2 threshold, one for red and one for green. To get the yellow I apply the green threshold to the red annotation to get the overlap. This works well but means that I need to open each image, I have a lot of images and qupath tends to get problems when I frequently open and close images. Currently I have 3 scripts and this is the workflow:</p>
<p>1 Run the script that detects the workflow<br>
Select the dermal annotation<br>
2 Run the script that detects the green and red areas<br>
Select the red annotation<br>
3 Run the script that detects green area but creates a detection called yellow as it is only applied to the red area</p>
<p>I would like to have this as one script that automatically selects the dermis annotation before running the red:green script and then automatically selects the red annotation before running the yellow script.</p>
<p>Sadly I don’t have any experience with groovy or java so I’m currently trying to learn.</p>
<p>This is an example of how I would like the annotation to look in the end. The green strip is a membrane used for embedding but present in some of the pictures so I don’t mind opening those specific ones.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934.jpeg" data-download-href="/uploads/short-url/gBp0csd9UbU0PhqndvD1f3A1KTi.jpeg?dl=1" title="Screenshot 2023-03-16 at 16.15.40" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_690x477.jpeg" alt="Screenshot 2023-03-16 at 16.15.40" data-base62-sha1="gBp0csd9UbU0PhqndvD1f3A1KTi" width="690" height="477" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_690x477.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_1035x715.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_1380x954.jpeg 2x" data-dominant-color="1C2F17"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-16 at 16.15.40</span><span class="informations">1920×1328 145 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thank you in advance</p> ;;;; <p>The Nikon Imaging Center at Harvard Medical School (NIC@HMS) is seeking candidates for a 2-3 year (renewable annually) post-doctoral training fellowship in advanced optical microscopy techniques and core facility management.</p>
<p>Optical microscopy has become central to progress in many areas of science. At the same time, the complexity of instruments and quantitative imaging experiments has dramatically increased, with many requiring extensive expertise to operate. Microscopy facilities managed by PhD-level scientists who advise and train researchers on imaging experimental design, the best instruments to use for their experiments, and proper use of instruments have become essential sources of expertise in many research institutions, and core facility management has become a stimulating career path for scientists with experience in advanced quantitative microscopy techniques and an interest in facilitating science broadly.</p>
<p>The NIC@HMS Fellow will learn advanced quantitative microscopy techniques including confocal, TIRF, FRET, FRAP, photo-activation, single-molecule imaging, light sheet and super-resolution microscopy, and key skills needed to manage a large, heavily-used core facility. The Fellow will learn to train core facility users to select and apply the appropriate techniques, using a wide range of biological specimens and experimental approaches. The Fellow will also have the opportunity to identify additional responsibilities that match her/his interests, such as: organizing and teaching microscopy courses in the NIC@HMS Educational Program; organizing microscopy discussion groups/journal clubs; troubleshooting equipment problems; developing protocols for testing equipment performance; designing and/or implementing novel or custom imaging techniques.</p>
<p>We are seeking candidates with optical microscopy experience, a PhD in biology, physics, or a related discipline, and interest in a career in optical microscopy. Start date is flexible. For more information and to apply, please visit <a href="https://microfellows.hms.harvard.edu/">microfellows.hms.harvard.edu</a>. Applications will be considered as they are received, but should be submitted no later than April 6, 2023.</p>
<p>Jennifer C. Waters, PhD<br>
Director of the Nikon Imaging Center &amp; Lecturer in Cell Biology, Harvard Medical School<br>
Chan Zuckerberg Initiative Imaging Scientist<br>
<a href="https://twitter.com/jencwaters" class="onebox" target="_blank" rel="noopener">https://twitter.com/jencwaters</a></p><aside class="onebox allowlistedgeneric" data-onebox-src="https://www.youtube.com/microcourses">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d17df8ddb48817e528e9038bc66d304cf122d9c5.png" class="site-icon" width="16" height="16">

      <a href="https://www.youtube.com/microcourses" target="_blank" rel="noopener">YouTube</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2_2_500x500.jpeg" class="thumbnail onebox-avatar" width="500" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2.jpeg 2x" data-dominant-color="EAEAEA">

<h3><a href="https://www.youtube.com/microcourses" target="_blank" rel="noopener">Microcourses</a></h3>

  <p>We are a team of light microscopists from core facilities at Harvard Medical School. We teach microscopy at HMS, and run the Quantitative Imaging: From Acquisition to Analysis course at Cold Spring Harbor Laboratory. On this channel, you can find...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>A few people made the macros and eventually left and I have been tasked with figuring it all out even though I HAVE NO experience in FIJI whatsoever. In short, I’ve hit a wall. I looked through the command list and compared it to the Macros Code and I see the line that is unrecognized but I am not sure why its not recognized because the line is there and there isn’t anything I can see that is wrong with it.<br>
The window that open says Macro Error unrecognized command “enhance local contrast (CLAHE)”, “blocksize=127 histogram=256 maximum=3 mask = *None fast_(less_accurate)” &lt;)&gt; ;<br>
I opened the debug window and got nada. Can someone please help me to get this macro to work! Thank you so much!!</p> ;;;; <p>Hello,</p>
<p>We encounter issues while importing specific .tif images on OMERO.<br>
Tiff Images are generated by a Vilber Lourmat machine (software FusionCapt Advanced Solo 7) and there are actually two types of tiffs we can use to export:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/3/33ed950f53aae4e061ce65edefb504a08f9e1a1a.png" alt="image" data-base62-sha1="7pnmR5dX25irzpvYgMoDcvdIJKO" width="645" height="66"></p>
<p>If images are encoded using <code>16 bits Tagged Image File Format</code>, then no images can be open by bioformats. It throws the following exception when we try to import them on OMERO, using the latest version of OMERO.insight.</p>
<pre><code class="lang-auto">java.io.EOFException: Attempting to read beyond end of file.
	at loci.common.NIOFileHandle.readInt(NIOFileHandle.java:415)
	at loci.common.RandomAccessInputStream.readInt(RandomAccessInputStream.java:564)
	at loci.common.RandomAccessInputStream.readUnsignedInt(RandomAccessInputStream.java:574)
	at loci.formats.tiff.TiffParser.getIFDValue(TiffParser.java:644)
	at loci.formats.tiff.TiffParser.fillInIFD(TiffParser.java:526)
	at loci.formats.in.MinimalTiffReader.initFile(MinimalTiffReader.java:483)
	at loci.formats.in.BaseTiffReader.initFile(BaseTiffReader.java:609)
	at loci.formats.FormatReader.setId(FormatReader.java:1443)
	at loci.formats.in.TiffDelegateReader.setId(TiffDelegateReader.java:92)
	at loci.formats.ImageReader.setId(ImageReader.java:849)
	at ome.formats.importer.OMEROWrapper$4.setId(OMEROWrapper.java:167)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at loci.formats.ChannelFiller.setId(ChannelFiller.java:234)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at loci.formats.ChannelSeparator.setId(ChannelSeparator.java:293)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at loci.formats.Memoizer.setId(Memoizer.java:662)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at ome.formats.importer.ImportCandidates.singleFile(ImportCandidates.java:427)
	at ome.formats.importer.ImportCandidates.handleFile(ImportCandidates.java:576)
	at ome.formats.importer.ImportCandidates.execute(ImportCandidates.java:384)
	at ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:222)
	at ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:174)
	at org.openmicroscopy.shoola.env.data.OMEROGateway.getImportCandidates(OMEROGateway.java:5894)
	at org.openmicroscopy.shoola.env.data.OmeroImageServiceImpl.importFile(OmeroImageServiceImpl.java:1104)
	at org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.importFile(ImagesImporter.java:103)
	at org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.access$000(ImagesImporter.java:49)
	at org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter$1.doCall(ImagesImporter.java:127)
	at org.openmicroscopy.shoola.env.data.views.BatchCall.doStep(BatchCall.java:144)
	at org.openmicroscopy.shoola.util.concur.tasks.CompositeTask.doStep(CompositeTask.java:226)
	at org.openmicroscopy.shoola.env.data.views.CompositeBatchCall.doStep(CompositeBatchCall.java:126)
	at org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.exec(ExecCommand.java:165)
	at org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.run(ExecCommand.java:276)
	at org.openmicroscopy.shoola.util.concur.tasks.AsyncProcessor$Runner.run(AsyncProcessor.java:91)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:509)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:373)
	at loci.common.NIOFileHandle.readInt(NIOFileHandle.java:413)
	... 34 more

	at org.openmicroscopy.shoola.env.data.util.Status.update(Status.java:608)
	at ome.formats.importer.ImportCandidates.safeUpdate(ImportCandidates.java:536)
	at ome.formats.importer.ImportCandidates.singleFile(ImportCandidates.java:471)
	at ome.formats.importer.ImportCandidates.handleFile(ImportCandidates.java:576)
	at ome.formats.importer.ImportCandidates.execute(ImportCandidates.java:384)
	at ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:222)
	at ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:174)
	at org.openmicroscopy.shoola.env.data.OMEROGateway.getImportCandidates(OMEROGateway.java:5894)
	at org.openmicroscopy.shoola.env.data.OmeroImageServiceImpl.importFile(OmeroImageServiceImpl.java:1104)
	at org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.importFile(ImagesImporter.java:103)
	at org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.access$000(ImagesImporter.java:49)
	at org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter$1.doCall(ImagesImporter.java:127)
	at org.openmicroscopy.shoola.env.data.views.BatchCall.doStep(BatchCall.java:144)
	at org.openmicroscopy.shoola.util.concur.tasks.CompositeTask.doStep(CompositeTask.java:226)
	at org.openmicroscopy.shoola.env.data.views.CompositeBatchCall.doStep(CompositeBatchCall.java:126)
	at org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.exec(ExecCommand.java:165)
	at org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.run(ExecCommand.java:276)
	at org.openmicroscopy.shoola.util.concur.tasks.AsyncProcessor$Runner.run(AsyncProcessor.java:91)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException: Attempting to read beyond end of file.
	at loci.common.NIOFileHandle.readInt(NIOFileHandle.java:415)
	at loci.common.RandomAccessInputStream.readInt(RandomAccessInputStream.java:564)
	at loci.common.RandomAccessInputStream.readUnsignedInt(RandomAccessInputStream.java:574)
	at loci.formats.tiff.TiffParser.getIFDValue(TiffParser.java:644)
	at loci.formats.tiff.TiffParser.fillInIFD(TiffParser.java:526)
	at loci.formats.in.MinimalTiffReader.initFile(MinimalTiffReader.java:483)
	at loci.formats.in.BaseTiffReader.initFile(BaseTiffReader.java:609)
	at loci.formats.FormatReader.setId(FormatReader.java:1443)
	at loci.formats.in.TiffDelegateReader.setId(TiffDelegateReader.java:92)
	at loci.formats.ImageReader.setId(ImageReader.java:849)
	at ome.formats.importer.OMEROWrapper$4.setId(OMEROWrapper.java:167)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at loci.formats.ChannelFiller.setId(ChannelFiller.java:234)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at loci.formats.ChannelSeparator.setId(ChannelSeparator.java:293)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at loci.formats.Memoizer.setId(Memoizer.java:662)
	at loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)
	at ome.formats.importer.ImportCandidates.singleFile(ImportCandidates.java:427)
	... 16 more
Caused by: java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:509)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:373)
	at loci.common.NIOFileHandle.readInt(NIOFileHandle.java:413)
	... 34 more
</code></pre>
<p>If we now encode them using <code>Compatibility Plus 16 bit Tagged Image File Format</code>, then some images can be imported on OMERO but some other not and we don’t know why (all images have been taken with the same machine). For images which cannot be imported, the exception is the same.</p>
<p>I attach here a zip file with a couple of images to that you can test and try to reproduce the bug.<br>
<a class="attachment" href="/uploads/short-url/eOUPEP6nm63KpsrkGzIHT1VKNtH.zip">images.zip</a> (11.9 MB)</p>
<p>Do you know where it is coming from and what we can do to import those files on OMERO ?<br>
Thanks for your help,</p>
<p>Rémy.</p> ;;;; <p>Hello.<br>
It’s my first time using DeepLabCut. Everytime I try to create a new project i receive the following error:</p>
<p>DirectWrite: CreateFontFaceFromHDC() failed (Indica um erro em um arquivo de entrada, como um arquivo de fonte.) for QFontDef(Family=“8514oem”, pointsize=9, pixelsize=20, styleHint=5, weight=400, stretch=100, hintingPreference=0) LOGFONT(“8514oem”, lfWidth=0, lfHeight=-20) dpi=144</p>
<p>Couldn’t find any solution for this.</p>
<p>Thank you in advance.</p> ;;;; <p>Hi <a class="mention" href="/u/jni">@jni</a>,</p>
<p>looks like one minute was all you need - this solution worked out for me <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Thanks so much,<br>
Christina</p> ;;;; <p>In Excel, save the x-y values in .csv (coma-separated values) format. In ImageJ, write a macro that opens the .csv file and uses</p>
<p>xvalues =Table.getColumn(“x”);<br>
yvalues =Table.getColumn(“y”);</p>
<p>to retrieve the values.</p> ;;;; <p>Thank you so much. Now it works. Actually I don’t think I need to display it, as long if <code>IJ.open</code> does not display it. Even before when using <code>IJ.openImage</code> it did run the SIFT. I just simply failed to retrieve the output… I could not run <code>inputImage.show()</code> as I am running headlessly. I run my script on a cluster so no GUI available.</p>
<p>This is my script updated.</p>
<pre><code class="lang-auto">def align_stack(source_file, output_file):
    input_image = IJ.open(source_file)
    IJ.run(input_image,"Linear Stack Alignment with SIFT", "initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate")
    image = IJ.getImage()
    IJ.save(image, output_file)
</code></pre> ;;;; <p>Thank you <a class="mention" href="/u/mark_zaidi">@Mark_Zaidi</a>, I will give it a go and see whether I can get improve on it.  For the over segmentation, I ran out without a script ie just using Qupath’s native built-ins. I’ve not actually observed this phenomenon before, not sure whether I accidentally changed a setting or two by alternating between scripting and running the inbuilt plugins.</p>
<p><a class="mention" href="/u/orchard">@Orchard</a>, thank you for your kind offer. I’ll see how far I get on with Mark’s suggestions and whether I manage to get cellpose up and running on my Mac…I may yet take you up on your offer!</p> ;;;; <p>Thanks <a class="mention" href="/u/lldelisle">@lldelisle</a><br>
Comment added</p> ;;;; <aside class="quote no-group" data-username="creativerror404" data-post="1" data-topic="74820" data-full="true">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png" class="avatar"> Kali Woods:</div>
<blockquote>
<p><strong>2.</strong> Deconvolution seems to have a number of potential pitfalls, especially if used in conjunction with quantitative image analysis.5</p>
<p><em><strong>2.a.</strong></em> I was wondering at the frequency that people measure their PSF for deconvolution. The book Microscope Image Processing Second Edition 2022, implies that experimental PSF for deconvolution should be tailored to and measured prior to the experiment that it will be used on.10 I was wondering if cores actually measure at this frequency for deconvolution or if they gather the PSF weekly, or monthly instead?</p>
</blockquote>
</aside>
<p>First you can use a simulated PSF  such as the one created by PSFGenerator for wide field fluorescence.<br>
There is 2 main reasons why the actual PSF deviate from the theoretical one:</p>
<ul>
<li>static aberration due to you microscope/objective , that should evolve smoothly with time and in a quite well controlled environment and probalbly making a psf from time to time is sufficient to control this part</li>
<li>aberration due to the oil/coverslip/sample. The main aberration is the spherical aberration due to refractive index mismatch between oil/coverslip/mounting medium. This can be mitigated using objective correcting collar but never perfectly. Keep in mind that the oil refractive index vary with temperature and as consequence the spherical aberration is vquite sensitive to the temperature and may vary during quite rapidly with time.</li>
</ul>
<p>If you need precise psf you will probably need to rely on a either beads in your medium or autocalibration procedure such as blind deconvolution (with EpiDemic) or PSF calibration as in [ Jizhou Li, Feng Xue, Fuyang Qu, Yi-Ping Ho, and Thierry Blu, “On-the-fly estimation of a microscopy point spread function,” Opt. Express <strong>26</strong>, 26120-26133 (2018)]</p>
<aside class="quote no-group" data-username="creativerror404" data-post="1" data-topic="74820" data-full="true">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png" class="avatar"> Kali Woods:</div>
<blockquote>
<p><em><strong>2.b.</strong></em> Assuming that a measured PSF is on hand, and a 3D image is collected what deconvolution algorithm would be recommended for these three situations:<br>
i. high signal, low noise<br>
ii. high signal, high noise<br>
iii. low signal, low noise.</p>
</blockquote>
</aside>
<p>I think that an algorithm that is efficient in the worse condition may be also efficient in more favorable solution so there must a single algorithm fits all for your situation. However in very favorable case (like case i) crude algorithm like  Richardson Lucy may work well so you can you whatever toolbox you want as most of them implement RL algorithm.</p>
<aside class="quote no-group quote-modified" data-username="creativerror404" data-post="1" data-topic="74820" data-full="true">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png" class="avatar"> Kali Woods:</div>
<blockquote>
<p><em><strong>2.c.</strong></em> I wanted to double check that the deconvolution follows after flat-field correction/background subtraction. Are there any other common pre-processing steps people perform prior to deconvolution? I know that deconvolution can increase the impact of artifacts and noise, does anyone perform deconvolution post noise removal?<br>
[…]<br>
<strong>4.</strong> Lastly, double checking that my ordering of Flatfield/background subtraction, Noise removal, and Deconvolution is correct. Does anyone perform any other standard quality control methodologies that limit batch effects between samples that I’m missing?<br>
i. Copy of image raw<br>
ii. Flatfield correction<br>
iii. Noise removal<br>
iv. Deconvolution<br>
vi. Pre-processing</p>
</blockquote>
</aside>
<p>Never never never, perform noise removal before deconvolution. Doing so you will introduce spatial correlation that will screw deconvolution algorithm.</p> ;;;; <p>Hi <a class="mention" href="/u/mzugravu">@mzugravu</a>,<br>
yes, it should be like that, opening the image without displaying and passing it to the plugin. However in this case it’s not possible, because this plugin does not accept an image as input, it grabs the active image from ImageJ. Therefore you need to display the image, so that the plugin can get it. To do this either replace <code>openImage</code> with <code>open</code> or call <code>show</code> on the image after opening it.<br>
For example:</p>
<pre><code class="lang-auto">from ij import IJ
def align_stack(source_file, output_file):
    inputImage = IJ.openImage(source_file)
    inputImage.show()
    IJ.run("Linear Stack Alignment with SIFT", "initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate")
    image = IJ.getImage()
    IJ.save(image, output_file)
    inputImage.close()
    image.close()
    
align_stack("/home/baecker/in.tif", "/home/baecker/out.tif")
</code></pre>
<p>Best,<br>
Volker</p> ;;;; <p>Thanks - I downloaded the NGFF plate, but I wondered if you could share the original file, so we can test the conversion?</p> ;;;; <p>Hi all,</p>
<p>I am currently working on my thesis about pose estimation using deep learning algorithms. Now, I am at a stage that I need obtain a comparison between software tools DeepLabCut and SLEAP on the same set of datasets. For the purpose, I need to manipulate the loss functions implemented in DeepLabCut and change it to become the same as in SLEAP, to get comparison of performances under that circumstance. I want to ask, from which portion of DeepLabCut’s source code can I play with the implemented loss functions?</p>
<p>Best regards,<br>
Özgür</p> ;;;; <p>Hi <a class="mention" href="/u/volker">@volker</a>,</p>
<p>Thank you for replying.</p>
<p>I’ve tried to add this line as you suggested <code>output = IJ.getImage()</code> after I run the plugin but I get this  <em>There are no images open</em>.</p>
<p>I am not sure I get the part where you say the plugin is not a plugin-filter… I believe I need to open the image like that and then pass it to the plugin because if I don’t do it I get again <em>There are no images open</em>.</p> ;;;; <p>The file is here:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.dropbox.com/s/t04gmbku6knxilc/w96_A1%2BA2_test_zarr.zip?dl=0">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/9303f04ca4e54b7a86074e7efd6051d682695017.png" class="site-icon" width="32" height="32">

      <a href="https://www.dropbox.com/s/t04gmbku6knxilc/w96_A1%2BA2_test_zarr.zip?dl=0" target="_blank" rel="noopener nofollow ugc">Dropbox</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f1f990cbac6b5571218c9046e9dead200fe6f1b.png" class="thumbnail onebox-avatar" width="160" height="160">

<h3><a href="https://www.dropbox.com/s/t04gmbku6knxilc/w96_A1%2BA2_test_zarr.zip?dl=0" target="_blank" rel="noopener nofollow ugc">w96_A1+A2_test_zarr.zip</a></h3>

  <p>Shared with Dropbox</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>thanks! I will try!</p> ;;;; <p>Out of curiousity, I asked this question to Bing Compose (which recently is revealed to be using GPT-4):</p>
<blockquote>
<p>Write a QuPath Groovy script to select annotations with intensity greater than 750 and change their class name from “Islet” to “NE-islet”</p>
</blockquote>
<p>and got this result:</p>
<pre><code class="lang-auto">// Get all annotations in the current image
def annotations = getAnnotationObjects()

// Loop through each annotation
for (annotation in annotations) {
  // Get the mean intensity value of the annotation
  def intensity = annotation.getMeasurementList().getMeasurementValue('Intensity')
  
  // Check if the intensity is greater than 750 and the class name is "Islet"
  if (intensity &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {
    // Change the class name to "NE-islet"
    annotation.setPathClass(getPathClass('NE-islet'))
  }
}

// Update the display
fireHierarchyUpdate()
</code></pre>
<p>Honestly, I think that’s impressive and I wouldn’t change much, if at all, to the structure of the script.</p>
<p>However, you will need to change the measurement name to match the exact measurement you made to the annotations, i.e. change <code>'Intensity'</code> to whatever measurement name you got. Also, you probably don’t need to use <code>fireHierarchyUpdate()</code> in QuPath v0.4.x anymore.</p>
<p>EDIT: I forgot that <code>getMeasurementValue()</code> is now deprecated, and get() should be used instead:</p>
<pre><code class="lang-auto">// Get all annotations in the current image
def annotations = getAnnotationObjects()

// Loop through each annotation
for (annotation in annotations) {
  // Get the mean intensity value of the annotation
  def intensity = annotation.measurements.get('Intensity') // or annotation.measurements['Intensity']
  
  // Check if the intensity is greater than 750 and the class name is "Islet"
  if (intensity &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {
    // Change the class name to "NE-islet"
    annotation.setPathClass(getPathClass('NE-islet'))
  }
}
</code></pre> ;;;; <p>Hello! Thanks you for the open source software, It is proving to be really useful for my current dataset. It has been great reading the discussion in the forum.</p>
<p>I do however need to clear a number of questions that I could not find answers to in the FAQ and the forum.</p>
<p>My dataset is stacks of time series microCT data and although I primarily use Avizo, due to the circumstances of the acquisition of these images there are differences in contrast in the regions of the image for same phases and to some extent across time points as well.</p>
<p>To give an example:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d.jpeg" data-download-href="/uploads/short-url/8Oec2wSztLNcILhogh0tx5vd19P.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d_2_690x419.jpeg" alt="image" data-base62-sha1="8Oec2wSztLNcILhogh0tx5vd19P" width="690" height="419" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d_2_690x419.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d_2_1035x628.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d.jpeg 2x" data-dominant-color="949996"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1132×688 76.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Here the region by the red arrow is the same phase and as you can see the results are great when trained with ilastik.</p>
<p>The problem is that this took about 9-10 hours to process a stack of 1500 images(2016x2016) using an autocontext workflow, which considering the number of scans I have is a problem.</p>
<p>Reading through this forum I have optimized the workflow a lot: by using smaller set of training data, sparse annotations, reducing the number of features selected and modifying the config files. It is now down to about 5 hours per stack, still not ideal but if I cannot reduce it further then its livable.</p>
<p>Now to the questions:</p>
<ul>
<li>
<p>I have to train a new workflow for each type of sample(considerable differences in structure), this is fine, however It seems that the project requires accompanying .h5 files to function, this balloons already large amount of data. But looking at the size of the .ilp file it looks like it already includes the raw data? It’s a bit redundant having to save the same data twice. <em>Is it possible to use the trained model without having the raw files with it?</em></p>
</li>
<li>
<p>There are 4 phases that I have to separate, reading through the forum it is recommended to export the probabilities and then threshold(using a value of 0.5) these to obtain the phases. I some scans where a specific region could correspond to one of two classes and they might have very similar probabilities, My question is what happens in regions where no class has the value 0.5. <em>Is it not possible to segment that region? Also could the probability export be in 8-bit?</em><br>
eg:( probabilities of the two phases, zoomed-in)<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb.jpeg" data-download-href="/uploads/short-url/tqN7rK25Wqxkif2e7FoTcxOAy8j.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb_2_689x364.jpeg" alt="image" data-base62-sha1="tqN7rK25Wqxkif2e7FoTcxOAy8j" width="689" height="364" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb_2_689x364.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb_2_1033x546.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb.jpeg 2x" data-dominant-color="8B8B8B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1279×676 69.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c.jpeg" data-download-href="/uploads/short-url/kaNwJdg95Ubu7RnGKDi1UsuNYEk.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c_2_689x376.jpeg" alt="image" data-base62-sha1="kaNwJdg95Ubu7RnGKDi1UsuNYEk" width="689" height="376" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c_2_689x376.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c_2_1033x564.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c.jpeg 2x" data-dominant-color="7CA2AC"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1268×691 86.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
</li>
<li>
<p>Regarding the auto-context workflow, generally I’ve seen that I need to annotate the 4 classes in the 1st stage and then the foreground and the background in the 2nd stage. <em>But how do I get the 4 classes from the export of 2nd stage then?</em> The way I’m doing it right now is in reverse, where separating the foreground in step 1 which works great and then annotating the classes in stage 2. <em>Am I approaching this wrong?</em></p>
</li>
</ul>
<p>*And finally regarding the <em>OOB error, what value is acceptable enough?</em> I know that it need to be as low as possible but what is an objective difference between 0.0001 and 0.001 when visually I cannot tell much difference between the two but the computational time is long. Some minor differences I do notice is for very few pixels and those are not really relevant in the context of the whole volume. Another issue is the multiple workflows for each volume, so might be good to know the maximum acceptable value to aim for each.</p>
<p>The quality of data I have is not the best and there is an acceptable degree of error that can be attributed to the circumstances of the scans, so a degree of uncertainty if unavoidable. But it would be great if I can optimize it further to have as best as feasible.</p>
<p>Thank you again! Any information regarding this would be helpful.</p> ;;;; <p>Hi <a class="mention" href="/u/thomasboudier">@ThomasBoudier</a>,</p>
<p>first thanks for the new options in the 3D Suite (e.g. the direct measurements in standard results tables).</p>
<aside class="quote no-group" data-username="ThomasBoudier" data-post="8" data-topic="78535">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/thomasboudier/40/17506_2.png" class="avatar"> Thomas Boudier:</div>
<blockquote>
<p>sometimes I forget my own development</p>
</blockquote>
</aside>
<p>I know that too well.</p>
<p>Yesterday I just had a quick look at the new V4 3D Roi Manager and figured out that the live ROI button might not work correctly. At least I could not manage to get the ROIs displayed on the image. However, I might have missed something.<br>
If you prefer that, I can also create a GitHub issue.</p> ;;;; <p>Thank you very much for your reply,<br>
I work in a conda environment in ubuntu which I create with:</p>
<p>module load anaconda/3.21.05<br>
module load cuda/11.3<br>
module load cudnn/8.2.1/cuda-11.X<br>
module load gcc/9.3.0<br>
module load cmake/3.24.0<br>
module load Qt/5.12.12</p>
<p>and</p>
<p>output of <code>napari --info</code>:</p>
<p>.local/lib/python3.8/site-packages/napari/_qt/<strong>init</strong>.py:50: UserWarning:<br>
napari was tested with QT library <code>&gt;=5.12.3</code>.<br>
The version installed is 5.9.7. Please report any issues with<br>
this specific QT version at <a href="https://github.com/Napari/napari/issues" class="inline-onebox" rel="noopener nofollow ugc">Issues · napari/napari · GitHub</a>.<br>
warn(message=warn_message)<br>
WARNING: QOpenGLWidget is not supported on this platform.<br>
WARNING:vispy:QOpenGLWidget is not supported on this platform.<br>
Segmentation fault (core dumped)</p>
<p>and</p>
<p>I installed napari with:<br>
python -m pip install “napari[all]”<br>
python -m pip install “napari[all]” --upgrade<br>
for the last time</p>
<p>thanks a lot…<br>
seher.</p> ;;;; <p>Hi guys，I am conducting research on atherosclerotic plaques of internal carotid artery on CT angiography. In order to analyze plaque components, I need to use the plug-in Polymeasure, but I cannot find the download method of this plug-in from any aspect. Can anyone help me?</p> ;;;; <p>Thank you <a class="mention" href="/u/thomasboudier">@ThomasBoudier</a> ! That’s great to know!</p> ;;;; <p>Ah, sorry, I should have realised you’re exporting a Plate, not an image.</p>
<p>The <code>w96_A1_A2_test_zarr/.zattrs</code> file that you’re seeing there with <code>{"bioformats2raw.layout":3}</code> should also contain `“plate”: {}`` data. E.g.<br>
<a href="https://ome.github.io/ome-ngff-validator/?source=https://cellpainting-gallery.s3.amazonaws.com/cpg0004-lincs/broad/images/2016_04_01_a549_48hr_batch1/images_zarr/SQ00014812__2016-05-23T20_44_31-Measurement1.ome.zarr" class="onebox" target="_blank" rel="noopener">https://ome.github.io/ome-ngff-validator/?source=https://cellpainting-gallery.s3.amazonaws.com/cpg0004-lincs/broad/images/2016_04_01_a549_48hr_batch1/images_zarr/SQ00014812__2016-05-23T20_44_31-Measurement1.ome.zarr</a></p>
<p>So something seems to have failed during conversion.</p>
<p>Did the conversion run to completion without errors?<br>
If you happen to have a sample plate that you could share, that would be helpful so we can test (I understand this isn’t so easy with plate data).</p>
<p>I actually haven’t tried NGFF converter with Plates myself. cc <a class="mention" href="/u/melissa">@melissa</a></p> ;;;; <p>Hi there,</p>
<p>I have no experience with scripting, other that running them and changing specific variables. I have detected the annotations with a pixel classifier and then added Intensity features to them to get the mean intensity of the structure.</p>
<p><strong>Could you provide a script that allows me to select some of my annotations based on their intensity</strong>? e.g. I would like to select the ones that have a value &gt;750, or 550-750.<br>
I did it manually for some of my images, but I believe there must be a more clever way to do it <img src="https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12" title=":sweat_smile:" class="emoji" alt=":sweat_smile:" loading="lazy" width="20" height="20"></p>
<p>Also it would be great if there was apossibility to <strong>exclude some of them based on their intensity values</strong>.</p>
<p>And finally I would like to be able to set a new class through the script: e.g from Islet → NE-islet</p>
<p>Thanks so much in advance!</p>
<p>Best,<br>
Ery</p> ;;;; <p>Hi,<br>
if it is not too late, I have some comments about your questions. As a disclamer, I’m more an astronomer than microscopist but a telescope and a microscope are not so different.</p>
<aside class="quote no-group" data-username="creativerror404" data-post="1" data-topic="74820" data-full="true">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png" class="avatar"> Kali Woods:</div>
<blockquote>
<p><em><strong>1.b.</strong></em> The use of background subtraction varies in some of the literature I’ve read. For example, in Quantitative analysis of Digital Microscope Images, Wolf describes using a background image, obtained from taking an image in a region of no fluorescence, to correct both the flatfield and the sample image. In the book Microscope Image Processing Second Edition 2022, the same basic equation is used but background is replaced with dark current image, which is defined as an image where all light to the dector is blocked. Background subtraction in the book is viewed as a completely alternative approach to flatfield/dark current. I was wondering if dark current image or the background image should be used for flatfield correction where A[x,y] is the corrected image, I[x,y] is the image measured, and F[x,y] is the flatfield image. Also, would like to know people’s arguments for and against using dark current or background.<br>
A[x,y] = (I[x,y] – C[x,y]) / (F[x,y] – C[x,y])</p>
</blockquote>
</aside>
<p>About flat/dark/background correction you have:</p>
<ul>
<li>
<code>D[x,y]</code> : the dark image where all light to the detector is blocked, it contains only dark current and thermal emission</li>
<li>
<code>B[x,y]</code>: the background that contain <code>D[x,y] + </code> background from rest of you setup up to your sample. It may contains some of the autofluorescence but if autofluorescence comes from your sample then it may no be present in the background. An idea to keep autofluorescence in the background in the use a very defocalized image as the backgroud</li>
<li>
<code>F[x,y]</code> is the flat field that gives information about differential throughput within the field of view (diffence in gain, dust on optics…). , it has to be corrected from the dark. It depends how it has been taken but usually it should not contains any autofluorescence.<br>
The final equation is then:<br>
<code>A[x,y] = (I[x,y] – B[x,y]) / (F[x,y] – D[x,y]) </code>
</li>
</ul>
<p>keep in mind that this is true only if other parameters are equals (same filters, same integration time,…)</p> ;;;; <p>I’ve been busy so only just got to test this - <code>as List</code> rather than <code>as Set</code> seems to fix that problem!</p>
<p>I’ll draw a hull around the clusters and get some shape measurements.</p> ;;;; <p>Hello poeple, I am reactivating this topic since I got the same issue.</p>
<p>I used to export full data csv with regions and import back to QuPath easily. After weeks, when I try to redo the same things, I was unable to export full data anymore. After select the folder, nothing happened, not even the error display in Matlab command window. I have tried both loading workplace from previous work, and loading a new table of cells then export. Still didn’t work.</p>
<p>I have checked the response to the similar issue and tried:</p>
<ol>
<li>Reinstall Matlab and CytoMAP</li>
<li>Run Matlab as administrator</li>
<li>Export to the folder in other disk</li>
</ol>
<p>Thanks for your help!</p> ;;;; <p>Dear Tobias,</p>
<p>I am the developer of <a href="https://github.com/MedVisBonn/eyepy" rel="noopener nofollow ugc">eyepy</a> a Python package for working with OCT data. I would be happy to include your code for reading the ThorLabs .oct format into the package, to make it available to the community. If you are interested in contributing your work you are welcome to join our <a href="https://github.com/MedVisBonn/eyepy/discussions/11" rel="noopener nofollow ugc">discussion</a> on supporting additional file formats.</p>
<p>Best Olivier</p> ;;;; <p>Hi<br>
I have experience with using stardist and cellpose in qupath. It also has some challenges and potential solutions similar to stardist like Mark said.</p><aside class="quote quote-modified" data-post="6" data-topic="78571">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mark_zaidi/40/37001_2.png" class="avatar">
    <a href="https://forum.image.sc/t/stardist-vs-qupath-cell-detection-oversegmentation-and-inadequate-seperation-of-cells/78571/6#:~:text=As%20for%20using%20DAPI%2Dtrained%20models%20to%20segment%20HDAB%20brightfield%20images%2C%20I%20discuss%20some%20of%20the%20challenges%20and%20potential%20solutions%20in%20this%20video%3A">Stardist vs Qupath cell detection, oversegmentation and inadequate seperation of cells</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Based on the picture you provided: 
 <a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6.jpeg" data-download-href="/uploads/short-url/hOuBaysMfvJmPDmhKkUqTLicqMe.jpeg?dl=1" title="image" rel="noopener nofollow ugc">[image]</a> 
the odd geometries could be caused by removing objects after the cells have been expanded with .cellExpansion(10), as noted by the object removal excerpt from your script below: 
//Clean up bad objects
removal = getCellObjects().findAll{it.getPathClass().toString().contains("Negative")}
removeObjects(removal, true)

As for the undersegmentation, I’d bet deconvolution issues are one major culprit. DSB2018 was trained on DAPI stained nuclei, and you’r…
  </blockquote>
</aside>

<p>The advantage of using cellpose is it has lots of models you can use and you can even retrain the model. I already got some nice results in HE and IHC(CD8, Ki67,HER2 ect…) slides. The disadvantage of it is also evident in that it needs python packages support and CUDA suppost. I’m not sure it works well or not in Mac M1.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/BIOP/qupath-extension-cellpose">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/BIOP/qupath-extension-cellpose" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26.png 2x" data-dominant-color="F2F0EC"></div>

<h3><a href="https://github.com/BIOP/qupath-extension-cellpose" target="_blank" rel="noopener nofollow ugc">GitHub - BIOP/qupath-extension-cellpose: an extension that wraps a Cellpose...</a></h3>

  <p>an extension that wraps a Cellpose environment such that WSI can be analyzed using Cellpose through QuPath. - GitHub - BIOP/qupath-extension-cellpose: an extension that wraps a Cellpose environment...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>I can give you some cellpose scripts if you need.</p>
<p>Best<br>
Yuan</p> ;;;; <p>I have an opening for a postdoctoral researcher position in my group in Warsaw:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://dioscuricentrebacteria.com/jobs/">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/ae921e180b49cad37cdd981f59955c1898cd2e7c.png" class="site-icon" width="32" height="32">

      <a href="https://dioscuricentrebacteria.com/jobs/" target="_blank" rel="noopener nofollow ugc" title="10:51PM - 18 May 2020">Dioscuri Centre for Physics and Chemistry of Bacteria – 18 May 20</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:440/134;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/6/267924fc84fd9ccad70c0bf34c1802a2ffb32847.png" class="thumbnail" width="440" height="134"></div>

<h3><a href="https://dioscuricentrebacteria.com/jobs/" target="_blank" rel="noopener nofollow ugc">Jobs</a></h3>

  <p>A postdoctoral researcher in biological physics We seek a talented post-doctoral researcher to contribute to the project “Transition from genetic to phenotypic antibiotic resistance in de novo bact…</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>The project will involve creating a very large microfluidic device for optical imaging of de novo bacterial mutations, and will combine experiments, image analysis, and mathematical modelling.</p>
<p>Although the job is ideally suited for a person with experimental background, theorists and computational researchers interested in image processing and seeking to gain wet lab experience are encouraged to apply. I have had some success in converting such researchers into half-time experimentalists. This project is an ideal place to make such a transition; theoretical/computational skills will be very useful and there is another post-doc (a biologist) to help with the “biology” part of the project.</p>
<p>Deadline for applications is 31 March 2023.</p>
<p>Best wishes,<br>
Bartek Waclaw</p> ;;;; <p>I used the NGFF converter (latest version). The structure looks like this:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7d0c0ac6a9154536a9fd7722fee066cd80f4c42.png" alt="image" data-base62-sha1="nWyZfVnEP28qKRE77DCR1ouYo4G" width="316" height="165"></p> ;;;; <p>Thank you so much <a class="mention" href="/u/smcardle">@smcardle</a>, it was the correct idea! I just needed to concatenate the labelServer to the imageServer (so the viceversa of your solution) because otherwise the original image is black and white. Now I have an image with 6 channel as expected (RGB for the original + 3 that I have defined for the labelServer). This is the final version of the code that works for me:</p>
<pre><code class="lang-auto">import static qupath.lib.gui.scripting.QPEx.*

import javafx.application.Platform
import qupath.lib.images.ImageData
import qupath.lib.images.servers.ImageServerProvider
import qupath.lib.images.servers.TransformedServerBuilder

def imageData = getCurrentImageData()

// Define output path (relative to project)
def outputDir = buildFilePath(PROJECT_BASE_DIR, 'export')
mkdirs(outputDir)
def name = GeneralTools.getNameWithoutExtension(imageData.getServer().getMetadata().getName())
def path = buildFilePath(outputDir, name + "-merged.tiff")

// Define how much to downsample during export (may be required for large images)
double downsample = 8

// Create an ImageServer where the pixels are derived from annotations
def labelServer = new LabeledImageServer.Builder(imageData)
  .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)
  .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported
  .addLabel('Other', 1)
  .setBoundaryLabel('Boundary*', 2) // Define annotation boundary label
  .multichannelOutput(true) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)
  .build()

def imageServer = getCurrentServer()

def serverMerged = new TransformedServerBuilder(imageServer)
    .concatChannels(labelServer)
    .build()


// Write the image
writeImage(serverMerged, path)

</code></pre>
<p>On a side note, my goal was to open the multichannel image in Imaris, which works. ImageJ is not really able to split the channels, so this code might not work for every software.</p> ;;;; <p>Hi <a class="mention" href="/u/mzugravu">@mzugravu</a>,</p>
<p>Let me first put your code here so that it is easier for readers to follow:</p>
<pre><code class="lang-auto">def align_stack(source_file, output_file):

    imp = IJ.openImage(source_file)
    IJ.run(imp, "Linear Stack Alignment with SIFT", "initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate")
    IJ.save(imp, output_file)
</code></pre>
<p>The problem is, that the plugin will not modify the input image, but create a separate output image. That is why imp is unmodified. You can just get the active image with <code> IJ.getImage()</code> after the SIFT and save that one.</p>
<p>Also be careful the plugin is not a plugin-filter, it takes the active image not the imp you pass. When I use the macro recorder I get</p>
<pre><code class="lang-auto">IJ.run("Linear Stack Alignment with SIFT", "initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 expected_transformation=Rigid interpolate");
</code></pre>
<p>Best,<br>
Volker</p> ;;;; <p>Ok, so here’s an approach:</p>
<p>My goal is to get the walls in the image. I’ve noticed that there’s a lot of brightness variation in the image <em>but</em> the sides of the walls are reliably dark. In order to make the inside of the walls dark as well, you can do an <em>opening</em>, which expands the dark regions, then expands the bright regions. The order of the operations means that if you have a bright gap smaller than the radius of the opening, it’ll stay dark, because the bright gap will be “obliterated” by the darkness. Sounds bleak but I guess it matches the theme. <img src="https://emoji.discourse-cdn.com/twitter/joy.png?v=12" title=":joy:" class="emoji" alt=":joy:" loading="lazy" width="20" height="20"></p>
<p>I measured the walls to be about 10 pixels thick, so I tried an opening with a radius of 5:</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib import patches
from skimage import morphology, color, filters, measure
import imageio.v3 as iio


image = iio.imread('dungeon.png')
gray = color.rgb2gray(image)
opened = morphology.opening(gray, morphology.disk(5))
fig, (ax0, ax1) = plt.subplots(1, 2)
ax0.imshow(gray, cmap='gray')
ax0.set_axis_off()
ax0.set_title('grayscale')
ax1.imshow(opened, cmap='gray')
ax1.set_axis_off()
ax1.set_title('opened')
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe.png" data-download-href="/uploads/short-url/2zgHkf32jZOqPt2tv2CQJrqgQRo.png?dl=1" title="opened-compare"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe_2_690x331.png" alt="opened-compare" data-base62-sha1="2zgHkf32jZOqPt2tv2CQJrqgQRo" width="690" height="331" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe_2_690x331.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe_2_1035x496.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe.png 2x" data-dominant-color="AFAFAF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">opened-compare</span><span class="informations">1097×527 152 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Cool! Now the walls are completely dark! One thing I noticed is that there’s a few wispy bits and, for example, the entrance to the central chamber thing still has a black line across it. We can do the reverse process now with a <em>closing</em>. As long as the radius is smaller than our walls, they should stay intact while thinner structures are removed.</p>
<pre><code class="lang-python">closed = morphology.closing(opened, morphology.disk(3))

fig, (ax0, ax1) = plt.subplots(1, 2)
ax0.imshow(opened, cmap='gray')
ax0.set_axis_off()
ax0.set_title('opened')
ax1.imshow(closed, cmap='gray')
ax1.set_axis_off()
ax1.set_title('closed')
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58.png" data-download-href="/uploads/short-url/dUrcOD33wBLluWdcYtI31mfkeV2.png?dl=1" title="closed-compare"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58_2_690x331.png" alt="closed-compare" data-base62-sha1="dUrcOD33wBLluWdcYtI31mfkeV2" width="690" height="331" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58_2_690x331.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58_2_1035x496.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58.png 2x" data-dominant-color="AAAAAA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">closed-compare</span><span class="informations">1097×527 119 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Minor difference, but you’ll notice the dark line closing off the chamber is gone. Now we need to threshold it — find only the dark bits. There’s lots of thresholding algorithms in scikit-image, but there’s a nice function to try them all in one go:</p>
<pre><code class="lang-python">from skimage import filters

filters.try_all_threshold(closed)
</code></pre>
<p>Which gives:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1.png" data-download-href="/uploads/short-url/iAn9B2bi7qYAWL6H7y9cRnpjnxL.png?dl=1" title="try-all-threshold"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_314x500.png" alt="try-all-threshold" data-base62-sha1="iAn9B2bi7qYAWL6H7y9cRnpjnxL" width="314" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_314x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_471x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_628x1000.png 2x" data-dominant-color="CFCFCF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">try-all-threshold</span><span class="informations">646×1026 60.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Minimum is looking pretty good! Now we can threshold the image, and use <code>skimage.measure.find_contours</code> to find the coordinates around the walls:</p>
<pre><code class="lang-python">thresholded_min = closed &lt; filters.threshold_minimum(closed)
contours = measure.find_contours(thresholded_min)

fig, ax = plt.subplots()
ax.imshow(gray, cmap='gray')
ax.set_axis_off()

for contour in contours:
    contour_xy = contour[:, [1, 0]]  # matplotlib uses xy coordinates, not row/col
    ax.add_patch(patches.Polygon(
            contour_xy,
            facecolor=(0, 0, 0, 0),
            edgecolor='cornflowerblue',
            linewdith=1))
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206.png" data-download-href="/uploads/short-url/9T4BgMoMsr8kfcGEGENbg2stezY.png?dl=1" title="contours"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206_2_597x500.png" alt="contours" data-base62-sha1="9T4BgMoMsr8kfcGEGENbg2stezY" width="597" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206_2_597x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206.png 2x" data-dominant-color="B4B5B6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">contours</span><span class="informations">891×746 327 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>So, that’s something! I’ll add two notes:</p>
<ol>
<li>If you’re going to be processing lots of these, it might be useful to use some machine learning tools to draw on the walls and get a really high contrast between walls and background. I got the following result by painting for about a minute in <a href="https://ilastik.org">Ilastik</a> using the pixel classification workflow:</li>
</ol>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41.png" data-download-href="/uploads/short-url/xd5mIiYxPrxPiDQTCzPwmjLbmBX.png?dl=1" title="Screen Shot 2023-03-16 at 5.50.03 pm"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_690x446.png" alt="Screen Shot 2023-03-16 at 5.50.03 pm" data-base62-sha1="xd5mIiYxPrxPiDQTCzPwmjLbmBX" width="690" height="446" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_690x446.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_1035x669.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_1380x892.png 2x" data-dominant-color="898F8D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-16 at 5.50.03 pm</span><span class="informations">3824×2474 1.97 MB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Looks pretty good I reckon! Check out some tutorials on their site. Once you’re done you can export the resulting probability maps and repeat the workflow above, but with a much nicer signal to noise I should think…</p>
<ol start="2">
<li>If you’re going to be doing a smaller batch, then before finding the contours, you might be able to refine the thresholded example in <a href="https://napari.org">napari</a>:</li>
</ol>
<pre><code class="lang-python">import napari

viewer, layer = napari.imshow(gray)
thresholded_edit = np.copy(thresholded_min)  # to preserve original
labels_layer = napari.add_labels(thresholded_edit)
# now use some of the paintbrush/eraser tools in napari
# the thresholded_edit array will be directly updated
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28.png" data-download-href="/uploads/short-url/17W1i8LoTNjCbfolkCFufMhkizS.png?dl=1" title="Screen Shot 2023-03-16 at 9.01.39 pm"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_613x500.png" alt="Screen Shot 2023-03-16 at 9.01.39 pm" data-base62-sha1="17W1i8LoTNjCbfolkCFufMhkizS" width="613" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_613x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_919x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_1226x1000.png 2x" data-dominant-color="646060"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-16 at 9.01.39 pm</span><span class="informations">3028×2468 2.19 MB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>(about 2 min of painting on napari — notice the fixed gaps around the central chamber.)</p>
<p>I hope all of the above is helpful and inspirational! <img src="https://emoji.discourse-cdn.com/twitter/blush.png?v=12" title=":blush:" class="emoji" alt=":blush:" loading="lazy" width="20" height="20"></p> ;;;; <p>Recently I’ve been generating a lot of multi-channel (this is what the Nikon software calls multiple acquisition settings) images and have been getting the same error when trying to import these into ImageJ/Fiji.<br>
ImageJ recognises the correct number of channels but I think gets confused between RGB channels and the ND2 channels, leading to N channels being opened in ImageJ (N being the number of channels taken with the microscope) but each is a mono rather than RGB image and seems to be made up of LCD-display-like pixels, with the image being recognisable but jumbled (see <a href="https://zenodo.org/record/7738355#.ZBLpFnbP1aR" rel="noopener nofollow ugc">Zenodo upload</a>).<br>
If it helps, I think the python nd2reader plugin can recognise the dimensions properly.</p>
<p>For those reading who also have this problem, the best fix I can come up with is exporting multi-channel ND2s as TIFFs in NIS E Viewer and then stacking them with ImageJ. Please let me know if you have a better idea!.</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://zenodo.org/record/7738355#.ZBLpFnbP1aR">
  <header class="source">
      

      <a href="https://zenodo.org/record/7738355#.ZBLpFnbP1aR" target="_blank" rel="noopener nofollow ugc">Zenodo</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://zenodo.org/record/7738355#.ZBLpFnbP1aR" target="_blank" rel="noopener nofollow ugc">20230315 ImageJ bio-format importer multi-channel ND2 bug</a></h3>

  <p>A bug wherein ND2 images taken with different camera settings are read incorrectly by ImageJ's bio-format importer imageJ_test.nd2 - The ND2 file produced by NIS Elements AR on a Nikon LV100 microscope. 11 time steps, 2 channels (images taken with...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Do you only have that one channel? brainreg was designed to work with autofluorescence channels with no labelling. I think there are two things limiting the registration:</p>
<ul>
<li>Bright labelling in the channel used for registration</li>
<li>Limited contrast between brain structures</li>
</ul> ;;;; <p>You likely don’t want to record a macro for this. The macro recording function is to make it easier to write scripts to repeat things you have done before, or batch-process many files, for example.</p>
<p>Is your data an image stack? What type of video do you want?</p>
<p>If it is a stack, and you want a video to just go through the slices, all you have to do is save it as and .avi file (With “Save as…”).</p>
<p>If you’d like to show it as a 3D object, and do a virtual rotation around it, the 3D viewer can do that very easily. Just open it in Plugins &gt; 3D Viewer,  and go to View &gt; Record 360 deg rotation.</p> ;;;; <p>Thanks <a class="mention" href="/u/biovoxxel">@biovoxxel</a> , sometimes I forget my own development <img src="https://emoji.discourse-cdn.com/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"></p>
<p>Just to complete, there is another method, not documented yet, called <strong>Manager3D_FillStack2</strong> that will not require to explicitly select the object, so you can replace the loop by :</p>
<pre><code class="lang-auto">for (i = 0; i &lt; nb_obj; i++) {
	Ext.Manager3D_FillStack2(i,i+1, i+1, i+1);
}
</code></pre>
<p>Best,</p>
<p>Thomas</p> ;;;; <p>Yeah I think tifffile is the way to go here, our command line tools can modify OME-XML but not the tiff tags.</p> ;;;; <p>Hi <a class="mention" href="/u/khinton">@Khinton</a>, do you have a link to a sample file that I can test? If you need a suitable upload location then we recommend <a href="https://zenodo.org/">Zenodo</a></p> ;;;; <p>Hi Tim,</p>
<p>“No route to host” means the client can’t connect to the server. I suspect you’re using the wrong hostname / ip address. It might be different from the hostname / url of the webclient. Best to ask the admin of your omero server to give you the hostname or ip address of the OMERO server. If you’re sure that you have the correct one, you should also check that you can actually access it (i.e. not blocked by a firewall or in a different network, etc.). Does a <code>ping hostname</code> work?</p>
<p>Kind Regards,<br>
Dominik</p> ;;;; <p>Heya, checking a few days later now and I now have a model zoo folder so I managed to add the file to it. Thank you very much for the help.</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696.png" data-download-href="/uploads/short-url/niGg8fh64SeHXsnBpAYUyDaci1M.png?dl=1" title="截屏2023-03-16 上午5.03.27" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696_2_690x487.png" alt="截屏2023-03-16 上午5.03.27" data-base62-sha1="niGg8fh64SeHXsnBpAYUyDaci1M" width="690" height="487" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696_2_690x487.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696.png 2x" data-dominant-color="E6E5E6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">截屏2023-03-16 上午5.03.27</span><span class="informations">989×699 70 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
When using the pixel classification, I cannot upload the PNG image for Raw Data part of Input Data. May I ask what is the solution to this situation? If PNG format is not acceptable, what kind of software or apps can be used for stack transformation from PNG to HDF5?</p>
<p>Thanks!</p> ;;;; <p>Can you provide some info like what platform you are on, how you set up your python env, how you installed napari, and what version of napari?<br>
(e.g. output of <code>napari --info</code>)?</p> ;;;; <p>I wrote a plugin to open a certain image format. I have the problem that two image instances are opened when using the ImagePlus.show function in run when using drag and drop. If I remove the ImagePlus.show function it opens a single instance but then if I use the plugin via Plugins-&gt;Input-Output it doesn’t open the image.</p>
<p>Thanks,<br>
Don</p> ;;;; <p>Great thanks very much !</p> ;;;; <p>I am trying to use the ilastik to quantify the percentage of virus-infected cells upon gene overexpression. In a word, I need to know the number of cells overexpressing the target genes (red) and the number of virus-infected cells upon gene overexpression (red+ green). I have attached the picture here fo<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806.jpeg" data-download-href="/uploads/short-url/rPqtcsJl9mLOtn2jqXN4BseXW9E.jpeg?dl=1" title="DeconvolvedTsfDAPI 377,447+TsfGFP 469,525+TsfTexas Red 586,647_D9_1_001" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_500x500.jpeg" alt="DeconvolvedTsfDAPI 377,447+TsfGFP 469,525+TsfTexas Red 586,647_D9_1_001" data-base62-sha1="rPqtcsJl9mLOtn2jqXN4BseXW9E" width="500" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_1000x1000.jpeg 2x" data-dominant-color="160D08"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">DeconvolvedTsfDAPI 377,447+TsfGFP 469,525+TsfTexas Red 586,647_D9_1_001</span><span class="informations">1920×1920 315 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
r your reference. May I ask what is the rational workflow of ilastik to achieve this? Additionally, sometimes the red signal is too strong and it covers the green signal so I cannot identify the green signals by my eyes during the training, is there any way to spilt the channel or another way to distinguish the signal (see the D9 picture for reference)？</p>
<p>percentage of virus-infected cells upon gene overexpression = the number of virus-infected cells upon gene overexpression (red+green) / the number of cells overexpressing the target genes (red)</p>
<p>Thanks for your time!</p> ;;;; <aside class="quote no-group" data-username="petebankhead" data-post="4" data-topic="78640">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
def server = imageData.getServer()
def otherServer = new TransformedServerBuilder(server).extractChannels(0, 1).build()
def sneakyImageData = new ImageData&lt;&gt;(otherServer, imageData.getHierarchy(), imageData.getImageType())
runPlugin(sneakyImageData, ...)
</code></pre>
</blockquote>
</aside>
<p>Hi <a class="mention" href="/u/petebankhead">@petebankhead</a><br>
this works with the adapted last line</p>
<pre><code class="lang-auto">runPlugin('qupath.imagej.superpixels.SLICSuperpixelsPlugin', sneakyImageData,
        '{"sigmaMicrons": 0.5,  ' +
         '"spacingMicrons": 5.0,  ' +
         '"maxIterations": 10,  ' +
         '"regularization": 0.9,  ' +
         '"adaptRegularization": false}')
</code></pre>
<p>But I have to admit, that the SLIC is not always behaving as I expect. Regularisation and smoothing seem only to have a limited influence on the outcome.<br>
I will look into pixel classification as indicated in the docs.</p>
<p>However, I second <a class="mention" href="/u/smcardle">@smcardle</a>, the old stuff is quite handy still.</p>
<p>Thanks for the quick and useful feedback.</p>
<p>Felix</p> ;;;; <p>thanks a lot！</p> ;;;; <p>Here’s a video in which I discuss some of the limitations of using QuPath (at least in it’s current state) for analyzing Visium-processed samples:</p><div class="onebox lazyYT lazyYT-container" data-youtube-id="o7jv4jUhK_8" data-youtube-title="Discussion of Spatial Transcriptomics in QuPath" data-parameters="feature=oembed&amp;wmode=opaque">
  <a href="https://www.youtube.com/watch?v=o7jv4jUhK_8" target="_blank" rel="noopener">
    <img class="ytp-thumbnail-image" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/098469283fe74e71b45f3101119cb070407b9ae3.jpeg" title="Discussion of Spatial Transcriptomics in QuPath" width="480" height="360">
  </a>
</div>
<p>
Is there a specific analysis you intend to conduct on these sections? There are several other packages that offer extensive support such as <a href="https://squidpy.readthedocs.io/en/stable/">Squidpy</a> if you have Python experience, or <a href="https://themilolab.github.io/SPATA2/">SPATA2</a> if you feel more comfortable in R.</p> ;;;; <aside class="quote no-group" data-username="Li1234" data-post="1" data-topic="78571">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/li1234/40/68757_2.png" class="avatar"> Li Yenn Yong :</div>
<blockquote>
<p>I then tried running stardist, and the opposite seems to happen, where there is an inadequate separation of cells.</p>
</blockquote>
</aside>
<p>Based on the picture you provided:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6.jpeg" data-download-href="/uploads/short-url/hOuBaysMfvJmPDmhKkUqTLicqMe.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6_2_586x500.jpeg" alt="image" data-base62-sha1="hOuBaysMfvJmPDmhKkUqTLicqMe" width="586" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6_2_586x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6_2_879x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6.jpeg 2x" data-dominant-color="D7D1D0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1051×896 111 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
the odd geometries could be caused by removing objects after the cells have been expanded with <code>.cellExpansion(10)</code>, as noted by the object removal excerpt from your script below:</p>
<pre><code class="lang-auto">//Clean up bad objects
removal = getCellObjects().findAll{it.getPathClass().toString().contains("Negative")}
removeObjects(removal, true)
</code></pre>
<p>As for the undersegmentation, I’d bet deconvolution issues are one major culprit. DSB2018 was trained on DAPI stained nuclei, and you’re trying to extract what appears to be the hematoxylin channel from HDAB stained sections. You might want to try different color vectors and see if that improves segmentation.</p>
<p>As for using DAPI-trained models to segment HDAB brightfield images, I discuss some of the challenges and potential solutions in this video:</p><div class="onebox lazyYT lazyYT-container" data-youtube-id="UI_Sfv3rNo4" data-youtube-title="HDAB Cell Detection using StarDist in QuPath" data-parameters="feature=oembed&amp;wmode=opaque">
  <a href="https://www.youtube.com/watch?v=UI_Sfv3rNo4" target="_blank" rel="noopener">
    <img class="ytp-thumbnail-image" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/1/617d2846647d4f2c4fc36af1b463ed9b5a9a551f.jpeg" title="HDAB Cell Detection using StarDist in QuPath" width="480" height="360">
  </a>
</div>
<p>
It’ll still fail in high intensity CD68 DAB cells, because deconvolution typically fails under high optical density conditions (i.e. can’t separate DAB from Hematoxylin when you have an almost black nucleus)</p>
<p>And finally, here’s a small collection of scripts that make parameter tuning of StarDist a bit simpler, especially when you want to apply models trained on different modalities from your own images. And doesn’t require extensive programming knowledge: <a href="https://github.com/MarkZaidi/Universal-StarDist-for-QuPath" class="inline-onebox">GitHub - MarkZaidi/Universal-StarDist-for-QuPath: Transfer trained StarDist models across imaging modalities</a><br>
It’s a bit of a “jack of all trades, master of none” script, so you’ll likely have better odds playing around with the script used in the video, since that merges the hematoxylin and DAB channels together. Useful when there’s overlap between your nuclear counterstain and your IHC marker. CD68 <em>should</em> be localized more to the cytoplasm/membrane, but it’s still worth a shot. Script from video is pasted below:</p>
<pre><code class="lang-auto">import qupath.ext.stardist.StarDist2D
var imageData = getCurrentImageData()
def stains = imageData.getColorDeconvolutionStains()
// Specify the model file (you will need to change this!)
def pathModel = 'C:/Users/Mark Zaidi/Documents/QuPath/Stardist Trained Models/dsb2018_heavy_augment.pb'

var stardist = StarDist2D.builder(pathModel)
        .preprocess(
            ImageOps.Channels.deconvolve(stains),
            ImageOps.Channels.extract(0,1),
            ImageOps.Channels.sum(),
            ImageOps.Filters.median(2),
            //ImageOps.Core.divide(2),
         )
        .threshold(0.5)              // Probability (detection) threshold
        //.channels('DAPI')            // Specify detection channel
        .normalizePercentiles(1, 99) // Percentile normalization
        .pixelSize(0.5)              // Resolution for detection
        .build()

// Run detection for the selected objects

var pathObjects = getSelectedObjects()
if (pathObjects.isEmpty()) {
    Dialogs.showErrorMessage("StarDist", "Please select a parent object!")
    return
}
stardist.detectObjects(imageData, pathObjects)
println 'Done!'
</code></pre> ;;;; <p>(Either way, loving the discussion here!)</p> ;;;; <p>I’m a little confused… In my mind the contrast limits should apply to the value <em>after</em> attenuation? This might explain why attenuation and the clims and the attenuation constant have such annoying interplay, where as you increase the attenuation you have to decrease the contrast limits?</p> ;;;; <p>I take following error in every platform</p>
<p>napari was tested with QT library <code>&gt;=5.12.3</code>.<br>
The version installed is 5.9.7. Please report any issues with<br>
this specific QT version at <a href="https://github.com/Napari/napari/issues" class="inline-onebox" rel="noopener nofollow ugc">Issues · napari/napari · GitHub</a>.<br>
warn(message=warn_message)<br>
WARNING: QOpenGLWidget is not supported on this platform.</p> ;;;; <p>I am taking following error everywhere: python, gui, command line, jupyter notebook. Kernel closes after that.<br>
I need help. Thanks.</p>
<p>napari was tested with QT library <code>&gt;=5.12.3</code>.<br>
The version installed is 5.9.7. Please report any issues with<br>
this specific QT version at <a href="https://github.com/Napari/napari/issues" class="inline-onebox" rel="noopener nofollow ugc">Issues · napari/napari · GitHub</a>.<br>
warn(message=warn_message)<br>
WARNING: QOpenGLWidget is not supported on this platform.</p> ;;;; <p>Hey <a class="mention" href="/u/morgan_marz">@morgan_marz</a><br>
I don’t have a full script worked out, but maybe something with TransformedServerBuilder would help. If you got your <code>labelServer</code> as you are doing now, and got your normal image server with <code>def imageserver = getCurrentServer()</code>, you could combine them with:</p>
<pre><code class="lang-auto">def serverMerged = new TransformedServerBuilder(labelServer)
    .concatChannels(imageServer)
    .build()
</code></pre>
<p>taken from <a href="https://forum.image.sc/t/qupath-multiple-channel-in-separate-files-how-to-merge-them/29455/22">here</a>.  Then pass serverMerged into your writeImage function. I haven’t tested this, but it might run into image type issues, since your labelServer is probably 8 bit? which may or may not match your data type. Important! This will not be a 4th channel. It will be either the last 3 channels (for an RGB image) or, for fluorescence, however many channels you have. If it runs at all, it will take a LOT longer to write, because there is significantly more information.</p> ;;;; <p>Hello! I performed brain slices alignment in ABBA and exported the atlas as annotations in QuPath. Now I would like to export the original image and the annotations in a single multichannel image. I am using this code:</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()

// Define output path (relative to project)
def outputDir = buildFilePath(PROJECT_BASE_DIR, 'export')
mkdirs(outputDir)
def name = GeneralTools.getNameWithoutExtension(imageData.getServer().getMetadata().getName())
def path = buildFilePath(outputDir, name + "-labels.ome.tif")

// Define how much to downsample during export (may be required for large images)
double downsample = 8

// Create an ImageServer where the pixels are derived from annotations
def labelServer = new LabeledImageServer.Builder(imageData)
  .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)
  .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported
  .addLabel('Other', 1)
  .setBoundaryLabel('Boundary*', 2) // Define annotation boundary label
  .multichannelOutput(true) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)
  .build()

// Write the image
writeImage(labelServer, path)
</code></pre>
<p>And I’m able to obtain a multichannel image with 3 channels: a black background, a binary image with the annotations filled in white and the annotation’s boundary. Is there a way to add also the original image, without the annotations, as a fourth channel?</p> ;;;; <p>Hi <a class="mention" href="/u/biovoxxel">@biovoxxel</a></p>
<p>Thank you so much for sharing this resource with me! That is exactly what I needed!</p>
<p>Have a great rest of your day <img src="https://emoji.discourse-cdn.com/twitter/laughing.png?v=12" title=":laughing:" class="emoji" alt=":laughing:" loading="lazy" width="20" height="20"></p> ;;;; <p>did not allow me to install tensorflow below 2.2.0<br>
when i wrote pip install tensorflow==2.0.0 or pip install tensorflow-gpu==2.0.0<br>
error: could not found a version that satisfies the required tensorflow==2.0.0</p>
<p>Best<br>
Julia</p> ;;;; <p>Thank you! That works great!</p> ;;;; <p>Hi christian,<br>
I have an lsm file and i did a reconstruction of it using SNT. I’d like to record a video of my reconstruction on the neuron. I used the macro plugin, and then record. But i get confused about the steps i should do inorder to generate a video at the end.</p> ;;;; <p><a class="mention" href="/u/zahraa_sweidan">@Zahraa_Sweidan</a><br>
What type of data do you have, what method are you using to process the data and what errors did you receive?</p> ;;;; <p>I am trying to opens PerkinElmer Nuance im3 files (cube files) in ImageJ or Fiji.</p>
<p>I found the IM3Reader (<a href="https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java" rel="noopener nofollow ugc">Source Code</a>, <a href="https://bio-formats.readthedocs.io/en/latest/metadata/IM3Reader.html" rel="noopener nofollow ugc">Supported Metadata Fields</a>)<br>
I am not sure if what I am doing is right and if it is possible to import the Bio-Format that is in the source code</p><aside class="onebox githubblob" data-onebox-src="https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java">
  <header class="source">

      <a href="https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java" target="_blank" rel="noopener nofollow ugc">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java" target="_blank" rel="noopener nofollow ugc">ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java</a></h4>


      <pre><code class="lang-java">/*#%L
 * BSD implementations of Bio-Formats readers and writers
 * %%
 * Copyright (C) 2005 - 2017 Open Microscopy Environment:
 *   - Board of Regents of the University of Wisconsin-Madison
 *   - Glencoe Software, Inc.
 *   - University of Dundee
 * %%
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 * 
 * 1. Redistributions of source code must retain the above copyright notice,
 *    this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 *    this list of conditions and the following disclaimer in the documentation
 *    and/or other materials provided with the distribution.
 * 
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
</code></pre>



  This file has been truncated. <a href="https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java" target="_blank" rel="noopener nofollow ugc">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
into ImgeJ or Fiji through &gt;Plugins&gt;Bio-Formats&gt;Bio-Formats Importer.</p>
<p>I tried to save the script from the link and copy into the text editor and import it as a IM3Reader.java file into the Bio-Format but it is still not opening the images with the im3 extension.</p>
<p>In Fiji I have the following “Exception”</p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_202 [64-bit]; Mac OS X 10.16; 296MB of 13333MB (2%)</p>
<p><em>java.lang.IllegalStateException: Too early in import process: current step is READER, but must be after STACK</em></p>
<ul>
<li>at loci.plugins.in.ImportProcess.assertStep(ImportProcess.java:778)*</li>
<li>at loci.plugins.in.ImportProcess.getReader(ImportProcess.java:306)*</li>
<li>at loci.plugins.in.Importer.run(Importer.java:99)*</li>
<li>at loci.plugins.LociImporter.run(LociImporter.java:78)*</li>
<li>at ij.IJ.runUserPlugIn(IJ.java:237)*</li>
<li>at ij.IJ.runPlugIn(IJ.java:203)*</li>
<li>at ij.Executer.runCommand(Executer.java:152)*</li>
<li>at ij.Executer.run(Executer.java:70)*</li>
<li>at java.lang.Thread.run(Thread.java:748)*</li>
</ul>
<p>Can anyone help with this?</p>
<p>Thank you</p> ;;;; <p>Hi <a class="mention" href="/u/slaine_troyard">@Slaine_Troyard</a>,</p>
<p>Not at the computer right now but you can check all macro commands here: <a href="https://mcib3d.frama.io/3d-suite-imagej/uploads/MacrosFunctionsRoiManager3D.pdf">https://mcib3d.frama.io/3d-suite-imagej/uploads/MacrosFunctionsRoiManager3D.pdf</a></p>
<p>With the <code>Measure3D</code> function you can get the volume of an object and based on it decide if you want to draw it into the image or not.</p> ;;;; <p><a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a> i will try with 2.0.0</p>
<p>if did not work, i will install everything again with the combination that you suggested. i have a Nvidia rtx a 4000 which drivers i should use with this combination?</p>
<p>i was avoiding to increase a lot the cuda because i saw a comment of <a class="mention" href="/u/mwmathis">@MWMathis</a> saying to did did not go further 10.2. however, in the website now said " <strong>Note, DeepLabCut is up to date with the latest CUDA and tensorflow versions</strong>" Probably, this comment was outdated!</p>
<p>Thanks for the fast replies!<br>
julia</p> ;;;; <p>Thanks, I was thinking of you when I added the bit in parentheses <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <aside class="quote no-group" data-username="petebankhead" data-post="2" data-topic="78640">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<p>The superpixel stuff is very old and rather (by me) unloved code.</p>
</blockquote>
</aside>
<p>Just throwing this out there… I <img src="https://emoji.discourse-cdn.com/twitter/heart.png?v=12" title=":heart:" class="emoji" alt=":heart:" loading="lazy" width="20" height="20"> superpixels. I use them routinely for situations where I want detailed, non-blocky lines but require large-scale tissue context.</p> ;;;; <p>Those cuda and cudnn are compatible with tensorflow 2.0.0. But I would recommend installing newer CUDA (11.8 for instance) with cudnn 8.4.1.50 and tensorflow 2.10.0</p> ;;;; <p>In 2.2.3 adding new videos through the GUI doesn’t work. You have to do it through ipython</p> ;;;; <p><a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a></p>
<p>cuda 10.0<br>
cuDnn 7.4.2<br>
tensorflow 2.3.0</p>
<p>thanks for your time!</p> ;;;; <p>What are you cudatoolkit, cudnn and tensorflow versions?</p> ;;;; <p>Thanks <a class="mention" href="/u/konrad_danielewski">@Konrad_Danielewski</a> and <a class="mention" href="/u/mwmathis">@MWMathis</a> for the reply.</p>
<p>I download the new Deeplabcut-master and I install it.</p>
<p>The error is fixed, but now I have a new one!</p>
<p>The error now is DLL load, very common in the forum.</p>
<p>I tried to update the Path, install a different python version, update opencv-python, install h5py==2.7.0, different version of cuda,different version of tensorflow basically all the tips that I read in the forum. But nothing works!</p>
<p>I notice in the forum that <a class="mention" href="/u/mwmathis">@MWMathis</a> faced the same problem in feb 2019, maybe you have another suggestion to me!</p>
<p>thanks a lot!<br>
Julia</p> ;;;; <p>Hi<br>
I am using the cellpose 2.2 in cell segmentation.<br>
This system was built in WEB platform and the cellpose was realized by using command.</p>
<p>Firstly, while researching the cellpose, I found that cellpose provide 14 pretrained models.<br>
But I am not sure what the differences(and their utilizing examples) are between them because I am not an expert in this field.<br>
Please give me good advice about this problem.<br>
Secondly, I also found that the cellpose provide “*_output.png” file when we get cell segment result by using commands.<br>
But sometimes there are some cases that it’s not created(Because I used the same command, I think that it’s related with the kind of image file).<br>
Which case is it created or not in?<br>
Any help would be appreciated.</p> ;;;; <p>Hi <a class="mention" href="/u/biovoxxel">@biovoxxel</a></p>
<p>Thank you so much for helping me with the code! I’m also trying to run a size exclusion filter on imageJ to remove ROIs that are too large or too small. Do know if there is a way to extract measurements from 3D measure on 3D manager and delete the corresponding ROIs?</p> ;;;; <p>Thank you for the reply. Could you please explain where I can put my x-y values into the code?</p>
<p>The values are in an Excel file and are around 500 points. Sorry, but I don’t have any experience in using ImageJ. So, I would appreciate it if you could explain each step.</p> ;;;; <p>We are labeling spiders. We’ve labeled a few neural networks, but one thing that we’ve always run into is that legs hide underneath other things, and the tips of the legs are never tracked very well. Even if the tip is not there, the network will try to label it. We have an idea of just labeling where the tips of the legs even if we can’t see them so the nueral network has an idea of where the tip is supposed to be relative to everything else. Sometimes it overlaps with other dots, or is on a different body part. I was just wondering if this is something that makes sense.<br>
Thanks!</p> ;;;; <p>Hi,</p>
<p>I am working with imaging mass cytometry data and see variability in my nuclear staining that I would like to minimize. I want to split the multi.tiff into it´s 25 channels, edit the DNA channel, and then save the image as H5 to import into ilastik. However, when I try splitting the channels with “ColorToGray” - Channels, and then specifiy the channels, CP complains that the input image isn´t in color (which is fair). What are alternative ways to split these channels and edit only one of them?</p>
<p>Thanks!</p> ;;;; <p>Yes! I think this is what I meant by a smarter way to check for “complete attenuation” but your comment helped me think through it.</p>
<p>I think this would mean breaking if <code>maxval &gt; clim.y * scale</code>? This gives me up to 80fps with the big cube of random values from above.</p> ;;;; <p>Hello,<br>
I would like to record an animation/video of my reconstruction of the neuron using FIJI, i tried to use Macro plugin but it gave me errors. Any help on the procedure to do so?</p> ;;;; <p>Good idea; actually, can’t we be <em>sure</em> when the attenuation is “effectively zero” because we know the contrast limits (and thus the maximum possible value)?</p>
<p>EDIT: by that I mean, there is an exact value, rather than a heuristic like <code>1e6</code>.</p> ;;;; <p>I just had a thought for this after my brief introduction to the problem in the napari community meeting today.</p>
<p>I think it was <a class="mention" href="/u/myrk">@Myrk</a> who mentioned attenuated mip is even worse performance, however it can be even more amenable to such optimization. We should be able to break the raycasting loop once the attenuation is effectively zero.</p>
<p>For anyone familiar with the shader code, it could look something like this:</p>
<pre><code class="lang-diff">        sumval = sumval + clamp((val - clim.x) / (clim.y - clim.x), 0.0, 1.0);
+        scale = exp(-u_attenuation * (sumval - 1) / u_relative_step_size);
+        if( scale &lt; 1e-6 * clim.y ) {
+            iter = nsteps;  // this breaks the loop on the next iteration
+        } else if( val * scale &gt; maxval ) {
-        scaled = val * exp(-u_attenuation * (sumval - 1) / u_relative_step_size);
-        if( scaled &gt; maxval ) {
            maxval = val * scale;
            maxi = iter;
            max_loc_tex = loc;
         }
</code></pre>
<p>I just tested this with the provided example and see pretty significant performance improvement (up to 60 fps), but it’s hard to tell how it might affect image quality depending on specifics of a dataset. There’s probably a smarter way to check for “complete attenuation” to break the loop.</p> ;;;; <p>Um, it’s not a big code change so could be patched locally? but that seems less than ideal.<br>
It is a pretty annoying bug though, which all the more emphasizes that we need a release if only to get some of the bugfixes in the wild.</p>
<p>Regarding running dev version, we do have nightlies on conda, so this works pretty well without requiring anything special:<br>
<code>conda create -n napari-main -c "napari/label/nightly" -c conda-forge napari python=3.10</code><br>
(I use mamba, but whatever)<br>
Other option is to install via pip from GitHub:<br>
<code>python -m pip install "git+https://github.com/napari/napari.git#egg=napari[all]"</code></p> ;;;; <p>Thank you. I’ll give a go and see how I get on</p> ;;;; <p>There are a decent number of posts for calculating Feret angles if you have an object that can be used for that function.</p>
<p>Regardless, I think in your case you would need some way to pair the two lines (proximity of points?), and then it would be easiest to script the calculation of the angle directly from the XY coordinates of the points, assuming you had 3 points and knew which of the points was the connection.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://qupath.github.io/javadoc/docs/qupath/lib/roi/LineROI.html#getAllPoints()">
  <header class="source">

      <a href="https://qupath.github.io/javadoc/docs/qupath/lib/roi/LineROI.html#getAllPoints()" target="_blank" rel="noopener">qupath.github.io</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://qupath.github.io/javadoc/docs/qupath/lib/roi/LineROI.html#getAllPoints()" target="_blank" rel="noopener">LineROI (QuPath 0.4.0)</a></h3>

  <p>declaration: package: qupath.lib.roi, class: LineROI</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/andreas.m.arnold">@andreas.m.arnold</a>, this bug was recently fixed in <a href="http://github.com/napari/napari/pull/5449" class="inline-onebox" rel="noopener nofollow ugc">Bugfix: Ensure layer._fixed_vertex is set when rotating by psobolewskiPhD · Pull Request #5449 · napari/napari · GitHub</a>; unfortunately, it’s not yet released.</p>
<p>I don’t know if there’s a workaround other than using the dev version. <a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a> ?</p> ;;;; <p>Hello all,</p>
<p>First to say I’m new to programming i.e python etc so please forgive me and please correct me if I use the wrong terminology!</p>
<p>Secondly, Cellpose is brilliant, so thank you to the creators.</p>
<p>I have a question about how I could potentially utilise the Cellpose cell segmentation with multi class instance segmentation.</p>
<p>I have human tissue FFPE sections that are H&amp;E stained, I’ve uploaded an example of what the tissue looks like, and I am trying to segment cells and then assign a class to each cell “instance” e.g. lymphocyte, plasma cell, tumour cell etc.</p>
<p>I’ve partly trained a Cellpose custom model,  which works fairly well on my H&amp;E images and currently I have been using the Cellpose GUI to get an idea of its potential. From my understanding Cellpose isn’t built for H&amp;E images, however this has worked well so far.</p>
<p>Is there a way that I can also use Cellpose cell segmentation and multi class prediction? Is there a way of scripting in python to then predict cell type?<br>
Would I need to create another deep learning model to then predict cell type? or can this be integrated into the Cellpose model?</p>
<p>Depending on how this is possible will depend on how I create my masks. One method I could think of was including a class dictionary which contains a label for each cells instance ID.</p>
<p>Thank you in advance for your help, if any further information is required please let me know <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>FYI I had considered using Stardist but not all my cells may be star convex, tumour cells can have all sorts of shapes and are atypical by nature so I wanted to see if I could avoid running into this problem.</p>
<p>Furthermore, I have already created python script using unet and Tensorflow to perform semantic segmentation of tumour and stroma. So I have some basic (very) understanding. Although I know that Cellpose uses PyTorch.</p>
<p><a class="attachment" href="/uploads/short-url/unPmejI5AuBJQDsWaRfQu6NXFxn.tif">TCGA-BA-4074-01Z-00-DX1 [x=71168,y=11776,w=512,h=512].tif</a> (768.2 KB)</p> ;;;; <p>Hi all,</p>
<p>OMERO newbie here. I’m currently trying to access an OMERO server through my Ubuntu terminal. I can log in through a web browser, but when I try the steps described here: <a href="https://omero.readthedocs.io/en/stable/users/cli/installation.html" class="inline-onebox" rel="noopener nofollow ugc">Installation — OMERO documentation</a></p>
<p>I get as far as the “omero login” command. When I put in the server address and login details, I get the following:</p>
<p>WARNING:omero.client:…Ignoring error in client.<strong>del</strong>:&lt;class ‘Ice.ConnectFailedException’&gt;<br>
InternalException: Failed to connect: Ice.ConnectFailedException:<br>
No route to host</p>
<p>This is in a miniconda environment, where I have installed omero-py. It’s likely that I may have missed an obvious step somewhere, but I’m completely new to OMERO and also don’t have a lot of experience with Linux systems.</p>
<p>Thanks in advance for any help!</p>
<p>Tim</p> ;;;; <p>You can use something like this at the start:</p>
<pre><code class="lang-groovy">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)
if (getSelectedObjects().isEmpty())
    return;
</code></pre> ;;;; <p>I’m having this same exact issue. Did you guys figure this out?</p> ;;;; <p>Hi <a class="mention" href="/u/slaine_troyard">@Slaine_Troyard</a> and <a class="mention" href="/u/thomasboudier">@ThomasBoudier</a>,</p>
<p>if I understood correctly what you want to do, the following macro should no the job:</p>
<pre><code class="lang-java">//the image you got the ROIs from needs to be active, so IJ can catch its dimensions
newImage("Splitted ROIs", "16-bit black", getWidth(), getHeight(), nSlices);

run("3D Manager");

Ext.Manager3D_Count(nb_obj);

for (i = 0; i &lt; nb_obj; i++) {
	Ext.Manager3D_Select(i);
	Ext.Manager3D_FillStack(i, i, i);
	
}
setMinAndMax(0, nb_obj);
</code></pre> ;;;; <p>The pull request is here: <a href="https://github.com/bioconda/bioconda-recipes/pull/39932" class="inline-onebox" rel="noopener nofollow ugc">Add omero_ij plugin (ImageJ/Fiji) by lldelisle · Pull Request #39932 · bioconda/bioconda-recipes · GitHub</a><br>
Please <a class="mention-group notify" href="/groups/ome">@ome</a> have a look and comment on it.</p> ;;;; <p>We are still working on resolving this issues.</p>
<p>We tried reuploading the image that doesn’t generate a thumbnail and the Blitz log file throws this error;</p>
<pre><code class="lang-auto">2023-03-15 11:03:58,069 ERROR [        ome.services.util.ServiceHandler] (Server-161) java.lang.Error:  Wrapped Exception: (java.lang.UnsatisfiedLinkError):
'void org.libjpegturbo.turbojpeg.TJDecompressor.init()'
java.lang.UnsatisfiedLinkError: 'void org.libjpegturbo.turbojpeg.TJDecompressor.init()'
        at org.libjpegturbo.turbojpeg.TJDecompressor.init(Native Method)
        at org.libjpegturbo.turbojpeg.TJDecompressor.&lt;init&gt;(TJDecompressor.java:81)
        at loci.formats.services.JPEGTurboServiceImpl.getTile(JPEGTurboServiceImpl.java:349)
        at loci.formats.services.JPEGTurboServiceImpl.getTile(JPEGTurboServiceImpl.java:251)
        at loci.formats.in.NDPIReader.openBytes(NDPIReader.java:217)
        at loci.formats.ImageReader.openBytes(ImageReader.java:465)
        at loci.formats.ChannelFiller.openBytes(ChannelFiller.java:167)
        at loci.formats.ChannelFiller.openBytes(ChannelFiller.java:159)
        at loci.formats.ChannelSeparator.openBytes(ChannelSeparator.java:200)
        at loci.formats.ChannelSeparator.openBytes(ChannelSeparator.java:151)
        at loci.formats.ReaderWrapper.openBytes(ReaderWrapper.java:341)
        at ome.io.bioformats.BfPixelsWrapper.getWholePlane(BfPixelsWrapper.java:366)
        at ome.io.bioformats.BfPixelsWrapper.getPlane(BfPixelsWrapper.java:263)
        at ome.io.bioformats.BfPixelBuffer.getPlane(BfPixelBuffer.java:209)
        at omeis.providers.re.data.PlaneFactory.createPlane(PlaneFactory.java:208)
        at omeis.providers.re.HSBStrategy.getWavelengthData(HSBStrategy.java:100)
        at omeis.providers.re.HSBStrategy.makeRenderingTasks(HSBStrategy.java:231)
        at omeis.providers.re.HSBStrategy.render(HSBStrategy.java:328)
        at omeis.providers.re.HSBStrategy.renderAsPackedInt(HSBStrategy.java:292)
        at omeis.providers.re.Renderer.renderAsPackedInt(Renderer.java:558)
        at ome.services.ThumbnailBean.createScaledImage(ThumbnailBean.java:648)
        at ome.services.ThumbnailBean.retrieveThumbnail(ThumbnailBean.java:1255)
        at ome.services.ThumbnailBean.getThumbnailWithoutDefault(ThumbnailBean.java:1127)
at jdk.internal.reflect.GeneratedMethodAccessor1622.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
        at ome.security.basic.EventHandler.invoke(EventHandler.java:154)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at ome.tools.hibernate.SessionHandler.doStateful(SessionHandler.java:216)
        at ome.tools.hibernate.SessionHandler.invoke(SessionHandler.java:200)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99)
        at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:282)
        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at ome.tools.hibernate.ProxyCleanupFilter$Interceptor.invoke(ProxyCleanupFilter.java:249)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at ome.services.util.ServiceHandler.invoke(ServiceHandler.java:121)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)
        at com.sun.proxy.$Proxy110.getThumbnailWithoutDefault(Unknown Source)
        at jdk.internal.reflect.GeneratedMethodAccessor1622.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
        at ome.security.basic.BasicSecurityWiring.invoke(BasicSecurityWiring.java:93)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at ome.services.blitz.fire.AopContextInitializer.invoke(AopContextInitializer.java:43)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)
        at com.sun.proxy.$Proxy110.getThumbnailWithoutDefault(Unknown Source)
        at jdk.internal.reflect.GeneratedMethodAccessor1625.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at ome.services.blitz.util.IceMethodInvoker.invoke(IceMethodInvoker.java:172)
        at ome.services.throttling.Callback.run(Callback.java:56)
        at ome.services.throttling.InThreadThrottlingStrategy.callInvokerOnRawArgs(InThreadThrottlingStrategy.java:56)
        at ome.services.blitz.impl.AbstractAmdServant.callInvokerOnRawArgs(AbstractAmdServant.java:140)
        at ome.services.blitz.impl.ThumbnailStoreI.getThumbnailWithoutDefault_async(ThumbnailStoreI.java:132)
        at jdk.internal.reflect.GeneratedMethodAccessor1624.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
        at omero.cmd.CallContext.invoke(CallContext.java:85)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)
        at com.sun.proxy.$Proxy111.getThumbnailWithoutDefault_async(Unknown Source)
        at omero.api._ThumbnailStoreTie.getThumbnailWithoutDefault_async(_ThumbnailStoreTie.java:162)
        at omero.api._ThumbnailStoreDisp.___getThumbnailWithoutDefault(_ThumbnailStoreDisp.java:689)
        at omero.api._ThumbnailStoreDisp.__dispatch(_ThumbnailStoreDisp.java:1108)
        at IceInternal.Incoming.invoke(Incoming.java:221)
        at Ice.ConnectionI.invokeAll(ConnectionI.java:2536)
        at Ice.ConnectionI.dispatch(ConnectionI.java:1145)
        at Ice.ConnectionI.message(ConnectionI.java:1056)
        at IceInternal.ThreadPool.run(ThreadPool.java:395)
        at IceInternal.ThreadPool.access$300(ThreadPool.java:12)
        at IceInternal.ThreadPool$EventHandlerThread.run(ThreadPool.java:832)
        at java.base/java.lang.Thread.run(Thread.java:829)
2023-03-15 11:04:00,187 INFO  [        ome.services.util.ServiceHandler] (Server-162)  Excp:    java.lang.UnsatisfiedLinkError: 'void org.libjpegturbo.turbojpeg.TJDecompressor.init()'
</code></pre>
<p>Seems to be related to the thumbnail not been generated?, but only for a specific file? [ <a href="https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi" rel="noopener nofollow ugc">Llano_de_la_cruz_R15 - 2023-02-01 09.45.40.ndpi</a>]</p>
<p>Because as you can see in this screenshot, the other image thumbnail gets generated, even thou there is no pyramid …<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3.png" data-download-href="/uploads/short-url/AiYqJ7NjIVokO6zWCwapAug4bFF.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_690x388.png" alt="image" data-base62-sha1="AiYqJ7NjIVokO6zWCwapAug4bFF" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_1380x776.png 2x" data-dominant-color="DFE0E2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1080 167 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Dear Community,<br>
I ran into a problem when handling shapes in Napari.<br>
Here’s an examplary code snippet to reproduce the error:</p>
<pre><code class="lang-auto">import napari
import numpy as np

from skimage.data import astronaut

# set up viewer
viewer = napari.Viewer()
# add an image
viewer.add_image(astronaut())
# add a shapes layer
ellipse = np.array([[59, 222], [110, 289], [170, 243], [119, 176]])
viewer.add_shapes(ellipse,
                  shape_type='ellipse',
                  edge_width=5,
                  edge_color='coral',
                  face_color='purple',
                  )
# run viewer
napari.run()
</code></pre>
<p>The problem arises when I select the ellipse shape and try to rotate it (by clicking on the extra vertice that appears a bit farther out from the displayed bounding box). This seems to crash Napari and results in the following error message:</p>
<pre><code class="lang-auto">TypeError                                 Traceback (most recent call last)
File ~\MiniConda\envs\napari-env\lib\site-packages\vispy\app\backends\_qt.py:532, in QtBaseCanvasBackend.mouseMoveEvent(self=&lt;vispy.app.backends._qt.CanvasBackendDesktop object&gt;, ev=&lt;PyQt5.QtGui.QMouseEvent object&gt;)
    530 if self._vispy_canvas is None:
    531     return
--&gt; 532 self._vispy_mouse_move(
        self = &lt;vispy.app.backends._qt.CanvasBackendDesktop object at 0x000001D96CE223A0&gt;
        ev = &lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt;
    533     native=ev,
    534     pos=_get_event_xy(ev),
    535     modifiers=self._modifiers(ev),
    536 )

File ~\MiniConda\envs\napari-env\lib\site-packages\vispy\app\base.py:216, in BaseCanvasBackend._vispy_mouse_move(self=&lt;vispy.app.backends._qt.CanvasBackendDesktop object&gt;, **kwargs={'button': 1, 'buttons': [1], 'last_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_mouse_press': None, 'modifiers': (), 'native': &lt;PyQt5.QtGui.QMouseEvent object&gt;, 'pos': (693, 194), 'press_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;})
    213 else:
    214     kwargs['button'] = self._vispy_mouse_data['press_event'].button
--&gt; 216 ev = self._vispy_canvas.events.mouse_move(**kwargs)
        self = &lt;vispy.app.backends._qt.CanvasBackendDesktop object at 0x000001D96CE223A0&gt;
        self._vispy_canvas.events.mouse_move = &lt;vispy.util.event.EventEmitter object at 0x000001D96CE23550&gt;
        kwargs = {'native': &lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt;, 'pos': (693, 194), 'modifiers': (), 'buttons': [1], 'press_event': &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=False last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[694 194] press_event=None source=None sources=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_event': &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=False last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[694 194] press_event=None source=None sources=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_mouse_press': None, 'button': 1}
        self._vispy_canvas.events = &lt;vispy.util.event.EmitterGroup object at 0x000001D96CE23400&gt;
        self._vispy_canvas = &lt;VispyCanvas (PyQt5) at 0x1d971739130&gt;
    217 self._vispy_mouse_data['last_event'] = ev
    218 return ev

File ~\MiniConda\envs\napari-env\lib\site-packages\vispy\util\event.py:453, in EventEmitter.__call__(self=&lt;vispy.util.event.EventEmitter object&gt;, *args=(), **kwargs={'button': 1, 'buttons': [1], 'last_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_mouse_press': None, 'modifiers': (), 'native': &lt;PyQt5.QtGui.QMouseEvent object&gt;, 'pos': (693, 194), 'press_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;})
    450 if self._emitting &gt; 1:
    451     raise RuntimeError('EventEmitter loop detected!')
--&gt; 453 self._invoke_callback(cb, event)
        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;
        self = &lt;vispy.util.event.EventEmitter object at 0x000001D96CE23550&gt;
        cb = &lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;
    454 if event.blocked:
    455     break

File ~\MiniConda\envs\napari-env\lib\site-packages\vispy\util\event.py:471, in EventEmitter._invoke_callback(self=&lt;vispy.util.event.EventEmitter object&gt;, cb=&lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object&gt;&gt;, event=&lt;MouseEvent blocked=False button=1 buttons=[1] d...urces=[] time=1678893048.7535458 type=mouse_move&gt;)
    469     cb(event)
    470 except Exception:
--&gt; 471     _handle_exception(self.ignore_callback_errors,
        self = &lt;vispy.util.event.EventEmitter object at 0x000001D96CE23550&gt;
        cb = &lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;
        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;
        (cb, event) = (&lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;, &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;)
    472                       self.print_callback_errors,
    473                       self, cb_event=(cb, event))

File ~\MiniConda\envs\napari-env\lib\site-packages\vispy\util\event.py:469, in EventEmitter._invoke_callback(self=&lt;vispy.util.event.EventEmitter object&gt;, cb=&lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object&gt;&gt;, event=&lt;MouseEvent blocked=False button=1 buttons=[1] d...urces=[] time=1678893048.7535458 type=mouse_move&gt;)
    467 def _invoke_callback(self, cb, event):
    468     try:
--&gt; 469         cb(event)
        cb = &lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;
        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;
    470     except Exception:
    471         _handle_exception(self.ignore_callback_errors,
    472                           self.print_callback_errors,
    473                           self, cb_event=(cb, event))

File ~\MiniConda\envs\napari-env\lib\site-packages\napari\_qt\qt_viewer.py:1077, in QtViewer.on_mouse_move(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, event=&lt;MouseEvent blocked=False button=1 buttons=[1] d...urces=[] time=1678893048.7535458 type=mouse_move&gt;)
   1069 def on_mouse_move(self, event):
   1070     """Called whenever mouse moves over canvas.
   1071 
   1072     Parameters
   (...)
   1075         The vispy event that triggered this method.
   1076     """
-&gt; 1077     self._process_mouse_event(mouse_move_callbacks, event)
        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;
        self = &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;

File ~\MiniConda\envs\napari-env\lib\site-packages\napari\_qt\qt_viewer.py:1026, in QtViewer._process_mouse_event(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, mouse_callbacks=&lt;function mouse_move_callbacks&gt;, event=&lt;ReadOnlyWrapper at 0x000001D97E64A5C0 for MouseEvent&gt;)
   1024 layer = self.viewer.layers.selection.active
   1025 if layer is not None:
-&gt; 1026     mouse_callbacks(layer, event)
        event = &lt;ReadOnlyWrapper at 0x000001D97E64A5C0 for MouseEvent at 0x000001D97B124AC0&gt;
        layer = &lt;Shapes layer 'ellipse' at 0x1d9730a4460&gt;
        mouse_callbacks = &lt;function mouse_move_callbacks at 0x000001D96E445280&gt;

File ~\MiniConda\envs\napari-env\lib\site-packages\napari\utils\interactions.py:172, in mouse_move_callbacks(obj=&lt;Shapes layer 'ellipse'&gt;, event=&lt;ReadOnlyWrapper at 0x000001D97E64A5C0 for MouseEvent&gt;)
    169 obj._persisted_mouse_event[gen].__wrapped__ = event
    170 try:
    171     # try to advance the generator
--&gt; 172     next(gen)
        gen = &lt;generator object select at 0x000001D97E3BD5F0&gt;
    173 except StopIteration:
    174     # If done deleted the generator and stored event
    175     del obj._mouse_drag_gen[func]

File ~\MiniConda\envs\napari-env\lib\site-packages\napari\layers\shapes\_shapes_mouse_bindings.py:63, in select(layer=&lt;Shapes layer 'ellipse'&gt;, event=&lt;ReadOnlyWrapper at 0x000001D97E651080 for ReadOnlyWrapper&gt;)
     61     _drag_selection_box(layer, coordinates)
     62 else:
---&gt; 63     _move(layer, coordinates)
        layer = &lt;Shapes layer 'ellipse' at 0x1d9730a4460&gt;
        coordinates = (82.56905945524973, 190.80641155985776)
     65 # if a shape is being moved, update the thumbnail
     66 if layer._is_moving:

File ~\MiniConda\envs\napari-env\lib\site-packages\napari\layers\shapes\_shapes_mouse_bindings.py:492, in _move(layer=&lt;Shapes layer 'ellipse'&gt;, coordinates=(82.56905945524973, 190.80641155985776))
    487     offset = handle - layer._fixed_vertex
    488     layer._drag_start = -np.degrees(
    489         np.arctan2(offset[0], -offset[1])
    490     )
--&gt; 492 new_offset = coord - layer._fixed_vertex
        coord = [82.56905945524973, 190.80641155985776]
        layer = &lt;Shapes layer 'ellipse' at 0x1d9730a4460&gt;
        layer._fixed_vertex = None
    493 new_angle = -np.degrees(np.arctan2(new_offset[0], -new_offset[1]))
    494 fixed_offset = handle - layer._fixed_vertex

TypeError: unsupported operand type(s) for -: 'list' and 'NoneType'
</code></pre>
<p>Meanwhile, moving or resizing shapes works just fine.</p>
<p>Here some additional information on my Napari installation:</p>
<pre><code class="lang-auto">
(base) C:\&gt;conda activate napari-env

(napari-env) C:\&gt;napari --info
napari: 0.4.17
Platform: Windows-10-10.0.19045-SP0
Python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]
Qt: 5.15.2
PyQt5: 5.15.7
NumPy: 1.24.1
SciPy: 1.9.3
Dask: 2022.12.1
VisPy: 0.11.0
magicgui: 0.6.1
superqt: 0.4.0
in-n-out: 0.1.6
app-model: 0.1.1
npe2: 0.6.1

OpenGL:
  - GL version:  4.6.0 NVIDIA 411.31
  - MAX_TEXTURE_SIZE: 16384

Screens:
  - screen 1: resolution 1920x1200, scale 1.0
  - screen 2: resolution 1920x1200, scale 1.0

Plugins:
  - Partial-Aligner: 0.0.1 (2 contributions)
  - napari: 0.4.17 (77 contributions)
  - napari-console: 0.0.6 (0 contributions)
  - napari-crop: 0.1.7 (2 contributions)
  - napari-sift-registration: 0.1.2 (2 contributions)
  - napari-svg: 0.1.6 (2 contributions)
  - napari-tools-menu: 0.1.19 (0 contributions)

(napari-env) C:\&gt;
</code></pre>
<p>Does anybody have an idea what could cause this behavior and how to fix it?</p>
<p>Best,<br>
Andreas</p> ;;;; <p>Have you tried <a href="https://forum.image.sc/t/qupath-and-visium-json/59908/7" class="inline-onebox">QuPath and Visium JSON - #7 by petebankhead</a></p> ;;;; <p>Hi,</p>
<p>So I am using a script in python that I run headlessly where I call the plugin Linear Stack Alignment with SIFT. Here is my code.<br>
<a class="attachment" href="/uploads/short-url/rwbhoLdtqNSPYjUIPrnyidSYixh.txt">script.txt</a> (471 Bytes)<br>
What is strange is that after reading the documentation when I do IJ.run the imp variable should be override and contain the registered stack. However, after careful examination the saved file is exactly as my source file. I confirm that the paths are correct, that the stack is well read and I believe that the plugin is working as the prompt is showing for each slice that features are extracted with SIFT (exactly as if I was using Fiji from the interface).</p>
<p>Does anyone have an idea why is this happening?</p>
<p>Thank you.</p> ;;;; <p>Hello,<br>
Tried more approaches to this with no success, from what I can extract from the API the code below should work:</p>
<pre><code>    IJ.setTool("freeline");
	IJ.run("Line Width...", "line=15");
	RoiManager rm = new RoiManager();
	new WaitForUserDialog("Draw Line", "Draw a line on the central neuron").show();
	rm.addRoi(null);

	WritablePolyline testRoi = (WritablePolyline) roiService.getROIs(currentData);
	Roi[] theLine = rm.getRoisAsArray();
	roiService.add(theLine[0], skeletonisedImage);
	ROITree theRoi = roiService.getROIs((Dataset) skeletonisedImage);
	PolylineWrapper newRoi = new PolylineWrapper((WritablePolyline) theRoi);
	Point[] testVals = newRoi.getContainedPoints();
</code></pre>
<p>Unfortunately I get the error  class ij.gui.FreehandRoi is not a supported ROI type. I’m beginning to wonder if IJ1 ROI can be used at all with ImageJ2 and equally do the imgLib2 ROI’s actually work as I can find no way to select them.</p>
<p>The obvious workaround is to do all the ROI work in IJ1, however, if its an IJ2 plugin you cannot pass any IJ1 variables between functions as it causes errors. The IJ1 variables have to be kept in 1 function which leads to ugly, difficult to read code.</p>
<p>It would be great if any of the imgLib2 developers could comment on this, regions of interest feature somewhere in just about every plugin I’ve ever done. Some documentation would be great, something like the Jupyter notebooks documentation for ops which is very good. I realise documentation like that takes time and isn’t a fun task but it would make the transition to IJ2 easier/possible for IJ1 users such as myself.</p>
<p>Thank you</p>
<p>David</p> ;;;; <p>Using brightfield methods on a black background image will likely be problematic, I suspect you will want to treat the image as a fluorescence image. That lets you perform thresholding on red, on green, and then checking overlap for yellow (unfortunately).<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888.jpeg" data-download-href="/uploads/short-url/n3oZ33agrif0yyV9l0DfAGbhtb2.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_455x375.jpeg" alt="image" data-base62-sha1="n3oZ33agrif0yyV9l0DfAGbhtb2" width="455" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_455x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_682x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_910x750.jpeg 2x" data-dominant-color="302D2A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1276×1050 70 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f.jpeg" data-download-href="/uploads/short-url/gnf4NobtI0gEkOl0vSDVM4Z1Fld.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_471x375.jpeg" alt="image" data-base62-sha1="gnf4NobtI0gEkOl0vSDVM4Z1Fld" width="471" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_471x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_706x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_942x750.jpeg 2x" data-dominant-color="2A2E29"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1407×1118 80 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Also useful for this <a href="https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579" class="inline-onebox">Script for generating double threshold classifier</a></p> ;;;; <p>Dera QuPath developper,</p>
<p>Is there a way to load a json file containing the coordinate of spots for spatial transcriptiomics (10x Visum) in QuPath. It seems than TMA is close to spatial trans in term of format even if I don’t know if spatial trans spots have similar shape ?<br>
An evolution of the software in this direction could be really interesting…Thanks,</p> ;;;; <p><a class="mention" href="/u/petebankhead">@petebankhead</a>, everything worked well for my first project running in batch! I then tried for another project and it seems the script stalls in batch mode. The second project has the same annotation hierarchy, image sizes, and number of images. I did notice that I included an image that had no annotations when I ran in batch mode and it stalled there as it looks like it tried to classify the whole slide. Is there a way for the script to skip these images? I know I can not include them in the batch run list but I don’t have a working list of the images with no annotations at the moment.</p> ;;;; <p>Yes, these are the rectangles <img src="https://emoji.discourse-cdn.com/twitter/smile.png?v=12" title=":smile:" class="emoji" alt=":smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Yes, but you’d need to make a new <code>ImageData</code>.</p>
<p>Entirely untested code to demonstrate the idea:</p>
<pre><code class="lang-groovy">def imageData = getCurrentImageData()
def server = imageData.getServer()
def otherServer = new TransformedServerBuilder(server).extractChannels(0, 1).build()
def sneakyImageData = new ImageData&lt;&gt;(otherServer, imageData.getHierarchy(), imageData.getImageType())
runPlugin(sneakyImageData, ...)
</code></pre>
<p>Not sure if it works, but it’s what I’d try.</p> ;;;; <p>Hi <a href="https://forum.image.sc/groups/team">@team</a>!</p>
<p>We would like to join as a community partner with BioImage Model Zoo: <a href="https://bioimage.io/#/" rel="noopener nofollow ugc">bioimage.io</a>. Regarding the requirements:</p>
<ul>
<li>The source code of <a href="http://bioimage.io" rel="noopener nofollow ugc">bioimage.io</a> platform is released under MIT license. Available here: <a href="https://github.com/bioimage-io/bioimage.io/blob/8a1bd04b86da51d24eadb80f8c71dc14e8f80aca/LICENSE" rel="noopener nofollow ugc">bioimage.io MIT License</a>
</li>
<li>We already reference the forum as primary point of contact in our <a href="https://github.com/bioimage-io/bioimage.io/blob/8a1bd04b86da51d24eadb80f8c71dc14e8f80aca/README.md" rel="noopener nofollow ugc">readme.md in github</a> and in the about page of our website.</li>
<li>I could serve as community representative.</li>
<li>And here’s our logo (transparent background, so should work for both light and dark themes):</li>
</ul>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/1/c18b6788250f7b1a8273a7676902b52e4ec70efa.svg" alt="bioimage-io-icon" data-base62-sha1="rCaP6XAcln76LHRccH5M5PU61QK" width="500" height="500"></p>
<p>Thanks a lot!</p> ;;;; <p>sorry to bring up old stuff…</p>
<aside class="quote no-group" data-username="petebankhead" data-post="2" data-topic="78640">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<p>and then temporarily trick QuPath into passing that to the superpixel command</p>
</blockquote>
</aside>
<p>this would be by passing the <code>ImageData</code> to the <code>runPlugin</code> method?</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#runPlugin(java.lang.String,qupath.lib.images.ImageData,java.util.Map)">
  <header class="source">

      <a href="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#runPlugin(java.lang.String,qupath.lib.images.ImageData,java.util.Map)" target="_blank" rel="noopener nofollow ugc">qupath.github.io</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#runPlugin(java.lang.String,qupath.lib.images.ImageData,java.util.Map)" target="_blank" rel="noopener nofollow ugc">QP (QuPath 0.4.0)</a></h3>

  <p>declaration: package: qupath.lib.scripting, class: QP</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hey there,<br>
we have two issues usng the acquisition engine in pycromanager:</p>
<ol>
<li>When we take a multidimensional acquisition (MDA) using Acquisition.acquire(events) we get a resource warning from python (ResourceWarning: unclosed file &lt;_io.BufferedReader name=‘C:\temp\scan_5\scan_NDTiffStack.tif’&gt;). The acquisition works fine, however we are wondering if that is a big issue or if we can ignore the warning. Especially, if this slows down our code?!</li>
<li>Gerneral question: If we initialise xyz-events for a z-scan, the events give us a the correct z position (eg. 0.25um). However this seems not to be stored in the metadata (eg. when opening in ImageJ), it always shows 1um as z-step. Do we have to write this separately to the metadata or are we doing something wrong with intialising the xyz-events. Similar issue, if we initialise t-events it is still saved as a z-stack with an 1um z-step.</li>
</ol>
<p>Any help is greatly appreciated.</p>
<p><em>ev = multi_d_acquisition_events(xyz_positions=xyz)<br>
with Acquisition(directory=save_directory, name=save_name, pre_hardware_hook_fn=preHardwareFn,<br>
post_camera_hook_fn=cameraHookFn, show_display=False, saving_queue_size=5000) as acq:<br>
acq.acquire(ev)</em></p> ;;;; <p>Hi all, I am launching a new online course for bioimage analysis scientist about pathology called “Pathology 101 for tissue image analysis”. This is not the place to promote products, so just wanted to let you know about the launch and if you would like to learn more, comment below or send me a message, and I let you know more about it <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi <a class="mention" href="/u/thomasboudier">@ThomasBoudier</a></p>
<p>Thank you so much for the recommendation! I was able to create a labeled image with V4, but I couldn’t record the macros code for all the steps. Do you know if there is a reference page for batch analysis with 3d Manager V4? Thank you!</p> ;;;; <p>I guess <a class="hashtag" href="/tag/tifffile">#<span>tifffile</span></a> could probably help you here. From the <a href="https://pypi.org/project/tifffile/#examples">list of examples on the documentation page</a>:</p>
<blockquote>
<p>Overwrite the value of an existing tag, e.g., XResolution:</p>
<pre><code class="lang-auto">with TiffFile('temp.tif', mode='r+') as tif:
...     _ = tif.pages[0].tags['XResolution'].overwrite((96000, 1000))
</code></pre>
</blockquote>
<hr>
<p>This reply by <a class="mention" href="/u/cgohlke">@cgohlke</a> illustrates re-writing (which I understand you’d like to avoid), but shows the flexibility that <code>tifffile</code> offers when manipulating tiff pages and metadata:</p>
<aside class="quote quote-modified" data-post="2" data-topic="75627">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cgohlke/40/20613_2.png" class="avatar">
    <a href="https://forum.image.sc/t/resaving-ome-tiff-as-compressed-while-keeping-metadata-intact/75627/2">Resaving OME-TIFF as compressed while keeping metadata intact</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Consider the bfconvert tool with the -compression argument for re-compressing OME-TIFF files: <a href="https://docs.openmicroscopy.org/bio-formats/6.11.1/users/comlinetools/conversion.html#cmdoption-bfconvert-compression" rel="noopener nofollow ugc">https://docs.openmicroscopy.org/bio-formats/6.11.1/users/comlinetools/conversion.html#cmdoption-bfconvert-compression</a>. That should be the safest option for OME-TIFF. 
Another option to convert standard TIFF files between different compression schemes is the tiffcp tool, part of the libtiff library: <a href="http://www.simplesystems.org/libtiff/tools/tiffcp.html" rel="noopener nofollow ugc">http://www.simplesystems.org/libtiff/tools/tiffcp.html</a>. It won’t work with JPEG2000 compression and I’m no…
  </blockquote>
</aside>
 ;;;; <p>So it is 2022 and I am experiencing the same issue. I would be curious to know if anyone understood what is happening here…</p>
<p><a class="mention" href="/u/dgault">@dgault</a> do note that the values in the above example don’t match:<br>
the original values: 0.05-0.836ns / 3.195-6.373ns<br>
values in TIF exported via LASX (leica’s software): 50-836 / 3195-6373 (this is fine, a factor of 1000)<br>
<strong>values when importing .lif file directly into ImageJ:</strong> 13529-15073 / 17278-17917</p>
<p>it would be great to be able to extract the FLIM data without the need to access LASX…</p> ;;;; <p>Different LUT folders are now automatically detected, if you place them in the normal LUT folder of Fiji/ImageJ and can be selected via a right-click on the empty areas of the button panel.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026.png" data-download-href="/uploads/short-url/5B7lHspitqLHwDT8nIrQHFsoBRs.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_690x303.png" alt="image" data-base62-sha1="5B7lHspitqLHwDT8nIrQHFsoBRs" width="690" height="303" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_690x303.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_1035x454.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_1380x606.png 2x" data-dominant-color="A2A2A1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1480×652 60.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>It would be nice to change this whole ijm macro to a groovy script, but trying to just change the file extension and running that on FIJI doesn’t seem to work.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb.png" data-download-href="/uploads/short-url/wreYNjfxpUmVGo1ESj8AY3mFM15.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_690x50.png" alt="image" data-base62-sha1="wreYNjfxpUmVGo1ESj8AY3mFM15" width="690" height="50" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_690x50.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_1035x75.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_1380x100.png 2x" data-dominant-color="E6E6E6"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1886×138 81.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I essentially get this unexpected input error for every line. In the window that pops up when dragging the groovy file onto FIJI (the console?), the language tab at the top left does correctly indicate that it’s a groovy file that is being run.</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/22a0fac488de941112ff661a0e931189e0104065.png" alt="image" data-base62-sha1="4Wl880lu40GydrOtJeSqf51IFVz" width="330" height="489"></p> ;;;; <p>So I do get that log message, perhaps it’s a bit hard to see in the screenshot I sent but the log message doesn’t include .tiff at the end of it, it just ends with the name of the image.</p>
<p>In the code, I tried adding a</p>
<pre><code class="lang-auto">suffix = ".tif"
</code></pre>
<p>immediately prior to the proccessTiff function, but that didn’t change anything.</p>
<p>If what I have to adjust is the <code>file</code> then I don’t know how to do that. I don’t see a <code>file =</code> anywhere and the first reference to it comes as an input to the processFile function after the processFolder function (where it was presumably created).</p>
<p>Also, should the processTiff function be taking in <code>input</code>? Since the saveTiff function is saving to <code>output</code> where presumably the raw/unadjusted tiffs are located. To get around this for now I’ve just been selecting the same folder for both <code>input</code> and <code>output</code><br>
Edit: Probably nevermind on this input / output thing, I may have misunderstood the code</p> ;;;; <p>dear brainreg gurus,<br>
ping <a class="mention" href="/u/adamltyson">@adamltyson</a> <a class="mention" href="/u/alessandrofelder">@alessandrofelder</a> <a class="mention" href="/u/paddyroddy">@paddyroddy</a></p>
<p>we have acquired a full mouse brain on a zeiss lightsheet 7 (iDisco clearing) but struggle with the alignment to the Allen atlas, see command and screenshots of the output below.</p>
<p><code>brainreg ./C0_ds ./brainreg_out10 --atlas allen_mouse_25um --orientation sal -v 17.5011 3.7279 3.7279 --save-original-orientation</code></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5.png" data-download-href="/uploads/short-url/6rvUYHmXMsAr5JKmcDfg2KkqYjX.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5_2_690x434.png" alt="image" data-base62-sha1="6rvUYHmXMsAr5JKmcDfg2KkqYjX" width="690" height="434" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5_2_690x434.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5_2_1035x651.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5.png 2x" data-dominant-color="555759"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1098×691 158 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8.png" data-download-href="/uploads/short-url/vnRa9SAF2V5vPTc8E2kPqdENZvG.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8_2_690x430.png" alt="image" data-base62-sha1="vnRa9SAF2V5vPTc8E2kPqdENZvG" width="690" height="430" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8_2_690x430.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8_2_1035x645.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8.png 2x" data-dominant-color="535556"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1102×687 160 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I also tried reducing the <code>--bending-energy-weight </code> but the result did not improve much.</p>
<p>Would you have a pointer for me on how the registration could be improved?<br>
I have uploaded the (downsampled) raw data as well as some of my trials in case you’d like to have a look or give it a trial:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://drive.switch.ch/index.php/s/k2CqpRT6FKR88Zp">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/c/2cbb91e9d80628fed03b013cee5d9fe41a5aea94.png" class="site-icon" width="32" height="32">

      <a href="https://drive.switch.ch/index.php/s/k2CqpRT6FKR88Zp" target="_blank" rel="noopener nofollow ugc">SWITCHdrive</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/0/30b920a598e0f67f5231643db766096701cb7c7e.png" class="thumbnail onebox-avatar" width="194" height="194">

<h3><a href="https://drive.switch.ch/index.php/s/k2CqpRT6FKR88Zp" target="_blank" rel="noopener nofollow ugc">SWITCHdrive -</a></h3>

  <p>brainreg is publicly shared</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>If not, is my raw data maybe not of sufficient quality? If so, what should be better in the ideal case?</p>
<p>Thank you for your help!</p> ;;;; <p>Hi,</p>
<p>to calculate this particular property, you need to have a single cell segmentation of your data. Currently, you can achieve this in BiofilmQ only if you have either very well separated cells or if you import an external segmentation. You should not calculate this property with the cubed segmentation of BiofilmQ.</p>
<p>We are planning an update of BiofilmQ in the near future which will include single cell segmentation and also allow you to calculate the properties you are interested in directly with the software. Depending on how urgently you need your results, you could wait for this update, or, if you already have a single-cell segmentation from an external program, I could guide you through the process of writing a custom script.</p>
<p>Best,</p>
<p>Hannah</p> ;;;; <p>Thanks Henry,<br>
it did not work, but it is not a big issue to load the config twice. We will continue with loading it twice</p> ;;;; <p>Hi,</p>
<p>I stained sections with picrosirius red and took images with a polarised light microscope. I’m trying to use qupath to differentiate between red, green and yellow. I tried it by setting the image type to brightfieqld and then using colour deconvolution to separate the three. However, when I try to use them as a threshold nothing is selected, no matter what values I use. They also don’t appear in brightness and contrast menu so I’m not sure if I applied it correctly. I would use a normal threshold using RGB but since yellow is red+green that won’t work. Does anyone have experience analysing PSR with qupath, this is my first time using this stain.</p>
<p>thanks in advance!</p>
<p>The command I’ve been using</p>
<p>setColorDeconvolutionStains(‘{“Name” : “Collagen Types”, “Stain 1” : “Red Collagen”, “Values 1” : “0.06515 0.65628 0.66028”, “Stain 2” : “Yellow Collagen”, “Values 2” : “0.35717 0.56026 0.74735”, “Stain 3” : “Green Collagen”, “Values 3” : “0.47394 0.55093 0.68692”, “Background” : " 255 255 255"}’);</p>
<p>Example image</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71272c07e9c64f341bbbde6edbc71669413ff8bd.jpeg" data-download-href="/uploads/short-url/g8ZSxQUBFWe2FO81dQAQ9NXTzWd.jpeg?dl=1" title="Screenshot 2023-03-15 at 15.09.59" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71272c07e9c64f341bbbde6edbc71669413ff8bd.jpeg" alt="Screenshot 2023-03-15 at 15.09.59" data-base62-sha1="g8ZSxQUBFWe2FO81dQAQ9NXTzWd" width="531" height="500" data-dominant-color="080D07"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-15 at 15.09.59</span><span class="informations">778×732 30.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>An other option would be to install Cellpose plugin for Cellprofiler.<br>
However that plugin doesn’t work with the standalone Cellprofiler. You would need to install Cellprofiler from github repository.<br>
(Check here: <a href="https://forum.image.sc/t/new-cellprofiler-4-plugin-runcellpose/56858" class="inline-onebox">New CellProfiler 4 Plugin: RunCellpose</a>)</p>
<p>If all your images are like those you shared here <img src="https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12" title=":star_struck:" class="emoji" alt=":star_struck:" loading="lazy" width="20" height="20"> It should give very very accurate segmentation.</p> ;;;; <p>So cool, this is great news. Yes, I can do it. Which github account should I pin for approval?</p> ;;;; <p>Hi <a class="mention" href="/u/icala">@ICala</a>, can you confirm which version of Bio-Formats you are using? The original issue in this thread should have been fixed in any version over 6.7.0.</p> ;;;; <p>That looks like it has likely failed to find the first of the tiff files. You should see a log message “Processing: fileName”. If that filename is incorrect, or I suspect it may be missing the correct tiff extension, then you need to modify the processTiff function to ensure that the <code>open(input+file);</code> command has the correct filename and extension included.</p> ;;;; <p>Thank you. I’ll see if I can hire someone to my team that can work on it over the next few months.</p> ;;;; <p>I have uninstalled and reinstalled fiji (imagej) twice, ensured imagej and the bio-formats plug-in are up to date, and I am still unable to open mvd2 files (obtained using volocity). I verified in the bio-formats configuration menu that volocity file extensions were enabled. I am getting the following message:</p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 138MB of 24287MB (&lt;1%)</p>
<p>java.lang.IllegalStateException: Too early in import process: current step is FILE, but must be after STACK<br>
at loci.plugins.in.ImportProcess.assertStep(ImportProcess.java:778)<br>
at loci.plugins.in.ImportProcess.getReader(ImportProcess.java:306)<br>
at loci.plugins.in.Importer.run(Importer.java:99)<br>
at loci.plugins.LociImporter.run(LociImporter.java:78)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p> ;;;; <p>Adding the memoizer capability to the Bio-Formats plugin is certainly something that has been requested a number of times and is on our backlog of features we would like to include at some-point (<a href="https://github.com/ome/bioformats/issues/3823" class="inline-onebox">Plugins: Add option for memoizer · Issue #3823 · ome/bioformats · GitHub</a>).</p> ;;;; <p>Hi Greg, unfortunately there is still no fix in place for <a href="https://github.com/ome/bioformats/issues/3525">this issue</a>. If you are able to work on a fix for the reader we would love to help you include that in Bio-Formats. You can see some links to docs on how to contribute <a href="https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/CONTRIBUTING.md">here</a>, I can offer guidance and help with testing and integration if needed.</p> ;;;; <p>Hi <a class="mention" href="/u/lldelisle">@lldelisle</a><br>
That will certainly be useful<br>
The artifacts are built via GHA when a new tag is pushed. The permanent URL will the GitHub one. This is the license file for omero-insight <a href="https://github.com/ome/omero-insight/blob/0d1a4092ae773e505029a334eb96ae02086ad787/LICENSE.txt" class="inline-onebox">omero-insight/LICENSE.txt at 0d1a4092ae773e505029a334eb96ae02086ad787 · ome/omero-insight · GitHub</a>. It will also apply to the jar file.</p>
<p>Will you be working on the packaging?</p>
<p>Cheers<br>
Jean-Marie</p> ;;;; <p>Not familiar with cellprofiler, but if it has edge detection, running that in the membrane channel might generate a good binary image. Possibly through ImageJ integration if nothing else.</p> ;;;; <p>Hello Stephane,</p>
<p>I sent you some images yesterday. Can you confirm that you received them ?<br>
Thank you,</p>
<p>Alix</p> ;;;; <p>the error is due to the fact that the version of the library used by FlimFIT to connect to the OMERO server is out-of-date.<br>
FLIMFit connection to OMERO has not been updated for a while now.</p>
<p>Could you add the <code>flimfit</code> tag to the post so people using or working on FlimFit are aware?<br>
Thanks<br>
Cheers<br>
Jmarie</p> ;;;; <p>I’m having issues where only one quarter of the image shows up in the snapshot. It isn’t shrunk into one corner as with your images, but looks like it tiled  four images, and three are blank. Reducing the snapshot size produces a correct image. My guess was that it had to do with screen resolution, but I haven’t dwelved into it any further. It doesn’t always happen, sometimes it is fine.</p>
<p>I’m having other issues with it, the rotation pivot is set to the origin, and I don’t seem to be able to change it. I thought I just didn’t do it right, but it doesn’t happen on my laptop, so maybe this is somehow related?</p>
<p>As far as I can tell, these are the same versions of the Fiji and 3D viewer, and both on Windows 10.</p> ;;;; <p>In desperation, I created now below groovy script that I can launch via Fiji with the CLI.</p>
<p>(the fact that it creates another file with a new file name is a NextFlow issue and would normally not be needed)</p>
<pre><code class="lang-auto">// UI and CLI
//
#@File(label="Reference image file", style = "file") referenceImageFile
#@File(label="Target image file", style = "file") targetImageFile
#@File(label="Output image file", style = "file") outputImageFile

// code
//
DebugTools.setRootLevel( "OFF" )

// open image
//
println("Opening: " + referenceImageFile)
def reference = IJ.openImage( referenceImageFile.getAbsolutePath() )
println("Opening: " + targetImageFile)
def target = IJ.openImage( targetImageFile.getAbsolutePath() )

if (reference == null || target == null ) {
    println("Could not open all image files");
    System.exit(1)
}

// transfer calibration
//
target.setCalibration(reference.getCalibration())

// re-save target
//
println("Saving: " + outputImageFile)
IJ.save(target, outputImageFile.getAbsolutePath())

// exit
//
println("Calibration transfer done.")
System.exit(0)
</code></pre> ;;;; <p>Hi <a class="mention" href="/u/luigi_marongiu">@Luigi_Marongiu</a>,</p>
<p>the gaussian 50 was just to get the background area extracted to restrict the analysis, which is optional.<br>
the <code>setAutothreshold</code> method is recorded if you specify in the normal thresholding dialog from the dropdown menu a specific automatic thresholding algorithm. This has the advantage, that it adapts better to slight differences in your imaging setup.<br>
Best check the <code>Plugins &gt;Macros...&gt;Record...</code> function which records all your clicking steps.</p> ;;;; <p>thank you,<br>
you right about your assume.<br>
I use in ilastik 1.4.orc8 (I tried GPU too).<br>
here the error message.<br>
thank you<br>
<a class="attachment" href="/uploads/short-url/pn5YQVdaoVevPIpH2QM6It6kjOj.txt">log.txt</a> (67.3 KB)</p> ;;;; <p>Hi Trond,</p>
<p>Thanks for the explanation! There are often multiple ways to solve the same problem, glad that one is working well for you.</p> ;;;; <aside class="quote group-team" data-username="Stephane" data-post="2" data-topic="78132">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/stephane/40/361_2.png" class="avatar"> Stephane Dallongeville:</div>
<blockquote>
<p>you need to use <em>Protocols</em> plugin which allow design graphical analysis workflow and doing batches.<br>
You can take example on this protocol for instance :</p>
</blockquote>
</aside>
<p>I will try this. Merci Stephane</p> ;;;; <p>Sorry Jean-Yves it is a mistake. It was a question about Icy spot detector !!!</p>
<p>But I will have a question about trackmate.</p> ;;;; <p>Hi,</p>
<p>Yes it really helped, thanks a lot!! Thank you for explaining that it was not intended for RGB images and how I can show the single channels. I do work mainly with tiff and czi images but also ran into occasions where I only have jpg images available.</p>
<p>Since it is very difficult to add single layer powders with no overlaps you ran into segmentation problems and I am interested to try it out to see how well it performs <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>BR<br>
Fredrik</p> ;;;; <p>Hi, so I tried with the ‘C’ button pressed and without and it still gave me all these errors and the process was going on forever. I am working on 0.2.3, as this is an old analysis. When I tried on 0.4.3 though it completed the process in 30 sec, without any problem.</p> ;;;; <p>I realise that my question probably doesn’t even make sense, because for TIFF files this probably means rewriting the whole image including the pixel data, does it? Which really is not cool…especially for big data (that is an advantage of formats such as OME-Zarr or XML/BDV where one could modify metadata without touching the voxel data).</p> ;;;; <p><a class="mention" href="/u/joshmoore">@joshmoore</a> <a class="mention" href="/u/dgault">@dgault</a></p>
<p>I think I need a command line tool that transfers the pixel calibration from one TIFF to another.</p>
<p>Reason: some tools like CellPose and ilastik, afaik, currently don’t propagate the pixel calibration of the input image into their respective output images.</p>
<p>Does such a command line tool exist? Or is there some other solution?</p>
<p>ping <a class="mention" href="/u/nicokiaru">@NicoKiaru</a> <a class="mention" href="/u/imagejan">@imagejan</a> <a class="mention" href="/u/k-dominik">@k-dominik</a> <a class="mention" href="/u/cellpose">@cellpose</a></p> ;;;; <p>I’m afraid not. The superpixel stuff is very old and rather (by me) unloved code. Currently, QuPath will always request all channels for any image tile – even if it doesn’t need them.</p>
<p>I guess you <em>could</em> try creating a <a href="https://qupath.github.io/javadoc/docs/qupath/lib/images/servers/TransformedServerBuilder.html">TransformedImageServer</a> that extracts channels, and then temporarily trick QuPath into passing that to the superpixel command. It would be awkward though, and it would still have the overhead of requesting all the channels at the start.</p> ;;;; <p>Hi Julia,</p>
<p>I received a macro from Herbie, where he used getProfile to acquire and subsequently evaluate the derivative of the grayscale image intensity, and then crop at the position of the maximum derivative, since the intensity change is strongest at the substrate/bead transition. The rest was handled by thresholding and measuring the area.</p>
<p>Kind regards,<br>
Trond</p> ;;;; <p>Hi</p>
<p>working on a multi-channel image, I would like to generate superpixels based on a single channel. But so far I was unable to find a way to tell QuPath to only pass a selected channel to the plugin.</p>
<p>And given how the plugin loads the image data, I wonder if there is an easy way to do this:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/qupath/qupath/blob/88c7cc45648c1d5b09a840bd1e497ea9a46453aa/qupath-core-processing/src/main/java/qupath/imagej/superpixels/SLICSuperpixelsPlugin.java#L183">
  <header class="source">

      <a href="https://github.com/qupath/qupath/blob/88c7cc45648c1d5b09a840bd1e497ea9a46453aa/qupath-core-processing/src/main/java/qupath/imagej/superpixels/SLICSuperpixelsPlugin.java#L183" target="_blank" rel="noopener nofollow ugc">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/qupath/qupath/blob/88c7cc45648c1d5b09a840bd1e497ea9a46453aa/qupath-core-processing/src/main/java/qupath/imagej/superpixels/SLICSuperpixelsPlugin.java#L183" target="_blank" rel="noopener nofollow ugc">qupath/qupath/blob/88c7cc45648c1d5b09a840bd1e497ea9a46453aa/qupath-core-processing/src/main/java/qupath/imagej/superpixels/SLICSuperpixelsPlugin.java#L183</a></h4>



    <pre class="onebox"><code class="lang-java">
      <ol class="start lines" start="173" style="counter-reset: li-counter 172 ;">
          <li>				lastResultSummary = "No ROI selected!";</li>
          <li>				return null;</li>
          <li>			}</li>
          <li>			// Get a PathImage if we have a new ROI</li>
          <li>			if (!pathROI.equals(this.pathROI)) {</li>
          <li>				ImageServer&lt;BufferedImage&gt; server = imageData.getServer();</li>
          <li>				</li>
          <li>				double downsample = getPreferredDownsample(imageData, params);</li>
          <li>				</li>
          <li>				// Create an expanded request (we will clip to the actual ROI later)</li>
          <li class="selected">				var request = RegionRequest.createInstance(server.getPath(), downsample, pathROI)</li>
          <li>						.pad2D((int)Math.ceil(downsample * 2), (int)Math.ceil(downsample * 2))</li>
          <li>						.intersect2D(0, 0, server.getWidth(), server.getHeight());</li>
          <li>				</li>
          <li>				this.pathImage = IJTools.convertToImagePlus(server, request);</li>
          <li>				this.pathROI = pathROI;</li>
          <li>			}</li>
          <li>			</li>
          <li>			// Define maximum iterations</li>
          <li>			int maxIterations = params.getIntParameterValue("maxIterations");</li>
          <li>			double m = params.getDoubleParameterValue("regularization");</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>As I read it, the plugin uses RGB or colour deconvolved colour stacks, without providing an option to select one single colour.</p>
<p>So if the plugin does not have a colour parameter, is there a possibility in a groovy script to select a single channel?</p>
<p>ping <a class="mention" href="/u/petebankhead">@petebankhead</a> and colleagues.</p>
<p>greetings,</p>
<p>Felix</p> ;;;; <p>Thank you very much! It works!</p> ;;;; <p>Thank you, that is very kind. I am trying to replicate the procedure but I can’t find the command <code> setAutoThreshold</code>; is that an additional plugin? Also, a gaussian blur of 50 was too much, but it worked for me at 2.5.</p> ;;;; <p>Here I have four more pictures so you can get an impression.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad.jpeg" data-download-href="/uploads/short-url/f6pqeiHv10NkXTbnJf3GcFd7rEp.jpeg?dl=1" title="David_roh_2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_667x500.jpeg" alt="David_roh_2" data-base62-sha1="f6pqeiHv10NkXTbnJf3GcFd7rEp" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_1334x1000.jpeg 2x" data-dominant-color="3E1F29"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">David_roh_2</span><span class="informations">1388×1040 101 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd.jpeg" data-download-href="/uploads/short-url/2PF6lBAfqnzvNgheiq68i8SUuhn.jpeg?dl=1" title="Test4_automatischeBelichtung" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_667x500.jpeg" alt="Test4_automatischeBelichtung" data-base62-sha1="2PF6lBAfqnzvNgheiq68i8SUuhn" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_1334x1000.jpeg 2x" data-dominant-color="1E0F26"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Test4_automatischeBelichtung</span><span class="informations">1388×1040 72.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e.jpeg" data-download-href="/uploads/short-url/cwc2CemlbbZzwOa5ku77OlLnHNc.jpeg?dl=1" title="Test3_AlexaFluor250" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_667x500.jpeg" alt="Test3_AlexaFluor250" data-base62-sha1="cwc2CemlbbZzwOa5ku77OlLnHNc" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_1334x1000.jpeg 2x" data-dominant-color="28141F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Test3_AlexaFluor250</span><span class="informations">1388×1040 87.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428.jpeg" data-download-href="/uploads/short-url/l3ffxwNBcvPkRyQa592OUQ0wpKw.jpeg?dl=1" title="groß2_automatischeBelichtung" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_667x500.jpeg" alt="groß2_automatischeBelichtung" data-base62-sha1="l3ffxwNBcvPkRyQa592OUQ0wpKw" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_1334x1000.jpeg 2x" data-dominant-color="160B20"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">groß2_automatischeBelichtung</span><span class="informations">1388×1040 48.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p><a class="attachment" href="/uploads/short-url/lhe9ef8xSjl6wdR35ie39GurX9E.cpproj">David_roh.cpproj</a> (702.3 KB)<br>
<a class="attachment" href="/uploads/short-url/amZgRzf971WPs5qtgdIMgVGP2uS.cpproj">groß2_AlexaFluor150.cpproj</a> (702.3 KB)</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f.jpeg" data-download-href="/uploads/short-url/znCbf4xVCsE36LLE3pnFx0L70UT.jpeg?dl=1" title="David_roh_automatisch_belichtet" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_667x500.jpeg" alt="David_roh_automatisch_belichtet" data-base62-sha1="znCbf4xVCsE36LLE3pnFx0L70UT" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_1334x1000.jpeg 2x" data-dominant-color="1A1525"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">David_roh_automatisch_belichtet</span><span class="informations">1388×1040 56.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4.jpeg" data-download-href="/uploads/short-url/akcuJVgazKVFhpnNT6dCwAkmUF6.jpeg?dl=1" title="David_roh" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_667x500.jpeg" alt="David_roh" data-base62-sha1="akcuJVgazKVFhpnNT6dCwAkmUF6" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_1334x1000.jpeg 2x" data-dominant-color="1A0D2E"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">David_roh</span><span class="informations">1388×1040 56.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd.jpeg" data-download-href="/uploads/short-url/1yHp9zI2ePDOmT4OJUF33FIuA9L.jpeg?dl=1" title="groß_automatischeBelichtung" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_667x500.jpeg" alt="groß_automatischeBelichtung" data-base62-sha1="1yHp9zI2ePDOmT4OJUF33FIuA9L" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_1334x1000.jpeg 2x" data-dominant-color="170B1F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">groß_automatischeBelichtung</span><span class="informations">1388×1040 53.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8.jpeg" data-download-href="/uploads/short-url/tObNxTSMUFtZnW9rdiVeq3mU9VS.jpeg?dl=1" title="groß2_AlexaFluor150" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_667x500.jpeg" alt="groß2_AlexaFluor150" data-base62-sha1="tObNxTSMUFtZnW9rdiVeq3mU9VS" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_1334x1000.jpeg 2x" data-dominant-color="32181F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">groß2_AlexaFluor150</span><span class="informations">1388×1040 84.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017.jpeg" data-download-href="/uploads/short-url/sQC0JhGxwgcvfVu4zJpwyeno7PN.jpeg?dl=1" title="Test5_AlexaFluor370" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_667x500.jpeg" alt="Test5_AlexaFluor370" data-base62-sha1="sQC0JhGxwgcvfVu4zJpwyeno7PN" width="667" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_1334x1000.jpeg 2x" data-dominant-color="3B1D48"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Test5_AlexaFluor370</span><span class="informations">1388×1040 118 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Hello everybody,<br>
I am trying to determine the cell boundaries on the images in the attachment. Unfortunately, my results are not accurate enough so far. Does anyone of you maybe know a way how I can best determine the cell borders?<br>
I have attached two pipelines of mine. Maybe one of you has an idea how I should improve them for more accurate results. Furthermore, I have attached some more sample images so that you know how the images look like with which a pipeline should work.</p>
<p>I would really appreciate your help.<br>
With kind regards,<br>
Justine</p> ;;;; <p>Hey,</p>
<p>I am writing a short script to import a (log)file as OriginalFile.</p>
<p>This seems to work (seeing the script logs).<br>
But the script does not allow the user to download even though this is described <a href="https://omero.readthedocs.io/en/stable/developers/scripts/style-guide.html#script-outputs" rel="noopener nofollow ugc">here</a> and generally works for <em>File_Annotations</em> (that are also described there).</p>
<p>Script code snippet:</p>
<pre><code class="lang-auto">                mimetype = 'text/plain'
                obj = client.upload(export_file, type=mimetype)
                obj_id = obj.id.val
                print(f"Uploaded object {obj_id}/{type(obj)}:{obj}")
                
                message += output_display_name
                client.setOutput("Original_File", robject(obj))  # Original_File
</code></pre>
<p>I don’t have a particular file or dataset to attach this file to, so I’m just using Original File only.</p>
<p>How can I make this work so that you can download the file from response message?</p>
<p>Thanks in advance,</p>
<p>Torec</p> ;;;; <p>Hello <a class="mention" href="/u/liam">@liam</a> , it seems that the error is happening on the yaml file as you suggest.<br>
As you mention too, the python tool is mostly suggested for image2image models,<br>
For your model, the yaml file should be slightly different. If you want to return a table, the axis will have to be different. It seems that the output has only two dims (shape: [100, 6]) whereas the axes specified show 4 dimensions (bcyx).<br>
If you could provide the torchscript model, I will be able to give you a more detailed solution.<br>
REgards,<br>
Carlos</p> ;;;; <aside class="quote no-group" data-username="ADW123" data-post="5" data-topic="78608">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p><code>selectObjectsByClassification('ACST*','DC*','LCST*')</code></p>
</blockquote>
</aside>
<p>Just to clear the confusion for me, these regions are classifications of your rectangles? If it’s the classes of the larger annotations, then it makes sense again that it’s slow. I see that you are using<br>
<code>selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)</code> in the latest script, so I know that the point is somewhat moot, but I want to be sure.</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg" data-download-href="/uploads/short-url/hrsyEwW0fgLIWgVqLo2PLA1jWnl.jpeg?dl=1" title="Angle between annotations" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg" alt="Angle between annotations" data-base62-sha1="hrsyEwW0fgLIWgVqLo2PLA1jWnl" width="570" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 2x" data-dominant-color="D7D4D5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Angle between annotations</span><span class="informations">838×735 182 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h3>
<a name="sample-image-andor-code-1" class="anchor" href="#sample-image-andor-code-1"></a>Sample image and/or code</h3>
<h3>
<a name="background-2" class="anchor" href="#background-2"></a>Background</h3>
<p>I am doing a study on invasive fungal infections in which I need to analyze the fungal elements within tissue samples. In the picture, it is a fungal infection in a human lung, stained with Grocott methamine silver.</p>
<h3>
<a name="analysis-goals-3" class="anchor" href="#analysis-goals-3"></a>Analysis goals</h3>
<p>One of the factors I want to evaluate is the angle with which the fungal hyphae branch (yellow on the picture). Is it possible to measure the angle between two linear annotations in QuPath?</p>
<h3>
<a name="challenges-4" class="anchor" href="#challenges-4"></a>Challenges</h3>
<p>I am a fairly new user of QuPath and are only familiar with the fundamentals. I have tried to look through previous questions in this forum as well as available tutorials, without any luck. Hopefully some of you out there can help me! <img src="https://emoji.discourse-cdn.com/twitter/pray.png?v=12" title=":pray:" class="emoji" alt=":pray:" loading="lazy" width="20" height="20"><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg" data-download-href="/uploads/short-url/hrsyEwW0fgLIWgVqLo2PLA1jWnl.jpeg?dl=1" title="Angle between annotations" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg" alt="Angle between annotations" data-base62-sha1="hrsyEwW0fgLIWgVqLo2PLA1jWnl" width="570" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 2x" data-dominant-color="D7D4D5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Angle between annotations</span><span class="informations">838×735 182 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Actually one more thing to be sure - colors of trajectories are generated randomly, so could you pleas track and visualize trajectories once again and see if it helps (or confirm that there are no trajectory detected for middle particle)?</p> ;;;; <p>Hi <a class="mention" href="/u/amitbenben">@Amitbenben</a>,</p>
<p>You can run the cell detection algorithm on any 3D image by using the napari plugin. The instructions are here:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://docs.brainglobe.info/cellfinder-napari/introduction">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/9995eaa313ce50caffc90f5047ed4758d1c96a3e.png" class="site-icon" width="256" height="256">

      <a href="https://docs.brainglobe.info/cellfinder-napari/introduction" target="_blank" rel="noopener">docs.brainglobe.info</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/362;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da_2_690x362.png" class="thumbnail" width="690" height="362" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da_2_690x362.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da_2_1035x543.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da.png 2x" data-dominant-color="F8FAFC"></div>

<h3><a href="https://docs.brainglobe.info/cellfinder-napari/introduction" target="_blank" rel="noopener">Introduction</a></h3>

  <p>About the cellfinder napari plugin</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>However, as your images are fairly small, you may want to use a cell segmentation (rather than just soma detection) tool instead, something like <a href="https://www.cellpose.org/">cellpose</a>.</p>
<p>Adam</p> ;;;; <p>Dear all,</p>
<p>I would like to get DeepLabCut (DLC) set up on Amazon Web Service EC2 for the training and analyzing part of the pipeline.  I would like to use one of their instances designed for deep learning. For those who already managed to run DLC using AWS could you please share with the community the <strong>AWS configuration you use? For instance, which Amazon Machine Image (AMI) you chose?</strong> It would be great if you share your experience or give any other recommendation here since there is not much information online about this.</p>
<p>Thank you in advance.</p>
<p>Best,</p> ;;;; <p>Have you tried deleting and reinstalling? If a mixture of old and new files exist in the folder then I’d expect things to fail.</p>
<p>Although in Ubtunu there isn’t really any installation involved – just extracting the compressed files, setting the permissions, and running: <a href="https://qupath.readthedocs.io/en/0.4/docs/intro/installation.html" class="inline-onebox">Installation — QuPath 0.4.3 documentation</a></p> ;;;; <p>Progress! Did you restart QuPath in between though, or  use <em>View → Show memory monitor</em> to clear the tile cache? If not, then the improved performance could simply be that the pixels are already available when you run the second script – so the slow bit of the task if already complete.</p>
<p>I don’t have any ideas right now how to improve things further. If you really want to dig into it, then <a href="https://visualvm.github.io">VisualVM</a> with CPU sampling can help identify precisely where the slow bits are…  which can be useful, but doesn’t mean we’ll necessarily be able to find a way to speed them up anyway.</p>
<p>It seems we really need to look deeper into optimizing the performance within QuPath itself.</p> ;;;; <aside class="quote no-group" data-username="ravikumar" data-post="1" data-topic="78603">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/r/ed8c4c/40.png" class="avatar"> Ravi Kumar:</div>
<blockquote>
<p>How can I change the color or the tracked trajectories? or Can I make it all with a single color? (i.e. blue/green/etc. for all particles)</p>
</blockquote>
</aside>
<p>Unfortunately it is not possible. Visualization of trajectories is just a helper tool and hence it is kept as simple as possible.</p>
<p>Could you please check if this middle particle is a part of any trajectory?<br>
Maybe you cannot see any trajectory since there is no one… You can just open “All Trajectories to Table” and check if there is trajectory with coordinates of your  particle in the middle.</p> ;;;; <p>I was able to resolve the problem myself. I turns out the <em>roiManager(“save”)</em> command automatically saves all ROIs in the Group selected as default except when the default group is 0.<br>
So I was able to circumvent the problem by adding <em>Roi.setDefaultGroup(0)</em> in the beginning of my macro. The previous success with adding a line of code was only an accident because to test the system I marked ROIs randomly to see if it works and by chance I ended with the default group = 0 after editing the macro.</p> ;;;; <p>I update the old version (0.3.2) with the related version (v.0.4.3) by overwriting all files. Then this  main program can not be opened as following notes:</p>
<blockquote>
<p>Exception in thread “main” java.lang.NoSuchMethodError: ‘java.lang.String ch.qos.logback.core.util.EnvUtil.logbackVersion()’<br>
at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:81)<br>
at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:77)<br>
at ch.qos.logback.classic.spi.LogbackServiceProvider.initializeLoggerContext(LogbackServiceProvider.java:50)<br>
at ch.qos.logback.classic.spi.LogbackServiceProvider.initialize(LogbackServiceProvider.java:41)<br>
at org.slf4j.LoggerFactory.bind(LoggerFactory.java:167)<br>
at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:154)<br>
at org.slf4j.LoggerFactory.getProvider(LoggerFactory.java:437)<br>
at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:423)<br>
at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:372)<br>
at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:398)<br>
at qupath.QuPath.(QuPath.java:90)</p>
</blockquote>
<p><strong>Desktop (please complete the following information):</strong></p>
<ul>
<li>OS:  Ubuntu 20.04</li>
<li>QuPath Version:  0.4.3</li>
</ul> ;;;; <p>Thanks, <a class="mention" href="/u/petebankhead">@petebankhead</a>!</p>
<p>So I made the following changes:</p>
<pre><code class="lang-auto">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)

def imageData = getCurrentImageData()
def classifier = loadPixelClassifier('PGM1')
def classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)
def manager = classifierServer.getTileRequestManager()
def tileRequests = new HashSet&lt;&gt;()
for (def annotation in getSelectedObjects()) {
    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())
    tileRequests.addAll(manager.getTileRequests(request))
}
tileRequests.parallelStream()
    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }
    
addPixelClassifierMeasurements("PGM1", "PGM1")

</code></pre>
<p>It seems to run in half the time. I then ran the following which seems to run faster:</p>
<pre><code class="lang-auto">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)

def tempRequest = RegionRequest.createInstance(getCurrentServer().getPath(), 1.0, 0, 0, 10, 10)
def tempImg = getCurrentServer().readBufferedImage(tempRequest)

def imageData = getCurrentImageData()
def classifier = loadPixelClassifier('PGM1')
def classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)
def manager = classifierServer.getTileRequestManager()
def tileRequests = new HashSet&lt;&gt;()
for (def annotation in getSelectedObjects()) {
    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())
    tileRequests.addAll(manager.getTileRequests(request))
}
tileRequests.parallelStream()
    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }
    
addPixelClassifierMeasurements("PGM1", "PGM1")

</code></pre>
<p>Anything else you think I should adjust?</p> ;;;; <p>In DLC-live-ma I am tracking 18 body parts in <strong>10 animals</strong>, so the output from DLC is a numpy array of shape <strong>10</strong>, 18, 4.</p>
<p>Then I feed a video with only <strong>5</strong> animals and all are detected. So the array’s first dimension gets populated until position 5, and from 5 onward it’s al <em>nan</em>.</p>
<p>Every time DLC loses track of an animal for a few frames and then re-aquires it, it gives this animal a new ID. So the next position in the array gets populated.</p>
<p>The problem is that after it populates the last position (<strong>10</strong>), it stops tracking any “new” animals. Since every once in a while tracking of an animal is lost, in a few minutes the entire array is nan and no new detections are made.</p>
<p>To get around that I set the number of animals as high as possible, such as 5000 animals. But even then, after a few minutes the entire array is used and I start getting all <em>nan</em>.</p>
<p>Is there a setting to ignore animal ID, or how can I make DLC re-use animal IDs (for example if the animal has not been detected after 5 or 10 frames)?</p> ;;;; <p>Hi everyone,</p>
<p>I am relatively new to ImageJ and to coding in general, so I do not have a firm background in the inner workings of Java or FiJi. For a project I want to mark cells in a culture as either belonging to one group or another. I am using the ROI-group feature in FiJi to manually sort them into either group 0 or group 1. When I then try to save these ROIs either manually in the ROI manager or via the <em>rioManager(“save”)</em><br>
command in a macro it sometimes happens that the command moves all the ROIs to group 1 before saving. When I then try it again after opening the macro in the editor, changing any line of code (not necessarily in the vicinity of the <em>save</em>-command like adding a <em>print</em>-statement) and then saving the macro, the command works as intended. What might be the source of this error and how can I prevent it in the future?</p>
<p>Thanks in advance!</p>
<p>Lucas</p> ;;;; <p>Hello everyone,</p>
<p>I have trained a Classifier to detect objects in order to create Annotations.<br>
The next step would be to detect the Cells, while being able to tell in which Annotation they can be found.</p>
<p>Problem: After I have run the cell detection, those Annotations have been deleted. Only the annotations, that I have made myself (without the Classifier) are left.</p>
<p>What I have already tried:</p>
<ul>
<li>run the Cell detection before training the Classifier: when I create the Superpixels for the Classifier, the Cells get deleted and vice versa</li>
<li>locking the Annotations</li>
</ul>
<p>Thank you so much in advance, I really appreciate your help.</p>
<p>Regards,<br>
Alexandra <img src="https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12" title=":slightly_smiling_face:" class="emoji" alt=":slightly_smiling_face:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi <a class="mention" href="/u/jennifer_fessler">@Jennifer_Fessler</a>,</p>
<p>you could use a variable for the name of the table, that way if ever you want to change the name you have to do it only in one place.</p>
<p>I don’t think that you need to call the <code>Table.update</code>after each <code>set</code>. Either once at the end or once per row might be enough, but I admit that the updating behavior of the ImageJ tables is a mystery to me.</p>
<p>Best,<br>
Volker</p> ;;;; <aside class="quote no-group" data-username="ADW123" data-post="7" data-topic="78608">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>Given that the tissue itself has been annotated (image attached below) would the new script still be requesting classifications for every pixel in the image?</p>
</blockquote>
</aside>
<p>Hmmm, it <em>shouldn’t</em> be.</p>
<p>But you ought to put the <code>selectObjects</code> line up at the top of the script in your last post – since <code>getSelectedObjects()</code> is used later. So in that version, it’s likely that the ‘requesting tiles in parallel’ isn’t actually doing anything, and the performance is the same as if the 2-lines at the end were run only.</p> ;;;; <p>Hello,<br>
I’ve made a little progress on this, I can get the line coordinates for a single line using the code below</p>
<pre><code>    IJ.setTool("freeline");
	IJ.run("Line Width...", "line=1");
   	
	RoiManager rm = new RoiManager();
	new WaitForUserDialog("Draw Line", "Draw a line on the central neuron").show();
	rm.addRoi(null);
	
	//Works for single line
	PolygonRoi theRoi = (PolygonRoi) rm.getRoi(0);
	UnmodifiablePolylineRoiWrapper test = new UnmodifiablePolylineRoiWrapper(theRoi);
	List&lt;RealLocalizable&gt; coordVals = test.vertices(); 
</code></pre>
<p>However, this wrapper isn’t usable on thicker lines. There are other classes within the package</p>
<h1>
<a name="netimagejlegacyconvertroipolylinehttpsjavadocscijavaorgimagej2netimagejlegacyconvertroipolylinepackage-summaryhtml-1" class="anchor" href="#netimagejlegacyconvertroipolylinehttpsjavadocscijavaorgimagej2netimagejlegacyconvertroipolylinepackage-summaryhtml-1"></a><a href="https://javadoc.scijava.org/ImageJ2/net/imagej/legacy/convert/roi/polyline/package-summary.html" rel="noopener nofollow ugc">net.imagej.legacy.convert.roi.polyline</a>
</h1>
<p>I can’t seem to get any of the ones for thicker lines to provide any coordinates. Has anyone used them and can give advice.</p>
<p>Thanks</p>
<p>David</p> ;;;; <p>I was wondering if cellprofiler offers Java APIs so I can use the particular module (i.e feature extraction). I found on the GitHub it is possible on Python but no info on Java.</p>
<p>Thanks everyone in advance.</p> ;;;; <p>Exactly what I was looking for!<br>
Thank you <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi, <a href="https://gist.github.com/petebankhead/bea34ee1716b67246c2308808dd3c025" rel="noopener nofollow ugc">this</a> was the script I was using.</p> ;;;; <p>Thanks, <a class="mention" href="/u/petebankhead">@petebankhead</a>! I have tried the more targeted script:</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
def classifier = loadPixelClassifier('PGM1')
def classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)
def manager = classifierServer.getTileRequestManager()
def tileRequests = new HashSet&lt;&gt;()
for (def annotation in getSelectedObjects()) {
    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())
    tileRequests.addAll(manager.getTileRequests(request))
}
tileRequests.parallelStream()
    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }
    
selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)
addPixelClassifierMeasurements("PGM1", "PGM1")

</code></pre>
<p>This still seems to have the same time-related concern as before (ie over 10 minutes on one slide). Given that the tissue itself has been annotated (image attached below) would the new script still be requesting classifications for every pixel in the image?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg" data-download-href="/uploads/short-url/zA4foiBfkVdcs62ejjYW44cbxsq.jpeg?dl=1" title="Screen Shot 2023-03-15 at 2.41.35 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg" alt="Screen Shot 2023-03-15 at 2.41.35 AM" data-base62-sha1="zA4foiBfkVdcs62ejjYW44cbxsq" width="646" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_969x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg 2x" data-dominant-color="D9D6D9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-15 at 2.41.35 AM</span><span class="informations">1270×982 113 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I tried adjusting the script to the following but it didn’t change anything:</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
def classifier = loadPixelClassifier('PGM1')
def classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)
def manager = classifierServer.getTileRequestManager()
def tileRequests = new HashSet&lt;&gt;()
for (def annotation in getSelectedObjects()) {
    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI(){it.getROI().getNumPoints() == 4})
    tileRequests.addAll(manager.getTileRequests(request))
}
tileRequests.parallelStream()
    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }
    
selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)
addPixelClassifierMeasurements("PGM1", "PGM1")

</code></pre> ;;;; <p>Close, but not the only problem. <em>“… based on the current camera settings”</em> made me aware that the camera device methods GetImageWidth() etc. MUST reflect what the images will be like. After fixing that some other issues also disappeared, so I’ll call that a solution <img src="https://emoji.discourse-cdn.com/twitter/wink.png?v=12" title=":wink:" class="emoji" alt=":wink:" loading="lazy" width="20" height="20"></p>
<p>Unfortunately I still can’t get a live-view in napari-micromanager, so this is probably not the same problem.</p> ;;;; <p>Dear all,</p>
<p>I’m happy to announce Fast4DReg, a new tool for correcting drift in 4D microscopy data. This tool was specifically built to remove lateral and axial drift in fluorescent 3D live cell imaging data. Fast4DReg does so by creating intensity projections of each time point and using cross-correlation to estimate the amount of drift. This estimated drift can be directly used to correct the drift in the same video, or it can correct drift in another video, for example, another channel. Using the intensity projections for drift estimation makes this tool especially fast. Additionally, Fast4DReg can correct misaligned channels, for example in the case where multiple cameras are used for multichannel image acquisition. The drift correction is performed the same way as for 3D videos, but prior to drift estimation, the channels are converted into time points (and back to the channel after correction).</p>
<p>Availability:</p>
<ul>
<li>
<p>Fast4DReg is available through Fiji update sites.</p>
</li>
<li>
<p>In <a href="https://github.com/guijacquemet/Fast4DReg" rel="noopener nofollow ugc">Github</a> we provide detailed step-by-step instructions on how to use the tool.</p>
</li>
<li>
<p>In <a href="https://zenodo.org/record/7514913#.ZAc-W3ZBwQ8" rel="noopener nofollow ugc">Zenodo</a>, we share all data used for testing the tool.</p>
</li>
<li>
<p>The publication can be found here: <a href="https://doi.org/10.1242/jcs.260728" rel="noopener nofollow ugc">Journal of Cell Science: Fast4DReg – fast registration of 4D microscopy datasets</a>.</p>
</li>
<li>
<p>Read more in my <a href="https://focalplane.biologists.com/2023/03/10/fast4dreg-to-the-rescue-of-your-drifty-microscopy-data/" rel="noopener nofollow ugc">Focal Plane Blog post.</a></p>
</li>
</ul>
<p>Please don’t hesitate to reach out if you need help getting started!</p>
<p>Best regards,<br>
Joanna Pylvänäinen and <a class="mention" href="/u/guillaume_jacquemet">@Guillaume_Jacquemet</a><br>
email: <a href="mailto:joanna.pylvanainen@abo.fi">joanna.pylvanainen@abo.fi</a><br>
web: <a href="https://cellmig.org/" rel="noopener nofollow ugc">https://cellmig.org/</a><br>
Twitter: <a href="https://twitter.com/JwPylvanainen" rel="noopener nofollow ugc">https://twitter.com/JwPylvanainen</a></p> ;;;; <aside class="quote no-group" data-username="oburri" data-post="4" data-topic="78608">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/oburri/40/3464_2.png" class="avatar"> Olivier Burri:</div>
<blockquote>
<p>Is it just me or is it running slowly because you’re running the pixel classifier on all your annotations each time you go through the loop.</p>
</blockquote>
</aside>
<p>Well-spotted! I didn’t notice this when I looked at the thread this morning…</p>
<p>Another two-line version of the script would be:</p>
<pre><code class="lang-groovy">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)
addPixelClassifierMeasurements("PGM1", "PGM1")
</code></pre>
<aside class="quote no-group" data-username="ADW123" data-post="1" data-topic="78608">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>I did also try pre-requesting tiles using this script:</p>
</blockquote>
</aside>
<p>That will probably make it much slower, since it’s requesting classifications for every pixel in the image – even outside the annotations.</p>
<p>You’d need something more selected. This might work (although I haven’t properly tested it to check):</p>
<pre><code class="lang-groovy">def manager = classifierServer.getTileRequestManager()
def tileRequests = new HashSet&lt;&gt;()
for (def annotation in getSelectedObjects()) {
    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())
    tileRequests.addAll(manager.getTileRequests(request))
}
tileRequests.parallelStream()
    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }
    
</code></pre> ;;;; <p>Thanks <a class="mention" href="/u/oburri">@oburri</a>! I’ve updated to the following code:</p>
<pre><code class="lang-auto">selectObjectsByClassification('ACST*','DC*','LCST*')
addPixelClassifierMeasurements("PGM1", "PGM1")
</code></pre>
<p>However, that seems to also be quite time-intensive in contrast to the GUI.</p>
<p>I then tried the following to no avail:</p>
<pre><code class="lang-auto">def tempRequest = RegionRequest.createInstance(getCurrentServer().getPath(), 1.0, 0, 0, 10, 10)
def tempImg = getCurrentServer().readBufferedImage(tempRequest)

def imageData = getCurrentImageData()
def classifier = loadPixelClassifier('PGM1')
def classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)

classifierServer.getTileRequestManager().getAllTileRequests()
    .parallelStream()
    .forEach { PixelClassificationImageServer.readRegion(it.getRegionRequest()) }

</code></pre>
<p>Not sure if I’m going wrong somewhere still?</p> ;;;; <p>Only 2 more hours to go for the first community call! For both sessions, this is the hackmd: <a href="https://hackmd.io/BqnK9Wm4QpGYAhYOoaFBQQ" rel="noopener nofollow ugc">OME-NGFF community call: 2023-03-15 - HackMD</a></p>
<p>Hope to see many of you there!<br>
Wouter-Michiel</p> ;;;; <aside class="quote no-group" data-username="ADW123" data-post="1" data-topic="78608">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<pre><code class="lang-auto">def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}

for (annotation in annotations) {

getCurrentHierarchy().getSelectionModel().setSelectedObjects(annotations, null)

addPixelClassifierMeasurements("PGM1", "PGM1")

}
</code></pre>
</blockquote>
</aside>
<p>Is it just me or is it running slowly because you’re running the pixel classifier on all your annotations each time you go through the loop.</p>
<p>The whole point of selecting the objects is that you do not have to do it in a loop <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>you could also make it more explanable by using <code>selectObjectsByClassification()</code> instead of using <code>findAll()</code></p>
<pre><code class="lang-auto">selectObjectsByClassification(null) // or a string with the classification of your boxed if they have one
addPixelClassifierMeasurements("PGM1", "PGM1")
</code></pre> ;;;; <p><a class="attachment" href="/uploads/short-url/1eWjYG2N5N03AVm3kaXo6EDfQyn.tif">2 mm after.tif</a> (177.4 KB)<br>
<a class="attachment" href="/uploads/short-url/59t0kg2XjEoUSbkOH6AKLsNIcAh.tif">2 mm before.tif</a> (177.4 KB)</p>
<p>Thank you very much for your reply. I really appreciate your help. Here are my images.</p> ;;;; <p>Hi everyone,</p>
<p>Thank you in advance for any help. I have a quick question about relate objects and colocalization. I noticed today that, unlike other times I’ve used relate objects for fos and another stain, I was getting a colocalization count higher than possible if it was 1-to-1. Does the order of the relate objects for parent and child matter such that the parent should always be the stain with the smaller number of objects? Or is there another step I can take to ensure there’s only 1 fos object per stained cell? Thank you!!</p>
<p>Best,<br>
Brandon</p> ;;;; <p>Hi<br>
It is difficult to give you advice without seeing the study support. Can you upload two images:</p>
<ul>
<li>a precisely annotated</li>
<li>one not annotated<br>
Thanks in advance.</li>
</ul>
<pre><code class="lang-auto">macro "Thinnest area of dentin measurement"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
img=getImageID();
// Start batch mode
//------------------------------
selectImage(img);
setBatchMode(true);
run("Duplicate...", "title=1");
close("\\Others");
run("Duplicate...", "title=[The result]");
run("Duplicate...", "title=2");
// Start processing
//------------------------------
run("Convert to Mask");
run("Duplicate...", "title=temp");
run("Invert");
//------------------------------
run("Find Maxima...", "prominence=10 strict exclude output=[Point Selection]");
run("Find Maxima...", "prominence=10 strict exclude output=List");
close("temp");
Table.sort("X");
//------------------------------
//distance between points
dx=getResult("X",0)-getResult("X",1);
dy=getResult("Y",0)-getResult("Y",1);
d=sqrt(pow(dx,2)+pow(dy,2));
R=d;// User choice
//------------------------------
for(j=0;j&lt;2;j++){
//------------------------------
Table.create("Measure Results for point  n°: "+(j+1));
//------------------------------
for(i=0;i&lt;36;i++){
t=i*10*(PI/180); // i*(10 degrees) 
angle=i*10;
// add to a table
//------------------------------
Table.set("Idx", i, i);
Table.set("Angle_degrees", i, angle);
Table.set("Angle_rad", i, t);
//------------------------------
selectWindow("2");
makeLine(getResult("X",j),getResult("Y",j),getResult("X",j)+d*cos(t),getResult("Y",j)+d*sin(t));
roiManager("Add");
profile=getProfile();
	n=0;
		for(b=0; b&lt;profile.length; b++){
		if(profile[b]==255)
                        n++;
		}
// add to a table
//------------------------------
Table.set("Minimum distance",i, n);
Table.update;
Table.sort("Minimum distance"); 
//------------------------------
}
print("Point "+(j+1)+"\nMinimal distance: "+ Table.get("Minimum distance", 0),"\nAngle: "+Table.get("Angle_degrees", 0)+" degrees");
print("-------");
selectWindow("The result");
run("RGB Color");
t=Table.get("Angle_rad", 0);
setColor("red");
drawLine(getResult("X",j),getResult("Y",j),getResult("X",j)+d*Math.cos(t),getResult("Y",j)+d*Math.sin(t));
//------------------------------
//setTool("text");
x=getResult("X",j)-50;
y=getResult("Y",j);
setFont("SansSerif", 18, " antialiased");
setColor("green");
Overlay.drawString(""+(j+1), x, y, 0.0);
Overlay.show();
}
//------------------------------
close("Results");
// End of processing
//End of batch mode
setBatchMode(false);
run("Tile");
exit();
}
type or paste code here
</code></pre> ;;;; <p>I certainly see some issues, but not the ones you are seeing. Also Win10.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9.png" data-download-href="/uploads/short-url/8QhvMjfFTK1aMSv6ywJT3pMBVsB.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_467x500.png" alt="image" data-base62-sha1="8QhvMjfFTK1aMSv6ywJT3pMBVsB" width="467" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_467x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_700x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_934x1000.png 2x" data-dominant-color="181716"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1035×1107 83 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Using the sample image, one of the three test images worked, the other two stuck the sample down in the lower corner, despite the 3Dviewer being in the state as shown top left.</p> ;;;; <p>I did see that link as well. I haven’t tried running in batch yet as I can’t get things to run efficiently via script with one image. Even after troubleshooting, the image was still processing getting the measurements via script after 10 minutes.</p> ;;;; <p>I think this post is more related <a href="https://forum.image.sc/t/batch-processing-unusually-slow/49633/28" class="inline-onebox">Batch processing unusually slow? - #28 by petebankhead</a></p>
<aside class="quote no-group" data-username="ADW123" data-post="1" data-topic="78608">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>In addition, how can I keep the pixel classification overlay visible after adding measurements?</p>
</blockquote>
</aside>
<p>The measurements shouldn’t turn your pixel classifier on or off. You can leave it on while generating measurements, or re-load it to see it.  If you want to script adding areas, you can, but that will take even longer.</p> ;;;; <p>I am trying to use a pixel classifier I trained and apply it to selected smaller annotations on a slide that has other larger annotations and then run in batch. When I run the pixel classifier on a slide through the GUI with my annotations selected (ie load pixel classifier, region: any annotation ROI, measure: current selection) it seems to work fine and fairly quickly (it takes ~1 minute). However, I tried scripting it using the following script:</p>
<pre><code class="lang-auto">def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}

for (annotation in annotations) {

getCurrentHierarchy().getSelectionModel().setSelectedObjects(annotations, null)

addPixelClassifierMeasurements("PGM1", "PGM1")

}

</code></pre>
<p>It seems that this runs very slowly. Is there something wrong with my code leading to this? I’m using version 0.4.2 and my memory is not maxing out. I found the following thread (<a href="https://forum.image.sc/t/qupath-measure-pixel-classifier-area-per-cell-detection-for-wsis/72701/7" class="inline-onebox">Qupath measure pixel classifier area per cell detection for WSIs - #7 by petebankhead</a>) but still a bit confused on how I should amend my script since I’m not detecting cells.</p>
<p>I did also try pre-requesting tiles using this script:</p>
<pre><code class="lang-auto">def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}

getCurrentHierarchy().getSelectionModel().setSelectedObjects(annotations, null)

def imageData = getCurrentImageData()
def classifier = loadPixelClassifier('PGM1')
def classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)

classifierServer.getTileRequestManager().getAllTileRequests()
    .parallelStream()
    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }
    
    addPixelClassifierMeasurements("PGM1", "PGM1")

</code></pre>
<p>However, that didn’t seem to help.</p>
<p>In addition, how can I keep the pixel classification overlay visible after adding measurements? In other words, after I run the script I want to be able to QC the areas that were classified? I don’t want to export the images but have everything stay in QuPath.</p>
<p>Would I just add in something like the following to the script?</p>
<pre><code class="lang-auto">createAnnotationsFromPixelClassifier("PGM1", 0.0, 0.0)

</code></pre>
<p>My images look like this (they are .svs files that are less than 1GB each) and I am only interested in getting information from the rectangular boxes:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg" data-download-href="/uploads/short-url/zA4foiBfkVdcs62ejjYW44cbxsq.jpeg?dl=1" title="Screen Shot 2023-03-15 at 2.41.35 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg" alt="Screen Shot 2023-03-15 at 2.41.35 AM" data-base62-sha1="zA4foiBfkVdcs62ejjYW44cbxsq" width="646" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_969x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg 2x" data-dominant-color="D9D6D9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-15 at 2.41.35 AM</span><span class="informations">1270×982 113 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Background subtraction for the tile. The black is probably throwing it off. If you change the pixel size that changes the tile size, so you get different results.</p><aside class="quote quote-modified" data-post="6" data-topic="77196">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar">
    <a href="https://forum.image.sc/t/questions-and-problems-when-using-qupath-cellpose/77196/6">Questions and problems when using Qupath Cellpose</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Most likely due to how background is handled. This is not new or specific to CellPose. 


etc.
  </blockquote>
</aside>
<p>
Not a new problem, and not specific to StarDist, sadly.</p>
<p>Have you tried making your annotation only cover the area where the cells are, and no black space?</p> ;;;; <p>Hey <a class="mention" href="/u/volker">@volker</a><br>
So I tried your update suggestions by adding Table.update and calling the table at the end of each Table.set call.<br>
This worked. Thank you so much for your help!<br>
//working code</p>
<p>Table.create(“Cortical Width Measurements”);<br>
Table.setLocationAndSize(240, 125, 600, 700);<br>
imagePath = getDirectory(“Where are the images located?”);<br>
filelist = getFileList(imagePath)<br>
for (rowIndex = 0; rowIndex &lt; lengthOf(filelist); rowIndex++) {<br>
if (endsWith(filelist[rowIndex], “.tif”)) {<br>
open(imagePath + File.separator + filelist[rowIndex]);<br>
waitForUser;<br>
imageTitle = getTitle();<br>
basename = substring(imageTitle, 0, indexOf(imageTitle, “.tif”));<br>
animal = “77R”;<br>
section = “2.3”;<br>
Dialog.create(“Fill in animal name and section number”); //title on window<br>
Dialog.addString(“Image Name”, basename); //correct image name<br>
Dialog.addString(“Animal Name”, animal);<br>
Dialog.addString(“Section Number”, section);<br>
Dialog.show();<br>
imageName = Dialog.getString();<br>
animal = Dialog.getString();<br>
section = Dialog.getString();<br>
lobule = getNumber(“Which Lobule are you measuring?”, 8);</p>
<p>Table.set(“Image”, rowIndex, imageName,“Cortical Width Measurements”);<br>
Table.update;<br>
Table.set(“Animal”, rowIndex, animal,“Cortical Width Measurements”);<br>
Table.update;<br>
Table.set(“Section”, rowIndex, section,“Cortical Width Measurements”);<br>
Table.update;<br>
Table.set(“Lobule”, rowIndex, lobule, “Cortical Width Measurements”);<br>
Table.update;<br>
selectWindow(imageTitle);<br>
close();<br>
}<br>
}</p> ;;;; <p>Hello <a class="mention" href="/u/volker">@volker</a><br>
Yes, this code does work. I will try your suggestions, but what about opening an image is causing the Table functions to stall?<br>
Thanks, Jenny</p> ;;;; <p><strong>University of Birmingham - Institute of Clinical Sciences, College of Medical and Dental Sciences</strong><br>
<strong>Research Fellow in 3D Bioimaging of Vasculature</strong></p><aside class="onebox allowlistedgeneric" data-onebox-src="https://www.jobs.ac.uk/job/CYD110/research-fellow-in-3d-bioimaging-of-vasculature">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/662736cf8236ea9a6f73f45cf77c0cd85b341026.png" class="site-icon" width="16" height="16">

      <a href="https://www.jobs.ac.uk/job/CYD110/research-fellow-in-3d-bioimaging-of-vasculature" target="_blank" rel="noopener nofollow ugc">Jobs.ac.uk</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/313;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0_2_690x313.jpeg" class="thumbnail" width="690" height="313" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0_2_690x313.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0.jpeg 2x" data-dominant-color="AEC2DA"></div>

<h3><a href="https://www.jobs.ac.uk/job/CYD110/research-fellow-in-3d-bioimaging-of-vasculature" target="_blank" rel="noopener nofollow ugc">Research Fellow in 3D Bioimaging of Vasculature at University of Birmingham</a></h3>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p><strong>Closing date 20th March 2023</strong><br>
Salary £33,348 to £43,155 with potential progression once in post to £45,737</p>
<p>Research Fellow position available for Bioimaging, image analysis and machine learning PhD’s interested to work on “placental of tissue and vasculature” in a cross-disciplinary project involving University of Birmingham, University of Manchester, Rosalind Franklin Institute, Diamond Light Source, Swansea University and University of Auckland. The position is fixed term until Sept 2023 to image and analyse human placenta ex vivo. The position is suitable for a candidate with a PhD (or near to completion) in Bioimaging, Biomaterials or Date Science and experience relevant to research areas in tomography, image processing and analysis.</p>
<p>For further details please contact Dr Gowsihan Poologasundarampillai (<a href="mailto:g.poologasundarampillai@bham.ac.uk">g.poologasundarampillai@bham.ac.uk</a>).</p> ;;;; <p>Hi Konrad,</p>
<p>Just letting you know that when I looked at the labeled-data folder and the coordinate CSV file and compare it to the coordinates in the CSV file in the videos folder the individuals have been swapped throughout the whole video.</p>
<p>Please find the CSV files attached</p>
<p><a class="attachment" href="/uploads/short-url/oqMD41YJ6Ll9Txe9OpSjFveTkaX.csv">CollectedData_grant1.csv</a> (5.2 KB)</p>
<p><a class="attachment" href="/uploads/short-url/l0KUNofCKytdGxKpXrLmA9B0EQi.csv">assembly3DLC_resnet50_multiproject1Mar13shuffle1_30000_el.csv</a> (453.1 KB)</p>
<p>Also, I’ve noticed in the CSV file in the videos folder that the likelihood calculation is 1 across all frames and coordinates which must be an error… or a sign of overfitting…?</p>
<p>I only performed 30,000 iterations during training thinking that would be suitable.</p>
<p>I’m not sure if this is relevant or not, but I thought it was worth bringing it to your attention.</p>
<p>Lastly, I noticed that after running the stitich_tracklets command the output doesn’t mention anything about being successful or to move to the refine_tracklets process - not sure if this is an indication the function didn’t work properly - I’m not sure</p>
<p>Hopefully this extra information helps you work out what’s happening <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Thanks, again</p> ;;;; <p>I believe it must be a bug, because moving the images used for training to another folder helped. I moved them to a folder where I trained previous models.</p> ;;;; <p>Yes, the tensorflow plugin in fiji (which csbdeep/stardist relies on) is still tied to 1.x, so indeed sticking to python is preferable</p> ;;;; <p>For anyone else with this problem, I would like to add that it may seem like CUDA installation is stuck but it just happens to take forever (~40 min sometimes).</p>
<p>You can really see if conda is doing something by typing -vv (verbose output) at the end of the command. Lots of debugging info will appear but it will give you a sense of progression.</p> ;;;; <p>Cool! I was trying to run it under Fiji, and that lists only Tensorflow 1.  In the mean time switched gears to Python.  Is there a way to use Tensorflow 2 with the CSBDeep plugin in Fiji?</p> ;;;; <p>I am using the MOSAIC plugin to track the particles and in trajectory visualization one particle track is not visible ( I think it’s in black color and my background is also black)</p>
<p>How can I change the color or the tracked trajectories? or Can I make it all with a single color? (i.e. blue/green/etc. for all particles)</p>
<p>attached two images for reference (trajectory of the particle in middle is not visible):</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5.png" data-download-href="/uploads/short-url/vOYeNUP0hN6zzSY0gyjK3Xzx8Cp.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5_2_486x500.png" alt="image" data-base62-sha1="vOYeNUP0hN6zzSY0gyjK3Xzx8Cp" width="486" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5_2_486x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5_2_729x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5.png 2x" data-dominant-color="040404"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">800×822 23 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c0eaf055b92a599d07df318055949e187916314.png" data-download-href="/uploads/short-url/d8nnZhIJeblFw71rKlHjjt0ZtVq.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c0eaf055b92a599d07df318055949e187916314_2_465x500.png" alt="image" data-base62-sha1="d8nnZhIJeblFw71rKlHjjt0ZtVq" width="465" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c0eaf055b92a599d07df318055949e187916314_2_465x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c0eaf055b92a599d07df318055949e187916314_2_697x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c0eaf055b92a599d07df318055949e187916314.png 2x" data-dominant-color="060608"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">752×808 21.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello,</p>
<p>I have trained a variation of the YOLOX object detection neural network for an ecological application that I am attempting to export for use in DeepImageJ. This type of network is different from many of the existing models in the bioimage model zoo because instead of returning a segmentation mask, it returns a list of predictions in the from [(x1, y1, x2, y2, confidence, class label), …] which represent predicted bounding boxes. I would like these predictions to be returned as a table in Fiji, similar to <a href="https://bioimage.io/#/?tags=deepimagej%2Fskinlesionclassification&amp;id=deepimagej%2Fskinlesionclassification" rel="noopener nofollow ugc">this example</a></p>
<p>I have exported my model in torchscript format, and I can view the model in Fiji, but when I try to run the model on the example image, Fiji crashes. I assume that this is due to an issue with the rdf.yml configuration file I am using, or the format of the outputs returned by the model. I will provide this configuration file below, and I would be grateful for any suggestions on how to modify it to correctly return the predictions to deepImageJ, or really any more documentation on how to format the file. I was originally attempting to create the file using the <a href="https://github.com/bioimage-io/core-bioimage-io-python" rel="noopener nofollow ugc">bioimage.io python tool</a> but I could not get it to run without errors and switched to trying to create the configuration manually.</p>
<p>I can provide the torchscript model on request as well if that would be helpful.</p>
<p>rdf.yml:</p>
<pre><code class="lang-auto">attachments:
  files: [results.csv, test.jpg]
authors:
- {name: Jemison}
cite:
- {text: Jemison et al., url: tbd}
config:
  deepimagej:
    allow_tiling: true
    model_keys: null
    prediction:
      prediction:
            preprocess:
              - {spec: null}
            postprocess:
              - {spec: null}
    pyramidal_model: false
    test_information:
      inputs:
      - name: test.jpg
        pixel_size: {x: 1.0, y: 1.0, z: 1.0}
        size: 896 x 684 x 3 x 1
      memory_peak: null
      outputs:
      - {name: results.csv, size: 100x6, type: ResultsTable}
      runtime: null
covers: [cover.png]
description: kelp sporophyte object detection
documentation: doc.md
format_version: 0.4.8
inputs:
  - name: input0
    axes: bcyx
    data_type: float32
    data_range: [-inf, inf]
    shape:
        min: [1, 1, 3, 1]
        step: [0, 0, 1, 1]
license: CC-BY-4.0
links: [deepimagej/deepimagej]
name: YOLOX-kelp
outputs:
- axes: bcyx
  data_range: [-.inf, .inf]
  data_type: float32
  name: output0
  postprocessing:
  - {spec: null}
  shape: [100, 6]
sample_inputs: [test.jpg]
sample_outputs: [results.csv]
tags: [kelp-object-detection]
test_inputs: [test.jpg]
test_outputs: [results.csv]
timestamp: '2023-02-17T11:56:47.272556'
type: model
weights:
  torchscript: {sha256: 0b65224af72b746b8c5c431eed9d1ea5b74a40883ee8b7688e41505cb738fc51,
    source: yolox_kelp_l_combined.pt}
</code></pre> ;;;; <p>Hi <a class="mention" href="/u/nicost">@nicost</a></p>
<p>There should actually not be any problem - tensorflow/csbdeep/stardist should be fully compatible with your GPU (and cuda 11 or 12). What kind of issue do you currently face?</p>
<p>Cheers,<br>
Martin</p> ;;;; <p>I am lucky enough to have access to an NVidia A40 GPU.  However, it is not obvious to me how to get Startdist (and CSBDeep) to use the GPU.  It looks like the minimum Cuda version for this GPU is 11.0, and that the minimum Tensorflow version for Cuda 11is 2.4. Am I out of luck or is there a way to get this to work?</p> ;;;; <p>Many have successfully connected MM.  Best thing to do would be to test yourself (or ask Zeiss to ensure it will work).  A laptop with USB to serial adapter should be enough to test.</p> ;;;; <p>Hello everyone,</p>
<p>I am having an issue I have not dealt with before where cells are being counted in a peculiar manner. Specifically, the cells are counted in a square box at the center and also on the edges of the image rather than the whole area. This seems to be dependent on the pixel size of the stardist script provided. Please let me know what this may be caused by.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a.jpeg" data-download-href="/uploads/short-url/sk8iwamMKF659wTiFnGVyOQV5X4.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_690x349.jpeg" alt="image" data-base62-sha1="sk8iwamMKF659wTiFnGVyOQV5X4" width="690" height="349" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_690x349.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_1035x523.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_1380x698.jpeg 2x" data-dominant-color="908F7A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×973 277 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e.jpeg" data-download-href="/uploads/short-url/ulLm1R1k8H1gBUK9EBh1xiuUbU2.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_690x328.jpeg" alt="image" data-base62-sha1="ulLm1R1k8H1gBUK9EBh1xiuUbU2" width="690" height="328" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_690x328.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_1035x492.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_1380x656.jpeg 2x" data-dominant-color="959385"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×913 218 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>FYI - when I open the refine_tracklets GUI and scroll through the frame slider it does include the whole video with 4,000 frames</p> ;;;; <p>Thank you, Konrad.</p>
<p>And thank you for your help this past week</p>
<p>It’s greatly appreciated</p>
<p>Fingers crossed we can get this worked out</p>
<p>I’m looking forward to hearing back from you</p>
<p>Thanks</p> ;;;; <p>It looks like it’s trying to open the csv file as if it was an image. Can you try to just open the folder from the dropdown menu? The second error like I said, you can follow to this line in the code and comment it out or remove completely</p> ;;;; <p>Are you sure you’re on 2.3.1?</p> ;;;; <p>I know of some projects where DLC was used on octopi and cuttlefish so definitely yes. The rules are the same, keep your labelling as precise and as consistent as possible.</p>
<p>The post pose estimation analysis depends on your goal, you should have one before you invest significant amounts of time into training a pose estimation model</p> ;;;; <ol>
<li>Rename them in the config and all the <code>CollectedData_name.h5</code> files (copy them first just to be safe)</li>
<li>Remove the names you don’t want to use from the config and CollectedData files like above</li>
</ol> ;;;; <p>Ok. good news, we fixed the reload loop issue, the fpbioimage folder on omero-web static/ didn’t have the proper permission, (don’t know why, we follow the standard installation instructions).</p>
<p>The other problems persist.</p> ;;;; <p>The matplotlib was for fixing this:  <code>AttributeError: ‘ArtistList’ object has no attribute ‘clear’</code></p>
<p>I’ll try to reproduce it tomorrow. Intuitively it would seem you’re trying to flag a single frame</p> ;;;; <p>Hi, were you eventually able to solve this?</p> ;;;; <aside class="quote no-group" data-username="RyanC" data-post="1" data-topic="78585">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/r/a4c791/40.png" class="avatar"> RyanC:</div>
<blockquote>
<p>if it is possible to perform the “split” function on ROIs (available in ImageJ) within Python?</p>
</blockquote>
</aside>
<p>Try <code>RoiFile.coordinates(True)</code> to split “multi_coordinates” into separate coordinates. There is currently no convenience function to split a ROI with multi_coordinates into separate ROI instances but that could be added.</p> ;;;; <p>Hi,</p>
<p>I’m a former ImageJ user from over 20 years ago, where I used ImageJ to prototype processing pipelines for real-time neutron and X-Ray radiography systems, then subsequently to develop color management/recovery algorithms for a 100K fps camera and other industrial/scientific imaging systems, after which my career moved away from imaging. I’m now semi-retired and am looking to refresh those ancient skills.</p>
<p>My initial target is my forearm and my new (and first) tattoo, where I hope to construct a flattened image suitable for printing or as a desktop/phone background. While this could be trivial to do using Photoshop or photogrammetry tools, I’d prefer to use ImageJ.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd.jpeg" data-download-href="/uploads/short-url/8NLDFs1lemsp21hrfcO9reVo2v3.jpeg?dl=1" title="My Tattoo 5 Days Later" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_374x500.jpeg" alt="My Tattoo 5 Days Later" data-base62-sha1="8NLDFs1lemsp21hrfcO9reVo2v3" width="374" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_374x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_561x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_748x1000.jpeg 2x" data-dominant-color="898280"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">My Tattoo 5 Days Later</span><span class="informations">1666×2222 310 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Below is the artwork the tattoo artist used for inspiration:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7.jpeg" data-download-href="/uploads/short-url/k6bNopOXsfpArsM3j5yYF20z2bt.jpeg?dl=1" title="Bee Art" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7_2_500x500.jpeg" alt="Bee Art" data-base62-sha1="k6bNopOXsfpArsM3j5yYF20z2bt" width="500" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7.jpeg 2x" data-dominant-color="B4BAB4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Bee Art</span><span class="informations">768×768 124 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I installed Fiji and was instantly gratified to see that <em><strong>very</strong></em> familiar UI, seemingly unchanged! Digging into the menus revealed extensive growth and evolution, leaving me feeling a bit lost.</p>
<p>I know I need to start with capturing a suitable image set.</p>
<ul>
<li>What constitutes a “suitable” image set? I’m guessing at least 10 images taken around my forearm at a distance great enough to minimize lens distortion. I know I’ve previously computed optimal acquisition metrics for similar problems in the 2D and 3D industrial inspection domains, but the specific memories elude me.</li>
<li>At a low level, I know I’ll need to import, resize and rotate the images to get closer to a common image geometry. So, it may be good if each image contained a common central feature, such as the centerline of the bee.</li>
<li>I could take images with my arm pressed against a pane of glass, providing a physically flattened local geometry. Though I may need to account for skin elasticity…</li>
<li>I can also shoot 4K video, though compression artifacts may be an issue.</li>
</ul>
<p>From there, I see several paths to pursue:</p>
<ul>
<li>My forearm is roughly conic: Map the topology, unwarp each image, then identify and combine the “best center strip” from each image.</li>
<li>The image contains (hopefully) uniformly sized tiled hexagons: Extract the edges, identify the vertices, then warp each to an ideal hexagon.</li>
<li>Use the original art as a template.</li>
</ul>
<p>Once results are obtained, the flattened skin image could be perceived as a harvested human hide, triggering intense ick incidents, so additional processing may be warranted. It may be fun to try to subtract the skin texture and color (as pre- and/or post-processing) to create a drawing-like image, then process that into directional color gradients and lines to create an idealized vector (SVG) representation.</p>
<p>As I wandered through the menus I stumbled upon the <a href="http://bigwww.epfl.ch/thevenaz/UnwarpJ/" rel="noopener nofollow ugc"><code>bUnwarpJ</code></a> plugin, triggering “<em>One &amp; Done</em>” fantasies. Which were not to be realized, as only the central bee was flattened, not the entire tattoo. I suspect the differences in background grid size confused the plugin.</p>
<p>Clearly, a pipeline is needed. Rather than dive into the deep end right away, I’m looking for ImageJ “hints” as starting points, a list of modern features or plugins I should initially investigate.</p>
<p>I’m also thinking it may be fun to try a few different approaches, then compare/contrast them, which would inspire the creation of metrics for a comparison/evaluation/grading pipeline (noise content, color fidelity, geometry accuracy, compute cost, etc.). I see the potential for some really crazy approaches, including ones in the Rube Goldberg tradition. If anyone would like to play along, I’ll post my image sets as I acquire them.</p>
<p>Ideally, I’d like to record progress in this thread, then contribute a document covering the results back to the community as a (possibly bizarre) tutorial. I like to leave tutorials or blog posts behind whenever I enter a new domain: We’re only beginners once! (Well, twice for me, I suppose.)</p>
<p>While I’m certainly willing to proceed on my own, I was wondering if anyone here would wish to mentor me on this journey? All contributions are welcome!</p>
<p>Thanks,</p>
<p>-BobC</p> ;;;; <p>Dominik,</p>
<p>Thanks for your response. I’m not familiar with FIJI. What I did try was using CellProfiler. I used the original images and the probabilities map. I split out the channels on the probabilities, using the red channel to convert to gray then I thresholded on this gray image (the rough part), using the IdentifyPrimaryObjects (see below). Based on your response I’m wondering if I should try manual Threshold and set to 0.5? I also ran it again with the green split to gray to look at the smooth portion….those two portions don’t add up to 100% which is why I was hoping to use the segmentation. I can’t get the segmentation to work in CellProfiler.</p>
<p>Do you have any other suggestions using CellProfiler. Btw, right now I’m not worried about the background or the writing, this is just testing the feasibility, in the future yes, I would use images without the writing and crop the background out if needed.</p>
<p>Thanks,</p>
<p>Kristin</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6.png" data-download-href="/uploads/short-url/hV4mf0CmHIKWetBFEwBNRA6yv7E.png?dl=1" title="image001.png" rel="noopener nofollow ugc"><img width="551" height="430" src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_551x430.png" data-base62-sha1="hV4mf0CmHIKWetBFEwBNRA6yv7E" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_551x430.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_826x645.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_1102x860.png 2x" data-dominant-color="ECECEC"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image001.png</span><span class="informations">1172×913 106 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello!</p>
<p>My research focuses on treating S. aureus biofilms with a chemical that disrupts their biofilm formation. Because our flow cell was made in-house, it is too thick for most objective lenses on our confocal and I can only use a 10x objective.</p>
<p>I am not seeing formation of clear microcolonies (see example image) and I am not sure if this is due to S. aureus not forming microcolonies (literature is not consistent on if S. aureus does or not) or if this is a product of low magnification. Either way, I have essentially been treating each z-stack as a full biofilm. Does this seem viable?</p>
<p>I am also curious about the strength of my single object parameters considering the magnification. Is it better to focus on global biofilm parameters?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01.png" data-download-href="/uploads/short-url/wtuqT7lrxJDNEWs3PDxK3gsBxWp.png?dl=1" title="example_4EB_biofilm" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01_2_504x500.png" alt="example_4EB_biofilm" data-base62-sha1="wtuqT7lrxJDNEWs3PDxK3gsBxWp" width="504" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01_2_504x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01.png 2x" data-dominant-color="626261"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">example_4EB_biofilm</span><span class="informations">531×526 295 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <ol>
<li>Is there a way to rename the bodyparts that I already labeled?</li>
<li>Is there a way to select a subset of labeled bodyparts (say I have 31 body parts defined and labeled but I only want to train with 21 of them for some reason), and create training set?</li>
</ol> ;;;; <p>Hi,<br>
I’m new to cellfinder, and I was able to run the tutorial smoothly (looks great - good job!).<br>
In the documentation you indicate I need to use a fully reconstructed brain, but I was wondering if I could only use the cell detection part on a smaller stack, without registration to the atlas.<br>
If not, what would you recommend instead of cellfinder?<br>
Thank you very much,<br>
Amit<br>
Here are two examples (two channels of the same section):<br>
<a href="https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/ERyg9NTENd9Cg0jqK4suH64BhjGX6cFrntpTu2SfyQGtLQ?e=JbgtD9" class="onebox" target="_blank" rel="noopener nofollow ugc">https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/ERyg9NTENd9Cg0jqK4suH64BhjGX6cFrntpTu2SfyQGtLQ?e=JbgtD9</a></p>
<p><a href="https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/EdVprenLy8tJhevWYKdibiUBhL1XM2uduZlkL2VdT6SsHw?e=zGhYdQ" class="onebox" target="_blank" rel="noopener nofollow ugc">https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/EdVprenLy8tJhevWYKdibiUBhL1XM2uduZlkL2VdT6SsHw?e=zGhYdQ</a></p> ;;;; <p>We installed all the Omero Web appsthe same day, we think the problem is fpbioimage because the reload stopped after we uninstalled this specific app and started again after we reinstall  it (we tried uninstalling and reinstalling every app).</p>
<p>The continuous reload happen after login.  Once the reload loop stops, you can refresh the page and there is no reload loop. If you logout and login, the reload loop happens again. I haven’t tried the incognito mode, ill do it as soon as i can.</p>
<p>The image that doesn’t generate the thumbnail is 5gb, but the other one that does generate the thumbnail but doesn’t do the pyramidal conversion is 10gb.</p>
<p>Here are the dropbox link to the images.</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12643b243ca3bc4e2fa6489fc836c2f58e96d9a5.png" class="site-icon" width="32" height="32">

      <a href="https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi" target="_blank" rel="noopener nofollow ugc">Dropbox</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png" class="thumbnail onebox-avatar" width="160" height="160">

<h3><a href="https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi" target="_blank" rel="noopener nofollow ugc">Llano_de_la_cruz_R15 - 2023-02-01 09.45.40.ndpi</a></h3>

  <p>Shared with Dropbox</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.dropbox.com/s/qbkszwh5kxa9rpk/STRI_Fm_Ferreira_IIES-PALY-7822%20-%202023-01-23%2014.10.35.ndpi">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12643b243ca3bc4e2fa6489fc836c2f58e96d9a5.png" class="site-icon" width="32" height="32">

      <a href="https://www.dropbox.com/s/qbkszwh5kxa9rpk/STRI_Fm_Ferreira_IIES-PALY-7822%20-%202023-01-23%2014.10.35.ndpi" target="_blank" rel="noopener nofollow ugc">Dropbox</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png" class="thumbnail onebox-avatar" width="160" height="160">

<h3><a href="https://www.dropbox.com/s/qbkszwh5kxa9rpk/STRI_Fm_Ferreira_IIES-PALY-7822%20-%202023-01-23%2014.10.35.ndpi" target="_blank" rel="noopener nofollow ugc">STRI_Fm_Ferreira_IIES-PALY-7822 - 2023-01-23 14.10.35.ndpi</a></h3>

  <p>Shared with Dropbox</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>thanks,</p> ;;;; <p>Hi <a class="mention" href="/u/fredrik">@Fredrik</a> ,</p>
<p>I just ran your code from a Python Jupyter lab and do see an image when executing it.</p>
<p>When running this, it shows a very thin image:</p>
<pre><code class="lang-auto">import sys
import numpy as np
from matplotlib import pyplot as plt
import pyclesperanto_prototype as cle
from skimage import io

img1 = io.imread("Own image.jpg")
img2 = io.imread("IXMtest_A02_s9_w1.tif")

cle.imshow(img1)          #Not shown
plt.show()
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b.png" data-download-href="/uploads/short-url/eHMn5L1XIrpcA1CWlCyUu2slupZ.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b_2_294x500.png" alt="image" data-base62-sha1="eHMn5L1XIrpcA1CWlCyUu2slupZ" width="294" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b_2_294x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b.png 2x" data-dominant-color="F8F8F8"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">407×690 18.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The underlying reason is that clesperanto is not made for RGB images (such as jpeg). <a href="https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/12a_image_file_formats/image_file_formats.html#why-jpeg-should-be-avoided">Read more why working with JPG is a bad idea in scientific image analysis</a>.</p>
<p>You could show the individual channels of the RGB image like this:</p>
<pre><code class="lang-auto">cle.imshow(img1[...,0])
</code></pre>
<p>or this</p>
<pre><code class="lang-auto">cle.imshow(img1[:,:,0])
</code></pre>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb341749a2825c5b03b88ec3a47c047c0d8dd3b6.jpeg" alt="image" data-base62-sha1="sZCwUIvwY7CE2z1vKpPIFaoxioC" width="647" height="498"></p>
<p>Btw. here is the <a href="https://github.com/clEsperanto/pyclesperanto_prototype/">link to the actively maintained repo</a>, not the repo of its website.</p>
<p>Let us know if this helps!</p>
<p>Best,<br>
Robert</p> ;;;; <aside class="quote no-group" data-username="RyanC" data-post="1" data-topic="78585">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/r/a4c791/40.png" class="avatar"> RyanC:</div>
<blockquote>
<p>Secondly, does anyone know if it is possible to perform the “split” function on ROIs (available in ImageJ) within Python?</p>
</blockquote>
</aside>
<p>I imagine using PyImageJ it should be. Don’t have much experience with it though. Not really clear to me what you mean by splitting the annotations though. If you want the sections of each class that exist within the particular 100x100 tile, you could do that with a second mask file. Then you don’t need to deal with ROIs at all, you have the exact pixel masks, which can be created in QuPath using the labeledImageServer.</p> ;;;; <p>Hi,</p>
<p>Did you change anything else about your OMERO.web installation when you installed <code>fpbioimage</code>?<br>
Does it do a “reload loop” on every login now? What about if you logout first?</p>
<p>If you try to login from a new browser (or use “incognito” mode) does that help?</p>
<p>What size is the image which fails to generate a thumbnail?<br>
It might simply be taking a long time to generate a pyramid. You can check the Processor log.</p>
<p>We generally recommend that if you’re having pyramid generation issues, it might help to build a pyramid image offline first, then import that into OMERO - Either ome.tiff of OME-NGFF.<br>
A UI tool for doing this conversion is <a href="https://www.glencoesoftware.com/products/ngff-converter/" class="inline-onebox">NGFF-Converter | Glencoe Software, Inc.</a><br>
This would also allow you to check the pyramid image before importing.</p>
<p>Or if you would like to share the image, we can test…?</p> ;;;; <p>Hey!</p>
<p>I’m not familiar with clesperanto, but based on their <a href="https://github.com/clEsperanto/clesperanto.github.io" rel="noopener nofollow ugc">repo</a> it doesn’t seem like an active project. I would be weary of adopting it (even though it seems super cool).</p>
<p>Maybe if you explain what is your overall goal, as opposed to “why doesn’t X work?”, someone could offer you an alternative. <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Leo</p> ;;;; <p>I’m trying to train a new cellpose model in GUI. I have green cytoplasm and red nuclei in RGB images. Segmentation with pretrained models looks ok.<br>
When I train a new model, I receive this error in the terminal:</p>
<pre><code class="lang-auto">Traceback (most recent call last):
  File "/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/gui/gui.py", line 1631, in new_model
    self.train_model()
  File "/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/gui/gui.py", line 1655, in train_model
    self.new_model_path = self.model.train(self.train_data, self.train_labels, 
  File "/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/models.py", line 764, in train
    train_data, train_labels, test_data, test_labels, run_test = transforms.reshape_train_test(train_data, train_labels,
  File "/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/transforms.py", line 422, in reshape_train_test
    if train_labels[0].ndim &lt; 2 or train_data[0].ndim &lt; 2:
IndexError: list index out of range
</code></pre>
<p>It seems similar to <a href="https://github.com/MouseLand/cellpose/issues/571#:~:text=422%20if%20train_labels%5B0%5D.ndim%20%3C%202%20or%20train_data%5B0%5D.ndim%20%3C%202%3A" rel="noopener nofollow ugc">this previously reported issue</a> on GitHub.<br>
Reinstalling cellpose did not help. Same problem occurs irrespective to initial model I choose for training, also irrespective to channel settings.<br>
Any ideas? Could it be a bug?<br>
Thanks a lot.</p> ;;;; <p>Hi,</p>
<p>I’m currently making some tools that will likely have to parse and modify cellprofiler pipelines. It looks like there’s work towards <a href="https://github.com/CellProfiler/core/pull/89" rel="noopener nofollow ugc">JSON support</a>, so I’m hoping I don’t have to write my own parser, but I can’t seem to find how to export a pipeline as JSON format in 4.2.5.</p>
<p>Is this still work in progress, or have a missed a big obvious button somewhere?</p> ;;;; <p>Regarding scripting Jython &amp; more flexible parameters, how about this approach:</p>
<ul>
<li>Whenever you save a TrackMate xml, it includes all the config options you set</li>
<li>I wrote a small <a href="https://github.com/leogolds/MicroscopyPipeline/blob/3346facb359cfb448e01514a45aaa80fa58381f1/containers/TrackMate/read_settings_and_process_tiff_stack.py#L38" rel="noopener nofollow ugc">script</a> to load this config file and run TrackMate headlessly<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf.png" data-download-href="/uploads/short-url/assJZyhPxUAvFVJS0fpyx7ztuzB.png?dl=1" title="Screenshot_20230314-132630" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_225x500.png" alt="Screenshot_20230314-132630" data-base62-sha1="assJZyhPxUAvFVJS0fpyx7ztuzB" width="225" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_225x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_337x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_450x1000.png 2x" data-dominant-color="1E2127"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot_20230314-132630</span><span class="informations">1080×2400 290 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div>
</li>
<li>By <a href="https://github.com/leogolds/MicroscopyPipeline/tree/main/containers/TrackMate" rel="noopener nofollow ugc">containerizing</a> Fiji+TrackMate with the script, you get a fairly flexible way to operate TrackMate</li>
</ul>
<p>I like to have a few commonly used .xml files on a repo for reuse. As to generating the config files, notice you can click the save xml button before you do any tracking. That way you don’t waste space on the actual tracking info and just get the settings you used.</p>
<p><a href="https://github.com/leogolds/MicroscopyPipeline/blob/3346facb359cfb448e01514a45aaa80fa58381f1/models/trackmate/basic_settings.xml" rel="noopener nofollow ugc">Here’s</a> an example of the most basic config file (label image detector, simple LAP).</p>
<p>Leo</p>
<p>P.s. shameless plug</p> ;;;; <p>Thanks for the prompt response <a class="mention" href="/u/jni">@jni</a> !</p>
<p>What I’m trying to do is use the 2d ‘blueprint’ to get approximate coordinates that I can feed into Unity and build a 3d model from it.   So it doesn’t have to be exact.    I think I can use the yellow mask for extracting coordinates, although as you mention Felzenszwalb may not be be doing a great job of finding ‘walls and gaps’, and it requires tweaking the scale quite a bit.    Since I’m not CV or ML expert, I’m not sure if this is best route to go, so if you have any suggestions for segmentation or coordinate extraction alternatives, I’m completely open <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>As you requested, am including the code and image source.    Really appreciate the insights you can provide.<br>
For it to work, I created subdirectories under content in Collab:  ‘content\data\dungeons*.png’<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c51b89f03c75ed1a2b44ef9854a95c4b59200ab8.png" data-download-href="/uploads/short-url/s7H3VrSV4XiFXWxgSt28DEawBYY.png?dl=1" title="dung1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c51b89f03c75ed1a2b44ef9854a95c4b59200ab8.png" alt="dung1" data-base62-sha1="s7H3VrSV4XiFXWxgSt28DEawBYY" width="500" height="500" data-dominant-color="79715F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">dung1</span><span class="informations">512×512 463 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-auto"># Importing the required libraries
from skimage.segmentation import felzenszwalb
from skimage.color import rgb2gray
from skimage import data
from skimage.filters import gaussian
from skimage.segmentation import active_contour
import matplotlib.pyplot as plt
import os
from skimage.color import label2rgb
from pathlib import Path
import numpy as np

data_path = Path("data/")

from skimage.segmentation import slic, mark_boundaries
from PIL import Image
 
# Setting the figure size as 15, 15
plt.figure(figsize=(15,15))
 
image_path = data_path /"dungeons"
print(image_path)

# Get image class from path name
image_class = image_path.parent.stem
print(image_class)
#Open image
suffix = '.png'
base_filename = 'dung1'
dungImage = os.path.join(image_path, base_filename + suffix)
interDungeon = Image.open(dungImage)
dungeon = np.asanyarray(interDungeon)
#dungeon = dungImage()
gray_dungeon = rgb2gray(dungeon)
 
# computing the Felzenszwalb's
# Segmentation with sigma = 5 and minimum
# size = 100
dungeon_segments = felzenszwalb(dungeon,
                                  scale = 80,
                                  sigma=5,
                                  min_size=100)

# Plotting the original image
plt.subplot(1,2,1)
plt.imshow(dungeon)
 
# Marking the boundaries of
# Felzenszwalb's segmentations
plt.subplot(1,2,2)
plt.imshow(mark_boundaries(dungeon,
                           dungeon_segments))
print (dungeon_segments)
</code></pre> ;;;; <p>Hey!</p>
<p>I am working on a project and have become a little stuck.</p>
<p>I am looking to use the output of a pixel classifier in QuPath to analyse specific subsections of a whole slide image of a lung slice. The subsection must contain annotations for three different classifications in close proximity. (eg: a 500um x 500um region containing: a tumour, lung vasculature and parenchyma.)</p>
<p>So far I have:<br>
a. Created a pixel classifier in QuPath for the images.<br>
b. Worked out how to filter the generated annotations from the pixel classifier by size.</p>
<p>My problem is what to do next.</p>
<ol>
<li>I want to automate the analysis so that (for example) we loop through all of the tumour annotations and check if there are both lung vasculature and parenchyma annotations within +/- 250 pixels of the tumour annotation edge.</li>
<li>I then would create 100um x 100um tiles from the parenchyma annotation and export these regions for analysis.</li>
<li>I will then run the chosen 100um x 100um tiles through a texture analysis program.</li>
</ol>
<p>I plan to export the annotations (un-split) to ROIs and then perform steps 1 and 2 of the above plan in a separate python file and send the 100um x 100um tiles back into QuPath as new ROIs.</p>
<p>I am now reaching a bit of a roadblock though, hopefully, some kind people can help answer a few of my questions:</p>
<p>Firstly, if all of the above analysis can be done exclusively in QuPath, please let me know and I will focus on keeping the analysis within the one program.</p>
<p>Secondly, does anyone know if it is possible to perform the “split” function on ROIs (available in ImageJ) within Python?</p>
<p>I have tried opening the ROIs using roifile in python after splitting the larger annotation in QuPath first. The classifier name is not exported in the metadata for the ROIs when split in QuPath, which makes my analysis rather tricky. Can this characteristic also be exported with the ROIs?</p> ;;;; <p>Looks like it was from here <a href="https://forum.image.sc/t/cell-classification-within-a-segmented-region-in-histology-images/55507/50" class="inline-onebox">Cell classification within a segmented region in histology images - #50 by petebankhead</a> with a similar comment about pop.</p> ;;;; <aside class="quote no-group" data-username="Li1234" data-post="3" data-topic="78571">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/li1234/40/68757_2.png" class="avatar"> Li Yenn Yong :</div>
<blockquote>
<p>Presumably if I increase resolution to 0.24, which is the native resolution it may help with segmentation?</p>
</blockquote>
</aside>
<p>Not really. Most segmentation models are based on certain sized objects in pixels. So you want the size of the nucleus to match up with what the model or method expects. If you are getting too much splitting, you generally reduce that by increasing the effective pixel size (downsampling the image). The opposite if you are getting too much merging.</p>
<p>At some point there should be a balance between oversplitting and merging.</p> ;;;; <p>That makes sense. Happy to (try to) help<br>
Good luck!</p>
<p>Leo</p> ;;;; <p>Thank you for the response.</p>
<p>I seem to be getting an error</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e.png" data-download-href="/uploads/short-url/1A4M5ltasL10ls2u4sg2ghPHmtU.png?dl=1" title="Untitled" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_690x240.png" alt="Untitled" data-base62-sha1="1A4M5ltasL10ls2u4sg2ghPHmtU" width="690" height="240" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_690x240.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_1035x360.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_1380x480.png 2x" data-dominant-color="CED0D0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Untitled</span><span class="informations">1751×611 117 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>In the output folder, the first image in .lif file is successfully converted to a raw .tiff, but nothing else is present.</p>
<p>I would guess this error is happening because the script is getting through the first part, converting one image from .lif to a raw .tiff, but then the error occurs when this first image is attempted to be adjusted. No prompt comes up for any adjustment parameters, so the crash is likely occurring before that.</p> ;;;; <aside class="quote no-group" data-username="Demehdix" data-post="81" data-topic="42019">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/demehdix/40/68621_2.png" class="avatar"> Mohammad Mehdi Johari Moghadam:</div>
<blockquote>
<p>first I used vedo module and python to draw two lines and calculate the distance in between, which didn’t work out.</p>
</blockquote>
</aside>
<p>may I ask you why that didn’t work for you?</p> ;;;; <p>One way that might happen if something changes the image properties, like binning, bit depth or crop size. Whenever that changes you need to <a href="https://valelab4.ucsf.edu/~MM/doc/mmcorej/mmcorej/CMMCore.html#initializeCircularBuffer--" rel="noopener nofollow ugc">initialize the buffer</a>.</p> ;;;; <p>Hi, try this:</p>
<pre><code class="lang-python">from vedo import *
from time import time

def loop_func(event):
    #print(event)
    msh.rotate_z(0.1)
    txt.text(f"time: {time()-t0} sec")
    plt.render()

msh = Cube() #Mesh("file.obj")
txt = Text2D(bg='yellow')
t0 = time()

plt = Plotter(axes=1)
plt += [msh, txt]
plt.add_callback("timer", loop_func)
plt.timer_callback("start")
plt.show()
plt.close()
</code></pre>
<p>See also <a href="https://github.com/marcomusy/vedo/blob/8039c5ea96a61d6801a74be501a54c72ffc07419/examples/simulations/airplane1.py">this example</a>.</p> ;;;; <aside class="quote no-group" data-username="ChrisStarling" data-post="6" data-topic="78562">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png" class="avatar"> Chris Starling:</div>
<blockquote>
<p>I was trying to use this script from Pete:</p>
</blockquote>
</aside>
<p>Can you link to the source? I don’t remember where or when I wrote it… <a href="https://docs.groovy-lang.org/latest/html/groovy-jdk/java/util/Set.html">Groovy sets don’t have a pop() method</a> but maybe using <code>as List</code> instead of <code>as Set</code> would work. Or maybe it will have horrible performance and give duplicate objects – I’m not sure.</p>
<hr>
<p>Approaching it in a different way, this concave hull script might be worth a try:</p>
<aside class="quote quote-modified" data-post="2" data-topic="76833">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar">
    <a href="https://forum.image.sc/t/qupath-script-command-to-draw-polygon-annotation-from-points/76833/2">Qupath - script command to draw polygon annotation from points?</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hi <a class="mention" href="/u/smalldogworld">@SmallDogWorld</a> 
This script should create a convex hull annotation around selected objects: 

Note that it wouldn’t give an annotation quite like the one in your screenshot, because yours isn’t convex. 
QuPath v0.4.x contains Java Topology Suite 1.19.0, which gives support to create a concave hull instead. There’s a script showing that at <a href="https://gist.github.com/petebankhead/13e1d7ec8635b24e6fc0cd7ed3a44810" class="inline-onebox">Generate a concave hull using JTS in QuPath v0.4.0 · GitHub</a> 
Here’s an adapted version that works with selected objects again: 
import org.locationtech.jts.…
  </blockquote>
</aside>
 ;;;; <p>Thanks Pete, I will try and let you know.</p> ;;;; <p>I’m having the same problem, but in the metadata I can see the appropriate wavelengths…<br>
I’m wondering if it’s possible to retrieve wavelengths info from metadata with a macro to then merge the mono channels based on their original wavelengths</p>
<p>Thanks</p> ;;;; <aside class="quote no-group" data-username="Research_Associate" data-post="4" data-topic="78561">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p>Was the pixel classifier still open in the background?</p>
</blockquote>
</aside>
<p>Yeah, that would have been my question as well.</p>
<aside class="quote no-group" data-username="EPet" data-post="5" data-topic="78561" data-full="true">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/epet/40/46601_2.png" class="avatar"> Ery Petropoulou:</div>
<blockquote>
<p>No it was not. Anything else that might be happening?</p>
</blockquote>
</aside>
<p>Hmmmm, does the problem ever occur whenever you <em>haven’t</em> use the pixel classifier since (re)starting QuPath?</p>
<p>I wonder if there could be a bug that means the pixel classifier is still being active even when it shouldn’t be. If so, then pressing the ‘C’ button to turn off the previous might help.</p>
<p>(I don’t know of such a bug, and maybe the pixel classifier has nothing to do with the problem, it’s just my best guess for now)</p> ;;;; <p>Dear <a class="mention-group notify" href="/groups/ngff">@ngff</a> group,</p>
<p>One day reminder for this community call! <img src="https://emoji.discourse-cdn.com/twitter/smile.png?v=12" title=":smile:" class="emoji" alt=":smile:" loading="lazy" width="20" height="20"></p>
<p>~J</p>
<hr>
<p><em>p.s. I just noticed that use of the <code>@ngff</code> handle had stopped contacting the group because of the size it reached. (I guess that counts as a success!) I’ve temporarily changed the setting in order to notify everyone, but we may need to set up a different form of communication soon.</em></p> ;;;; <p>OK, here’s your problem (I think). Your nested loop structure is causing problems because it doesn’t break out of the loop once it finds a match.</p>
<p>So if you have folders that look like this:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/987e858f6f9d6104660f587b0a81ef90d26ea09b.png" alt="2023-03-14" data-base62-sha1="lL1E7esGn0iDYu48BVaEGL5pglt" width="312" height="123"></p>
<p><code>list4</code> will look like this: <code>M1_70_c4.tif, M1_70_c4.zip, M1_71_c4.tif, M1_71_c4.zip</code></p>
<p>The first <code>endsWith</code> check will pick up <code>M1_70_c4</code> then will go to <code>list3</code> and find the first matching image. First round goes OK.</p>
<p>The problem is when you run <code>j++</code> (having closed both your aSMA and Ki67 images at the end of the last <code>j</code>’ loop), you will eventually find another Ki67 image ending in <code>3.tif</code>, open it and try to process it against an <code>aSMA</code> image which you already closed!</p>
<p>The not terribly elegant solution, is to break the loop once you’ve found a match by artifically iterating your count variable.</p>
<pre><code class="lang-auto">				selectWindow(aSMA);
				close();
				selectWindow(Ki67);
				close();
				List.clear();
				j=1000000;
			} //-- ends with 3.tif
		} //-- j loop to find matching file
	} //-- ends with 4.tif
} //- i loop all files in main folder
</code></pre>
<p>Note the addition of the <code>j=1000000;</code> line. This basically prevents any more searches on the file list and kicks you back to the next <code>i</code> iteration.</p>
<p>Super! Problem solved right?</p>
<p><strong>BUT</strong> … there is a bigger problem, while you are iterating through your list4, you are always selecting the first match in <code>list3</code> that ends with <code>3.tif</code>. This is evidenced by the output table which shows every time that the image is <code>M1_70_c3.tif</code>.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/239211967e4a1528d6c71433fc7748abc5317602.png" alt="2023-03-14-table" data-base62-sha1="54FF3z1sBxdr8dmd8CQzBL0WjYu" width="496" height="293"></p>
<p>So, what you actually need to do is look for a matching string to your <code>list[i]</code> filename, that ends in <code>3.tif</code>. Something like</p>
<pre><code class="lang-auto">replace(list[i],"4.tif","3.tif");
</code></pre>
<p>on line ~37.</p>
<p>Hopefully that should get you back on track!</p> ;;;; <p>But modelzoo is a .py file. So it is not a folder where I can paste it.</p> ;;;; <p>No it was not. Anything else that might be happening?</p> ;;;; <aside class="quote no-group" data-username="SpecialK" data-post="3" data-topic="72737">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/s/ec9cab/40.png" class="avatar"> Kristin:</div>
<blockquote>
<p><a class="mention" href="/u/k-dominik">@k-dominik</a>, why is the recommendation to export the probabilities and then threshold?</p>
</blockquote>
</aside>
<p>Hi <a class="mention" href="/u/specialk">@SpecialK</a>,</p>
<p>If you only have two classes, then exporting the “Simple Segmentation”, rather than Probabilities is ok(ish). With more classes, the above holds true, that you will get pixels classified to one class, despite all probabilities being low.</p>
<p>If you’re looking “only” for the ratio of two (or more) different classes, then I’d probably recommend doing that in Fiji, rather then in ilastik object classification (this would make most sense if you were to train a classifier afterwards).</p>
<p>From the images you posted it looks like you want to quantify “rough” vs “smooth” areas. Challenging here is that the image is watermarked and also has other text on it. Ideally you’d be working on the raw data without that (if you can get access to it). In any case, I would propose to treat this as a 3-class problem in Pixel Classification: 1) Smooth areas 2) Rough Areas 3) Background (anything that is not considered your sample).</p>
<p>Then with the exported probablilities, I’d suggest to go to either Fiji, or something like Python (would be my choice), and 1) Load the probabilities 2) Thresholding the “rough” image class at 0.5 to get a binary image of the rough area 3) Threshold the sum of the “smooth” and “rough” channel at 0.5, to get a binary image of the total sample area. Then you could compare the two areas.</p>
<p>Does this make sense?</p>
<p>Cheers<br>
Dominik</p> ;;;; <p>I was trying to use this script from Pete:</p>
<pre><code class="lang-auto">import qupath.lib.objects.PathObject
import static qupath.lib.scripting.QP.*

def imageData = getCurrentImageData()
def hierarchy = imageData.getHierarchy()

// Get cells
def cells = getCellObjects()

// Extract Delaunay info (should have run clustering plugin already)
def connections = imageData.getProperties().get('OBJECT_CONNECTIONS')

// Assign labels to clusters
int label = 0
for (group in connections.getConnectionGroups()) {
    def allObjects = group.getPathObjects() as Set
    while (!allObjects.isEmpty()) {
        def nextObject = allObjects.pop()
        label++
        def cluster = new HashSet()
        def pending = [nextObject]
        while (!pending.isEmpty()) {
            nextObject = pending.pop()
            addToCluster(group, nextObject, cluster, pending)
        }
        for (pathObject in cluster) {
            // REMOVE THE OPTIONS YOU DON'T WANT!
            // Show cluster as classification
            //pathObject.setPathClass(getPathClass("Cluster " + label))
            // Show cluster as name
            pathObject.setName("Cluster " + label)
            // Add cluster as measurement (but don't use this in an object classifier!)
            pathObject.getMeasurementList().putMeasurement("Cluster", label)
            pathObject.getMeasurementList().close()        
        }
        allObjects.removeAll(cluster)
        
    }
}
fireHierarchyUpdate()

void addToCluster(group, pathObject, cluster, pending) {
    if (cluster.add(pathObject)) {
        for (next in group.getConnectedObjects(pathObject)) {
            if (!cluster.contains(next))
                pending.add(next)
        }
    }
}
</code></pre>
<p>To label the clusters but QuPath doesn’t seem to like the <code>pop()</code>  command:</p>
<pre><code class="lang-auto">ERROR: No signature of method: java.util.LinkedHashMap$LinkedKeySet.pop() is applicable for argument types: () values: []
</code></pre>
<p>I’ve googled about but I can’t figure out what I need to import or what method I can substitute</p> ;;;; <p>Thank you, that works perfectly!</p> ;;;; <p>Thanks for your thoughts.</p>
<p>Presumably if I increase resolution to 0.24, which is the native resolution it may help with segmentation? But the downside is it may increase the length of time of processing?</p> ;;;; <p>Hi everyone thank you for your help with this issues.</p>
<p>After installing OMERO.fpbioimage app the Omero web client seem to be stuck in a reload loop after the users login. This continuous reload stops by itself after a minute or two, no idea what’s causing it, the omero web log files say nothing about it.</p>
<p>We then proceeded to upload two .ndpi images,</p>
<p>For one of them omero doesn’t generate a thumbnail, you can see in the screenshot what omero web displays.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724.png" data-download-href="/uploads/short-url/2BEDBK3qBjVzpLEZOoHvhoUOsVS.png?dl=1" title="Screen Shot 2023-03-09 at 9.59.21 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724_2_690x256.png" alt="Screen Shot 2023-03-09 at 9.59.21 AM" data-base62-sha1="2BEDBK3qBjVzpLEZOoHvhoUOsVS" width="690" height="256" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724_2_690x256.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724_2_1035x384.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724.png 2x" data-dominant-color="E0E2E5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-09 at 9.59.21 AM</span><span class="informations">1338×498 85.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The other one seems ok but when you zoom in it gets pixelated, which could mean that there was a problem with the piramidal transformation?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab.jpeg" data-download-href="/uploads/short-url/tpM64vErEz2S3GuazT3kM2IscbN.jpeg?dl=1" title="Screen Shot 2023-03-09 at 10.00.34 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_690x444.jpeg" alt="Screen Shot 2023-03-09 at 10.00.34 AM" data-base62-sha1="tpM64vErEz2S3GuazT3kM2IscbN" width="690" height="444" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_690x444.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_1035x666.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_1380x888.jpeg 2x" data-dominant-color="CACFCE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-09 at 10.00.34 AM</span><span class="informations">1556×1002 171 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8.jpeg" data-download-href="/uploads/short-url/jV2N1GRNYF0lHqvVh2NitKpQf56.jpeg?dl=1" title="Screen Shot 2023-03-09 at 10.00.46 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_690x427.jpeg" alt="Screen Shot 2023-03-09 at 10.00.46 AM" data-base62-sha1="jV2N1GRNYF0lHqvVh2NitKpQf56" width="690" height="427" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_690x427.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_1035x640.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_1380x854.jpeg 2x" data-dominant-color="CBCECD"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-09 at 10.00.46 AM</span><span class="informations">1748×1082 55.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Again, thank you for any help,<br>
Saludos,</p> ;;;; <p>Hello <a class="mention" href="/u/yeshurun">@yeshurun</a>,</p>
<p>sorry you ran into problems using ilastik again. What version of ilastik are you using?</p>
<p>So I take it you found good parameters for your Block Size and Halo on a representative, smaller, image and added the “big” image along with the corresponding probability maps in the batch processing?</p>
<p>The error itself is not super informative (unfortunately). ilastik writes a log file, however, that usually has more information. Would you be up for making it available? The cleanest solution would be to start ilastik, got to <em>Settings → Open Log Folder</em>, then delete the <code>log.txt</code> file you find there. Then restart ilastik, and go on processing the big image in the same project file in batch processing. After the crash, you could go to <em>Settings → Open log folder</em> again, open the txt file and post the contents here.</p>
<p>Cheers<br>
Dominik</p> ;;;; <p>Thank you, Chris. That was very helpful</p> ;;;; <pre><code class="lang-auto">// Close all open images and clear previous results
run("Close All");
run("Clear Results");

//kernen input

//aSMA input
dir4 = getDirectory("Choose Directory 4."); 
list4=getFileList(dir4);
for (i=0; i&lt;list4.length; i++){ // for loop to parse through names in main folder
	if(endsWith(list4[i], "4.tif")){   // if the filename ends with 4.tif file, we enter the following part:
		input = dir4 + list4[i];
		input_ROI = replace(input,"4.tif","4.zip");
		print("now processing: "+input);
		print("together with: "+input_ROI);

		open(input);	
		//get image title for later use
		aSMA = getTitle();
	
		roiManager("reset");
		run("Select None");

		roiManager("open", input_ROI);
		roiManager("Show All");
		roiManager("Select", 2);
		setBackgroundColor(0, 0, 0);
		run("Clear Outside");
		run("8-bit");
		//get image title for later use
		aSMA = getTitle();

		//Ki67 input
		dir3 = getDirectory("Choose Directory 3."); 
		list3=getFileList(dir3);
			for (j=0; j&lt;list3.length; j++){ // for loop to parse through names in main folder
				if(endsWith(list3[j], "3.tif")){   // if the filename ends with 4.tif file, we enter the following part:
				input = dir3 + list3[j];
				input_ROI = replace(input,"3.tif","3.zip");


				print("now processing: "+input);
				print("together with: "+input_ROI);

				open(input);
	
				//get image title for later use
				Ki67 = getTitle();
	
				roiManager("reset");
				run("Select None");

				roiManager("open", input_ROI);
				roiManager("Show All");
				roiManager("Select", 2);
				setBackgroundColor(0, 0, 0);
				run("Clear Outside");
				run("8-bit");

				//get image title for later use
				Ki67 = getTitle();

				Stack.setXUnit("pixel");
				run("Properties...", "channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000");
	
	
	
				//select the window with ki67 staining
				selectWindow(Ki67);
				//correct for background in ki67 staining
				run("Subtract Background...", "rolling=50");
				//threshold based on the ki67 staining and convert to a binary image
				setAutoThreshold("Default dark");
				setOption("BlackBackground", true);
				run("Convert to Mask");
				//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
				run("Close-");
				run("Open");
				//watershed to split up nuclei that are lying against each other
				run("Watershed");
				run("Select All");
				roiManager("Delete");
				run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
	
				//select the window with aSMA staining
				selectWindow(aSMA);
	
				//threshold based on the aSMA staining and convert to a binary image
				setAutoThreshold("Default dark");
				setOption("BlackBackground", true);
				run("Convert to Mask");
		
				//select all ROIs from ki67 and transfer to aSMA
				run("Select All");
				roiManager("XOR");
				count = roiManager("count");
				roiManager("select", count-1);
				roiManager("Add");
				
				//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
				run("Close-");
				run("Open");
				//select all ROIs and dilate in order to catch all potential aSMA  signal
				run("Select All");
				run("Dilate");
				run("Dilate");
				//watershed to split up dilated ROIs that are lying against each other
				run("Watershed");
				//calculate overlap
				imageCalculator("AND", Ki67, aSMA);
				run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
				waitForUser("Press OK to continue","press ok to continue!");
				print("aSMA has run");
				selectWindow(aSMA);
				close();
				selectWindow(Ki67);
				close();
				List.clear();
			} //-- ends with 3.tif
		} //-- j loop to find matching file
	} //-- ends with 4.tif
} //- i loop all files in main folder
</code></pre>
<p>Thanks for the suggestions, i have implemented them. Unfortunately, the script remains to have the same error. The first loop with the first pair of 3.tif and 4.tif images goes as planned, but it does not correctly identify the second “4.tif”  image in the directory, causing the above error in this part of the code:</p>
<p>//select the window with aSMA staining<br>
selectWindow(aSMA);</p>
<p>I think it is because of the macro not opening the second image in the 4. directory, hence it is searching for the previous window (M11_34)</p> ;;;; <p>Following Henry Pinkard’s advice (<a href="https://forum.image.sc/t/how-to-convert-fast-stream-of-images-to-stream-of-z-stacks/77670">How to convert fast stream of images to stream of z stacks</a>) I wrote a Beanshell script to acquire images and sort them to stacks. I’m also implementing a configurable Java plugin version of the same.</p>
<p>Sometimes it works, but often image acquisition fails. From the logfile I can see that my device adapter reports result code 31 (DEVICE_INCOMPATIBLE_IMAGE) when trying to insert the captured image. Strangely it always works when I use Live-View.</p>
<p>I also tried to acquire images from napari-micromanager and got snapshots but no live-view. Unfortunately no logfile is written but I suspect the error is the same as above.</p>
<p>Any hints at what could cause this error? What does DEVICE_INCOMPATIBLE_IMAGE actually mean?</p> ;;;; <p>Hi, I’m using ImageJ SNT to construct the paths from a .swc files and do the sholl analysis. I would like to have the color-coded paths and try the application : Path Mnager&gt;Analyze&gt;Color Coding&gt;Color Code Path(s). Color by: Sholl inters. (root centered). LUT: mpl-inferno.lut.<br>
However, the colored path are not continuous. Some segments didn’t follow the color codes, and just showed the color as ‘Deselected’ paths (magenta ones here, and those segments change color with color assigned for ‘Deselected’ in the SNT control panel).<br>
p.s. those ‘deselected’ segments seems to be the seeds(?) in original Imaris filaments.<br>
Is there any possible way to solve this problem? Thank you.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e.png" data-download-href="/uploads/short-url/4pOVfLhXXM73k6Zmoir9gkdo1z0.png?dl=1" title="SNTquestion20230314" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e_2_375x500.png" alt="SNTquestion20230314" data-base62-sha1="4pOVfLhXXM73k6Zmoir9gkdo1z0" width="375" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e_2_375x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e.png 2x" data-dominant-color="050203"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">SNTquestion20230314</span><span class="informations">400×532 27.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/jennifer_fessler">@Jennifer_Fessler</a>,<br>
does this (even more simplified code) work ?</p>
<pre><code class="lang-auto">Table.setLocationAndSize(240, 125, 600, 700);
for (rowIndex = 0; rowIndex &lt; 10; rowIndex++) {
    if (true) {
        selectWindow("Cortical Width Measurements");
        Table.set("Image", rowIndex, 1);
        Table.set("Animal", rowIndex, 2);
        Table.set("Section", rowIndex, 3);
        Table.set("Lobule", rowIndex, 4);    
    }
}
</code></pre>
<p>For me it does.</p>
<p>Instead of selecting the window you can also add the name of the table as the last parameter to all Table-commands. You can also try to call <code>Table.update</code>at the end or after each row.</p>
<p>Best,<br>
Volker</p> ;;;; <p>OK, so there doesn’t appear to be an image under <code>w96_A1_A2_test_zarr/0/</code> which is where it should be for <code>bioformats2raw</code> layout.</p>
<p>How was the image generated?<br>
Via <code>bioformats2raw</code>? or via NGFF-converter? Or custom code?</p> ;;;; <p>Hey <a class="mention" href="/u/volker">@volker</a><br>
Thanks for getting back with me. I pulled the Table.create out of the for loop. I forgot last night in playing around with the code I left it in there. Here is the altered code. The table does not update. I even inserted a bit of code to select the table window to avoid confusion on which window I am updating and it still does not update the table. I am playing with this simplified code because the table was not updating in larger macro I had set up and I was trying to isolate the issue. Thanks for the help!</p>
<p>Table.create(“Cortical Width Measurements”);<br>
Table.setLocationAndSize(240, 125, 600, 700);<br>
imagePath = getDirectory(“Where are the images located?”);<br>
filelist = getFileList(imagePath)<br>
for (rowIndex = 0; rowIndex &lt; lengthOf(filelist); rowIndex++) {<br>
if (endsWith(filelist[rowIndex], “.tif”)) {<br>
open(imagePath + File.separator + filelist[rowIndex]);<br>
waitForUser;<br>
imageTitle = getTitle();<br>
basename = substring(imageTitle, 0, indexOf(imageTitle, “.tif”));<br>
animal = “77R”;<br>
section = “2.3”;<br>
Dialog.create(“Fill in animal name and section number”); //title on window<br>
Dialog.addString(“Image Name”, basename); //correct image name<br>
Dialog.addString(“Animal Name”, animal);<br>
Dialog.addString(“Section Number”, section);<br>
Dialog.show();<br>
imageName = Dialog.getString();<br>
animal = Dialog.getString();<br>
section = Dialog.getString();<br>
lobule = getNumber(“Which Lobule are you measuring?”, 8);<br>
selectWindow(“Cortical Width Measurements”);<br>
Table.set(“Image”, rowIndex, imageName);<br>
Table.set(“Animal”, rowIndex, animal);<br>
Table.set(“Section”, rowIndex, section);<br>
Table.set(“Lobule”, rowIndex, lobule);<br>
selectWindow(imageTitle);<br>
close();<br>
}<br>
}</p> ;;;; <aside class="quote no-group" data-username="Li1234" data-post="1" data-topic="78571">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/li1234/40/68757_2.png" class="avatar"> Li Yenn Yong :</div>
<blockquote>
<p>Also, I noticed that stardist was taking &gt;10mins per annotation vs 2-3 mins using the native Qupath plugin, just wondering whether I missed out a command string somewhere to help with stardist processing time?</p>
</blockquote>
</aside>
<p>Not really, StarDist takes longer. You can get minor speed increases using the GPU, sometimes, but it’s mostly system dependent.<br>
Adjusting the parameters for StarDist is probably your best bet. Over and under segmentation is sometimes dependent on how accurate the color channels passed to the analysis are or the pixel size predicted. Larger pixel size makes for more merged objects. Same thing goes for the built in cell detection.</p>
<p>In general, if the DAB obscures the nucleus, you will get poorer segmentation. That’s just the price you pay for using DAB as opposed to fluorescence, not much you can do about that part now.</p> ;;;; <p>You can use the Set Class button in the Annotation tab I think. Or use a script.</p><aside class="quote quote-modified" data-post="4" data-topic="45382">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar">
    <a href="https://forum.image.sc/t/fluorescent-image-manually-set-class-to-a-detected-cell/45382/4">Fluorescent image: manually set class to a detected cell?</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    The most straightforward method will be recreating the composite classifier with only the two classes that work, and then finding some way to add in the third positive class. The “Set class” will only work if you have a list of complex classes to set, I think. So you would have to choose from triple, two different double positive classes, or single positive, when manually assigning the class. Kind of awkward, but doable as long as you have created those classes in your Annotation tab. You could…
  </blockquote>
</aside>

<p><a href="https://gist.github.com/Svidro/5b016e192a33c883c0bd20de18eb7764#file-set-selected-object-class-groovy">https://gist.github.com/Svidro/5b016e192a33c883c0bd20de18eb7764#file-set-selected-object-class-groovy</a></p> ;;;; <p>Hi, I was wondering if any progress has been made on this issue? I am in need of analyzing my time courses that have the dropped data. I understand that the community has a priority list and might not be able to get to my personal bug <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20">  I am just wondering to see if I need to pursue a fix myself or if it would be better to wait.</p>
<p>Thank you!</p> ;;;; <p>Hi <a class="mention-group notify" href="/groups/ome">@ome</a> ,<br>
I would like to have the jar omero_ij on bioconda like <a href="https://github.com/bioconda/bioconda-recipes/tree/master/recipes/fiji-max_inscribed_circles" class="inline-onebox" rel="noopener nofollow ugc">bioconda-recipes/recipes/fiji-max_inscribed_circles at master · bioconda/bioconda-recipes · GitHub</a>. Would you agree? Could I have a permanent URL for the version 5.8.0? What is the licence of the plugin?<br>
Thank you very much,<br>
Best Regards,<br>
Lucille Delisle</p> ;;;; <p>Generally you can’t. You first need to select the objects you want to measure, then perform the measurement. If you select the objects through the GUI (annotations, detections, some class) and then create the measurement through the GUI, the lines of script should show up in the workflow. <a href="https://qupath.readthedocs.io/en/stable/docs/scripting/workflows_to_scripts.html" class="inline-onebox">Workflows to scripts — QuPath 0.4.3 documentation</a></p> ;;;; <aside class="quote no-group" data-username="EPet" data-post="3" data-topic="78561">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/epet/40/46601_2.png" class="avatar"> Ery Petropoulou:</div>
<blockquote>
<p>Yes I used a pixel classifier to detect my islets. Then I created 3 new classes because I wanted to be able to distinguish three types of islets, but this was done manually by selecting the islets and setting the new classes.</p>
</blockquote>
</aside>
<p>Was the pixel classifier still open in the background?</p> ;;;; <p>Hi all,</p>
<p>I’m trying to figure out the best way to count positive cells. I’m working on a CD68 IHC-DAB stain as my initial stain to figure this out. I’ve tried both using Qupath’s native cell detection, then setting cell intensity based on cell-DAB stdv as it seems to separate the best, but what I’ve noticed is that some of my cells seem to be oversegmented.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e.jpeg" data-download-href="/uploads/short-url/zTyBUHqSetWRdyAxKXIUKrLEiiq.jpeg?dl=1" title="Screenshot 2023-03-14 at 14.03.43" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_644x500.jpeg" alt="Screenshot 2023-03-14 at 14.03.43" data-base62-sha1="zTyBUHqSetWRdyAxKXIUKrLEiiq" width="644" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_644x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_966x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_1288x1000.jpeg 2x" data-dominant-color="EDEDED"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-14 at 14.03.43</span><span class="informations">1676×1300 136 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<a class="attachment" href="/uploads/short-url/ebBBbqFwpJ06UlW6A0Xb3UlgYLS.tif">Screenshot - slide-2023-02-24T09-20-06-R9-S10-1.tif</a> (3.3 MB)</p>
<p>I then tried running stardist, and the opposite seems to happen, where there is an inadequate separation of cells.<br>
<a class="attachment" href="/uploads/short-url/haWqdlp81Ftxde3eE1Ryp5lnxRV.tif">Screenshot - slide-2023-02-24T09-20-06-R9-S10.mrxs (1)-1.tif</a> (3.3 MB)</p>
<pre><code class="lang-auto">
// Get current image - assumed to have color deconvolution stains set
var imageData = getCurrentImageData()
var stains = imageData.getColorDeconvolutionStains()

import qupath.ext.stardist.StarDist2D

// Specify the model file (you will need to change this!)
var pathModel = '/Applications/QuPath.app/dsb2018_paper.pb'

var stardist = StarDist2D.builder(pathModel)
        .preprocess( // Extra preprocessing steps, applied sequentially
            ImageOps.Channels.deconvolve(stains),
            ImageOps.Channels.extract(0),
            ImageOps.Filters.median(2),
            ImageOps.Core.divide(1.5)
         )
        .threshold(0.1)              // Probability (detection) threshold
        .normalizePercentiles(1, 99) // Percentile normalization
        .pixelSize(0.5)              // Resolution for detection
        .cellExpansion(10)          // Approximate cells based upon nucleus expansion
        .cellConstrainScale(2)     // Constrain cell expansion using nucleus size
        .measureShape()              // Add shape measurements
        .measureIntensity()          // Add cell measurements (in all compartments)
        .includeProbability(true)    // Add probability as a measurement (enables later filtering)
		 .tileSize(1024)
        .build()



// Run detection for the selected objects
var pathObjects = getSelectedObjects()
if (pathObjects.isEmpty()) {
    Dialogs.showErrorMessage("StarDist", "Please select a parent object!")
    return
}
stardist.detectObjects(imageData, pathObjects)


stardist.close() // This can help clean up &amp; regain memory

setCellIntensityClassifications("Hematoxylin: Nucleus: Mean", 0.15)

//Clean up bad objects
removal = getCellObjects().findAll{it.getPathClass().toString().contains("Negative")}
removeObjects(removal, true)

setCellIntensityClassifications("Nucleus: Area µm^2", 3)

//Clean up bad objects
removal = getCellObjects().findAll{it.getPathClass().toString().contains("Negative")}
removeObjects(removal, true)

println 'Done!'
</code></pre>
<p>Just wondering whether anyone had any suggestions on how to improve this? I did then try running cellpose to see if it made a difference, but am completely new to programming and having much difficulty trying to get it up and running on a Mac M1.</p>
<p>Also, I noticed that stardist was taking &gt;10mins per annotation vs 2-3 mins  using the native Qupath plugin, just wondering whether I missed out a command string somewhere to help with stardist processing time?</p> ;;;; <p>This is the version of the ImageJ I am using:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png" data-download-href="/uploads/short-url/wHHVEARNewpXphnJDKGeFnnveO6.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png" alt="image" data-base62-sha1="wHHVEARNewpXphnJDKGeFnnveO6" width="674" height="500" data-dominant-color="4D4A1D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">758×562 103 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I am operating on Windows 10.<br>
The problem is when I open a colorized stack composite in 3D viewer plugin, I go to view, and take snapshot, and nothing is in frame, and a black screen results:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238.jpeg" data-download-href="/uploads/short-url/g6LaNsmsUqFvLDHHVK3g9nw25As.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg" alt="image" data-base62-sha1="g6LaNsmsUqFvLDHHVK3g9nw25As" width="690" height="398" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1035x597.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1380x796.jpeg 2x" data-dominant-color="171714"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1467×847 28.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg" data-download-href="/uploads/short-url/bM3txLoZ9CSkkGWNbCPKzu7ylSh.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg" alt="image" data-base62-sha1="bM3txLoZ9CSkkGWNbCPKzu7ylSh" width="690" height="404" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_1035x606.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg 2x" data-dominant-color="100E0D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1350×791 54.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>It will take a snapshot of the bounding box, but the sample is missing from the snapshot. I tried on a mac laptop, and it worked fine, but I’m wondering if anyone else is having this problem in ImageJ Windows 10? Is there a solution to this?</p> ;;;; <p>Dear QuPath user, I am trying  to correct some cells identifiacation/classification. For some reason (cells overalpping), some cells are not well identified. Is there a way to change the cells class manualy to correct bad results ? Of course it is possible to do it after export. Thanks, Mathieu</p> ;;;; <p>Thanks, I’ll look into those and see if they get me anywhere. Thank you!</p> ;;;; <aside class="quote no-group" data-username="jkh1" data-post="2" data-topic="78562">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jkh1/40/20446_2.png" class="avatar"> Jean-Karim Hériché:</div>
<blockquote>
<p>f the clusters of tumor cells have been segmented,</p>
</blockquote>
</aside>
<p>I suspect they meant the clusters themselves, as in <a href="https://forum.image.sc/t/annotations-based-on-cell-density/51021/3" class="inline-onebox">Annotations Based on Cell Density - #3 by Research_Associate</a><br>
Once you have the outline of each cluster, you could take shape measurements.</p>
<p>You <em>might</em> also be able to use density maps to detect small clusters and create annotations for them.</p>
<p>Another option is to convert the objects into annotations, then dilate/erode them a certain distance so that they form a single object - <a href="https://forum.image.sc/t/create-annotation-from-selected-objects/77529/5" class="inline-onebox">Create annotation from selected objects - #5 by David_Garcia_Ros</a></p>
<p>Or create a <em><strong>concave</strong></em> hull around them <a href="https://forum.image.sc/t/qupath-script-command-to-draw-polygon-annotation-from-points/76833/2" class="inline-onebox">Qupath - script command to draw polygon annotation from points? - #2 by petebankhead</a></p> ;;;; <p>I think something broke the plugin in the recent changes indeed.<br>
And yeah just having a sample image to test would help (a small one is better) as i don’t remember the format required by the plugin and i’m not sure to have one of these with me. Thanks !</p>
<p>You can send me a link to a sample by email (or directly joined in the mail if it’s small enough) <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Best,</p>
<p>– Stephane</p> ;;;; <p>I believe the calipers are the min and max lengths of the object, as measured by calipers, see <a href="https://en.wikipedia.org/wiki/Feret_diameter" class="inline-onebox">Feret diameter - Wikipedia</a><br>
Std dev is Standard Deviation and Range is… range from min to max. OD ranges tend to be from 0 to ~1.5-3, but can change depending on the color vectors for specific stains.</p> ;;;; <p>Hi <a class="mention" href="/u/luigi_marongiu">@Luigi_Marongiu</a>,</p>
<p>The following macro might help. It works on the original image you posted here. If your original images are different (e.g. in size) you will need to adapt it, but at least can orient on it.</p>
<pre><code class="lang-auto">setOption("BlackBackground", true);
original = getTitle();
run("Duplicate...", " ");
copy = getTitle();
run("8-bit");
run("Gaussian Blur...", "radius=50");
setAutoThreshold("Minimum dark");
run("Convert to Mask");
run("Create Selection");
selectWindow(original);
run("Restore Selection");
run("Enlarge...", "enlarge=-20");
run("Crop");
run("Duplicate...", " ");
copy2 = getTitle();
run("Select None");
run("8-bit");
run("Invert");
run("Enhance Contrast...", "saturated=1 normalize");
run("Median...", "radius=3");
run("Top Hat...", "radius=100");
setAutoThreshold("Li dark");
run("Convert to Mask");
run("Analyze Particles...", "size=0-Infinity show=Masks exclude in_situ");
run("Watershed");
run("Analyze Particles...", "  show=Nothing display clear summarize add");
run("Set Measurements...", "area mean standard modal min centroid center perimeter bounding fit shape feret's integrated median skewness kurtosis area_fraction display redirect=None decimal=3")
run("Analyze Particles...", "  show=Nothing display clear summarize add");
close(copy);
selectWindow(original);
roiManager("show all without labels");
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c.jpeg" data-download-href="/uploads/short-url/q22XpKL4Ej9wiISnEsiVAawN6ri.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_690x353.jpeg" alt="image" data-base62-sha1="q22XpKL4Ej9wiISnEsiVAawN6ri" width="690" height="353" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_690x353.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_1035x529.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_1380x706.jpeg 2x" data-dominant-color="92836B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1526×782 285 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi,<br>
I am now writing some plugins that write a memoizer file first. Not done for a generic case. The first reading is slow but all other ones work quite fast. Typically you read then each series.</p>
<p>The <a href="https://imagej.net/plugins/big-data-processor" rel="noopener nofollow ugc">big-data-processor</a> does a similar thing when loading data. First loading is slow, but all subsequent loading work quite well.</p>
<p>It would be super-nice if the bioformat people could have an option to create bfmemo file. This would make the usage much more efficient.</p>
<p>Antonio</p> ;;;; <p>Hello <a class="mention" href="/u/stephane">@Stephane</a></p>
<p>Unfortunately now none of my image are working with that plugin ! It used to work (back in 2018).<br>
Maybe it is because of the upgraded version of ICY?<br>
I have many ScanR images from well plates that I could send you if you need a sample to try.</p>
<p>Thank you for your help,</p>
<p>Best,</p>
<p>Alix</p> ;;;; <p>I can’t replicate the problem <em>exactly</em> but it looks like you’re missing a pair of closing braces.</p>
<p>Generally, it’s good practice (although not required) to indent your code blocks for each nested loop to keep track of when you close each block. With the addition of two closing braces, the code below seems to work:</p>
<pre><code class="lang-auto">// Close all open images and clear previous results
run("Close All");
run("Clear Results");

//kernen input

//aSMA input
dir = getDirectory("Choose Directory 4."); 
for (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder
	if(endsWith(list[i], "4.tif")){   // if the filename ends with 4.tif file, we enter the following part:
		input = dir + list[i];
		input_ROI = replace(input,"4.tif","4.zip");
		print("now processing: "+input);
		print("together with: "+input_ROI);

		open(input);	
		//get image title for later use
		aSMA = getTitle();
	
		roiManager("reset");
		run("Select None");

		roiManager("open", input_ROI);
		roiManager("Show All");
		roiManager("Select", 2);
		setBackgroundColor(0, 0, 0);
		run("Clear Outside");
		run("8-bit");
		//get image title for later use
		aSMA = getTitle();

		//Ki67 input
		dir = getDirectory("Choose Directory 3."); 

		list=getFileList(dir);
			for (j=0; j&lt;list.length; j++){ // for loop to parse through names in main folder
				if(endsWith(list[j], "3.tif")){   // if the filename ends with 4.tif file, we enter the following part:
				input = dir + list[j];
				input_ROI = replace(input,"3.tif","3.zip");


				print("now processing: "+input);
				print("together with: "+input_ROI);

				open(input);
	
				//get image title for later use
				Ki67 = getTitle();
	
				roiManager("reset");
				run("Select None");

				roiManager("open", input_ROI);
				roiManager("Show All");
				roiManager("Select", 2);
				setBackgroundColor(0, 0, 0);
				run("Clear Outside");
				run("8-bit");

				//get image title for later use
				Ki67 = getTitle();

				Stack.setXUnit("pixel");
				run("Properties...", "channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000");
	
	
	
				//select the window with ki67 staining
				selectWindow(Ki67);
				//correct for background in ki67 staining
				run("Subtract Background...", "rolling=50");
				//threshold based on the ki67 staining and convert to a binary image
				setAutoThreshold("Default dark");
				setOption("BlackBackground", true);
				run("Convert to Mask");
				//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
				run("Close-");
				run("Open");
				//watershed to split up nuclei that are lying against each other
				run("Watershed");
				run("Select All");
				roiManager("Delete");
				run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
	
				//select the window with aSMA staining
				selectWindow(aSMA);
	
				//threshold based on the aSMA staining and convert to a binary image
				setAutoThreshold("Default dark");
				setOption("BlackBackground", true);
				run("Convert to Mask");
		
				//select all ROIs from ki67 and transfer to aSMA
				run("Select All");
				roiManager("XOR");
				count = roiManager("count");
				roiManager("select", count-1);
				roiManager("Add");
				
				//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
				run("Close-");
				run("Open");
				//select all ROIs and dilate in order to catch all potential aSMA  signal
				run("Select All");
				run("Dilate");
				run("Dilate");
				//watershed to split up dilated ROIs that are lying against each other
				run("Watershed");
				//calculate overlap
				imageCalculator("AND", Ki67, aSMA);
				run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
				waitForUser("Press OK to continue","press ok to continue!");
				print("aSMA has run");
				selectWindow(aSMA);
				close();
				selectWindow(Ki67);
				close();
				List.clear();
			} //-- ends with 3.tif
		} //-- j loop to find matching file
	} //-- ends with 4.tif
} //- i loop all files in main folder
</code></pre>
<p>This is very much a personal choice, but I also like to label the closing braces so I know what loop is closing:</p>
<pre><code class="lang-auto">			} //-- ends with 3.tif
		} //-- j loop to find matching file
	} //-- ends with 4.tif
} //- i loop all files in main folder
</code></pre>
<p>Can you try that and see if it works for you?</p> ;;;; <p>Hello again, Please i need your help<br>
How can I choose what I want to measure on qupath as a parameter with one line of code (script)</p> ;;;; <p>Hi <a class="mention" href="/u/alix_boucharlat">@Alix_Boucharlat</a></p>
<p>Can you share a sample image that we can use with this plugin ? It looks like it needs a very specific format for the image (I made some tests but my images weren’t recognized).<br>
That would be really useful in understanding the problem, thanks !</p>
<p>– Stephane</p> ;;;; <p>I know it’s surprising to me too, because I ran the same yesterday and it was totally fine.</p>
<p>Yes this happens only to the expand annotations command and only to some of the annotations.</p>
<p>Hmmm, I am not so sure what you mean with the last question. Yes I used a pixel classifier to detect my islets. Then I created 3 new classes because I wanted to be able to distinguish three types of islets, but this was done manually by selecting the islets and setting the new classes. Also some of the annotations were manually corrected, but I do it in all of my images, so I guess in the one of yesterday as well.</p> ;;;; <p>I am not interested in the individual cell shapes but how they relate to each other: In linear clusters the angles between cells will tend to be in the 90 - 180 degrees range (in green below):<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/939fd03b962a4e7820982e43677ddf9edabd1fbb.jpeg" alt="linear1" data-base62-sha1="l3WAnGkEbBLtnAZiWqhBgYhrfpV" width="291" height="177"></p>
<p>Whereas in round clusters the angles between cells will tend to be less than 90 degrees (in blue):<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/5407863e7d5e9dfe95726d251984ae97165f51d7.jpeg" alt="cluster1" data-base62-sha1="bZmeaaiBxOGgmSPpdh2zBa9PPDx" width="269" height="243"></p>
<p>I was thinking by averaging these angles I could work out if a cluster tends towards round or linear but I haven’t been able to figure out how to calculate the angles…</p> ;;;; <p>Hi <a class="mention" href="/u/amadeus">@amadeus</a>,</p>
<p>This will do…</p>
<pre><code class="lang-auto">original_image = getTitle();
run("Split Channels");
selectWindow(original_image + " (green)");

</code></pre> ;;;; <p>I’m afraid I don’t know. It’s a really surprising error, because it seems to be related to running out of memory when reading pixels from the image – but that wouldn’t be relevant to expanding the annotations.</p>
<p>Does the problem <em>only</em> occur when running the command to expand annotations?</p>
<p>When it occurs, does it happen for <em>all</em> annotations in the image – or just for some annotations in the image?</p>
<p>Are you using a pixel classifier at all, or is there anything else running that might use a lot of memory?</p> ;;;; <p>Hi all,</p>
<p>I have quite a simple problem I’d like some help with.</p>
<p>In my image analysis pipeline, I currently open an RGB image, split the channels, select the green channel and then run an image analysis macro on that channel alone.</p>
<p>I currently do the first steps (1-3) manually:</p>
<ol>
<li>open image</li>
<li>split channels</li>
<li>select green channel</li>
<li>run my analysis macro.</li>
</ol>
<p>How could I add a few lines into my current macro to automate steps 2 and 3?<br>
I guess it’ll be something like:</p>
<p>run(“Split Channels”);<br>
selectWindow(“green channel”); ← I’m not sure how to properly write this line…! (the window name would be “filename.jpg (green)”)</p>
<p>Hope that makes sense! Any help would be much appreciated, thanks in advance!</p>
<p>Amadeus</p> ;;;; <p>Yeah, isosurface rendering is really expensive in napari, it’s not well optimized. The main issue is that the shader is re-sampling the texture a <em>bunch</em> of times in order to calculate lighting, and it’s not being smart about caching it.</p> ;;;; <p>Thanks <a class="mention" href="/u/ym.lim">@ym.lim</a> I’ve created an issue for this at <a href="https://github.com/qupath/qupath/issues/1252" class="inline-onebox">PathIO doesn't restore backup if writing ImageData fails · Issue #1252 · qupath/qupath · GitHub</a></p>
<p>Specially QuPath <em>should</em> automatically read from the backup file if the original is missing or broken. The stack trace in your message is really helpful and shows me why that doesn’t always work.</p> ;;;; <p>I’ve thought about this for about a minute, so this may not be the best solution, but here’s one idea: layers have a <code>.metadata</code> attribute that is a dictionary explicitly for “arbitrary” use. You could do something like:</p>
<pre><code class="lang-python">from contextlib import contextmanager

@contextmanager
def set_dict_key(dictionary, key, value):
    dictionary[key] = value
    yield
    del dictionary[key]

# ...
# now in your slider widget class/function

def update_confidence_threshold(
        shapes_layer, confidence_level, boxes
        ):
    with set_dict_key(
            shapes_layer.metadata,
            'my-plugin-name:update_confidence_threshold',
            True
            ):
        shapes_layer.data = filter_by_confidence(boxes, confidence_level)
</code></pre>
<p>Then in the function that you’re hooking up to events:</p>
<pre><code class="lang-python">def respond_to_layer_data(ev):
    key = 'my-plugin-name:update_confidence_threshold'
    if key in shapes_layer.metadata:
        return
    # rest of function goes here
</code></pre> ;;;; <p>Ok, now it works and i give me:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb.jpeg" data-download-href="/uploads/short-url/zz3QDvZCBWi0woRbpnlkqSVtYzp.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb_2_690x257.jpeg" alt="image" data-base62-sha1="zz3QDvZCBWi0woRbpnlkqSVtYzp" width="690" height="257" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb_2_690x257.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb_2_1035x385.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb.jpeg 2x" data-dominant-color="DF8E5F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1091×407 80.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/altairch95">@Altairch95</a>!</p>
<p>Once you have computed the transform you can apply it to a set of coordinates by calling it:</p>
<pre><code class="lang-python">tf = PiecewiseAffineTransform()
tf.estimate(src_points, dst_points)
new_dst = tf(new_src)
</code></pre> ;;;; <p>sure! thanks again for the quick reply. The following error message is popping up:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/1/115735aee8706756bd3c57062b8dafaca083779d.png" alt="image" data-base62-sha1="2toXHsqvcy91dZ5tzvOM0TI1l5X" width="340" height="118"><br>
This error messgae is caused by line 92 (selectwindow(asma)) in the second run</p>
<p>and the two images:<br>
<a class="attachment" href="/uploads/short-url/qKrNlnCHmC03x3G15yW7EvxMyXE.tif">M1_70_c3.tif</a> (5.5 MB)<br>
<a class="attachment" href="/uploads/short-url/rHRtmPpGxyjHH5dNZ5n0ih3aQo9.tif">M1_70_c4.tif</a> (13.2 MB)</p>
<p>please note that the macro starts with the image called m11_34 and then proceeds onto M11_45</p> ;;;; <p>Thank you, that option was already checked. It should have been unchecked:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/c/3cfd5fe3f5734caa2674fa9db10e07ae9f1d4ad7.png" alt="Screenshot from 2023-03-14 12-55-35" data-base62-sha1="8Hxv2RlfFzaOdQSqjVBOpSkJnSf" width="481" height="499"></p>
<p>But I think the issue is about defining the size of the things to count and their circularity.<br>
I changed a bit the values with <code>run("Analyze Particles...", "size=5-10 display clear summarize add");</code> and I got the results above.<br>
Better but not good enough. Is there a systematic approach to how to define the size of the colonies for counting?</p> ;;;; <p>Thank you very much!!</p> ;;;; <p>I tried to classify objectives from whole slide image in ilastik (raw data, and segmented image), but I get the following error massage when I try to export the image by Blockwise Object Classification (as recommended in ilastik website for big image analysis <a href="https://www.ilastik.org/documentation/objects/objects" class="inline-onebox" rel="noopener nofollow ugc">ilastik - Object Classification</a>):</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b.jpeg" data-download-href="/uploads/short-url/nFZqayVhLL7iuQf0RfNtTTu939V.jpeg?dl=1" title="20230314_132752" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_666x500.jpeg" alt="20230314_132752" data-base62-sha1="nFZqayVhLL7iuQf0RfNtTTu939V" width="666" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_1332x1000.jpeg 2x" data-dominant-color="9A9B9C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">20230314_132752</span><span class="informations">1920×1440 237 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Do someone here know how to export big image with object classification in ilastik?</p> ;;;; <p>If the clusters of tumor cells have been segmented, you can compute any number of shape-related features such as circularity, elongation or eccentricity.</p> ;;;; <p>It would be useful for something I’m working on to be able to distinguish tumour cells that form linear clusters rather than round nests:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b071800aff6fef77c072abdff6d139810d508669.jpeg" alt="Classic" data-base62-sha1="paTcSJtzXryXMmovY3pkpuL8OCZ" width="302" height="185"> Linear</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/581f3ff48d7985d03331646f59f21abfbd976f4b.jpeg" alt="Alveolar" data-base62-sha1="czyY84nQG4ppsMTAKEoFXLefpFN" width="215" height="171"> Round</p>
<p>I was thinking that if I took the centroid of a tumour cell and the centroid of its nearest tumour cell neighbour then the angle between them could be calculated. Or the angle between three cells (two ‘lines’) If this was done for all tumour centroids within (say) 15um of each other then the tendency of each group to follow a linear or round pattern could be calculated…</p>
<p>The problem is that I have fallen at the first hurdle - I can’t figure out how to calculate angles between centroids. Any ideas would be gratefully appreciated! <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> Or any ideas of better ways to get this sort of data! Many thanks!</p> ;;;; <p>Hi there,</p>
<p>I am trying to expand my islet annotations, and while some do get expanded, others do<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70.jpeg" data-download-href="/uploads/short-url/yVGWnhc9IPuwFql5uAI29tA2tK8.jpeg?dl=1" title="annot expansion error" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_571x500.jpeg" alt="annot expansion error" data-base62-sha1="yVGWnhc9IPuwFql5uAI29tA2tK8" width="571" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_571x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_856x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_1142x1000.jpeg 2x" data-dominant-color="2E3031"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">annot expansion error</span><span class="informations">1897×1659 397 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
not and the process goes on forever with QuPath non-responding. What is causing these exceptions? How can I get past them?</p>
<p>Thank you,<br>
Ery</p> ;;;; <p>Hi all,</p>
<p>I would like to apply a piecewise affine transformation to align two images from two set of coordinates that are chromatically shifted: mov → ref., and save the transformation map to correct other sets of coordinates. I’m trying with the skimage.transform PiecewiseAffineTransform package, but I’m not sure how to apply the transformation map to other sets of coordinates.<br>
Any help will be appreciated!</p>
<p>Thanks!</p> ;;;; <p>Hi James, I have roughly tried to combine the two as below though I haven’t tested it and you could likely tidy it up to make the code much neater. It simply skips the processing of the folder in the second script and jumps straight to processing the file (now renamed processTiff) after saving the unadjusted tiffs. I have left a TODO where you could add the extra user dialog at a later point.</p>
<pre><code class="lang-auto">run("Bio-Formats Macro Extensions");
setBatchMode(true); 
input = getDirectory("Input directory, folder where your .lif file is");
output = getDirectory("Output directory, where you'd like your raw .tiff files to go");


suffix = ".lif";

processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
}

function processFile(input, output, file) {
	Ext.setId(input + file);
	Ext.getSeriesCount(count);
	print("Processing: " + input + file+ " - "+d2s(count,0)+" images");

	for (f=0;f&lt;count;f++) {
		openLif(input+file,f);
		print("Saving to: " + output);
		listImages();
	}
}

function openLif(input,f){
	Ext.setSeries(f);
	run("Bio-Formats Importer", "open=[" +input +"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_" + d2s(f+1,0));
}

//Create string "image_1 image_2 image_3 image_4...."
function seriesN(num){
	str = "";
	for (i=0; i&lt;num; i++){
		ii = i+1;
        str = str+"image_"+ii+" ";
     }
     return str;
}

//Save
function listImages(){
	imageList = getList("image.titles");
	if (imageList.length==0)
	     print("No image windows are open");
	else {
    	print("Image windows:");
     	for (i=0; i&lt;imageList.length; i++){
        	print("   "+imageList[i]);
        	// TODO: Add condition for saving unadjusted files
                saveTiff(imageList[i]);
                
                // Process the files as per the second script
                processTiff(output, output, imageList[i]);
     	}
	}
	print("");
}


function saveTiff(winName){
	selectWindow(winName); 
	saveAs("Tiff", output + winName);
	close();
}

function processTiff(input, output, file) {

	print("Processing: " + input + file);
	
	
	open(input+file);
	renderColor(file);
	brightnessNcontrast(file);
	deleteSlices(file);

	saveAs("Tiff", output + file + "_BCadjusted");
	rename(file);
	
	RGBmerge(file);
	scaleBar();
	makeMontage(file);

	//selectWindow("Montage1to5");
	//saveAs("Jpeg", output + file + "_montage1to5");
	selectWindow("MontageHorizontal");
	saveAs("Jpeg", output + file + "_BCmontage");

	print("Saving to: " + output);
	run("Close All");
}



// Give colors for each slice
function renderColor(file){
	color = newArray("Blue","Green","Red","Grays");
	//color = newArray("Blue","Green","Red");

	run("Make Composite", "display=Color");
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		run(color[i]);
	}
}
// Change brightness and contrast
function brightnessNcontrast(file){
	//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		setMinAndMax(BC_range[2*i],BC_range[2*i+1]);
	}
	
}

// Delete slices. You want to delete slice a, b
function deleteSlices(file){
	deleteA = 0;
	deleteB = 0;
	//deleteB &gt; deleteA. deleteB should be larger than deleteA

	print("deleting the slice "+deleteA+" and "+deleteB);
	selectWindow(file);
	if(deleteB&gt;0){
		setSlice(deleteB);
		run("Delete Slice", "delete=channel");
		}
	if(deleteA&gt;0){
		setSlice(deleteA);
		run("Delete Slice", "delete=channel");
	}	
}

// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.
function RGBmerge(file){
	run("Duplicate...", "title=4channels duplicate");
	run("RGB Color");
		
	selectWindow(file);
	run("Duplicate...", "title=Merge duplicate");
	Stack.setDisplayMode("composite");
	run("RGB Color");
	run("Copy");

	selectWindow("4channels (RGB)");
	setSlice(nSlices);
	run("Add Slice"); 
	run("Paste"); 

	close("4channels");
	close("Merge");
	close("Merge (RGB)");
	
	// "4channels (RGB)" is made.
}

// Add scale bar
function scaleBar(){
	setSlice(nSlices);
	run("Set Scale...", "distance=311.0016 known=100 pixel=1 unit=µm");
	//run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] hide");
	run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] bold");
}

function makeMontage(input){
	//selectWindow("4channels (RGB)");
	//run("Make Montage...", "columns=1 rows=5 scale=0.25 border=2");
	//rename("Montage1to5");
	selectWindow("4channels (RGB)");
	run("Make Montage...", "columns="+nSlices+" rows=1 scale=0.5 border=2");
	rename("MontageHorizontal");
}
function GetTime(){
     MonthNames = newArray("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec");
     DayNames = newArray("Sun", "Mon","Tue","Wed","Thu","Fri","Sat");
     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);
     TimeString ="Date: "+DayNames[dayOfWeek]+" ";
     if (dayOfMonth&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+dayOfMonth+"-"+MonthNames[month]+"-"+year+"\nTime: ";
     if (hour&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+hour+":";
     if (minute&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+minute+":";
     if (second&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+second;
     print(TimeString);
}

</code></pre> ;;;; <p>Hi <a class="mention" href="/u/slaine_troyard">@Slaine_Troyard</a> ,</p>
<p>In the new version of 3D Manage, called <strong>3D Manager V4</strong>, there is the option to <em>export</em> your roi list. The split option is not implemented yet in this new version, so in the meanwhile you can save your roi list from the legacy 3D Manager, load it into the new version and export it.</p>
<p>It seems what would be ideal for your case is to have a macro function <em>export</em> for the legacy 3D Manager, I will try to implement it.</p>
<p>Best,</p>
<p>Thomas</p> ;;;; <p>Hi James, those messages can largely be ignored. Unfortunately there isn’t a simple way of turning them off, though a number of users have now requested a way of controlling the logging levels in the plugin.</p>
<p>Currently the only way to change the logging levels is through the DebugTools class in the Bio-Formats API as per the <a href="https://bio-formats.readthedocs.io/en/stable/developers/logging.html">logging docs</a>. If you are using a jython or groovy script then it can be set using the below:</p>
<pre><code class="lang-auto">from loci.plugins import BF
from loci.common import DebugTools

DebugTools.setRootLevel("OFF")
</code></pre> ;;;; <p>It is when QuPath is attempting to read the .qpdata later, when the data is already corrupted.</p> ;;;; <p>Thanks <a class="mention" href="/u/ym.lim">@ym.lim</a> is the the log from when QuPath was force-quit, or is it from the attempt to read the .qpdata later?</p>
<p>It looks like it’s from the attempt to read the data (at which time the data is already corrupted), but I’m not sure.</p> ;;;; <p>Hi <a class="mention" href="/u/petebankhead">@petebankhead</a> ,</p>
<p>This just happened to me again, and I managed to log it.</p>
<p>What happened to me was that I ran a script for several project entries that took an unusually long time (it was only <code>println getProjectEntry().getImageName()</code>).</p>
<p>So I did what I normally do to stop the running script by closing QuPath (see <a href="https://github.com/qupath/qupath/issues/1167" class="inline-onebox" rel="noopener nofollow ugc">Cancelling a script run takes too long to regain control of QuPath · Issue #1167 · qupath/qupath · GitHub</a> and <a href="https://forum.image.sc/t/feature-suggestion-option-to-revert-to-last-state-on-cancelling-script-run/74486" class="inline-onebox">Feature suggestion: option to revert to last state on cancelling script run</a>). This somehow now causes the data.qpdata file to become corrupted and losing hierarchy data, probably due to QuPath unable to read the .qpdata properly.</p>
<p>I managed to restore the data from the automatic backup as mentioned in the posts above. Screenshot of the automatic backup:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754.jpeg" data-download-href="/uploads/short-url/7sJP8za5Q1axJRQaARH1YO18vjK.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754_2_517x154.jpeg" alt="image" data-base62-sha1="7sJP8za5Q1axJRQaARH1YO18vjK" width="517" height="154" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754_2_517x154.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754_2_775x231.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754.jpeg 2x" data-dominant-color="282828"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1028×307 50.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The full log of the error from QuPath trying to read the corrupted .qpdata file:</p>
<pre><code class="lang-auto">ERROR: Reached end of file...
ERROR: null
java.io.EOFException: null
    at java.base/java.io.ObjectInputStream$BlockDataInputStream.peekByte(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream$FieldValues.&lt;init&gt;(Unknown Source)
    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at qupath.lib.objects.PathCellObject.readExternal(PathCellObject.java:94)
    at java.base/java.io.ObjectInputStream.readExternalData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at qupath.lib.objects.PathObject.readExternal(PathObject.java:1201)
    at java.base/java.io.ObjectInputStream.readExternalData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream$FieldValues.&lt;init&gt;(Unknown Source)
    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
    at qupath.lib.io.PathIO.readImageDataSerialized(PathIO.java:353)
    at qupath.lib.io.PathIO.readImageData(PathIO.java:488)
    at qupath.lib.projects.DefaultProject$DefaultProjectImageEntry.readImageData(DefaultProject.java:696)
    at qupath.lib.gui.QuPathGUI.openImageEntry(QuPathGUI.java:2991)
    at qupath.lib.gui.panes.ProjectBrowser.lambda$new$4(ProjectBrowser.java:201)
    at com.sun.javafx.event.CompositeEventHandler.dispatchBubblingEvent(CompositeEventHandler.java:86)
    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:234)
    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:191)
    at com.sun.javafx.event.CompositeEventDispatcher.dispatchBubblingEvent(CompositeEventDispatcher.java:59)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:58)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.EventUtil.fireEventImpl(EventUtil.java:74)
    at com.sun.javafx.event.EventUtil.fireEvent(EventUtil.java:54)
    at javafx.event.Event.fireEvent(Event.java:198)
    at javafx.scene.Scene$ClickGenerator.postProcess(Scene.java:3599)
    at javafx.scene.Scene$MouseHandler.process(Scene.java:3903)
    at javafx.scene.Scene.processMouseEvent(Scene.java:1887)
    at javafx.scene.Scene$ScenePeerListener.mouseEvent(Scene.java:2620)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:411)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:301)
    at java.base/java.security.AccessController.doPrivileged(Unknown Source)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler.lambda$handleMouseEvent$2(GlassViewEventHandler.java:450)
    at com.sun.javafx.tk.quantum.QuantumToolkit.runWithoutRenderLock(QuantumToolkit.java:424)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler.handleMouseEvent(GlassViewEventHandler.java:449)
    at com.sun.glass.ui.View.handleMouseEvent(View.java:551)
    at com.sun.glass.ui.View.notifyMouse(View.java:937)
    at com.sun.glass.ui.win.WinApplication._runLoop(Native Method)
    at com.sun.glass.ui.win.WinApplication.lambda$runLoop$3(WinApplication.java:184)
    at java.base/java.lang.Thread.run(Unknown Source)
ERROR: Load ImageData: Cannot invoke "qupath.lib.objects.hierarchy.PathObjectHierarchy.getRootObject()" because "hierarchy" is null
ERROR: Load ImageData
java.lang.NullPointerException: Cannot invoke "qupath.lib.objects.hierarchy.PathObjectHierarchy.getRootObject()" because "hierarchy" is null
    at qupath.lib.objects.hierarchy.PathObjectHierarchy.setHierarchy(PathObjectHierarchy.java:832)
    at qupath.lib.io.PathIO.readImageDataSerialized(PathIO.java:412)
    at qupath.lib.io.PathIO.readImageData(PathIO.java:488)
    at qupath.lib.projects.DefaultProject$DefaultProjectImageEntry.readImageData(DefaultProject.java:696)
    at qupath.lib.gui.QuPathGUI.openImageEntry(QuPathGUI.java:2991)
    at qupath.lib.gui.panes.ProjectBrowser.lambda$new$4(ProjectBrowser.java:201)
    at com.sun.javafx.event.CompositeEventHandler.dispatchBubblingEvent(CompositeEventHandler.java:86)
    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:234)
    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:191)
    at com.sun.javafx.event.CompositeEventDispatcher.dispatchBubblingEvent(CompositeEventDispatcher.java:59)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:58)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)
    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)
    at com.sun.javafx.event.EventUtil.fireEventImpl(EventUtil.java:74)
    at com.sun.javafx.event.EventUtil.fireEvent(EventUtil.java:54)
    at javafx.event.Event.fireEvent(Event.java:198)
    at javafx.scene.Scene$ClickGenerator.postProcess(Scene.java:3599)
    at javafx.scene.Scene$MouseHandler.process(Scene.java:3903)
    at javafx.scene.Scene.processMouseEvent(Scene.java:1887)
    at javafx.scene.Scene$ScenePeerListener.mouseEvent(Scene.java:2620)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:411)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:301)
    at java.base/java.security.AccessController.doPrivileged(Unknown Source)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler.lambda$handleMouseEvent$2(GlassViewEventHandler.java:450)
    at com.sun.javafx.tk.quantum.QuantumToolkit.runWithoutRenderLock(QuantumToolkit.java:424)
    at com.sun.javafx.tk.quantum.GlassViewEventHandler.handleMouseEvent(GlassViewEventHandler.java:449)
    at com.sun.glass.ui.View.handleMouseEvent(View.java:551)
    at com.sun.glass.ui.View.notifyMouse(View.java:937)
    at com.sun.glass.ui.win.WinApplication._runLoop(Native Method)
    at com.sun.glass.ui.win.WinApplication.lambda$runLoop$3(WinApplication.java:184)
    at java.base/java.lang.Thread.run(Unknown Source)
</code></pre> ;;;; <aside class="quote no-group" data-username="19cent" data-post="3" data-topic="78510">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/19cent/40/68978_2.png" class="avatar"> 19cent:</div>
<blockquote>
<p>the same error keeps popping up</p>
</blockquote>
</aside>
<p>Can you clarify what is the error message? Also, if you can provide even one of each image, then it would enable other people to replicate your analysis and more easily solve the problem.</p>
<p>Thanks!</p> ;;;; <p>Hi <a class="mention" href="/u/nicokiaru">@NicoKiaru</a> and <a class="mention" href="/u/dgault">@dgault</a>,</p>
<p>Apologies for the delay in responding and thanks for the questions. I had to spend some time trying to work out if there was anything special about the ROIs that wouldn’t open (e.g. were they always in the same location on the slide?) and concluded that, no, there was nothing special about to ROIs that failed.</p>
<p>But in trying to work out why I could replicate the issue on one machine but not another I discovered that the md5 checksums of different copies of the same file did not match.</p>
<p>So in conclusion, the (perhaps boring) solution is that the data were corrupted during transfer from one of our instrument servers to our cloud storage. I have managed to re-upload the data and now everything is working as intended.</p>
<p>Thanks for your engagement!</p>
<p>Jaime</p> ;;;; <p>Hi <a class="mention" href="/u/eric_denarier">@Eric_Denarier</a></p>
<p>The <em>Spot Detector</em> plugin only has a fixed export type (the one you described) and when used in batch you can only use that one. I guess when you speak about doing the export individually you mean using the ROI export tool which is not part of the <em>Spot Detector</em> plugin. If you want a custom export for batch mode then you need to use <em>Protocols</em> plugin which allow design graphical analysis workflow and doing batches.<br>
You can take example on this protocol for instance :<br>
<a href="https://icy.bioimageanalysis.org/protocol/batch-spots-detections-and-statistics/" class="onebox" target="_blank" rel="noopener">https://icy.bioimageanalysis.org/protocol/batch-spots-detections-and-statistics/</a></p>
<p>You can access online protocols directly from the Icy top search bar by taping its name (just try “batch spot” here for instance).</p>
<p>Hope that helps !</p>
<p>– Stephane</p> ;;;; <p>Hi <a class="mention" href="/u/kapoorlab">@kapoorlab</a></p>
<p>Thank you very much for your suggestion. I agree this is an important topic for the publication of image analysis in particular but overall also for Microscopy and Image Analysis facilities.</p>
<p>As for this particular guideline publication I agree with <a class="mention" href="/u/marie-nkaefer">@Marie-nkaefer</a>. We want to stay very close to the technical aspects of image figures and image analysis. Focusing on easy comprehension and reproducibility. As there is also currently a gap in unified guidelines in this particular area.</p>
<p>Authorship would not really fit well into this focus and is very well covered with journal guidelines and rules already.</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f.png" data-download-href="/uploads/short-url/4DgXc2GoljqBABpBGWtgWoia7G7.png?dl=1" title="qUPATH" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_690x15.png" alt="qUPATH" data-base62-sha1="4DgXc2GoljqBABpBGWtgWoia7G7" width="690" height="15" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_690x15.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_1035x22.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_1380x30.png 2x" data-dominant-color="C9C9C9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">qUPATH</span><span class="informations">1419×32 20.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Hello everyone, I need your help in the sense that I do not fully understand the different Qupath parameters, what do they mean exactly<br>
Max caliper/Min caliper<br>
OD std dev<br>
OD Range<br>
And especially in term of interpretation (that is a good circularity of the core =1, but for the other parameters how can I know the reference values)<br>
Thank you for your HEEELP ! <span class="hashtag">#URGENT</span></p> ;;;; <p>Hello<br>
I’m trying to write a plugin to get the values from freeline drawn over a skeletonised image. The skeleton is from neurons where the idea is follow the pixel line and look for branch points. I’ll be linking this to AnalyseSkeleton plugin which already does most of the heavy lifting but I need to be able to follow the “skeleton”. I cannot get the code to accept the polyline, I’m getting the error: Cannot convert class Lij.gui.Roi; to MaskInterval. My guess is that even though the line is 15 pixels thick its not seen as an Roi but I can’t get any polyline options to even pass the compile stage in Eclipse. This is my code so far:</p>
<p>public void MeasureMinora(Img skeletonisedImage) {</p>
<pre><code> //Select Line Tool and set width to 15 pixels
	IJ.setTool("freeline");
	IJ.run("Line Width...", "line=15");
   	
	RoiManager rm = new RoiManager();
	new WaitForUserDialog("Draw Line", "Draw a line on the central neuron").show();
	rm.addRoi(null);
	Roi[] theLine = rm.getRoisAsArray();
	MaskInterval maskInterval = roiService.toMaskInterval(theLine);
	
	//Apply the MaskInterval to the skeleton image to get an IterableInterval just in the ROI
   IterableInterval linePos = Views.interval(skeletonisedImage, maskInterval);
	
    Cursor cur = linePos.cursor();
    while (cur.hasNext()) {
    	Object val = cur.get();
    	cur.fwd();
    }	
	 
}
</code></pre>
<p>Any help appreciated</p>
<p>Thanks</p>
<p>David</p> ;;;; <p>Thanks for the swift reply and the suggestion! Unfortunately, the same error keeps popping up</p>
<pre><code class="lang-auto">// Close all open images and clear previous results
run("Close All");
run("Clear Results");

//kernen input

//aSMA input
dir = getDirectory("Choose Directory 4."); 

list=getFileList(dir);
for (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder
if(endsWith(list[i], "4.tif")){   // if the filename ends with 4.tif file, we enter the following part:
		input = dir + list[i];
		input_ROI = replace(input,"4.tif","4.zip");


	print("now processing: "+input);
	print("together with: "+input_ROI);

	open(input);
	
	//get image title for later use
	aSMA = getTitle();
	
	roiManager("reset");
	run("Select None");

	roiManager("open", input_ROI);
	roiManager("Show All");
roiManager("Select", 2);
setBackgroundColor(0, 0, 0);
run("Clear Outside");
run("8-bit");
//get image title for later use
aSMA = getTitle();

//Ki67 input
dir = getDirectory("Choose Directory 3."); 

list=getFileList(dir);
for (j=0; j&lt;list.length; j++){ // for loop to parse through names in main folder
if(endsWith(list[j], "3.tif")){   // if the filename ends with 4.tif file, we enter the following part:
		input = dir + list[j];
		input_ROI = replace(input,"3.tif","3.zip");


	print("now processing: "+input);
	print("together with: "+input_ROI);

	open(input);
	
	//get image title for later use
	Ki67 = getTitle();
	
	roiManager("reset");
	run("Select None");

	roiManager("open", input_ROI);
	roiManager("Show All");
roiManager("Select", 2);
setBackgroundColor(0, 0, 0);
run("Clear Outside");
run("8-bit");

//get image title for later use
Ki67 = getTitle();

Stack.setXUnit("pixel");
	run("Properties...", "channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000");
	
	
	
	//select the window with ki67 staining
	selectWindow(Ki67);
	//correct for background in ki67 staining
	run("Subtract Background...", "rolling=50");
	//threshold based on the ki67 staining and convert to a binary image
	setAutoThreshold("Default dark");
	setOption("BlackBackground", true);
	run("Convert to Mask");
	//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
	run("Close-");
	run("Open");
	//watershed to split up nuclei that are lying against each other
	run("Watershed");
	run("Select All");
	roiManager("Delete");
	run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
	
	//select the window with aSMA staining
	selectWindow(aSMA);
	
	//threshold based on the aSMA staining and convert to a binary image
	setAutoThreshold("Default dark");
	setOption("BlackBackground", true);
	run("Convert to Mask");
	
	//select all ROIs from ki67 and transfer to aSMA
	run("Select All");
	roiManager("XOR");
	count = roiManager("count");
	roiManager("select", count-1);
	roiManager("Add");
				
	//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
	run("Close-");
	run("Open");
	//select all ROIs and dilate in order to catch all potential aSMA  signal
	run("Select All");
	run("Dilate");
	run("Dilate");
	//watershed to split up dilated ROIs that are lying against each other
	run("Watershed");
//calculate overlap
imageCalculator("AND", Ki67, aSMA);
run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
waitForUser("Press OK to continue","press ok to continue!");
print("aSMA has run");
selectWindow(aSMA);
close();
selectWindow(Ki67);
close();
List.clear()
}
}


</code></pre> ;;;; <p>Sorry, yes - It all depends on where you save the python script.<br>
If you save it into <code>\Testdata_Zeiss\OME_ZARR_Testfiles\simple-cors.py</code> then run it from there: <code>python simple-cors.py</code>, the URL will be <code>http://localhost8000/w96_A1+A2_test_zarr</code>.</p>
<p>I hope I’m not missing something that’s different on Windows here?</p> ;;;; <p>Hi <a class="mention" href="/u/jennifer_fessler">@Jennifer_Fessler</a>,</p>
<p>the <code>Table.create</code> should not be inside of the loop. Just call it once before the loop starts, otherwise it will clear the table in each iteration.</p>
<p>Best,<br>
Volker</p> ;;;; <p>Thanks for raising this - I replied on on the github issue…</p> ;;;; <p>Hi Konrad,</p>
<p>I was able to install matplotlib to 3.6.1 (please find output attached).</p>
<p><a class="attachment" href="/uploads/short-url/fsPuVAkWtKGPVjaWE0Cl0pvdb58.txt">Matpoltlib fixed about error remains.txt</a> (6.5 KB)</p>
<p>But the same error occurs when trying to flag a time window in the refine_tracklets GUI.</p>
<p>Please let me know if I’m performing the stitich_tracklets and refine_tracklets commands correctly?</p>
<p>Thanks <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20"></p>
<p>Grant</p> ;;;; <aside class="quote no-group quote-modified" data-username="Luigi_Marongiu" data-post="5" data-topic="78451">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/luigi_marongiu/40/68914_2.png" class="avatar"> Luigi Marongiu:</div>
<blockquote>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f6695be843c9939ae3ebc4a08d942dec6d195b0e.png" alt="Screenshot from 2023-03-13 15-14-39" data-base62-sha1="z9RbG6TmzKvWlCX8bYXqiINNNUO" width="333" height="348"></p>
</blockquote>
</aside>
<p>Hi <a class="mention" href="/u/luigi_marongiu">@Luigi_Marongiu</a>,<br>
Just to touch on one point, this “spider web” effect in watershed is quite a common issue when working with Binary masks and results from background not being set correctly. Can you check in <code>[Process &gt; Binary &gt; Options]</code> and make sure ther checkbox “Black Background” is checked.</p>
<p>This is equivalent to the line in your script: <code>setOption("BlackBackground", true);</code></p> ;;;; <p>Hi  <a class="mention" href="/u/kapoorlab">@kapoorlab</a></p>
<p>I don’t think this is an issue that lies within the scope of this publication. It is an important issue but I don’t think it should be part of these specific checklists.</p>
<p>Best wishes,<br>
Marie</p> ;;;; <p>Looks like your env doesn’t have java available? pip can’t install that.<br>
It looks like you might be using conda to manage the env? if so, try to install openjdk from conda.</p> ;;;; <p>Haha sorry</p>
<p>Told you I was a novice <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>I’ll try again</p> ;;;; <p>Hi everyone,<br>
May I ask for some help please? I plan to measure the thinnest part of dentin after performing mechanical instrument (MI) in CT image and would like to measure dentin thickness of the same area (before performing mechanical instrument).  I do not know which plugin should I use? Could you mind help suggest me please?</p>
<p>Adjabhak</p> ;;;; <p>HI both,</p>
<p>thank you very much for your comments and fast replies <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Indeed, that is what I did initially, but I actually want the number of boxes (<code>len(data)</code>) to be able to change also from the backend, e.g. one of the user sliders represents the model’s confidence in the predictions - so as you move it to the right the number of boxes should reduce, while moving it to the left the number of boxes should increase. This should be dealt differently than when a user manually adds or removes a box.</p>
<p>I was wondering if somehow combining the <code>data</code> event with some user event, e.g. <code>mouse_over_canvas</code> would work. Do you know if that would be possible? Thanks again!</p> ;;;; <p>Is DeepLabCut useful for analysing animals with no rigid skeletons/animals with fully muscular compositions? If yes, any particular steps to be mindful of/procedures to follow to analyse animals with don’t have bones/rigid skeletons?</p> ;;;; <p>I apologize for any inconvenience caused that I was unable to reproduce that error.<br>
However, I encountered a similar issue(I think) and here are the steps:</p>
<p>Load my project, Extract outline frames</p>
<ol>
<li>
<p>Click “Extract frames”<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469.png" data-download-href="/uploads/short-url/wPa1scoFXyV1DcvDfHl7A8iwGd.png?dl=1" title="click extract frames" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469_2_690x479.png" alt="click extract frames" data-base62-sha1="wPa1scoFXyV1DcvDfHl7A8iwGd" width="690" height="479" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469_2_690x479.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469_2_1035x718.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469.png 2x" data-dominant-color="2D303D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">click extract frames</span><span class="informations">1278×889 118 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
</li>
<li>
<p>Click “Labeling GUI”<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002.png" data-download-href="/uploads/short-url/pACMfYzgciyhKq7LXDgleOqgR9w.png?dl=1" title="click labeling GUI" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002_2_690x479.png" alt="click labeling GUI" data-base62-sha1="pACMfYzgciyhKq7LXDgleOqgR9w" width="690" height="479" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002_2_690x479.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002_2_1035x718.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002.png 2x" data-dominant-color="262129"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">click labeling GUI</span><span class="informations">1278×889 134 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
</li>
<li>
<p>Drop the folder located inside folder “labeled-data”  into the labele GUI and click “OK”<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c.png" data-download-href="/uploads/short-url/yWoIBLwhtDkyjuv06MzNYcBfo5e.png?dl=1" title="Ok" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c_2_690x351.png" alt="Ok" data-base62-sha1="yWoIBLwhtDkyjuv06MzNYcBfo5e" width="690" height="351" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c_2_690x351.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c_2_1035x526.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c.png 2x" data-dominant-color="2B2D30"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Ok</span><span class="informations">1247×635 71.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44.png" data-download-href="/uploads/short-url/ywO7x2RuxysqhHAJw1ojjEW1VDm.png?dl=1" title="after click OK" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_690x334.png" alt="after click OK" data-base62-sha1="ywO7x2RuxysqhHAJw1ojjEW1VDm" width="690" height="334" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_690x334.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_1035x501.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_1380x668.png 2x" data-dominant-color="3F1C34"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">after click OK</span><span class="informations">1408×682 157 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
</li>
</ol>
<hr>
<hr>
<p>Regarding the issue with, Refine tracklets, I wanted to share with you the steps I took and the results I obtained today. I have completed the correction of all 6300 frames and followed these steps:</p>
<ol>
<li>
<p>Launch track refinement GUI<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1.png" data-download-href="/uploads/short-url/fzeJUHG6jI0mayykk9dzRBoi0j7.png?dl=1" title="ref gui" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_690x423.png" alt="ref gui" data-base62-sha1="fzeJUHG6jI0mayykk9dzRBoi0j7" width="690" height="423" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_690x423.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_1035x634.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_1380x846.png 2x" data-dominant-color="3F424D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">ref gui</span><span class="informations">1587×974 131 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
</li>
<li>
<p>Fix some small jitters and click “save”</p>
</li>
<li>
<p>After clicking “Merge dataset”, there are sometimes issues with the deeplabcut GUI crashing, and other times an error message appears.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd.png" data-download-href="/uploads/short-url/3L4KKwGTevlXpinvlSiDcVXiF7D.png?dl=1" title="merge dataset(refine)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd_2_660x499.png" alt="merge dataset(refine)" data-base62-sha1="3L4KKwGTevlXpinvlSiDcVXiF7D" width="660" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd_2_660x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd_2_990x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd.png 2x" data-dominant-color="34303F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">merge dataset(refine)</span><span class="informations">1296×981 199 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
</li>
</ol>
<p>I hope my reply will be helpful to you.</p> ;;;; <p>Hi <a class="mention" href="/u/will-moore">@will-moore</a></p>
<p>Can you maybe give me a hint how to "construct the correct path?</p>
<p>So the local path is : c:\w96_A1+A2_test_zarr\</p>
<p>But I struggle with how to add this to:</p>
<p><a href="https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/???" rel="noopener nofollow ugc">https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/???</a></p>
<p>I truied to append the local path in various ways but nothing worked so far. The validator could not find my path so far.</p> ;;;; <p>Hi,</p>
<p>I have tried using pyclesperanto_prototype to perform segmentation and it works excellent on a demo image. However, I have problem when using my own images. I can show the images using imshow from matplotlib but when using the imshow from it pyclesperanto_prototype no image is shown.</p>
<p>My code:</p>
<pre><code class="lang-auto">import sys
import numpy as np
from matplotlib import pyplot as plt
import pyclesperanto_prototype as cle
from skimage import io

img1 = io.imread("Own image.jpg")
img2 = io.imread("IXMtest_A02_s9_w1.tif")

cle.imshow(img1)          #Not shown
plt.show()

plt.imshow(img1, cmap='gray')      #Shown
plt.show()

cle.imshow(img2)          #Shown
plt.show()

plt.imshow(img2, cmap='gray')         #Shown
plt.show()
</code></pre>
<p>Hope for help!<br>
BR<br>
Fredrik Olsson</p>
<p><a class="attachment" href="/uploads/short-url/9I5Dr2fSECOebnYg8cO05IgX7sT.tif">IXMtest_A02_s9_w1.tif</a> (709.3 KB)<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f.jpeg" data-download-href="/uploads/short-url/htpfHH2ziVLhTVRBoCYXhxCq87d.jpeg?dl=1" title="Own image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f_2_669x500.jpeg" alt="Own image" data-base62-sha1="htpfHH2ziVLhTVRBoCYXhxCq87d" width="669" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f_2_669x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f.jpeg 2x" data-dominant-color="885A56"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Own image</span><span class="informations">750×560 227 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hey <a class="mention" href="/u/pccross">@pccross</a> and welcome!</p>
<p>I’m curious whether you are interested in the coordinates of the yellow mask you showed, which I think are the <em>boundaries</em> between segments found by Felzenszwalb, or whether you want the coordinates of the pixels making up each segment?</p>
<p>I’m also curious about exactly what features you’re trying to find in the image, because it doesn’t look to me like Felzenszwalb actually does a great job of finding the “walls” and “gaps” in it. But I’m not sure if that’s exactly what you’re after.</p>
<p>In addition to answering those questions, would you be able to share (a) the source image itself, and (b) the code to get the figure you showed above? This can let us play around with it a bit more to see if we can come up with a solution to your problem.</p> ;;;; <p>You keep running terminal commands in the ipython console. Don’t go into ipython to install stuff</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/f/cfbff39ee1889cc23973dd01d2711de1fc7fe7e6.jpeg" data-download-href="/uploads/short-url/tDQ5DlCCKSAjv0CoTceioYU7p30.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/f/cfbff39ee1889cc23973dd01d2711de1fc7fe7e6.jpeg" alt="image" data-base62-sha1="tDQ5DlCCKSAjv0CoTceioYU7p30" width="690" height="325" data-dominant-color="989276"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">881×416 54.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
A bit new at using this great library, so be gentle.<br>
I’m using the felzenszwalb segmentation functions to create a mask for images, and it does great at that.   I was wondering if it’s possible to get the coordinate values for all (or a subset of) the lines making up the mask so I have a list of vertices I can feed to other applications?</p> ;;;; <p>Of course, here is a Drive link to a couple datasets in .tiff format<br>
<a href="https://drive.google.com/drive/folders/1BZ1vtbyh3sfQHPk3Po53-BXzouaihLZt?usp=sharing" class="onebox" target="_blank" rel="noopener nofollow ugc">https://drive.google.com/drive/folders/1BZ1vtbyh3sfQHPk3Po53-BXzouaihLZt?usp=sharing</a></p>
<p>The file <code>21316414z-8_scale-4.0_cdim-3_net-4_wmean--2_wstd-0.85_0.tif</code> is the one I’ve been showing here.</p>
<p>I would be happy to drop in on March 15: 8:30AM to talk about this <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi Konrad,</p>
<p>I performed the function:</p>
<p>pip install —upgrade matplotlib==3.6.1</p>
<p>and then</p>
<p>conda list matplotlib and I get the attached response:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73ceeb803d896860484cbd7be000c8b8edd92359.jpeg" data-download-href="/uploads/short-url/gwuerDPLVKNiR2ZbSR0FACfeOhP.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73ceeb803d896860484cbd7be000c8b8edd92359_2_375x500.jpeg" alt="image" data-base62-sha1="gwuerDPLVKNiR2ZbSR0FACfeOhP" width="375" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73ceeb803d896860484cbd7be000c8b8edd92359_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73ceeb803d896860484cbd7be000c8b8edd92359_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73ceeb803d896860484cbd7be000c8b8edd92359_2_750x1000.jpeg 2x" data-dominant-color="1D5E6E"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3024×4032 2.6 MB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>It looks like matplotlib version is still 3.7.0 ?</p>
<p>What do you think I should do?</p>
<p>Thanks</p> ;;;; <p>I am trying to write a macro that will build a table with the necessary image data and then record the measurements that I perform in the same table. This shouldn’t be a big deal. I am opening the image in a for loop and then using the table functions to record the information from the image and the measurements as I go. However, I have been working with a simplified version of the code because my created table is empty once I introduce the for loop that opens images. Here is the code for comparison.</p>
<p>//Working Code<br>
//image already open<br>
rowIndex = 0;<br>
imageTitle = getTitle();<br>
basename = substring(imageTitle, 0, indexOf(imageTitle, “.tif”));<br>
animal = “77R”;<br>
section = “2.3”;<br>
Dialog.create(“Fill in animal name and section number”); //title on window<br>
Dialog.addString(“Image Name”, basename); //correct image name<br>
Dialog.addString(“Animal Name”, animal);<br>
Dialog.addString(“Section Number”, section);<br>
Dialog.show();<br>
imageName = Dialog.getString();<br>
animal = Dialog.getString();<br>
section = Dialog.getString();<br>
lobule = getNumber(“Which Lobule are you measuring?”, 8);<br>
Table.create(“Cortical Width Measurements”);<br>
Table.setLocationAndSize(240, 125, 600, 700);<br>
Table.set(“Image”, rowIndex, imageName);<br>
Table.set(“Animal”, rowIndex, animal);<br>
Table.set(“Section”, rowIndex, section);<br>
Table.set(“Lobule”, rowIndex, lobule);<br>
selectWindow(imageTitle);<br>
close();</p>
<p>//Code with empty table<br>
imagePath = getDirectory(“Where are the images located?”);<br>
filelist = getFileList(imagePath)<br>
for (rowIndex = 0; rowIndex &lt; lengthOf(filelist); rowIndex++) {<br>
if (endsWith(filelist[rowIndex], “.tif”)) {<br>
//open(imagePath + File.separator + filelist[rowIndex]);<br>
waitForUser;<br>
imageTitle = getTitle();<br>
basename = substring(imageTitle, 0, indexOf(imageTitle, “.tif”));<br>
animal = “77R”;<br>
section = “2.3”;<br>
Dialog.create(“Fill in animal name and section number”); //title on window<br>
Dialog.addString(“Image Name”, basename); //correct image name<br>
Dialog.addString(“Animal Name”, animal);<br>
Dialog.addString(“Section Number”, section);<br>
Dialog.show();<br>
imageName = Dialog.getString();<br>
animal = Dialog.getString();<br>
section = Dialog.getString();<br>
lobule = getNumber(“Which Lobule are you measuring?”, 8);<br>
Table.create(“Cortical Width Measurements”);<br>
Table.setLocationAndSize(240, 125, 600, 700);<br>
Table.set(“Image”, rowIndex, imageName);<br>
Table.set(“Animal”, rowIndex, animal);<br>
Table.set(“Section”, rowIndex, section);<br>
Table.set(“Lobule”, rowIndex, lobule);<br>
selectWindow(imageTitle);<br>
close();<br>
}<br>
}</p>
<p>I thought I had read all the documentation to get this code to work but perhaps I missed something? Thank You!<br>
Jenny</p> ;;;; <p><a class="mention" href="/u/mathew">@Mathew</a><br>
Hi, Sorry to bother you again and again. Please help me with this project, since I am totally exhausted with it.<br>
So, first I used vedo module and python to draw two lines and calculate the distance in between, which didn’t work out. Then I used this binarized extracted image that you created and used this macro that you kindly mentioned in my post:</p><aside class="quote quote-modified" data-post="3" data-topic="78386">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/5daacb/40.png" class="avatar">
    <a href="https://forum.image.sc/t/measuring-the-distance-between-two-varying-contours/78386/3">Measuring the distance between two varying contours</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hi <a class="mention" href="/u/carsten2023">@Carsten2023</a> 
Please provide feedback. 
Thanks in advance. 
Here is the result of using the macro below. 
 <a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed.png" data-download-href="/uploads/short-url/yj2ShLVEtmbUrFX5M0CjL9vuSsJ.png?dl=1" title="Image capturée-10-03-2023 19-29-24" rel="noopener nofollow ugc">[Image capturée-10-03-2023 19-29-24]</a> 
macro "distance between contours"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
img=getImageID();
//---------------------------
// Start batch mode
setBatchMode(true);
selectImage(img);
run("Duplicate...", "title=1");
close("\\Others")
//-------------------------------
// Start image processing
h=getHeight();
w=ge…
  </blockquote>
</aside>
<p>
I got the results and then tried to calculate the data using SPSS:<br>
Min: 0 (how the min distance between two lines turned out to be zero?)<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/416826b34d9adca7b81327927c3d6deaa96fca1a.jpeg" alt="Retina" data-base62-sha1="9kCb3V3gcozIgqCs55F0IepMfPQ" width="504" height="494"><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/028dc4258969c1ef52443fb106b34e513daf9008.jpeg" data-download-href="/uploads/short-url/mAGDcOABDXOuwLI7MlY1e9fN6U.jpeg?dl=1" title="Retinal Layers" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/028dc4258969c1ef52443fb106b34e513daf9008_2_690x222.jpeg" alt="Retinal Layers" data-base62-sha1="mAGDcOABDXOuwLI7MlY1e9fN6U" width="690" height="222" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/028dc4258969c1ef52443fb106b34e513daf9008_2_690x222.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/028dc4258969c1ef52443fb106b34e513daf9008_2_1035x333.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/028dc4258969c1ef52443fb106b34e513daf9008_2_1380x444.jpeg 2x" data-dominant-color="191818"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Retinal Layers</span><span class="informations">1603×518 146 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Mean=265<br>
additionally, I am attaching an example of the segmented retinal image and unannotated format of my project sample in this post too.<br>
P.S.: As mentioned earlier I want to draw two lines manually and then measure the min, max and mean distance of them accurately as possible.</p>
<p>Again, thank you very much for your help and time.</p> ;;;; <p>I am experiencing the same issue using a macro to split .lif files into individual .tiff files and running MIPs on them. Were you able to figure out a solution?</p> ;;;; <p>Hey I’m using a macbook pro 2.3 GHz 8-Core Intel Core i9 Intel UHD Graphics 630 1536 MB ventura 13.2.1 .  Every time I try to install bioformats.  I get this error message</p>
<p>(tuto-env) (base) titan@Titans-MacBook-Pro Documents % pip install python-bioformats<br>
Collecting python-bioformats<br>
Using cached python_bioformats-4.0.7-py3-none-any.whl (40.6 MB)<br>
Collecting python-javabridge==4.0.3<br>
Using cached python-javabridge-4.0.3.tar.gz (1.3 MB)<br>
Preparing metadata (setup.py) … error<br>
error: subprocess-exited-with-error</p>
<p>× python setup.py egg_info did not run successfully.<br>
│ exit code: 1<br>
╰─&gt; [9 lines of output]<br>
Could not find Java JRE compatible with x86_64 architecture<br>
Traceback (most recent call last):<br>
File “”, line 2, in <br>
File “”, line 34, in <br>
File “/private/var/folders/4_/n6nzlmcd4b719txgbffmwsgr0000gn/T/pip-install-73y8gsxo/python-javabridge_35e6b47ca93d435cb6c56bb3134a6689/setup.py”, line 412, in <br>
ext_modules=ext_modules(),<br>
File “/private/var/folders/4_/n6nzlmcd4b719txgbffmwsgr0000gn/T/pip-install-73y8gsxo/python-javabridge_35e6b47ca93d435cb6c56bb3134a6689/setup.py”, line 98, in ext_modules<br>
raise Exception(“JVM not found”)<br>
Exception: JVM not found<br>
[end of output]</p>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.<br>
error: metadata-generation-failed</p>
<p>× Encountered error while generating package metadata.<br>
╰─&gt; See above for output.</p>
<p>note: This is an issue with the package mentioned above, not pip.<br>
hint: See above for details</p> ;;;; <p>Hello <a class="mention" href="/u/kota">@Kota</a> <a class="mention" href="/u/schmiedc">@schmiedc</a>,</p>
<p>I think we missed one important aspect in the paper which is defining authorship of analysts involved. If we have several analysts working on different aspects of a project with one making a novel plugin and other one writing a macro or a python script to do the analysis, how do we set the authorship versus acknowledgement boundary. This is very crucial in some situations where you build something novel but can not publish it as a standalone because of constraints of the tool being attached to the research project and then the research publication is the only way you can claim novelty of the tool whilst it not being main focus of the paper. In this situation we should also create some guideline to address such ambiguous/tricky cases, what do you think?</p>
<p>Cheers,<br>
Varun</p> ;;;; <p>I was able to add my custom atlas labels doing the following:</p>
<ol>
<li>extracted the label image at highest resolution from the ccf2017-mod65000-border-centered-mm-bc.h5 file</li>
<li>load that image into napari</li>
<li>create a new layer where I painted in my subregions</li>
<li>overwrite the existing label with my new ones</li>
<li>save as a tif (‘int16’)</li>
<li>used the following code to overwrite the .h5 file with my labels at various resolutions</li>
</ol>
<pre><code class="lang-auto">img_dir_atlas = r'D:\...'
img_path_atlas = os.path.join(img_dir_atlas, 'ccf2017-mod65000-border-centered-mm-bc.h5')
img_labels_path = os.path.join(img_dir_atlas, 'mylabels.tif')


mylabels = imread(img_labels_path)
mylabels2x = rescale(mylabels, 1/2, anti_aliasing=False, preserve_range=True).astype('int16')
mylabels4x = rescale(mylabels, 1/4, anti_aliasing=False, preserve_range=True).astype('int16')
mylabels8x = rescale(mylabels, 1/8, anti_aliasing=False, preserve_range=True).astype('int16')

# overwrite the data at a given resolution
with h5py.File(img_path_atlas,'r+') as atlas_h5:
    atlas_h5['t00000']['s03']['0']['cells'][...] = mylabels
    atlas_h5['t00000']['s03']['1']['cells'][...] = mylabels2x
    atlas_h5['t00000']['s03']['2']['cells'][...] = mylabels4x
    atlas_h5['t00000']['s03']['3']['cells'][...] = mylabels8x
</code></pre>
<ol start="7">
<li>updated the ontology in the ‘1.json’ file located in .\ABBA\abba_atlases by adding my subregions as children of the parent region. I gave them unique ids and graph orders and setting the atlas id to null.</li>
<li>replacing the ‘ccf2017-mod65000-border-centered-mm-bc.h5’ and ‘1.json’ files with my new versions</li>
</ol>
<p>Did not seem to break anything, as I was able to do the registrations and extract the cells in my new regions running the cells_in_regions.groovy script.</p> ;;;; <p><a class="mention" href="/u/moseyic">@moseyic</a> are you able to share your dataset? Even a small crop would be useful. It would be much easier for us to experiment if we had actual data to play around with.</p>
<p>I’m a little surprised that isosurface even works with RGB data but that might be something that we can improve with relatively little effort. We can also improve the performance — there’s already a PR in the works to dynamically change the step size while interacting with a volume so that it’s smooth:</p>
<aside class="onebox githubpullrequest" data-onebox-src="https://github.com/napari/napari/pull/4764">
  <header class="source">

      <a href="https://github.com/napari/napari/pull/4764" target="_blank" rel="noopener">github.com/napari/napari</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">



    <div class="github-icon-container" title="Pull Request">
      <svg width="60" height="60" class="github-icon" viewbox="0 0 12 16" aria-hidden="true"><path fill-rule="evenodd" d="M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"></path></svg>
    </div>

  <div class="github-info-container">



      <h4>
        <a href="https://github.com/napari/napari/pull/4764" target="_blank" rel="noopener">[WIP] adaptively set volume rendering ray step size</a>
      </h4>

    <div class="branches">
      <code>napari:main</code> ← <code>kevinyamauchi:monitor-fps</code>
    </div>

      <div class="github-info">
        <div class="date">
          opened <span class="discourse-local-date" data-format="ll" data-date="2022-06-29" data-time="14:54:17" data-timezone="UTC">02:54PM - 29 Jun 22 UTC</span>
        </div>

        <div class="user">
          <a href="https://github.com/kevinyamauchi" target="_blank" rel="noopener">
            <img alt="kevinyamauchi" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f99fd7b72021bce8ec2fe8bc1bff252b8ba4485.png" class="onebox-avatar-inline" width="20" height="20">
            kevinyamauchi
          </a>
        </div>

        <div class="lines" title="7 commits changed 5 files with 166 additions and 2 deletions">
          <a href="https://github.com/napari/napari/pull/4764/files" target="_blank" rel="noopener">
            <span class="added">+166</span>
            <span class="removed">-2</span>
          </a>
        </div>
      </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container"># Description
Inspired by a conversation that @jni had with @royerloic that was<span class="show-more-container"><a href="https://github.com/napari/napari/pull/4764" target="_blank" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden"> discussed during the Eurasia community meeting, This WIP PR introduces a `FrameRate` monitor that hooks into the canvas `monitor_fps()` callback. Using this, we can adaptively set rendering parameters when the framerate drops a specified value. Currently, this demo doubles the ray step size when the FPS &lt; 30 and halves the ray step size when FPS &gt; 60 (only for volume rendering).

Due to the way that framerate is calculated in the `canvas.monitor_fps()` function, the `FramerateMonitor` invalidates old measurements (marks them as stale) and implements some simple debouncing to prevent transient movements from causing false positives.

There is a demoscript below that displays the current measured frame rate and ray step size as a text overlay.

&lt;details&gt;&lt;summary&gt;example script&lt;/summary&gt;
&lt;p&gt;

```python
"""
Viewer FPS label
================

Display a 3D volume and the fps label.
"""
import time

import numpy as np
import napari



viewer = napari.Viewer(ndisplay=3)
image_layer = viewer.add_image(np.random.random((700, 500, 500)), colormap='red', opacity=0.8)
viewer.text_overlay.visible = True

visual = viewer.window._qt_window._qt_viewer.layer_to_visual[image_layer]
node = visual._layer_node.get_node(3)
print(node.relative_step_size)


def on_draw(event=None):
    """Display the current frame rate and step size as a text overlay"""

    fps_monitor = viewer.window._qt_window._qt_viewer._fps_monitor
    fps = viewer.window._qt_window._qt_viewer.canvas.fps
    fps_valid = fps_monitor.valid

    viewer.text_overlay.text = f"{fps:1.1f} FPS, valid: {fps_valid}, step: {node.relative_step_size}"


viewer.window.qt_viewer.canvas.events.draw.connect(on_draw)


if __name__ == '__main__':
    napari.run()

```

&lt;/p&gt;
&lt;/details&gt;

## Type of change
- [x] New feature (non-breaking change which adds functionality)


# References



# How has this been tested?

- [ ] example: the test suite for my feature covers cases x, y, and z
- [ ] example: all tests pass with my change
- [ ] example: I check if my changes works with both PySide and PyQt backends
      as there are small differences between the two Qt bindings.  

## Final checklist:
- [ ] My PR is the minimum possible work for the desired functionality
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] If I included new strings, I have used `trans.` to make them localizable.
      For more information see our [translations guide](https://napari.org/developers/translations.html).</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>But all of this is easier if we have a target dataset + visualisation in mind.</p>
<p>It would also be great if you could come to one or two of our <a href="https://napari.org/dev/community/meeting_schedule.html">napari community meetings</a> and demonstrate the issue.</p> ;;;; <p>Hello! I am using the Vedo library in Python to continuously rotate a .obj file. Although my code is partially working, the window freezes when I try to interact with it or minimize it. I would appreciate any guidance on what I might be doing wrong in my implementation.</p>
<pre><code class="lang-auto">from vedo import *
import time

# Load the object file
obj = load("file.obj")

while True:
    # Rotate the mesh around the y-axis by 10 degrees
    obj.rotate_y(10)

    # Wait for 0.1 seconds before rotating again
    time.sleep(0.1)

    obj.show(interactive=False)
</code></pre> ;;;; <aside class="quote no-group" data-username="psobolewskiPhD" data-post="2" data-topic="78529">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png" class="avatar"> Peter Sobolewski:</div>
<blockquote>
<p>But, you can try using a different event <code>set_data</code> instead of <code>data</code> which is not well documented</p>
</blockquote>
</aside>
<p>Agreed that <code>set_data</code> is not well documented, but I think it’s probably not what you’re looking for here either. Based on where it is emitted (i.e. in <code>Layer.refresh</code>), I’ve always interpreted it as indicating that new data is ready to be rendered either because the data itself changed or the view/slice of the data changed (e.g. the dimension slider positions moved).</p>
<p>As suggested, I think using the <code>data</code> event in combination with the <code>len()</code> of <code>Layer.data</code> is probably the best solution/workaround right now.</p> ;;;; <p>Cool- I’ll give it a go shortly</p>
<p>Thanks, again</p>
<p><img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji only-emoji" alt=":+1:" loading="lazy" width="20" height="20"></p> ;;;; <p>Here’s an example of the same data using ISO mode<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80.jpeg" data-download-href="/uploads/short-url/jfhMLRQgJ8ionrM6Jbq5v4DY6By.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80_2_690x459.jpeg" alt="image" data-base62-sha1="jfhMLRQgJ8ionrM6Jbq5v4DY6By" width="690" height="459" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80_2_690x459.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80_2_1035x688.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80.jpeg 2x" data-dominant-color="353639"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1082×721 105 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Its better, but is there any way to actually use the RGB valued data? The data itself is 1024x1024x1024x3, but this just looks grayscale.</p>
<p>P.S. Not sure what’s going on, but when I rotate the data in the napari viewer, my display seizes up / freezes for a couple seconds before loading. Sometimes it even crashes my display. I don’t think its a resource thing since my temps, and mem/cpu usage dont spike at all.</p>
<p>Since I’m already here: I tried this FPS counter on the data generated by <code>p.random.default_rng().random((345, 900, 1600)</code>, and wow I get like 5-8fps.<br>
i7-13700k, nvidia 3070ti, 64GB ram.</p>
<aside class="quote quote-modified" data-post="6" data-topic="68103">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jni/40/5991_2.png" class="avatar">
    <a href="https://forum.image.sc/t/poor-3d-viewer-performance-in-napari/68103/6">Poor 3D viewer performance in Napari</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Yeah, so then I think it’s a limitation with the Volume shader in VisPy, which doesn’t really have many tricks for accelerating performance in large volumes. The anisotropic behaviour makes less sense in GL though because in all cases you’re traversing every pixel — just in a different order and with different “ray” size. Perhaps sampling a short ray is nonlinearly faster than sampling a long ray? Or maybe long rays are also cast along the “corners”, so that you end up sampling more pixels? 
Any…
  </blockquote>
</aside>
 ;;;; <p>I have also seen the same issue with unknown errors resulting in an unrelated stacktrace and the incorrect <code>There is no registered plugin named 'napari-ome-zarr'</code> message.</p>
<p>It has also been reported at <a href="https://github.com/ome/ome-zarr-py/issues/258" class="inline-onebox">use declarative representation of the spec · Issue #258 · ome/ome-zarr-py · GitHub</a><br>
I can’t tell the cause of the issue in your case.</p>
<p>Can you try to open in <code>ome-ngff-validator</code>?<br>
You need to serve the data with CORs headers - I use this python script:</p>
<pre><code class="lang-auto"># From https://stackoverflow.com/questions/21956683/enable-access-control-on-simple-http-server

from http.server import HTTPServer, SimpleHTTPRequestHandler, test
import sys

class CORSRequestHandler (SimpleHTTPRequestHandler):
    def end_headers (self):
        self.send_header('Access-Control-Allow-Origin', '*')
        SimpleHTTPRequestHandler.end_headers(self)

if __name__ == '__main__':
    test(CORSRequestHandler, HTTPServer, port=int(sys.argv[1]) if len(sys.argv) &gt; 1 else 8000)
</code></pre>
<p>then go to <a href="https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/path/to/OME_ZARR_Testfiles%5Cw96_A1+A2_test_zarr">https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/path/to/OME_ZARR_Testfiles\w96_A1+A2_test_zarr</a></p> ;;;; <p>As long as you have the 3.6.1 it’s fine. <code>pip install --upgrade matplotlib==3.6.1</code> or <code>3.5.1</code> should work. I meant this part:</p>
<pre><code class="lang-auto">(deeplabcut) C:\WINDOWS\system32&gt;ipython
Python 3.8.16 | packaged by conda-forge | (default, Feb 1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]
Type ‘copyright’, ‘credits’ or ‘license’ for more information
IPython 8.10.0 – An enhanced Interactive Python. Type ‘?’ for help.

In [1]: pip install matplotlib==3.5.1
</code></pre>
<p>You can check which version is in the env with <code>conda list matplotlib</code></p> ;;;; <p>I don’t think we have an event specifically for that—there’s a long open issue:</p><aside class="onebox githubissue" data-onebox-src="https://github.com/napari/napari/issues/720">
  <header class="source">

      <a href="https://github.com/napari/napari/issues/720" target="_blank" rel="noopener nofollow ugc">github.com/napari/napari</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/napari/napari/issues/720" target="_blank" rel="noopener nofollow ugc">Receive events on shape create and delete</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2019-11-22" data-time="03:38:36" data-timezone="UTC">03:38AM - 22 Nov 19 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/cudmore" target="_blank" rel="noopener nofollow ugc">
          <img alt="cudmore" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/486cc9305fb394d18456198c9f60433d4ce94eae.jpeg" class="onebox-avatar-inline" width="20" height="20">
          cudmore
        </a>
      </div>
    </div>

    <div class="labels">
        <span style="display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;">
          feature
        </span>
        <span style="display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;">
          good first issue
        </span>
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">## 🚀 Feature
Can we get events when shape layer shapes are created and deleted?<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">

## Motivation

I am implementing a shape analysis plugin. It would be great if my code could receive an event when a shape is added and a shape is deleted.

I want to display the current shapes in list, something like a Qt TreeView. I need to append and delete from this list as shapes are added and deleted from the Napari interface. To do this, I need to receive events when a new shape is created and a shape is deleted.

## Pitch

Because there can be many shapes (e.g. ROIs), it is super useful to display them as a list. In a Qt TreeView for example. With this, the user can see the number of shapes, the properties of each (including my analysis results), can select them in the list and have them selected in the viewer, can delete them, etc. etc.

When there are many shapes, it is not useful to only display them on top of an image. This results in **user fatigue**. It is useful to have them in a list so the user can browse shapes from the list as compared to a visual search on top of the image. Thus, I need Napari to trigger events on shape addition and deletion.

When shapes/ROIs are only on top of the image, it becomes similar to [where is Waldo](https://www.google.com/search?q=where+is+waldo&amp;sxsrf=ACYBGNSnqjLq7LwPJcsMs8nydf4f6HKJ2Q:1574393515050&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwi9xs3x8PzlAhVIGDQIHYWWCBoQ_AUoAXoECA4QAw&amp;biw=1258&amp;bih=1231#imgrc=0W6qdOm2E9qAdM:)!

## Alternatives



I can find shape and shape layer events triggered when properties of a shape change, things like (edge_color, face_color, edge_width). But no code that generates an event when a shape is created or deleted?

 - New shape

For new shapes I am catching mouse down events using the `mouse_drag_callbacks` decorator. But this requires me to keep a backend list of current shapes and compare to the list when the mouse is clicked.

```
@self.shapeLayer.mouse_drag_callbacks.append
def shape_mouse_move_callback(layer, event):
    if event.type == 'mouse_press':
        #compare list of shapes in my backend shape layer
        #if there are more than in my backend then the last one is a new one
        pass
```

 - Delete shape

I am not sure how to do this? I found `napari.layers.shapes.shapeList.remove()` which does not trigger an event and seems to be called repeatedly as shape properties are updated? Properties like (edge_color, face_color, edge_width). Maybe it should be `_remove()`. I also found `remove_selected()` but it also does not trigger an event?

## Additional context


I am storing the analysis for each shape in the shape `metadata` data member. With this, my code does not need to know about added or deleted shapes. But I would like to keep a backend list of shapes to be displayed in a Tree View so the user can select shapes from the list as compared to selecting in the image.

![cudmore-napari-plugin-1](https://user-images.githubusercontent.com/1009168/69394311-825a4780-0c90-11ea-91f9-9b839b1c2f3c.gif)

On the left is the Napari viewer with two shapes, a line in blue and a polygon/rectangle in orange. On the right is my PyQtGraph interface for shape analysis. It has 4 plots., (i) a line intensity profile (white) along with my analysis including a gaussian fit (red) and my diameter estimate (blue). This is all updated in real time as the user drags a selected line shape and/or adjusts the viewed image plane/frame, (ii) my meta analysis of the diameter along the line, (iii) a plot of all the line profiles through the time series, and (iv) a plot of the mean intensity within the polygon/rectangle napari shape.

I would like to add a list of current shapes/ROIs to this interface. Something like a Qt TreeView. To do this, I need events that tell me when a shape is added and a shape is deleted...</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
But, you can try using a different event <code>set_data</code> instead of <code>data</code> which is not well documented but does fire when a shape is added or deleted (but also when shapes are modified, which might not be what you want…).<br>
Shapes <code>data</code> is a list of the shapes, so you could maybe check the len() of the layer data?</p>
<p>For debugging and exploring the UI you can try this nice plugin:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/dalthviz/napari-ui-tracer">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/dalthviz/napari-ui-tracer" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f.png 2x" data-dominant-color="E9EAEC"></div>

<h3><a href="https://github.com/dalthviz/napari-ui-tracer" target="_blank" rel="noopener nofollow ugc">GitHub - dalthviz/napari-ui-tracer: A plugin to help understand Napari UI...</a></h3>

  <p>A plugin to help understand Napari UI components and check their source code definition - GitHub - dalthviz/napari-ui-tracer: A plugin to help understand Napari UI components and check their source...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
See more here:</p><aside class="onebox githubissue" data-onebox-src="https://github.com/napari/docs/issues/93#issuecomment-1456590480">
  <header class="source">

      <a href="https://github.com/napari/docs/issues/93#issuecomment-1456590480" target="_blank" rel="noopener nofollow ugc">github.com/napari/docs</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/napari/docs/issues/93#issuecomment-1456590480" target="_blank" rel="noopener nofollow ugc">Add documentation mapping visual UI with code structure, modules and classes</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2023-01-18" data-time="18:13:36" data-timezone="UTC">06:13PM - 18 Jan 23 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/dalthviz" target="_blank" rel="noopener nofollow ugc">
          <img alt="dalthviz" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce9210e6fcf089db7e955f4e310d823aac2eb5a0.jpeg" class="onebox-avatar-inline" width="20" height="20">
          dalthviz
        </a>
      </div>
    </div>

    <div class="labels">
        <span style="display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;">
          content
        </span>
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">## 📚 New content request

After helping around fixing bugs, I was thinking tha<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">t maybe having some sort of mapping from the visual elements/UI to its related source code (directory structure, modules and classes) could be a good way to document the Napari GUI. This could help with the task of onboarding contributors and also document the general way things are currently working in terms of the GUI.

I think it could be nice to create documentation per interface section (layers controls, layers list, viewer, menus, dialogs, etc) and show for each of them the corresponding subset of the Napari source code directory layout: only the most related directories/files as well as some sort of dependency diagram(s) to show how the modules and classes depend between each other in that subset of the Napari source code.

Checking, seems like some exploration to do mappings of the source code have been done in a more or less automatic manner as well as an exploration of tooling to create diagrams (https://github.com/napari/napari/issues/1389). I would say that finding a way to at least semiautomatically create this documentation should be explored.

Also, talking about this with @melissawm a suggestion I got is asking core developers about the current code layout and the decisions behind the current code layout to document this, and seems like @psobolewskiPhD and @goanpeca did some notes about things that could be nice to document related to the Napari source code that could also apply to the idea here of creating some documentation for the visual UI/frontend (https://hackmd.io/2iq_CmjHR8mK0UL7e4bYaw#napari-codebase-related).

This effort could be related also with https://github.com/napari/docs/issues/49

### Outline

* UI sections:
  * Application menus
  * Application status bar
  * Layer controls
  * Layer list
  * Viewer
  * Console (probably plugins dockwidgets in general using the Console plugin as example)
  * Dialogs:
    * Preferences
    * Plugins
    * About
    * Warning/Error/Information notifications dialogs

The sections above come from what I have explored from the source code but probably there are more UI sections.

Creating an issue for this as discussed with @potating-potato</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hello everyone,</p>
<p>I’m trying to quantify RNA FISH in different cell types with 3D z-stack images that have 4 channels.<br>
C1 is RNA<br>
C2 is YFP+ cells<br>
C3 is RFP+ cells<br>
C4 is DNA</p>
<p>In order to do this, I coded a Macros pipeline in FIJI for 3D cell/spot segmentation. Now I want to use the <strong>3D MultiColoc</strong> of the ImageJ 3D Suite plugin to analyze colocalization between C2/3, C3/2, C2/1, and C3/1. However, I noticed that some of the labels counted 2+ cells as a single object, and I need to split the cells before I analyze the co-expression of YFP and RFP in different channels. Therefore, I used <strong>3D Manager</strong> from ImageJ 3D Suite to split objects and saved the new 3D ROIs as zip files ending with “filter_3droi”. However, I couldn’t find a way to convert these 3D ROIs into labeled images for the 3D MultiColoc analysis.</p>
<p><strong>Here are some sample images:</strong><br>
<a href="https://drive.google.com/drive/folders/1jwLyhxJHNeC8kzoFsRJx33zxryUft9Kd?usp=share_link" class="onebox" target="_blank" rel="noopener nofollow ugc">https://drive.google.com/drive/folders/1jwLyhxJHNeC8kzoFsRJx33zxryUft9Kd?usp=share_link</a></p>
<p>The most relevant posts I found were all using 2D ROIs:</p><aside class="quote quote-modified" data-post="2" data-topic="4256">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/imagejan/40/14151_2.png" class="avatar">
    <a href="https://forum.image.sc/t/creating-labeled-image-from-rois-in-the-roi-manager/4256/2">Creating Labeled Image from ROIs in the ROI Manager?</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hi Ofra, 
what you’re asking for is the general concept of a Labeling image. 
As far as I know, there’s no built-in way to do exactly what you want (converting ROI Manager entries to a labeling image), but there are several tools available from <a href="http://imagej.net/List_of_update_sites" rel="noopener nofollow ugc">update sites</a>: 


the LOCI update site provides a ROI Map command (probably the closest to what you want to do), that produces an 8-bit indexed color image for up to 255 ROIs, or a 16-bit image when the number of ROIs is higher, 


the 3D ImageJ Suite upda…
  </blockquote>
</aside>
<aside class="quote quote-modified" data-post="2" data-topic="55582">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/imagejan/40/14151_2.png" class="avatar">
    <a href="https://forum.image.sc/t/segmentation-masks-from-rois-plugins/55582/2">Segmentation masks from rois plugins</a> <a class="badge-wrapper  bullet" href="/c/announcements/10"><span class="badge-category-bg" style="background-color: #AB9364;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for announcements, such as new software releases and upcoming community events.">Announcements</span></a>
  </div>
  <blockquote>
    Thanks a lot for sharing, <a class="mention" href="/u/lthomas">@LThomas</a>! 
I agree there’s definitely a need for such tool, but I’m afraid that many people are re-creating almost identical functionality in various places: 



Maybe we could make a community effort to centralize this kind of stuff, and directly ship an implementation with Fiji? 
I opened an issue a while ago (albeit originally for the other direction, label image =&gt; ROI, but we should have both of course):
  </blockquote>
</aside>

<p>I also found plugins that convert ROIs into binary masks, but that ruins the whole point of splitting the objects in the first place. I would really appreciate it if someone could help me out with converting 3D ROI into labeled images.</p>
<p>Here are the websites for 3D Manager and 3D Multicoloc:<br>
<a href="https://mcib3d.frama.io/3d-suite-imagej/plugins/3DManager/3D-Manager/" class="onebox" target="_blank" rel="noopener nofollow ugc">https://mcib3d.frama.io/3d-suite-imagej/plugins/3DManager/3D-Manager/</a><br>
<a href="https://mcib3d.frama.io/3d-suite-imagej/plugins/Analysis/Relationships/3D-MultiColoc/" class="onebox" target="_blank" rel="noopener nofollow ugc">https://mcib3d.frama.io/3d-suite-imagej/plugins/Analysis/Relationships/3D-MultiColoc/</a></p>
<p>Thank you very much!</p> ;;;; <p>I’d love to combine two ImageJ/FIJI macros into one, but I am having trouble with the bio-formats portion.</p>
<p>The first macro converts images in a .lif to .tiff, the second takes .tiff images and allows for adjustment of brightness, then outputs the adjusted .tiffs. The first thing I tried was to just put the two scripts sequentially after the other, and line up the output file directory of the first to the input of the second, but that didn’t go well.</p>
<p>Here’s the convert lif to a tif script</p>
<pre><code class="lang-auto">run("Bio-Formats Macro Extensions");
setBatchMode(true); 
input = getDirectory("Input directory, folder where your .lif file is");
output = getDirectory("Output directory, where you'd like your raw .tiff files to go");


suffix = ".lif";

processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
}

function processFile(input, output, file) {
	Ext.setId(input + file);
	Ext.getSeriesCount(count);
	print("Processing: " + input + file+ " - "+d2s(count,0)+" images");

	for (f=0;f&lt;count;f++) {
		openLif(input+file,f);
		print("Saving to: " + output);
		listImages();
	}
}

function openLif(input,f){
	Ext.setSeries(f);
	run("Bio-Formats Importer", "open=[" +input +"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_" + d2s(f+1,0));
}

//Create string "image_1 image_2 image_3 image_4...."
function seriesN(num){
	str = "";
	for (i=0; i&lt;num; i++){
		ii = i+1;
        str = str+"image_"+ii+" ";
     }
     return str;
}

//Save
function listImages(){
	imageList = getList("image.titles");
	if (imageList.length==0)
	     print("No image windows are open");
	else {
    	print("Image windows:");
     	for (i=0; i&lt;imageList.length; i++){
        	print("   "+imageList[i]);
        	saveTiff(imageList[i]);
     	}
	}
	print("");
}


function saveTiff(winName){
	selectWindow(winName); 
	saveAs("Tiff", output + winName);
	close();
}
</code></pre>
<p>Here’s the brightness adjustment script, judging by the comments, this is likely a public script made by someone somewhere and handed down to me.</p>
<pre><code class="lang-auto">print("\\Clear");
GetTime();
setBatchMode(true); 
run("Input/Output...", "jpeg=100 gif=-1 file=.xls use_file copy_row save_column save_row");


input = getDirectory("Input directory, where your raw .tiff files are");
output = getDirectory("Output directory, where you'd like your adjusted .tiff files to go");

Dialog.create("File type");
Dialog.addString("File suffix: ", ".tif", 5);
Dialog.addNumber("Ch1:", 0);
Dialog.addNumber("Ch1:", 65535);
Dialog.addNumber("Ch2:", 0);
Dialog.addNumber("Ch2:", 65535);
Dialog.addNumber("Ch3:", 0);
Dialog.addNumber("Ch3:", 65535);
Dialog.addNumber("Ch4:", 0);
Dialog.addNumber("Ch4:", 65535);
Dialog.show();
suffix = Dialog.getString();
Range1 = Dialog.getNumber();
Range2 = Dialog.getNumber();
Range3 = Dialog.getNumber();
Range4 = Dialog.getNumber();
Range5 = Dialog.getNumber();
Range6 = Dialog.getNumber();
Range7 = Dialog.getNumber();
Range8 = Dialog.getNumber();

BC_range = newArray(Range1,Range2,Range3,Range4,Range5,Range6,Range7,Range8);

print("Brightness and contrast range is: ");
Array.print(BC_range);
print("Gray, Green, Red, Blue");



processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
	GetTime();
	selectWindow("Log");  
	saveAs("Text", output + "log.txt"); 
}

function processFile(input, output, file) {

	print("Processing: " + input + file);
	
	
	open(input+file);
	renderColor(file);
	brightnessNcontrast(file);
	deleteSlices(file);

	saveAs("Tiff", output + file + "_BCadjusted");
	rename(file);
	
	RGBmerge(file);
	scaleBar();
	makeMontage(file);

	//selectWindow("Montage1to5");
	//saveAs("Jpeg", output + file + "_montage1to5");
	selectWindow("MontageHorizontal");
	saveAs("Jpeg", output + file + "_BCmontage");

	print("Saving to: " + output);
	run("Close All");
}



// Give colors for each slice
function renderColor(file){
	color = newArray("Blue","Green","Red","Grays");
	//color = newArray("Blue","Green","Red");

	run("Make Composite", "display=Color");
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		run(color[i]);
	}
}
// Change brightness and contrast
function brightnessNcontrast(file){
	//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);
	for (i = 0; i &lt; nSlices; i++) {
		setSlice(i+1);
		setMinAndMax(BC_range[2*i],BC_range[2*i+1]);
	}
	
}

// Delete slices. You want to delete slice a, b
function deleteSlices(file){
	deleteA = 0;
	deleteB = 0;
	//deleteB &gt; deleteA. deleteB should be larger than deleteA

	print("deleting the slice "+deleteA+" and "+deleteB);
	selectWindow(file);
	if(deleteB&gt;0){
		setSlice(deleteB);
		run("Delete Slice", "delete=channel");
		}
	if(deleteA&gt;0){
		setSlice(deleteA);
		run("Delete Slice", "delete=channel");
	}	
}

// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.
function RGBmerge(file){
	run("Duplicate...", "title=4channels duplicate");
	run("RGB Color");
		
	selectWindow(file);
	run("Duplicate...", "title=Merge duplicate");
	Stack.setDisplayMode("composite");
	run("RGB Color");
	run("Copy");

	selectWindow("4channels (RGB)");
	setSlice(nSlices);
	run("Add Slice"); 
	run("Paste"); 

	close("4channels");
	close("Merge");
	close("Merge (RGB)");
	
	// "4channels (RGB)" is made.
}

// Add scale bar
function scaleBar(){
	setSlice(nSlices);
	run("Set Scale...", "distance=311.0016 known=100 pixel=1 unit=µm");
	//run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] hide");
	run("Scale Bar...", "width=100 height=4 font=14 color=White background=None location=[Lower Right] bold");
}

function makeMontage(input){
	//selectWindow("4channels (RGB)");
	//run("Make Montage...", "columns=1 rows=5 scale=0.25 border=2");
	//rename("Montage1to5");
	selectWindow("4channels (RGB)");
	run("Make Montage...", "columns="+nSlices+" rows=1 scale=0.5 border=2");
	rename("MontageHorizontal");
}
function GetTime(){
     MonthNames = newArray("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec");
     DayNames = newArray("Sun", "Mon","Tue","Wed","Thu","Fri","Sat");
     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);
     TimeString ="Date: "+DayNames[dayOfWeek]+" ";
     if (dayOfMonth&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+dayOfMonth+"-"+MonthNames[month]+"-"+year+"\nTime: ";
     if (hour&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+hour+":";
     if (minute&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+minute+":";
     if (second&lt;10) {TimeString = TimeString+"0";}
     TimeString = TimeString+second;
     print(TimeString);
}
</code></pre>
<p>Ideally, I would like to dialog the user as to whether or not they would also like to save the unadjusted .tiff files (and if so, where to do that, separately from the adjusted ones). But I am not as familiar with ImageJ macro language as I am with QuPath/groovy and don’t know how to go about doing that</p> ;;;; <p>Does calling pip install or conda install make a difference to downgrading matplotlib for our purposes?</p>
<p>I previously used pip install matplotlib ==3.5.1</p>
<p>Should it be conda install matplotlib ==3.5.1</p>
<p>Thanks</p> ;;;; <p>Sorry, Konrad</p>
<p>But I’m not sure what you mean by ‘use triple’.</p>
<p>Maybe it would be easier for me to understand if you note down the code how I should check the matpoltlib installation…?</p>
<p>Following this post, I believe I performed the matplotlib downgrade correctly</p>
<aside class="onebox stackexchange" data-onebox-src="https://stackoverflow.com/questions/62035127/not-able-to-downgrade-matplotlib-from-version-3-1-1-to-3-1-0">
  <header class="source">

      <a href="https://stackoverflow.com/questions/62035127/not-able-to-downgrade-matplotlib-from-version-3-1-1-to-3-1-0" target="_blank" rel="noopener nofollow ugc">stackoverflow.com</a>
  </header>

  <article class="onebox-body">
      <a href="https://stackoverflow.com/users/13497419/anmol-dwivedi" target="_blank" rel="noopener nofollow ugc">
    <img alt="anmol dwivedi" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/c/ac4e5939b9158db6dbd8beaeab952095d77d2987.jpeg" class="thumbnail onebox-avatar" width="256" height="256">
  </a>

<h4>
  <a href="https://stackoverflow.com/questions/62035127/not-able-to-downgrade-matplotlib-from-version-3-1-1-to-3-1-0" target="_blank" rel="noopener nofollow ugc">not able to downgrade matplotlib from version 3.1.1 to 3.1.0</a>
</h4>

<div class="tags">
  <strong>python, anaconda</strong>
</div>

<div class="date">
  asked by
  
  <a href="https://stackoverflow.com/users/13497419/anmol-dwivedi" target="_blank" rel="noopener nofollow ugc">
    anmol dwivedi
  </a>
  on <a href="https://stackoverflow.com/questions/62035127/not-able-to-downgrade-matplotlib-from-version-3-1-1-to-3-1-0" target="_blank" rel="noopener nofollow ugc">03:51AM - 27 May 20 UTC</a>
</div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>I’m sorry to be such a pain, but I’m very new to this</p>
<p>I really appreciate it</p> ;;;; <p>Beautiful, and yes I do see how looping through is a better idea!</p>
<p>One other side note, I see in the console as this is running, I get these warnings:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f.png" data-download-href="/uploads/short-url/tIfdK8JnPQaTVU61QSRLDwRuPkX.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f_2_690x229.png" alt="image" data-base62-sha1="tIfdK8JnPQaTVU61QSRLDwRuPkX" width="690" height="229" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f_2_690x229.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f.png 2x" data-dominant-color="F3DCDC"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">868×289 140 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The message is self-explanatory, but I was wondering if there’s a way to just prevent them from showing up as they’re not really relevant to anything I’m doing.</p> ;;;; <p>Any guidance for creating this file? The key elements seem to be the 4 3d images corresponding to channels downsampled 4 times:</p>
<pre><code class="lang-auto">        s00
        &lt;HDF5 dataset "cells": shape (1140, 800, 1320), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (570, 400, 660), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (285, 200, 330), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (142, 100, 165), type "&lt;i2"&gt;

        s01
        &lt;HDF5 dataset "cells": shape (1140, 800, 1320), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (570, 400, 660), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (285, 200, 330), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (142, 100, 165), type "&lt;i2"&gt;

        s02
        &lt;HDF5 dataset "cells": shape (1140, 800, 1320), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (570, 400, 660), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (285, 200, 330), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (142, 100, 165), type "&lt;i2"&gt;

        s03
        &lt;HDF5 dataset "cells": shape (1140, 800, 1320), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (570, 400, 660), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (285, 200, 330), type "&lt;i2"&gt;
        &lt;HDF5 dataset "cells": shape (142, 100, 165), type "&lt;i2"&gt;
</code></pre>
<p>Is there anything else I need to include in the .h5 file besides these images?<br>
What do I need to modify in the ontology? Do I just need to update the “1.json” file with the new subregions as children of the parent region, do I need to change anything in “mouse_brain_ccfv3.xml”?</p> ;;;; <p>Hi <a class="mention" href="/u/k-dominik">@k-dominik</a>, sorry for my late response.<br>
As you say, it was a matter of integers and channels I was considering. So all information clear and now it is working.</p>
<p>Sorry for my stupid doubt, and thanks for your help!<br>
Anna</p> ;;;; <p><a class="mention" href="/u/paschamber">@paschamber</a> sorry I missed your post. Here’s the other thread: <a href="https://forum.image.sc/t/customizing-atlas-labels-of-ccf2017-for-use-in-abba/78523" class="inline-onebox">Customizing atlas labels of ccf2017 for use in abba</a></p> ;;;; <p>No risk to break anything! Worst case scenario you have to re-install ABBA. Or just delete the <code>.\abba_atlases</code> folder and it will be re-downloaded from scratch.</p> ;;;; <p>Thanks, fixed</p> ;;;; <p>Hi Nicolas,</p>
<p>Thanks for your response. I wanted to clarify that I made my adjustments to the modulo 65000 atlas you referenced, by extracting the label image from the file “ccf2017-mod65000-border-centered-mm-bc.h5” and painting over those regions I wanted to add with unique ids. It might be easier if I can put the edited labels back into the .h5 file (by creating a new one) and replace the one located in the “.\abba_atlases” folder?</p>
<p>Do you think this will break anything?</p>
<p>Thanks,<br>
Pascal</p> ;;;; <p>Hello everyone,</p>
<p>I am new at using CellProfiler and I’m trying to quantify signal intensity for p16, a nuclear protein, in an artificial skin construct in 2D. I understand that this may be a simple request but I have tried following several approaches without good results. The following images are the DAPI channel (ch00), the p16 channel (ch01) and the image with all overlaid channels.</p>
<p><a class="attachment" href="/uploads/short-url/6vY482qrshx8ZujGF1mLaA9ILu1.tif">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1_ch00.tif</a> (1.3 MB)<br>
<a class="attachment" href="/uploads/short-url/hdLoMpGIe3zPJPRxduYSoXTiywi.tif">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1_ch01.tif</a> (1.0 MB)<br>
<a class="attachment" href="/uploads/short-url/f6zNMjhxiXL6WEMSh2wmEzSrKV6.tif">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1.tif</a> (2.6 MB)</p>
<p>My initial idea on how to go on about doing this was the following:</p>
<ol>
<li>Covert all channels to greyscale</li>
<li>Identify Primary Objects, in the DAPI channel, i.e. nuclei</li>
<li>Mask everything that is not identified as a nucleus in the DAPI channel image</li>
<li>Apply this mask to the p16 channel image</li>
<li>Define this as an object</li>
<li>Measure the signal intensity of this object in the p16 channel</li>
</ol>
<p>I could not get this to work so I did the following:</p>
<ol>
<li>Covert all channels to greyscale</li>
<li>Identify Primary Objects in the DAPI channel, i.e. nuclei</li>
<li>Identify Primary Objects in the p16 channel as “p16 signal”</li>
<li>Relate nuclei as the “parent” object to the p16 signal “children” object and define this as new object named “p16 positive nuclei”</li>
<li>Measure signal intensity of p16 positive nuclei</li>
</ol>
<p>Looking at the images this has not worked as I wanted to, as the object “p16 positive nuclei” does not seem to be defined by the area occupied by the object “nuclei”.</p>
<p>Essentially, what I want to do is define the area occupied by the nuclei in the DAPI channel and measure the intensity of the p16 signal from the p16 channel in that area, as p16 is a nuclear stain. Below you can find my pipeline.</p>
<p><a class="attachment" href="/uploads/short-url/mYeZDoco9za6pAjSiy0uOHdftwP.cpproj">UG3SEN_p16_53BP1.cpproj</a> (1.6 MB)</p>
<p>I apologize for the long post. I would be grateful for any help.</p>
<p>Thank you,<br>
Periklis</p> ;;;; <p>Hi <a class="mention" href="/u/pathai">@pathAI</a>,</p>
<p>if I got you right and you want to upsample a label map, one simple approach would be to use nearest neighbor interpolation:</p>
<pre><code class="lang-python">image_upsampled = skimage.transform.rescale(image, scale=4, order=0)
</code></pre>
<p>However, this creates relatively coarse boundaries. Alternatively, the next simplest way might be to use linear interpolation (or higher order, quadratic, cubic etc.) and reconstruct each label independently:</p>
<pre><code class="lang-python">image_upsampled =\
     np.max([(skimage.transform.rescale((image==l).astype(float),
                                        scale=4, order=1) &gt; 0.5).astype(np.uint64) * l
             for l in np.unique(image)], axis=0)
</code></pre>
<p>I might be misunderstanding your question though!</p> ;;;; <p>Hi <a class="mention" href="/u/kbellve">@kbellve</a><br>
Thanks für the update and the nice offer, I would definitively be still interested. I was also busy with other stuff so that I did not start yet. I did some (basic) arduino programming for other projects, so if I could be of any help, please let me know.</p> ;;;; <p>We are looking for a bioimage scientist.<br>
Remote meetings, participation in development, provision of data, product evaluation, introduction of customers, compliance with regulations.<br>
Please contact me directly if you are interested.</p> ;;;; <p>Hey Rafa,<br>
I think that when you pass a transform the scale information will be over-written. It would be good to handle this smarter in the MoBIE python library. I probably won’t have time to look into this until Friday.<br>
In the meantime, could you try scaling the transformation matrix accordingly and see if this fixes the issue?</p> ;;;; <aside class="quote no-group" data-username="Research_Associate" data-post="4" data-topic="78431">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<pre><code class="lang-auto">midpoint = getCurrentImageServer().getWidth()/2
for (annotation in getAnnotationObjects()){

  if (annotation.getROI().getCentroidX() &gt;midpoint){
    annotation.setName("Right side")
  } else {
    annotation.setName("Left side")
  }
}
</code></pre>
</blockquote>
</aside>
<p>Juste had to change ImageServer to Server and it works perfectly! Thank you!</p> ;;;; <p>Hi,</p>
<p>I’m working on version 2 of my plugin, here: <a href="https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/tree/dev-v.0.2" class="inline-onebox" rel="noopener nofollow ugc">GitHub - HelmholtzAI-Consultants-Munich/napari-organoid-counter at dev-v.0.2</a></p>
<p>I would like that the events.data event (here: <a href="https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/96843870fd8a7e942711737621c8d4f9be5cfac9/napari_organoid_counter/_widget.py#L138" class="inline-onebox" rel="noopener nofollow ugc">napari-organoid-counter/_widget.py at 96843870fd8a7e942711737621c8d4f9be5cfac9 · HelmholtzAI-Consultants-Munich/napari-organoid-counter · GitHub</a>) is triggered only when the user adds or removes a box. Currently it also triggered when the user plays around with the sliders to change the model parameters (which also changes the data of the layer).</p>
<p>Any help would much appreciated <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
Thanks,<br>
Christina</p> ;;;; <p>Hmm, this looks like a manifestation of this open issue:</p><aside class="onebox githubissue" data-onebox-src="https://github.com/ome/napari-ome-zarr/issues/71">
  <header class="source">

      <a href="https://github.com/ome/napari-ome-zarr/issues/71" target="_blank" rel="noopener nofollow ugc">github.com/ome/napari-ome-zarr</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/ome/napari-ome-zarr/issues/71" target="_blank" rel="noopener nofollow ugc">Support bioformats2raw.layout</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2022-10-18" data-time="09:52:33" data-timezone="UTC">09:52AM - 18 Oct 22 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/will-moore" target="_blank" rel="noopener nofollow ugc">
          <img alt="will-moore" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3b4abae5c68aa3acff11ae5b78492a1ed3968a9.jpeg" class="onebox-avatar-inline" width="20" height="20">
          will-moore
        </a>
      </div>
    </div>

    <div class="labels">
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">As noted in previous discussion at #21, the data generated with:

```$ bioform<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">ats2raw image directory.zarr```

cannot be opened with:

```$ napari directory.zarr```.

What should be the expected behaviour in this case?

 - Open ALL the images under `directory.zarr/` in multiple layers? The user could then turn off or delete the layers that they don't want to use. This should probably be the default behaviour in the case where there is only 1 image. But what if there are 3, or 100?
 - In the case of an OME-NGFF plate, we show a grid of thumbnails for the first image in each Well. We could try to do the same for non-plate bioformats2raw.layout, but this doesn't give users an easy way to open the Images themselves.
 - Any other way that a napari file-reader plugin can direct users to the images within the `directory.zarr`?</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Doesn’t look like there is any good solution yet, just some draft PR?</p>
<p>Certainly the error message could be improved though!</p> ;;;; <p>Thank you <a class="mention" href="/u/joshmoore">@joshmoore</a> for setting this up. It will be great to see community interchange about Quality Control, Reproducibility, and Data Management for Light Microscopy on Image.sc.</p> ;;;; <p>This bug is fixed in the ImageJ 1.54d6 daily build.</p> ;;;; <p>Exactly! this is what we were afraid of…Eosin and Saffron are quite similar only that Saffron has more “orange” and peachy tones than Eosin … guess we will have no choice but pixel classification and training for separating the 2 stainings… Our images are unfortunately more similar to your left pannel…</p> ;;;; <p>Interesting, I had not known that about saffron. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8009589/" class="inline-onebox">Modification of a Haematoxylin, Eosin, and Natural Saffron Staining Method for The Detection of Connective Tissue - PMC</a><br>
It looks like depending on the method you may get more or less separable stains. Stains on different color axes (red, green, blue) will be more accurately and usefully separable. Pink and red, for example, would not be since they are mostly <em>along the same color axis</em>, differing only in the intensity of the red.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f.jpeg" data-download-href="/uploads/short-url/s9Tj3kg7GIITELArsei3RFl2txZ.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_439x500.jpeg" alt="image" data-base62-sha1="s9Tj3kg7GIITELArsei3RFl2txZ" width="439" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_439x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_658x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_878x1000.jpeg 2x" data-dominant-color="D5A789"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1208×1375 187 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The images on the right would be much easier to separate.</p> ;;;; <p>Good to see! <img src="https://emoji.discourse-cdn.com/twitter/fireworks.png?v=12" title=":fireworks:" class="emoji" alt=":fireworks:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/fireworks.png?v=12" title=":fireworks:" class="emoji" alt=":fireworks:" loading="lazy" width="20" height="20"></p> ;;;; <p>Thanks for setting this up, now we have to collect some people from QUAREP-LiMi to give this activity</p> ;;;; <p>Replacing the atlas is going to be complicated unfortunately… Not impossible, but complicated.</p>
<p>The reason is because the Allen Brain Atlas uses gigantic numbers for their labels. What I mean by gigantic is 32 bits integers. They do not fit in 16-bits unsigned integers. (That does not make a lot of sense for me since the total number of labels do not exceed a few thousands).</p>
<p>BigDataViewer do not very easily support 32 bits integer labels, and there’s a passage using an ImageJ ImagePlus structure which do not support 32 bits integer at all anyway. So I had to cheat.</p>
<p>I noticed that I can remap the labels by doing a modulo 65000 on all labels, and I do not have any overlap, that’s why I packaged this new modulo atlas (<a href="https://zenodo.org/record/4173229#.ZA9Qeh_MI70" class="inline-onebox">Allen Mouse CCF v3 - Labels Modulo 65000 | Zenodo</a>).</p>
<p>Now if you had or resample labels, you will need to take care of this. I think it’s going to be a pain.</p>
<blockquote>
<p>Or is there a simpler way such as just exporting the transformations applied by abba and applying to my label image? If I can extract the transforms applied in abba to my own custom atlas, I can apply it to my section images to extract the regions in python.</p>
</blockquote>
<p>Yes, so I think this makes sense. Basically, using ABBA you can map slices to Allen CCFv3, and extract the coordinates in space. You can then use this coordinates in your atlas to know the corresponding labels.</p>
<p>I’ve updated <a href="https://github.com/NicoKiaru/ABBA-Python">ABBA-Python</a> (use the dev branch) and there are <a href="https://github.com/NicoKiaru/ABBA-Python/tree/dev/notebooks">a few notebooks</a> demoing how to access the coordinates of any pixel in the atlas after the registration. Maybe that can be helpful ?</p>
<p>Have a quick look and let me know what you think.</p> ;;;; <p>ok great seems to be what I need !! Thanks</p> ;;;; <p>If setting image type to Brightfield(other) and setting the 3 stain vectors still doesn’t separate the stain well, perhaps the next step should be to try training a pixel classifier.</p> ;;;; <p>Could increment using a value</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://www.imagescientist.com/scripting-export-images#regions">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c6b0cf7d4866291864036815c968bbc0536bd810.png" class="site-icon" width="100" height="100">

      <a href="https://www.imagescientist.com/scripting-export-images#regions" target="_blank" rel="noopener">Image Scientist</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://www.imagescientist.com/scripting-export-images#regions" target="_blank" rel="noopener">Exporting images from QuPath — Image Scientist</a></h3>

  <p>There are many ways to export many different types of image information from QuPath. Here you can find ways to export masks, areas with indexed masks (individual cells), measurement maps, pixel classifier predictions, and more. Also a section on...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
Or replace the increment using the annotation name itself. In your code you aren’t using any annotations, so it would need something like<br>
<code>getSelectedObject().getName()</code><br>
assuming you only had one object selected. In general, selecting objects to do things is slow and not really a great way to do things unless you are using plugin commands like cell detection.</p> ;;;; <p>Hi I had a somewhat similar question to the one raised here: <a href="https://forum.image.sc/t/custom-atlas-in-abba/77206" class="inline-onebox">Custom Atlas in ABBA</a>, but instead of a different atlas I just want to update the annotations. Specifically I have edited the labels of the ccf2017 atlas to divide a region into subregions, giving them unique id values. Ultimately, I want to use these custom labels to extract regions cell counts in qupath. In order to use these labels in abba what alterations need to be made?</p>
<p>I assume I need to update the ontology to include my new regions as children of the parent region. When setting the id value (and other parameters) is enough that it is unique or do they need to be inserted in order such that all subsequent ids are incremented? It would be easier if I can just make the ids (40,000, 40,001 …) something far outside the range.<br>
e.g. in the file “1.json” in ./abba_atlases</p>
<pre><code class="lang-auto">{
                     "id": 1085,
                     "atlas_id": 1125,
                     "ontology_id": 1,
                     "acronym": "MOs6b",
                     "name": "Secondary motor area, layer 6b",
                     "color_hex_triplet": "1F9D5A",
                     "graph_order": 29,
                     "st_level": 11,
                     "hemisphere_id": 3,
                     "parent_structure_id": 993,
                     "children": [
                        # proposed subdivisions
                        {"id": 1086, "atlas_id":1126, ...}, 
                        {"id":1087, "atlas_id":1127, ...}, 
]
                    }
</code></pre>
<p>As for the atlas images can I just “swap out” the existing labels for my own, update the region outlines (maybe unnecessary if only utilized for visualization), and recompile the images into a new .h5 with multiple resolutions? Then replace the atlas and ontologies located in the abba_atlases folder?</p>
<p>Or is there a simpler way such as just exporting the transformations applied by abba and applying to my label image? If I can extract the transforms applied in abba to my own custom atlas, I can apply it to my section images to extract the regions in python.</p> ;;;; <p>Maintain hierarchy: Select on the annotations you want to run the cell detection inside of, if they are on a lower level. If you run a cell detection on, for example Whole spleen, all of your other annotations will be deleted.</p>
<pre><code class="lang-auto">selectObjectsByClassification("a", "b", "c")
//run cell detection
</code></pre>
<p>Destroy Hierarchy quick and dirty:</p>
<pre><code class="lang-auto">annotations = getAnnotationObjects()
selectAnnotations()
//run cell detection here, annotations will be deleted
clearAnnotations() //not necessary, but will end up with duplicated parent annotations otherwise
createObjects(annotations)
</code></pre>
<p>Destroy Hierarchy 2:<br>
It would be project dependent, but follow nearly the same steps as above, but use getLevel and some print statements to figure out which annotations to store.</p>
<p><code>annotationsToRestore = getAnnotationObjects().findAll{it.getLevel !=1}</code><br>
or similar.</p> ;;;; <p>Hi! I am trying to export regions based on annotations, I am currently using this code, which works well to export them one by one but I constantly have to change the name in the script.</p>
<p>def roi = getSelectedROI()<br>
def requestROI = RegionRequest.createInstance(server.getPath(), 1, roi)<br>
writeImageRegion(server, requestROI, ‘/path/to/export/region.tif’)</p>
<p>I have already named all of my annotations on Qupath; how could I get to export all annotations once on the Image using the names that I gave them?</p>
<p>Thank you!</p> ;;;; <p>I think the only thing to try is setting the stain vector for the saffron itself using Brightfield(other). You can sometimes get the stain vectors more easily using ImageJ plugins on extracted regions of the image.<br>
Some of the information here might help <a href="https://forum.image.sc/t/double-positive-cell-detection-in-triple-stained-tissues/40218/2" class="inline-onebox">Double-positive cell detection in triple-stained tissues - #2 by Research_Associate</a><br>
Note the use of color inspector 3D etc.</p> ;;;; <p>yes sure (if they are not overlapping otherwise even if the annoation are not in a hirerachy but overlap Qupath  delete them I think by creating the hirerchy before detect the cells ). if I want the statistics in the whole annotations do you think the best way is to reload the annoations after the detections (not scriptable ?)  ?</p> ;;;; <p>Hello</p>
<p>I am trying to use the “Well plates” pluging to open my data that are coming from a scanR instrument. It used to work perfectly but now I cannot get it to work. Each time I find my folder and click on the “play” button an error message appears “java.lang.NullPointerException”<br>
Do you have any idea where it could come from?</p>
<p>Thank you for your help</p>
<p>Best,</p>
<p>Alix</p> ;;;; <p>Thank you for the insight. We were indeed trying to set vectors, however, it seems that Saffron is mixed with Eosin and we wanted to know whether there would be a ‘trick’ or procedure to unmix the two. <img src="https://emoji.discourse-cdn.com/twitter/smiling_face_with_tear.png?v=12" title=":smiling_face_with_tear:" class="emoji" alt=":smiling_face_with_tear:" loading="lazy" width="20" height="20"></p> ;;;; <p>Possibly something like</p>
<pre><code class="lang-auto">
midpoint = getCurrentServer().getWidth()/2
for (annotation in getAnnotationObjects()){

  if (annotation.getROI().getCentroidX() &gt;midpoint){
    annotation.setName("Right side")
  } else {
    annotation.setName("Left side")
  }
}
</code></pre>
<p>Winging it, haven’t tested the code.</p> ;;;; <p>If the database set up is fine then it looks like CP may have some issue writing to disk. Unfortunately I don’t know much about Windows to help with troubleshooting this kind of issue. Maybe someone more knowledgeable will come along.</p> ;;;; <aside class="quote no-group" data-username="ChrisStarling" data-post="7" data-topic="78488">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png" class="avatar"> Chris Starling:</div>
<blockquote>
<p>(P.S. It was also an 8Gb machine that was giving me problems - maybe QuPath needs a minimum of 8Gb to itself to process very large images?)</p>
</blockquote>
</aside>
<p>Vaguely also recall having issues with qptiffs from Vectra not being pyramidal, which stressed the memory far more. Less for QuPath to use for other stuff.</p> ;;;; <p>If the annotations are not in a hierarchy but all at the same level, I believe the annotations will not be deleted.</p> ;;;; <aside class="quote no-group" data-username="petebankhead" data-post="9" data-topic="29558">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<p>logarithm in there somewhere</p>
</blockquote>
</aside>
<p>Possibly inverted too? Based on the color scale the background has the highest OD? Or maybe this is a different measurement system showing transparency.<br>
Edit (or I guess negative log if that is the case)</p> ;;;; <aside class="quote no-group" data-username="Alvin" data-post="1" data-topic="78484">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png" class="avatar"> Alvin Lam:</div>
<blockquote>
<p>I have visited the <a href="http://imagej.net">imagej.net</a> page and the link to the Image J 1.x seemed to be dead.</p>
</blockquote>
</aside>
<p>Which link do you mean? The IJ links should all point to <a href="https://imagej.net/software/imagej/" class="inline-onebox">ImageJ</a> and seem to be working.</p> ;;;; <p>Dea Qupath user, I would like to quantify my cells(detetcions) in my whole tissue by annotation. I have different annotations (merge by type) with the following hierarchy : spleen tissue<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc.jpeg" data-download-href="/uploads/short-url/k3Ib9bub6tiBeg27Tkn0mgUYQ3i.jpeg?dl=1" title="Clipboard-1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc_2_578x500.jpeg" alt="Clipboard-1" data-base62-sha1="k3Ib9bub6tiBeg27Tkn0mgUYQ3i" width="578" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc_2_578x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc.jpeg 2x" data-dominant-color="8B8D8F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Clipboard-1</span><span class="informations">842×728 143 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>If I launch the cell detection module on the whole tissue, my annotations disapear…How can I do to quantify the cells number in each annotation ?<br>
I can launch in the MZ/B cell zone and T cell zonea and red pulp that should be exclusice but infortunaltely due to little errors in the annotation they are overlap a little so it clear the older detections…</p>
<p>One way is to save the annotations in Geojson and to reload them after the cells detection ?<br>
Tks, Mathieu</p> ;;;; <p>Unclear if you are using the predefined H&amp;E stain vectors (which we would nor expect to accomplish what you want) or you set stain vectors yourself as described in the <a href="https://qupath.readthedocs.io/en/0.4/docs/tutorials/separating_stains.html#setting-stain-vectors" rel="noopener nofollow ugc">documentation</a>.</p>
<p>You can set vectors for up to 3 stains, and in theory you may be able to separate hematoxylin, eosin, and saffron. In practice, I’m not sure how well it will separate eosin and saffron, because in my experience with this stain they are different but not very far in color from each other (eosin: pink, saffron: pinkish orange). I have worked with this stain but never attempted to separate it by deconvolution.</p> ;;;; <p>Regarding the error from refine_tracklets GUI, I made a pull request to fix this, it’s pending now (the line can be removed from the code to make it work - it’s just making an icon in a popup window).</p>
<p>Can you explain step by step what you’re doing that causes the second error?</p> ;;;; <p>Tried it. Usage of resources was the same, but in this case ‘CellProfiler.exe stopped working’ after 10-15 minutes from the moment it started running modules</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5.jpeg" data-download-href="/uploads/short-url/5HG9zIs3zlq9G1doLBjgQ2hXdbv.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_690x199.jpeg" alt="image" data-base62-sha1="5HG9zIs3zlq9G1doLBjgQ2hXdbv" width="690" height="199" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_690x199.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_1035x298.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_1380x398.jpeg 2x" data-dominant-color="F8F8F9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1830×528 111 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>CP 4.2.1</p>
<p>Workflow screenshot in the previous post.</p> ;;;; <p>Which version of DLC are you on?</p> ;;;; <p>Hi <a class="mention" href="/u/petebankhead">@petebankhead</a> ,</p>
<p>Sorry if I was not clear, I don’t have much experience in coding. So what I am trying to do using scripts is naming multiple annotations in a full image. Ideally the left side of the image would have a different name than the annotations on the left side of the image. Is there a way to script this? Or to be more clear, is there a way for the script to automatically recognize where the half of the image is (using the X coordinate)?</p>
<p>Thank you</p> ;;;; <p>These are not errors, just logs. Is the training going?</p> ;;;; <p>Just copy paste the file to your conda environemnt folder in the <code>lib\site-packages\deeplabcut\modelzoo\</code></p> ;;;; <blockquote>
<p>whether the ExportToSpreadhseet isn’t the cause for slow processing</p>
</blockquote>
<p>If that’s the case, try exporting to database instead.</p> ;;;; <blockquote>
<p>What kind of info will be useful in this case?</p>
</blockquote>
<p>Operating system and CP version but also possibly some details of the workflow.</p> ;;;; <p>You should add the whole folder of videos. Then in the gui you can tick off the videos you don’t want to add to the project</p> ;;;; <p>The paths to a video and the <code>h5</code> file should be strings, not lists of strings.</p>
<p>From what you copied it looks like you’ve run the installation of matplotlib from withing the ipython console not in the conda env (might be weird formatting - it’s better if you use triple <code> </code> ` and input code inside, to keep the formatting of the output from the terminal. I just wanted to see if the installation went ok.</p> ;;;; <p>Hi <a class="mention" href="/u/19cent">@19cent</a>,</p>
<p>It’s really helpful if you can format your code, not only is it easier to read but also it prevents errors when others want to copy your code. There should be a “Preformatted text” button in the editor.</p>
<p>Anyway, regarding your issue, from a brief look I would guess it’s a result of using <code>i</code> as your iterating variable in both an outside and inside loop. eg.</p>
<pre><code class="lang-auto"> for (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder
</code></pre>
<p>For each loop when nested, you need to pick a new variable name (eg. <code>j</code>,<code>k</code>, or ideally give it a descriptive name like <code>ki67_loop</code>). Otherwise, the loops will interfere with each other.</p>
<p>Hope it’s as simple a fix as that!</p> ;;;; <p>Hi <a class="mention" href="/u/sebi06">@sebi06</a>, thanks for the report.</p>
<p>As you will have seen, I have transferred the issue to the bioformats2raw GitHub repository as this is where the error came from and suggested a couple of options to move forward with the conversion. To avoid tracking too many places, I suggest we use GitHub until the issue feels resolved from your side. If some aspects of the discussion are deemed to be relevant to the wider bioimaging community, we can definitely update this post later on.</p>
<p>Best,<br>
Sebastien</p> ;;;; <p>Dear Histo analysts out here,</p>
<p>We are quite new to the analysis of histology tissues using quPath.  We have stainings with Hematoxylin and Eosin. But in France (mostly) we have an additional marker : Saffron which stains mostly collagen (and other things). Our issue is that we want to separate the stainings, and in quPath our Saffron is not separated properly (a mix between Eosin and the residuals)  here are the images of the problem :<br>
Original staining that shows saffron as a peachy color :<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792.png" data-download-href="/uploads/short-url/94PU59b1zGEqmj12Omr7XSAFSSK.png?dl=1" title="Original" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792_2_690x320.png" alt="Original" data-base62-sha1="94PU59b1zGEqmj12Omr7XSAFSSK" width="690" height="320" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792_2_690x320.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792_2_1035x480.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792.png 2x" data-dominant-color="D494BB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Original</span><span class="informations">1355×630 206 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>When separating Eosin (for intracellular/cytoplasm staining) you can see long and thick fibers of collagen  due to Saffron mixing :<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f.png" data-download-href="/uploads/short-url/xNaB90QBzSz3Uqy4xjik8URSSPZ.png?dl=1" title="Eosin" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f_2_690x339.png" alt="Eosin" data-base62-sha1="xNaB90QBzSz3Uqy4xjik8URSSPZ" width="690" height="339" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f_2_690x339.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f_2_1035x508.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f.png 2x" data-dominant-color="DB92C7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Eosin</span><span class="informations">1353×665 192 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And lastly part of the Saffron is also detected in the residuals :<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728.png" data-download-href="/uploads/short-url/4qe9H7jrs0tHbP9J9idsCIJoE64.png?dl=1" title="Residual" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728_2_690x385.png" alt="Residual" data-base62-sha1="4qe9H7jrs0tHbP9J9idsCIJoE64" width="690" height="385" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728_2_690x385.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728_2_1035x577.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728.png 2x" data-dominant-color="FBFA94"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Residual</span><span class="informations">1360×760 174 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Is there a way to correctly separates saffron from the rest of the HE staining?<br>
Thank you in advance,</p>
<p>Zeinab and Guil</p> ;;;; <p>To restore the backup, right click on the broken image. <em>Open directory</em> → <em>Project entry…</em> This will take you to the correct directory for that project entry. Delete the <code>data</code> file and rename the the <code>data.bkup</code> file by deleting the extension.</p>
<p>(You may want to move the broken data file to another location rather than delete it entirely. <em>Or</em> renaming the directory would mean that QuPath would create a new directory when you open the image again. You could copy the backup into the new folder then rename it. That should hopefully fix things!)</p> ;;;; <p>Oh I understand now, thanks Leo for the suggestion! That can be a great backup plan to use if I find that CP is not capable of doing this, thank you!! I’d like to stick with the same software that i have used for about 100 other experiments, as any internal differences in segmenting could give different population statistics (if I can).</p>
<p>I appreciate the clarification, as that may help me move forward with this new program in the future!</p>
<p>Thanks,</p>
<p>Nick</p> ;;;; <p>I had this happen to me a few times - as far as I could tell QuPath was running out of memory while trying to save the image data. I noticed that the data file was smaller than the backup even though I’d done more to the image. Replacing the data file with the backup seemed to fix the hierarchy.</p>
<p>I’ve upgraded to a machine with x4 the memory so <img src="https://emoji.discourse-cdn.com/twitter/crossed_fingers.png?v=12" title=":crossed_fingers:" class="emoji" alt=":crossed_fingers:" loading="lazy" width="20" height="20"> fingers crossed this shouldn’t be happening again!</p>
<p>(P.S. It was also an 8Gb machine that was giving me problems - maybe QuPath needs a minimum of 8Gb to itself to process very large images?)</p> ;;;; <p>Hello,<br>
I am trying to measure the intensity of point selection in all slices of a stack:</p>
<ul>
<li>In “Set Measurements” I choose “Mean gray value”.</li>
<li>I place one or more point selections on the stack and add the selections to the ROI Manager (either automatically in the Point Tool options menu or manually by pressing “t” after each point selection).</li>
<li>Then I choose “Multi measure” with “Measure all slices” and “One row per slice” in the ROI Manager menu.<br>
The Results table shows the same intensity value for all slices which is not correct. It works nicely when I try the same thing with eg circle ROIs or probably everything that is larger than a single pixel.<br>
My stacks are 8-bit or 16-bit gray scale and I can reproduce this also with the “T1 Head” sample stack.<br>
Could someone help me in finding out if the problem is in the computer or before the computer?</li>
</ul>
<p>Best,<br>
Klaus</p> ;;;; <p>Hello,</p>
<p>I am want to run inference on WSI, but the images are gigantic, so I thought to use lower, upsampled level ( I will go with level 1 from openslide with is 4x downsample). Now I run the inference, but I want to upsample without interpolation or having float values.</p>
<p>I tried the following for skimage without success.</p>
<pre><code class="lang-auto">patch = skimage.transform.pyramid_expand(patch,4, preserve_range = True)
</code></pre>
<p>Any help is appreciated.</p>
<p>Thanks in advance</p> ;;;; <p>Within the first 30 minutes, the high point of usage was 99% for CPU and 17% memory (around 70 GB). My pipeline isn’t very complex (attached screenshot), but I wonder whether the ExportToSpreadhseet isn’t the cause for slow processing. It’s over 35000 image sets in the end…</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b.jpeg" data-download-href="/uploads/short-url/dvn0zXIQZZEXD6tHvJQ3hJQSLEL.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_690x340.jpeg" alt="image" data-base62-sha1="dvn0zXIQZZEXD6tHvJQ3hJQSLEL" width="690" height="340" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_690x340.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_1035x510.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_1380x680.jpeg 2x" data-dominant-color="EEEEED"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×948 153 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I don’t think there is a problem of segmentation: the colonies are correctly outlined by the threshold tool. If I apply segmentation I get an error instead.<br>
This is the original image:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000.jpeg" data-download-href="/uploads/short-url/lUM4sXeN8MjmqGpT15eDoW33KAU.jpeg?dl=1" title="CT_e-4" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg" alt="CT_e-4" data-base62-sha1="lUM4sXeN8MjmqGpT15eDoW33KAU" width="375" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_750x1000.jpeg 2x" data-dominant-color="5D4019"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">CT_e-4</span><span class="informations">1200×1600 346 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>and this is the watershed:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f6695be843c9939ae3ebc4a08d942dec6d195b0e.png" alt="Screenshot from 2023-03-13 15-14-39" data-base62-sha1="z9RbG6TmzKvWlCX8bYXqiINNNUO" width="333" height="348"></p>
<p>The issue is that a single colony is counted several times…</p> ;;;; <p>Dear all,</p>
<p>I am trying to do the following:</p>
<p>This code needs to process Ki67 and aSMA images in a specified directory. It takes each Ki67 and aSMA image and its corresponding ROI (region of interest) file, opens the image, performs several image processing steps such as background subtraction, thresholding, and particle analysis, and then searches for the next corresponding aSMA image to finally calculate the aSMA positive region within the Ki-67 particles.</p>
<p>The issue is that, from the second loop onwards, the macro searches for the previous “4.tif”  image, instead of continuing the loop, resulting in an error. Please find the code below, any help is appreciated:</p>
<pre><code class="lang-auto">run("Close All");
run("Clear Results");

//kernen input

//aSMA input
dir = getDirectory("Choose Directory 4."); 

list=getFileList(dir);
for (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder 
	if(endsWith(list[i], "4.tif")){   // if the filename ends with 4.tif file, we enter the following part:
		input = dir + list[i];
		input_ROI = replace(input,"4.tif","4.zip");

		print("now processing: "+input);
		print("together with: "+input_ROI);

		open(input);
		
		//get image title for later use
		aSMA = getTitle();
		
		roiManager("reset");
		run("Select None");

		roiManager("open", input_ROI);
		roiManager("Show All");
		
roiManager("Select", 2);
setBackgroundColor(0, 0, 0);
run("Clear Outside");
run("8-bit");
//get image title for later use
		aSMA = getTitle();

//Ki67 input
dir = getDirectory("Choose Directory 3."); 

list=getFileList(dir);
for (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder 
	if(endsWith(list[i], "3.tif")){   // if the filename ends with 4.tif file, we enter the following part:
		input = dir + list[i];
		input_ROI = replace(input,"3.tif","3.zip");

		print("now processing: "+input);
		print("together with: "+input_ROI);

		open(input);
		
		//get image title for later use
		Ki67 = getTitle();
		
		roiManager("reset");
		run("Select None");

		roiManager("open", input_ROI);
		roiManager("Show All");
	
roiManager("Select", 2);
setBackgroundColor(0, 0, 0);
run("Clear Outside");
run("8-bit");

//get image title for later use
		Ki67 = getTitle();
		
		Stack.setXUnit("pixel");
		run("Properties...", "channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000");
		
		
		
		//select the window with ki67 staining
		selectWindow(Ki67);
		//correct for background in ki67 staining
		run("Subtract Background...", "rolling=50");
		//threshold based on the ki67 staining and convert to a binary image
		setAutoThreshold("Default dark");
		setOption("BlackBackground", true);
		run("Convert to Mask");
		//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
		run("Close-");
		run("Open");
		//watershed to split up nuclei that are lying against each other
		run("Watershed");
		run("Select All");
		roiManager("Delete");
		run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
		
		//select the window with aSMA staining
		selectWindow(aSMA);
		
		//threshold based on the aSMA staining and convert to a binary image
		setAutoThreshold("Default dark");
		setOption("BlackBackground", true);
		run("Convert to Mask");
		
		//select all ROIs from ki67 and transfer to aSMA
		run("Select All");
		roiManager("XOR");
		count = roiManager("count");
		roiManager("select", count-1);
		roiManager("Add");
					
		//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother
		run("Close-");
		run("Open");
		//select all ROIs and dilate in order to catch all potential aSMA  signal
		run("Select All");
		run("Dilate");
		run("Dilate");
		//watershed to split up dilated ROIs that are lying against each other
		run("Watershed");


//calculate overlap
imageCalculator("AND", Ki67, aSMA);
run("Analyze Particles...", "size=20-5000 pixel show=Overlay summarize add");
waitForUser("Press OK to continue","press ok to continue!");
print("aSMA has run");
		selectWindow(aSMA);
		close();
		selectWindow(Ki67);
		close();
		List.clear()
	}
}
</code></pre> ;;;; <p>Thanks for your reply!</p>
<p>I’ve restarted the analysis to now monitor the memory usage. When I saw the analysis running slow today, there was still plenty memory left (out of 512 GB, less than 5% was being used). I’ll report back how does the resource utilisation look like when it’s actively processing the entire set.</p>
<p>What kind of info will be useful in this case?</p> ;;;; <p>Hi <a class="mention" href="/u/jaimemcc">@jaimemcc</a>, can you give us some examples of regions that are failing for a particular series?</p> ;;;; <p>It looks like you’re running out of memory. Check if this is the case by monitoring memory usage while running your analysis. If that’s not the case, you’ll have to provide more info.</p> ;;;; <p>Regarding your initial problems:<br>
1- I suspect your problem is that the colonies are not well separated so are not adequately segmented. You may want to try something a bit more elaborate than simple thresholding. If you showed the original image, people may come up with suggestions.<br>
2- The simplest way of comparing colonie sizes across plates is to image all plates under the same conditions, not just magnification but also illumination as differences could also affect segmentation.</p> ;;;; <p>Thank you for the replies.<br>
The Measurements settings were these (which are, I guess, suitable for my task):<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50b960c063953d66bce756a38ccfcceb5224d085.png" alt="Screenshot from 2023-03-13 14-01-41" data-base62-sha1="bw7mOI9EteryVxZDcg236xobyoB" width="349" height="480"></p>
<p>The procedure carried out was:</p>
<pre><code class="lang-auto">open("~/Documents/LAB/Growth/exp_7/CT_e-4.jpg");
//setTool("oval");
makeOval(632, 334, 618, 586);
makeOval(572, 334, 678, 586);
makeOval(572, 334, 598, 586);
makeOval(552, 288, 618, 632);
setBackgroundColor(0, 0, 0);
run("Clear Outside");
run("16-bit");
setAutoThreshold("Default dark no-reset");
//run("Threshold...");
setOption("BlackBackground", true);
run("Convert to Mask");
run("Close");
//run("Threshold...");
//setThreshold(229, 255);
run("Convert to Mask");
run("Close");
run("Analyze Particles...", "size=2-Infinity display clear summarize add");
</code></pre>
<p>Which gave:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/58d3d13d768866a746d0d609307e023cd99100c8.png" alt="Screenshot from 2023-03-13 14-08-26" data-base62-sha1="cFNPHLvSp0kkBvCZcIXMSORKC0g" width="333" height="348"></p>
<p>How can I improve the analysis?<br>
Thanks</p> ;;;; <p><a class="attachment" href="/uploads/short-url/jQePmHyYW6r8Evkll47Ttihr2ZQ.tif">DIESTRUS All runs.lif_Run 3 F55 Diestrus A 10_ch03.tif</a> (616.6 KB)<br>
<a class="attachment" href="/uploads/short-url/7DHjXTkuinN7E1ikMtkE3wzaqct.tif">DIESTRUS All runs.lif_Run 3 F55 Diestrus B 32_ch00.tif</a> (878.4 KB)</p>
<p>Background:<br>
Marked sections with four stains: Nuclei, Dnmt, Vasopressin and Cre (The last three are just three different proteins). Images attached show two of these stains on the same section as an example.</p>
<p>Analysis goal:<br>
I’m trying to understand whether these proteins coexpress with eachother. For example, the cells that are expressing Dnmt, do they also express Cre? Do they also express Vasopressin? The nuclei stain is just to mark cells.</p>
<p>Challenges:<br>
I dont know which cell profiler modules I should use for this type of analysis. I tried doing MaskImages and then MeasureObjectIntensity, but I don’t understand what the resulting intensity values mean. Should I use Relate objects instead? Is Cell profiler the right software for this type of analysis?</p> ;;;; <p>Hi, Adam:</p>
<p>Thanks a lot for all your help!!!</p>
<p>I really found where my issues are!! Cellfinder requried directories of tif images but not tiff images!!!<br>
I preivous transformed my data into tiff images through the plugin of Fiji. and now when I transformed them into tif images, all the issues are sloved!!</p>
<p>Thanks a lot！！</p> ;;;; <p>I have no insight into the way this plugin works and what the parameters mean. What I know is that the “update template coefficient” in the menu is the “a” value in the formula on the ImageJ wiki website. There are also some explanations on the Wikipedia page for the Lucas-Kanade algorithm (<a href="https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method" class="inline-onebox">Lucas–Kanade method - Wikipedia</a>), which helped me in the past when the algorithm failed because there were shifts that were too big compared to the pixel size - in that case downsampling prior to correction helped.</p> ;;;; <p>Hi,</p>
<p>I am wondering what is the reason behind very long analysis times in CP when analysing multiple folders in one run. A single plate takes 30-40 minutes, but when I put all 27 folders (=plates), the analysis is much longer than 40 min * 27. It goes on for. 3-4 days, instead &lt;1 day.</p>
<p>Anyone knows any explanation for this?</p>
<p>Best,<br>
Bartek</p> ;;;; <p>Dear Pete,</p>
<p>I’ll keep the log in case it happens again. I increased the memmory, so it is working well again. Unfortunately, I dont know how to recover the backup files that are generated by qupath. What I have done is to keep a duplicate of the folders I have done many annotations.</p>
<p>Thank you for your support.</p> ;;;; <p>Thank you! <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi <a class="mention" href="/u/rianne">@Rianne</a> I don’t have an answer to your question, but I edited your post to format the code so it is more readable for others (using <code>&lt;/&gt;</code> in the toolbar when editing).</p> ;;;; <p><a class="mention" href="/u/ep.zindy">@EP.Zindy</a> very quick response to say that I think you’re missing a logarithm in there somewhere… In any case, I linked to some implementations of colour deconvolution (including some on Python) at</p><aside class="quote quote-modified" data-post="1" data-topic="38725">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar">
    <a href="https://forum.image.sc/t/color-deconvolution-implementations-best-practice/38725">Color deconvolution implementations &amp; best practice</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    The posts about colour deconvolution today have reminded me of a question I had some time ago… 
Color deconvolution, as described by Ruifrok and Johnston, involves generating a 3x3 stain matrix using three stain vectors. 
I understand that if two stains are available, then the remaining elements can be created by generating a third (pseudo)stain that is orthogonal to the first two. 
As far as I can tell, this third stain is generated using the cross product in several places: 

QuPath (<a href="https://github.com/qupath/qupath/blob/a03756328188999c0b7f12c290cda0589c50bd4b/qupath-core/src/main/java/qupath/lib/color/StainVector.java#L316">code</a>)
sci…
  </blockquote>
</aside>
 ;;;; <p>Keep getting this error:</p>
<p>Error using flim_omero_logon_manager/Omero_logon (line 125)<br>
Java exception occurred:<br>
Ice.UnmarshalOutOfBoundsException</p>
<pre><code>reason = ""
                                                                  
at IceInternal.BasicStream.readString(BasicStream.java:2038)
                    
at omero.sys.EventContext.__readImpl(EventContext.java:166)
                     
at Ice.ObjectImpl.__read(ObjectImpl.java:368)
                                   
at IceInternal.BasicStream$EncapsDecoder.unmarshal(BasicStream.java:3132)
       
at IceInternal.BasicStream$EncapsDecoder10.readInstance(BasicStream.java:3513)
  
at IceInternal.BasicStream$EncapsDecoder10.readPendingObjects(BasicStream.java:3440)
at IceInternal.BasicStream.readPendingObjects(BasicStream.java:566)
             
at omero.api.IAdminPrxHelper.end_getEventContext(IAdminPrxHelper.java:4062)
     
at omero.api.IAdminPrxHelper.getEventContext(IAdminPrxHelper.java:3935)
         
at omero.api.IAdminPrxHelper.getEventContext(IAdminPrxHelper.java:3922)
</code></pre>
<p>Error in front_end_menu_controller/menu_login_callback (line 369)</p>
<p>Error in front_end_menu_controller&gt;@(varargin)obj.menu_login_callback(varargin{:})</p>
<p>Error in EC (line 7)</p>
<p>Error in front_end_menu_controller&gt;@(x,y)EC(fcn) (line 263)</p> ;;;; <p>Added light and dark themes:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18aa2e685b654b9ee9d8b804dcdfd78c930bfa9e.png" alt="image" data-base62-sha1="3wc4oVBcXDAd9VMhZ2AmPZ9DWSG" width="280" height="138"><br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/27bed5b533923e1aa9c68851dc69bd1d63a9ebf7.png" alt="Screenshot 2023-03-13 at 12.33.32" data-base62-sha1="5FBtWHr9rf6CiXgCJsxf0gxO4Vp" width="243" height="132"></p>
<p>and created sidebar topic:</p>
<aside class="quote quote-modified" data-post="1" data-topic="78499">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/joshmoore/40/1634_2.png" class="avatar">
    <a href="https://forum.image.sc/t/tag-sidebard-quarep/78499">Tag sidebard: quarep</a> <a class="badge-wrapper  bullet" href="/c/community-partners/62"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category contains topics describing the Community Partners being discussed on this forum. A  Community Partner  is an open-source software project or community organization that uses this forum as a primary recommended discussion channel.">Community Partners</span></a>
  </div>
  <blockquote>
    <a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503.jpeg" data-download-href="/uploads/short-url/cpy0Ps2y4BNWIgg7eM2PetbwFCr.jpeg?dl=1" title="image">[image]</a> 
The <a href="https://quarep.org/">QUAREP-LiMi</a> (“Quality Assessment and Reproducibility for Instruments &amp; Images in Light Microscopy”) is a group of enthusiastic light microscopists from Academia and Industry all interested in improving quality assessment (QA) and quality control (QC) in light microscopy.
  </blockquote>
</aside>
 ;;;; <p>Here’s where I got to in terms of <em>passing custom stain vectors to a Python application</em>.</p>
<p>The strict minimum Python code to actually make use of the JSON export would be something along these lines:</p>
<pre><code class="lang-python">import json

def get_stain(stains, val):
    str_stain = None
    if isinstance(val,int):
        str_stain = f'stain{val+1}'
    else:
        for i in range(3):
            str_stain = f'stain{i+1}'
            if stains[str_stain]['name'] == val:
                break
            else:
                str_stain = None

    if str_stain is not None:
        ret = [stains[str_stain]['r'], stains[str_stain]['g'], stains[str_stain]['b']]
    else:
        ret = None

    return ret

def get_max(stains):
    return [stains['maxRed'], stains['maxGreen'], stains['maxBlue']]
</code></pre>
<p>I used the stain vectors I generated for <a href="https://user-images.githubusercontent.com/11299568/83185126-8318de00-a0f8-11ea-91fe-efb1bc84a860.png" rel="noopener nofollow ugc">this image</a> (from the thread: <a href="https://forum.image.sc/t/pathology-image-color-separation-scaling-artifacts/38393" class="inline-onebox">Pathology image color separation scaling artifacts</a>) and generated a JSON file with the groovy script above. Then in Python:</p>
<pre><code class="lang-python">fn = r"83185126-8318de00-a0f8-11ea-91fe-efb1bc84a860_stains.json"
with open(fn) as f:
    stains = json.load(f)

# Using the stain index:
print(get_stain(stains, 0))

# Using the stain name
print(get_stain(stains, 'Hematoxylin'))

# The background value:
print(get_max(stains))
</code></pre>
<p>Now, take the following with a large pinch of salt, here’s how I think colour deconvolution would be done in Python…</p>
<pre><code class="lang-auto">import numpy as np
import pyvips
from numpy.linalg import inv

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable

# Get the stains matrix and invert it

def get_matrix(stains):
    mat = np.zeros((3,3),float)
    for i in range(3):
        mat[i,:] = get_stain(stains,i)        
    return mat

mat = get_matrix(stains)
max_stains = get_max(stains)
matinv = inv(mat)

# Load the image using pyvips and convert to a numpy array
image = pyvips.Image.new_from_file('83185126-8318de00-a0f8-11ea-91fe-efb1bc84a860.png', access='sequential')
arr = image.numpy()

# Image normalisation using QuPath's background value
arr1 = arr[:,:,0:3]/[[max_stains]]
arr1[arr1 &gt; 1] = 1

# Deconvolve and display the stain images
fig, axes = plt.subplots(1,3,figsize=(16,8))
for i,ax in enumerate(axes):
    data = np.sum(arr1*matinv[:,i],axis=2)
    im = ax.imshow(data, cmap='gray')
    ax.axis('off')
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="8%", pad=0.1)
    cb = plt.colorbar(im, cax=cax)
</code></pre>
<p>The color deconvolution happens in this line (i: HEM=0, DAB=1, Residual=2). Be careful that numpy understands what I mean, but there probably are more correct ways to write this! <img src="https://emoji.discourse-cdn.com/twitter/innocent.png?v=12" title=":innocent:" class="emoji" alt=":innocent:" loading="lazy" width="20" height="20"></p>
<pre><code class="lang-auto">data = np.sum(arr1*matinv[:,i],axis=2)
</code></pre>
<p>And the result:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5.jpeg" data-download-href="/uploads/short-url/n36JfPs6voqwBz8l8s9on3yPRxr.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5_2_690x179.jpeg" alt="image" data-base62-sha1="n36JfPs6voqwBz8l8s9on3yPRxr" width="690" height="179" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5_2_690x179.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5_2_1035x268.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5.jpeg 2x" data-dominant-color="C7C7C7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1300×339 76.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Do not hesitate to correct me everywhere I made mistakes!</p>
<p>Cheers,<br>
Egor</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503.jpeg" data-download-href="/uploads/short-url/cpy0Ps2y4BNWIgg7eM2PetbwFCr.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_499x499.jpeg" alt="image" data-base62-sha1="cpy0Ps2y4BNWIgg7eM2PetbwFCr" width="499" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_499x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_748x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_998x998.jpeg 2x" data-dominant-color="82B2BF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1800×1801 275 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The <a href="https://quarep.org/">QUAREP-LiMi</a> (<em>“Quality Assessment and Reproducibility for Instruments &amp; Images in Light Microscopy”</em>) is a group of enthusiastic light microscopists from Academia and Industry all interested in improving quality assessment (QA) and quality control (QC) in light microscopy.</p> ;;;; <p><a class="attachment" href="/uploads/short-url/24p37YnrV3VOxEOl71pF3uXdG6O.tif">February 2022_RPG_BCAS-CC1-Olig2 staining 5 week con_5weekcon_left3_3_MIP.ome.tif (rearranged).tif</a> (4.0 MB)</p>
<p>Hi everybody,</p>
<p>I am quite new to Qupath. I have images of mouse brain and I am trying to quantify the oligodendrocyte cell population in these images.<br>
My images have four channels:</p>
<ul>
<li>DAPI</li>
<li>Olig2 (oligodendrocyte lineage marker)</li>
<li>CC1 (mature oligodendrocyte marker)</li>
<li>BCAS (immature oligodendrocyte marker)</li>
</ul>
<p>I trained classifiers to recognize Olig2, CC1 and BCAS. However, as BCAS is expressed in oligodendrocyte processes, cells that are not BCAS+ sometimes/often are counted as BCAS+ when an oligodendrocyte process ‘accidentaly’ lies close to the nucleus of that cell. I can recognize this by eye, but the classifier only gets about 70% of the BCAS+ cells correct, no matter how much I train it, how small I make the cell circumference and which parameters I use (I tried a bunch of different things).</p>
<p>So, I want to go through all the pictures and correct the assigned populations manually (mostly for BCAS, but also to correct misstakes of the other markers).</p>
<p>To ease the correction process, I would like to convert the detected cells to dots (one cell type = one annotation) for each category that I find relevant:</p>
<ul>
<li>DAPI+ cells</li>
<li>Olig2+ cells</li>
<li>Olig2+CC1+ cells</li>
<li>Olig2+BCAS+ cells</li>
<li>Olig2+CC1+BCAS+ cells</li>
</ul>
<p>It works to create a group of dots for the DAPI+ cells using this macro:</p>
<pre><code class="lang-auto">selectAllObjects();
runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImage":"DAPI","requestedPixelSizeMicrons":0.3,"backgroundRadiusMicrons":8.0,"backgroundByReconstruction":true,"medianRadiusMicrons":0.0,"sigmaMicrons":1.5,"minAreaMicrons":10.0,"maxAreaMicrons":400.0,"threshold":15.0,"watershedPostProcess":true,"cellExpansionMicrons":1.5,"includeNuclei":true,"smoothBoundaries":true,"makeMeasurements":true}');

import qupath.lib.roi.ROIs
import qupath.lib.objects.PathAnnotationObject

xs = []
ys = []

getCellObjects().forEach {
    xs &lt;&lt; it.getROI().getCentroidX()
    ys &lt;&lt; it.getROI().getCentroidY()
}

def roi = ROIs.createPointsROI(xs as double[], ys as double[], getCurrentViewer().getImagePlane())
def newAnn = new PathAnnotationObject(roi, null)
addObject(newAnn)

for (var ann: getAnnotationObjects()) {
    ann.setName("DAPI+cells")
}
</code></pre>
<p>Then, it also works to select the Olig2+cells using this macro:<br>
selectAllObjects();</p>
<pre><code class="lang-auto">runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImage":"DAPI","requestedPixelSizeMicrons":0.3,"backgroundRadiusMicrons":8.0,"backgroundByReconstruction":true,"medianRadiusMicrons":0.0,"sigmaMicrons":1.5,"minAreaMicrons":10.0,"maxAreaMicrons":400.0,"threshold":15.0,"watershedPostProcess":true,"cellExpansionMicrons":1.5,"includeNuclei":true,"smoothBoundaries":true,"makeMeasurements":true}');
runObjectClassifier("Olig2");
selectObjectsByClassification("Olig2");

import qupath.lib.roi.ROIs
import qupath.lib.objects.PathAnnotationObject

xs = []
ys = []

getSelectedObjects().forEach {
    xs &lt;&lt; it.getROI().getCentroidX()
    ys &lt;&lt; it.getROI().getCentroidY()
}

def roi = ROIs.createPointsROI(xs as double[], ys as double[], getCurrentViewer().getImagePlane())
def newAnn = new PathAnnotationObject(roi, null)
addObject(newAnn)

}
</code></pre>
<p>However, when I apply the following macro, which attempts to label the Olig2+CC1+ subsets by first deleting the ‘not Olig2+ cells’, my DAPI+cells annotation and my Olig2+ cell annotations are removed. Does anybody know how to prevent this from happening? I don’t have much coding experience and despite checking all blogposts related to this topic I haven’t managed to work around this. Thank you so much for your help!</p>
<pre><code class="lang-auto">selectAllObjects();
runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImage":"DAPI","requestedPixelSizeMicrons":0.3,"backgroundRadiusMicrons":8.0,"backgroundByReconstruction":true,"medianRadiusMicrons":0.0,"sigmaMicrons":1.5,"minAreaMicrons":10.0,"maxAreaMicrons":400.0,"threshold":15.0,"watershedPostProcess":true,"cellExpansionMicrons":1.5,"includeNuclei":true,"smoothBoundaries":true,"makeMeasurements":true}');

runObjectClassifier("Olig2");
selectObjectsByClassification(null);
clearSelectedObjects(true);

runObjectClassifier("CC1");
selectObjectsByClassification("CC1");

import qupath.lib.roi.ROIs
import qupath.lib.objects.PathAnnotationObject

xs = []
ys = []

getSelectedObjects().forEach {
    xs &lt;&lt; it.getROI().getCentroidX()
    ys &lt;&lt; it.getROI().getCentroidY()
}

def roi = ROIs.createPointsROI(xs as double[], ys as double[], getCurrentViewer().getImagePlane())
def newAnn = new PathAnnotationObject(roi, null)
addObject(newAnn)

}
</code></pre> ;;;; <p>I vaguely remember having this issue happening to me once recently. Fortunately, I managed to recover the hierarchy data from an automatic backup created within the data subfolder for the particular project entry.</p>
<p>Edit: I cannot remember the project and entry that this happened, so cannot show a screenshot example… I recall that the data.qpdata had a backup (I think appended as .bak or similar), which I renamed and got my hierarchy data back.</p> ;;;; <p>Hey <a class="mention" href="/u/itk_lighting">@ITK_Lighting</a>  <a class="mention" href="/u/alexjov">@alexjov</a> ,<br>
I am also having the same issue. Were you able to find out what the parameters mean?<br>
<a class="mention" href="/u/christlet">@christlet</a> seems to be the maintainer. Christopher, can you help us?<br>
All the best!<br>
Sarah</p> ;;;; <p>Hello,</p>
<p>Has anyone used the Quantile Based Normalization plugin in Fiji? I have tried all kinds of thing and keep getting Null Point Exception errors, and don’t know enough about programming to know what to do with it…</p>
<blockquote>
<p>java.lang.NullPointerException<br>
at util.Quantile_Based_Normalization.processToDirectory(Quantile_Based_Normalization.java:529)<br>
at util.Quantile_Based_Normalization.run(Quantile_Based_Normalization.java:862)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p>
</blockquote>
<p>I’ve checked Fiji is up to date, removed spaces from file names and file paths. Both images are identical in size.</p>
<p>Any thoughts would be appreciated! Thanks!</p> ;;;; <p><a class="mention" href="/u/matthew_hartley">@Matthew_Hartley</a> Thanks Matthew! Some of the points we discussed last week would tackle the flexibility/extensibility I think. I will also mention this during the meeting to see what people in the community think about it.</p> ;;;; <p>When QuPath crashes, if you copy the information from <em>View → Show log</em> then that can help us track down the source of the problem.</p>
<p>I’m afraid the error message you see now suggests that the data for that image is unavailable, but it can’t explain why (since that depends upon the original crash).</p>
<aside class="quote no-group" data-username="llordello" data-post="3" data-topic="78488">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png" class="avatar"> Leonardo:</div>
<blockquote>
<p>And indeed, never have issues with other softwares, even QuPath when crashed.</p>
</blockquote>
</aside>
<p>There are lots of details that might be relevant. QuPath is likely working with huge images. And if you run a pixel classifier at a high resolution across the entire image, then crashes are quite likely. The ‘workaround’ is usually to run a pixel classifier at a lower resolution, or only within smaller annotations.</p>
<aside class="quote no-group" data-username="llordello" data-post="1" data-topic="78488">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png" class="avatar"> Leonardo:</div>
<blockquote>
<p>It is the second time it happens to me and I cannot access the annotations I have done in my project.</p>
</blockquote>
</aside>
<p>Do you mean you lose all annotations for all images in the project? I wouldn’t expect a crash when saving one image to affect the others.</p>
<aside class="quote no-group" data-username="llordello" data-post="1" data-topic="78488">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png" class="avatar"> Leonardo:</div>
<blockquote>
<p>running a cell detection and then a classifier.</p>
</blockquote>
</aside>
<p>I’m not sure how you are doing that – interactively, or via a script (using either ‘Run’ or ‘Run for project’).</p>
<aside class="quote no-group" data-username="llordello" data-post="1" data-topic="78488">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png" class="avatar"> Leonardo:</div>
<blockquote>
<p>The last time I had to redo the annotations that took me a long time… Now it happens again… Any inputs about how to avoid this?</p>
</blockquote>
</aside>
<p>If you do time-consuming manual annotations, it’s a good idea to back these up regularly – especially before doing any complex processing that has crashed before. One way is to create a zip file of the entire project folder. Alternatively, you can use <em>File → Export objects as GeoJSON</em> to store the annotations in a text file. You can then import them again later by dragging them on top of QuPath when the image is open.</p> ;;;; <p>Sharing and working with large 3D image data can be very tedious without the right software. <a href="https://webknossos.org/" rel="noopener nofollow ugc">WEBKNOSSOS</a> makes this very easy. Free accounts on <a href="http://webknossos.org" rel="noopener nofollow ugc">webknossos.org</a> include some storage space to get your started. However, if you already have storage resources, you may not want to upgrade just for storage. With its OME-Zarr support, WEBKNOSSOS can access datasets that are stored externally. In this tutorial, we’ll explain how to convert data into OME-Zarr and how to set up a static file server for use with WEBKNOSSOS.</p>
<p>Using <a href="https://webknossos.org/" rel="noopener nofollow ugc">webknossos.org</a> instead of a self-hosted WEBKNOSSOS instance has the benefit that we maintain the server, install updates frequently, and backup your annotations. Also, you can upgrade to paid features of WEBKNOSSOS at any time.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df.png" data-download-href="/uploads/short-url/guYxuHlODIU4OHEvafji2udwlVJ.png?dl=1" title="Architecture diagram of WEBKNOSSOS with internal storage and remote storage (e.g. your own server and cloud storage)" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df_2_673x500.png" alt="Architecture diagram of WEBKNOSSOS with internal storage and remote storage (e.g. your own server and cloud storage)" data-base62-sha1="guYxuHlODIU4OHEvafji2udwlVJ" width="673" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df_2_673x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df.png 2x" data-dominant-color="F4F4F5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Architecture diagram of WEBKNOSSOS with internal storage and remote storage (e.g. your own server and cloud storage)</span><span class="informations">700×519 46.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h1>
<a name="getting-your-data-ready-1" class="anchor" href="#getting-your-data-ready-1"></a>Getting your data ready</h1>
<p>WEBKNOSSOS supports a range of chunked file formats for external access including OME-Zarr, N5 and Neuroglancer precomputed. We strongly recommend OME-Zarr, because it is best supported. Other file formats, such as TIFF, CZI, IMS or HDF5, need to be converted to OME-Zarr. There are multiple tools available to do that, e.g. bioformats2raw or NGFF-Converter. If you are savvy in Python, you can use the <strong><code>webknossos</code></strong> package:</p>
<pre><code class="lang-python">import webknossos as wk

ds = wk.Dataset.from_images(
  "path/to/tiff/stack",
  "dataset_wkw",
  voxel_size=(4, 4, 40),
  data_format="zarr"
)
ds.compress()
ds.downsample()
</code></pre>
<p>The Python library also has features for creating layers and mags (resolution levels) from arbitrary numpy arrays. You can easily integrate that into your existing scripts. Make sure to check out the <a href="https://docs.webknossos.org/webknossos-py/examples/dataset_usage.html" rel="noopener nofollow ugc">examples in the documentation</a>.</p>
<h1>
<a name="set-up-your-own-storage-server-2" class="anchor" href="#set-up-your-own-storage-server-2"></a>Set up your own storage server</h1>
<p>You can also use your own server as storage for WEBKNOSSOS. You need to have a server that is publicly available on the Internet and has a (sub)domain name attached to it. It needs to run a server application with HTTPS support. We also recommend basic auth to prevent unauthorized access to your data.</p>
<p>In this tutorial, we show how to set up <a href="https://caddyserver.com/" rel="noopener nofollow ugc">Caddy</a> on an Ubuntu server. Caddy is a great choice because it includes automatic HTTPS configuration and is easy to use. Other software such as Apache, nginx or traefik are also great options and there are many tutorials available on how to set them up.</p>
<p>First, you need to install Caddy as explained in the documentation: <a href="https://caddyserver.com/docs/install" class="inline-onebox" rel="noopener nofollow ugc">Install — Caddy Documentation</a>. Next, you need to assign a folder from where the data is served. In our example, we’ll use <code>/opt/webknossos</code>. Go ahead and create that folder.</p>
<p>Now, we need to configure Caddy. It should be located under <code>/etc/caddy/Caddyfile</code>. Depending on your Linux distribution, it might be located somewhere else. Copy the following content into your Caddyfile. Change the domain name in the first line and generate your own password for basic auth. You can use the <code>caddy hash-password</code> command to generate the password.</p>
<pre><code class="lang-auto">example.cloud.scm.io {
  root * /opt/webknossos
  file_server browse
  basicauth * {
    webknossos $2a$14$uGab5vbFo/VH1Jubz39/yOW57uQlPmhT//mbGvT85dDn.xIiqRJam
  }
}

</code></pre>
<p>Reload the Caddy service using <code>sudo systemctl reload caddy</code>. After a few seconds, Caddy should serve your data.</p>
<p>Now you are ready to add your data to this folder. If you don’t have data of your own, you can download and extract the following dataset to test your setup: <a href="https://static.webknossos.org/data/l4_sample.zarr.zip" rel="noopener nofollow ugc">https://static.webknossos.org/data/l4_sample.zarr.zip</a></p>
<p>The folder structure should look something like this:</p>
<pre><code class="lang-auto">/opt/webknossos/
└── l4_sample
    ├── .zgroup
    ├── color
    │   ├── .zattrs
    │   ├── .zgroup
    │   ├── 1
    │   ├── 2-2-1
    │   ├── 4-4-1
    │   ├── 8-8-2
    │   └── 16-16-4
    ├── datasource-properties.json
    └── segmentation
        ├── .zattrs
        ├── .zgroup
        ├── 1
        ├── 2-2-1
        ├── 4-4-1
        ├── 8-8-2
        └── 16-16-4
</code></pre>
<p>Your server is now fully prepared to serve data to WEBKNOSSOS. Head over to your account on <a href="http://webknossos.org" rel="noopener nofollow ugc">webknossos.org</a> and add your dataset. Go to <a href="https://webknossos.org/datasets/upload#remote" rel="noopener nofollow ugc">“Add Dataset” &gt; “Add Remote Dataset</a>” and enter the URL to your dataset, e.g. <a href="https://example.cloud.scm.io/l4_sample" rel="noopener nofollow ugc">https://example.cloud.scm.io/l4_sample</a>. Click add layer and voilá your data should be imported and ready to be visualized, annotated and shared.</p>
<p><div class="large-image-placeholder"><a href="https://miro.medium.com/v2/resize:fit:700/1*sjx5CSylNzqiMx8aDaf9jA.gif" target="_blank" rel="noopener nofollow ugc"><svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="url">https://miro.medium.com/v2/resize:fit:700/1*sjx5CSylNzqiMx8aDaf9jA.gif</span><span class="help">(image larger than 20 MB)</span></a></div></p>
<h1>
<a name="cloud-storage-3" class="anchor" href="#cloud-storage-3"></a>Cloud storage</h1>
<p>If you don’t want to manage your own storage servers, you can also buy storage from one of the many cloud providers. Amazon S3 is the most popular choice, but can be quite pricey. Especially costs for egress traffic add up quickly. While Google Cloud storage is a popular alternative, pricing is in the same ballpark. Cheaper alternatives include Backblaze B2, Cloudflare R2, and Scaleway Object Storage.</p>
<p>To use that, simply create an account with the provider of your choice, create a storage bucket, upload some datasets, and fetch the credentials. Now, you can import the data into WEBKNOSSOS using the URL, e.g. s3://webknossos-zarr/demodata/l4_sample, and the corresponding credentials.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867.png" data-download-href="/uploads/short-url/4gEpxHii4ulwGiG3fgIrqYK82WP.png?dl=1" title="" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867_2_690x498.png" alt="" data-base62-sha1="4gEpxHii4ulwGiG3fgIrqYK82WP" width="690" height="498" role="presentation" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867_2_690x498.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867.png 2x" data-dominant-color="E7E8EB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename"></span><span class="informations">700×505 69.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>To learn more about updates to WEBKNOSSOS, follow us on <a href="https://twitter.com/webknossos" rel="noopener nofollow ugc">Twitter</a> or <a href="https://mstdn.science/@webknossos" rel="noopener nofollow ugc">Mastodon</a>. If you haven’t already, go to <a href="https://webknossos.org/" rel="noopener nofollow ugc">webknossos.org</a> and sign up for a free account.</p> ;;;; <p>There are some documentation on how to install ImageJ plugins or change the ImageJ plugin directory for QuPath at <a href="https://qupath.readthedocs.io/en/stable/docs/advanced/imagej.html" class="inline-onebox" rel="noopener nofollow ugc">ImageJ — QuPath 0.4.3 documentation</a>.</p>
<p>Assuming that you managed to install the plugin without issues, I have had success with the following QuPath script to run ImageJ macros (in this case, using the Analyze Skeleton (2D/3D) plugin). Note that you will need to adapt some parts for your project (e.g. “Class Name” to whatever class your annotations of interest is; or the parameters as you see fit).</p>
<pre><code class="lang-auto">//// Run ImageJ macros in QuPath
import qupath.imagej.gui.ImageJMacroRunner

params = new ImageJMacroRunner(getQuPath()).getParameterList()

// Change the value of a parameter, using the JSON to identify the key
params.getParameters().get('downsampleFactor').setValue(1)
params.getParameters().get('sendROI').setValue(true)
params.getParameters().get('sendOverlay').setValue(false)
params.getParameters().get('getOverlay').setValue(false)
params.getParameters().get('getOverlayAs').setValue('Annotations') // or 'Detections'
params.getParameters().get('getROI').setValue(false)
params.getParameters().get('clearObjects').setValue(false)

macro = """
selectWindow("Class Name");
run("Create Mask");
selectWindow("Mask");
run("Skeletonize (2D/3D)");
run("Analyze Skeleton (2D/3D)", "prune=none");
"""

def imageData = getCurrentImageData()
def annotations = getAnnotationObjects().findAll{it.getPathClass() == getPathClass("Class Name")}

// Loop through the annotations and run the macro
for (annotation in annotations) {
    ImageJMacroRunner.runMacro(params, imageData, null, annotation, macro)
//    ij.IJ.run("Close All", "");
}
</code></pre> ;;;; <p>It is a 8Gb but I used to keep about 80% for QuPath. And indeed, never have issues with other softwares, even QuPath when crashed. With the new version, not sure how to set the memmory to be used…</p> ;;;; <p>Hi Trond,</p>
<p>Thanks for testing it out! It was just an intresting problem to try and solve, hah.I knew it wouldn’t be perfect, because it would only work it the substrate touched both sides of the image in each horizontal line.</p>
<p>Glad you found an approach that works for you. Do you mind sharing how you solved the problem?</p> ;;;; <p>Hi <a class="mention" href="/u/nopitynope">@nopitynope</a></p>
<p>Would this help at all: <a href="https://forum.image.sc/t/summarizing-results-in-one-table/57877" class="inline-onebox">Summarizing results in one table?</a></p>
<p>Best wishes,<br>
Marie</p> ;;;; <p>Hej <a class="mention" href="/u/constantinpape">@constantinpape</a>,</p>
<p>I think I am running into some problems again <img src="https://emoji.discourse-cdn.com/twitter/smiley.png?v=12" title=":smiley:" class="emoji" alt=":smiley:" loading="lazy" width="20" height="20"></p>
<p>Please have a look at the updated version of the notebook: <a href="https://github.com/CamachoDejay/mobie-python-examples/blob/388fcab147549c75e9a8b82633344cf389c03a11/mobie-project-views_tmatrix.ipynb" class="inline-onebox" rel="noopener nofollow ugc">mobie-python-examples/mobie-project-views_tmatrix.ipynb at 388fcab147549c75e9a8b82633344cf389c03a11 · CamachoDejay/mobie-python-examples · GitHub</a></p>
<p>As you will see in import option 3 I now add a transformation “on-the-fly” via:</p>
<pre><code class="lang-auto">input_file = "./data/blobs_crop_rot.ome.tif"
raw_name = "rot_816nmPix_03"
unit = "nanometer"
resolution = (1., 816, 816)

transformation = [0.8034685581387304,0.46418531456408085,0.0,17949.15847223811,
                 -0.46418531456408085,0.8034685581387304,0.0,206061.42501785012,
                  0.0,0.0,0.9279168767591658,0.0]

transformations = {'parameters': transformation}

mobie.add_image(
    input_path=input_file, 
    input_key='',  # the input is a single tif image, so we leave input_key blank
    root=mobie_project_folder,
    dataset_name=dataset_name,
    image_name=raw_name,
    menu_name=menu_name,
    resolution=resolution,
    chunks=chunks,
    scale_factors=scale_factors,
    transformation=transformations,
    is_default_dataset=True,  # mark this dataset as the default dataset that will be loaded by mobie
    target=target,
    max_jobs=max_jobs,
    unit=unit,
    file_format="bdv.n5"#"ome.zarr"
)
</code></pre>
<p>However, when I create a view using this <code>image_source</code> via:</p>
<pre><code class="lang-auto">view_title = "Overlay_C1"
source_list = [["blobs_454nmPix"], ["rot_816nmPix_03"]]
settings = [ 
    {"color": "green", "contrastLimits": [0., 255.], "blendingMode": "sum"},
    {"color": "magenta", "contrastLimits": [0., 255.], "blendingMode": "sum"},
]


mobie.create_view(dataset_folder, view_name=view_title,
                  sources=source_list,
                  display_group_names=[source_list[0][0], source_list[1][0]], 
                  display_settings=settings,
                  overwrite=True)
</code></pre>
<p>I run into problems.</p>
<p>It seems that, while the transformation information was stored, I lost the “scaling” information. Note how once I go into this view in MoBIE I the overlay is not workin, here if I focus on the non-transformed image source:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9.jpeg" data-download-href="/uploads/short-url/t31ukttM69mmJ6RfMp6pGd5I1Pb.jpeg?dl=1" title="Focus_on_454nm" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_200x250.jpeg" alt="Focus_on_454nm" data-base62-sha1="t31ukttM69mmJ6RfMp6pGd5I1Pb" width="200" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_200x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_300x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_400x500.jpeg 2x" data-dominant-color="163816"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Focus_on_454nm</span><span class="informations">524×649 29.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I can not see the second source because the pixel size after transformation is tiny. Here if I focus on that “transformed” source, note that the pixel size should be <code>unit = "nanometer"; resolution = (1., 816, 816)</code>:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f.jpeg" data-download-href="/uploads/short-url/bD6Aws3eFA8pDovIztho9DwLdCv.jpeg?dl=1" title="Focus_on_816nm" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_200x250.jpeg" alt="Focus_on_816nm" data-base62-sha1="bD6Aws3eFA8pDovIztho9DwLdCv" width="200" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_200x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_300x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_400x500.jpeg 2x" data-dominant-color="14DE14"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Focus_on_816nm</span><span class="informations">526×652 18.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Is there something off or am I misunderstanding when the transformation takes place? Because the rotation angle and the translation seem to be ok.</p>
<p>Saludos,<br>
Rafa</p> ;;;; <p>Thanks, Konrad.</p>
<p>Just to be clear: you want me to load the DeepLabCut environment (conda activate deeplabcut) and call the refine_tracklets command without calling ipython?</p>
<p>With the index error, you’re also saying I don’t need to put brackets around the el.h5 file and assembly3 file when calling the refine_tracklets command?</p>
<p>Makes sense because they are not lists… only single files for each component</p>
<p>Thanks</p>
<p><img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji only-emoji" alt=":+1:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi James,</p>
<p>your script fixes the number of images/series in the lif file to 80.<br>
Since I assume that their number actually changes from file to file, you need to use an extension of the bio-formats library that allows you to extract information from the file, like the number of images in it contained, before actually opening the single images.<br>
Specifically, you need to add at the beginning of the script the following line</p>
<pre><code class="lang-javascript">run("Bio-Formats Macro Extensions");`
</code></pre>
<p>Then you modify the two functions <code>ProcessFile</code> and <code>openLif</code> as follow</p>
<pre><code class="lang-javascript">function processFile(input, output, file) {
	// do the processing here by replacing
	// the following two lines by your own code
	Ext.setId(input + file);
	Ext.getSeriesCount(count);
	print("Processing: " + input + file+ " - "+d2s(count,0)+" images");

	for (f=0;f&lt;count;f++) {
		openLif(input+file,f);
		print("Saving to: " + output);
		listImages();
	}
}

function openLif(input,f){
	Ext.setSeries(f);
	run("Bio-Formats Importer", "open=[" +input +"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_" + d2s(f+1,0));
}
</code></pre>
<p>Notice that in this way the images in the lif file are opened one by one, which is a safer procedure.</p>
<p>I hope it helps.<br>
Giovanni</p> ;;;; <p><a class="mention" href="/u/wmv1992">@wmv1992</a> - happy to pitch from the <a href="http://www.ebi.ac.uk/bioimage-archive" rel="noopener nofollow ugc">BioImage Archive</a>/<a href="https://www.nature.com/articles/s41592-021-01166-8" rel="noopener nofollow ugc">REMBI</a> perspective!</p>
<p>We’re interested in OME-NGFF metadata from a couple of slightly different directions:</p>
<ol>
<li>To be able to use metadata from within OME-NGFF images submitted to us, so that submitters don’t have to supply this information manually. For this we need to be able to verify that it meets some set of minimal requirements and extract to enable indexing and search.</li>
<li>To be able to package metadata we have into OME-NGFF as a standardised distribution format.</li>
</ol>
<p>Flexibility/extensibility is important in both cases since we’ll need to cover varying community requirements - +1 for <a class="mention" href="/u/dmt">@dmt</a> ’s point above about coverage of consensus metadata &amp; extensibility. <a class="mention" href="/u/joshmoore">@joshmoore</a>’s point about being able to store some metadata separately and “register” in the model is likely to be pretty critical to avoid duplication.</p> ;;;; <p>Hi Leonardo,</p>
<p>Sorry to hear you are running into issues and having to repeat steps. Can I ask how much RAM is in the computer you are using?</p>
<p>There is the option to increase the memory available to QuPath via the preferences but I believe the default is 50% of the RAM in the machine. It’s wise to not increase it much more than this for the computers sake if you don’t have a lot of RAM in the first place (better QuPath grinds to a halt instead of the PC).</p>
<p>Thanks,</p>
<p>Fiona</p> ;;;; <p>Hey, I don’t have the environment to hand, but I ran:</p>
<pre><code class="lang-bash">conda create --name ENV_NAME python=3.10 -y
conda activate ENV_NAME
pip install cellfinder
</code></pre>
<p>I converted the image using <a href="https://imagej.net/software/fiji/">FIJI</a> by:</p>
<ul>
<li>Creating a directory to save the images (one called <code>signal</code>, and one <code>background</code>)</li>
<li>Opening FIJI</li>
<li>Dragging the 3D image onto the main FIJI window</li>
<li>Saving the image as a series of 2d tiffs by <code>File</code> → <code>Save as</code> → <code>Image Sequence</code> and choosing the appropriate directory (leaving the other options as default)</li>
</ul>
<p>I then ran cellfinder as per the instructions passing in the data directories, e.g.:</p>
<pre><code class="lang-auto">cellfinder -s signal -b background -o output -v 5 5 5 --orientation sal
</code></pre>
<p>Hope this helps.<br>
Adam</p> ;;;; <p>Hello everyone! I was wondering if this problem had ever been solved and if <a class="mention" href="/u/ayang">@ayang</a> was able to get Micromanager to work with the Axio Observer 7? My company is looking into potentially purchasing this microscope but we will not purchase it if we cannot easily get Micromanager to communicate with it. Thanks!</p> ;;;; <p>Join our upcoming Q&amp;A session dedicated to the First AI4Life Open Call happening on March 20, 2023, at 4 pm CET. This one-hour session is designed to help you with any queries or concerns you might have about the Open Call and provide support throughout the application process.</p>
<p>To register for the Q&amp;A session, visit <a href="https://bit.ly/oc-ask-us-anything" class="inline-onebox" rel="noopener nofollow ugc">Meeting Registration - Zoom</a>.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c.png" data-download-href="/uploads/short-url/x8WuPN4WclfLCFXBTzWvZbdEKvi.png?dl=1" title="Ask Us Anything!" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_690x388.png" alt="Ask Us Anything!" data-base62-sha1="x8WuPN4WclfLCFXBTzWvZbdEKvi" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_1380x776.png 2x" data-dominant-color="DCE4E2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Ask Us Anything!</span><span class="informations">1600×900 168 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>If you can’t make it to the live Q&amp;A, check out our video where Florian Jug addresses some of the frequently asked questions about the Open Call:</p>
<div class="onebox lazyYT lazyYT-container" data-youtube-id="ZwqL3AmU3og" data-youtube-title="FAQs about the AI4Life Open Calls" data-parameters="feature=oembed&amp;wmode=opaque">
  <a href="https://www.youtube.com/watch?v=ZwqL3AmU3og" target="_blank" rel="noopener nofollow ugc">
    <img class="ytp-thumbnail-image" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f97705ea26ec32c8e86a367f5db0de62b7584042.jpeg" title="FAQs about the AI4Life Open Calls" width="480" height="360">
  </a>
</div>

<p>Don’t miss this opportunity to learn more about the AI4Life Open Call and get the support you need. We look forward to seeing you there!</p> ;;;; <p>With Bioformats the label &amp; macro images of the slide are the last two resolutions of the image.</p>
<pre><code class="lang-auto">b.imRead = new ImageReader();
int macro = b.imRead.getSeriesCount() - 1;
int label = b.imRead.getSeriesCount() - 2;
b.imRead.setSeries(macro);
byte[] bts = b.imRead.openBytes(0, tilex, tiley, tilesx, tilesy);
</code></pre> ;;;; <p>Where can I download the unformatted data in the starfish instance</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25.jpeg" data-download-href="/uploads/short-url/7cA4lkgPiZsxqFvoiETwig5KbiZ.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_690x388.jpeg" alt="image" data-base62-sha1="7cA4lkgPiZsxqFvoiETwig5KbiZ" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_1380x776.jpeg 2x" data-dominant-color="B9B8BB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1080 107 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Hello, guys! I´m trying to understand what I am doing wrong. It is the second time it happens to me and I cannot access the annotations I have done in my project.</p>
<p>I got a message saying that I was out of memmory after running a cell detection and then a classifier. I could see the detections and also the categories. Then when I tried to save, it crashed and now this message appears and I cannot access the image.</p>
<p>The last time I had to redo the annotations that took me a long time… Now it happens again… Any inputs about how to avoid this?</p> ;;;; <p><a class="mention" href="/u/christian_tischer">@Christian_Tischer</a> - As far as I can tell, this code hasn’t been updated since I last modified it. It writes 0.4, but I imagine it still needs to be updated for any recent additions.</p> ;;;; <p>Ah, I see. Yeah, that looks like it could be the case, now that I look at it. And what is it about the isosurface mode in napari that looks worse compared to the imajej?</p> ;;;; <p>Hi Everyone,</p>
<p>Now I’m trying to get the label and macro image in a czi file using python. However, I searched many libraries and I cannot solve this problem. Is there anyone know how to access the label image in a czi file like the below screenshot in Zen Blue?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919.png" data-download-href="/uploads/short-url/t8G41E0MYO50WkOLnxbTATlugsh.png?dl=1" title="Label in Zen Blue" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919_2_690x253.png" alt="Label in Zen Blue" data-base62-sha1="t8G41E0MYO50WkOLnxbTATlugsh" width="690" height="253" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919_2_690x253.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919.png 2x" data-dominant-color="92938D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Label in Zen Blue</span><span class="informations">1016×373 351 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thanks a lot even if tell me it cannot be accessed now <img src="https://emoji.discourse-cdn.com/twitter/melting_face.png?v=12" title=":melting_face:" class="emoji" alt=":melting_face:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi all,</p>
<p>I have been experiencing difficulties in running AnalyseSkeleton in the ImageJ Extension of QuPath. I came across some very useful post about polygonal measurement with Skeletonizing/Distance Mapping and has successful experience in Fiji. The images were annotated with QuPath so at a certain level I am using them in conjunction.</p>
<p>Soon I saw some other topics regarding automated (via scripting/macros) measurements and results return to QuPath that involved similar techniques so I attempted to set up a similar macro. However, I was unable to install AnalyzeSkeleton into the QuPath version of ImageJ. I am not sure is the plugin on the Fiji side utilizing ImageJ2. I have visited the <a href="http://imagej.net" rel="noopener nofollow ugc">imagej.net</a> page and the link to the Image J 1.x seemed to be dead. May I ask that is there any way to workaround this issue? Thank you very much.</p>
<p>Alvin</p> ;;;; <p><code>assembly3</code> video and <code>_el.h5</code>. Can you reinstall it normally from the env and not ipython console, just to be sure? Though the new error seems to point to not having a second flag? <code>IndexError: invalid index to scalar variable.</code> means that you’re trying to get an index from a single value (not a list/array etc.)</p> ;;;; <p>Hi all,</p>
<p>i try to open a OME-ZARR by using Drag &amp; Drop or from CMD in the latest napari but I always get errors. Any idea what I am doing wrong here?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb.png" data-download-href="/uploads/short-url/y6PBvy8B4PWaOQKSuZS0sS5zRzB.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb_2_690x213.png" alt="image" data-base62-sha1="y6PBvy8B4PWaOQKSuZS0sS5zRzB" width="690" height="213" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb_2_690x213.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb_2_1035x319.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb.png 2x" data-dominant-color="393A3F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1363×422 12 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-plaintext">napari: 0.4.17
Platform: Windows-10-10.0.19045-SP0
Python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]
Qt: 5.15.6
PyQt5: 5.15.7
NumPy: 1.23.5
SciPy: 1.10.0
Dask: 2023.1.1
VisPy: 0.11.0
magicgui: 0.6.1
superqt: unknown
in-n-out: 0.1.6
app-model: 0.1.1
npe2: 0.6.2

OpenGL:
- GL version: 4.6.0 NVIDIA 512.36
- MAX_TEXTURE_SIZE: 32768

Screens:
- screen 1: resolution 1920x1080, scale 2.0
- screen 2: resolution 1920x1080, scale 1.0

</code></pre>
<pre><code class="lang-bash">(ia39) PS C:\Users\m1srh&gt; napari "f:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr" --plugin napari-ome-zarr
Traceback (most recent call last):
  File "F:\Documents\miniconda3\envs\ia39\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "F:\Documents\miniconda3\envs\ia39\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "F:\Documents\miniconda3\envs\ia39\Scripts\napari.exe\__main__.py", line 7, in &lt;module&gt;
    sys.exit(main())
  File "F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\__main__.py", line 561, in main
    _run()
  File "F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\__main__.py", line 341, in _run
    viewer._window._qt_viewer._qt_open(
  File "F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\_qt\qt_viewer.py", line 830, in _qt_open
    self.viewer.open(
  File "F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\components\viewer_model.py", line 1014, in open
    self._add_layers_with_plugins(
  File "F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\components\viewer_model.py", line 1216, in _add_layers_with_plugins
    layer_data, hookimpl = read_data_with_plugins(
  File "F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\plugins\io.py", line 104, in read_data_with_plugins
    raise ValueError(
ValueError: There is no registered plugin named 'napari-ome-zarr'.
Names of plugins offering readers are: {'bfio', 'ome-types'}
</code></pre>
<p>But the napari-ome-zarr plugin is installed an up-to-date</p>
<pre><code class="lang-plaintext">MultipleReaderError: Multiple plugins found capable of reading F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr. Select plugin from {'napari': 'napari builtins', 'napari-ome-zarr': 'napari-ome-zarr'} and pass to reading function e.g. `viewer.open(..., plugin=...)`.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\_qt\qt_viewer.py:1191, in QtViewer.dropEvent(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, event=&lt;PyQt5.QtGui.QDropEvent object&gt;)
   1188     else:
   1189         filenames.append(url.toString())
-&gt; 1191 self._qt_open(
        self = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;
        filenames = ['F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr']
        shift_down = &lt;PyQt5.QtCore.Qt.KeyboardModifiers object at 0x000000007F46D350&gt;
        alt_down = &lt;PyQt5.QtCore.Qt.KeyboardModifiers object at 0x000000007F46D2E0&gt;
   1192     filenames,
   1193     stack=bool(shift_down),
   1194     choose_plugin=bool(alt_down),
   1195 )

File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\_qt\qt_viewer.py:848, in QtViewer._qt_open(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, filenames=[r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr'], stack=False, choose_plugin=False, plugin=None, layer_type=None, **kwargs={})
    838     handle_gui_reading(
    839         filenames,
    840         self,
   (...)
    845         **kwargs,
    846     )
    847 except MultipleReaderError:
--&gt; 848     handle_gui_reading(filenames, self, stack, **kwargs)
        filenames = ['F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr']
        self = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;
        stack = False
        kwargs = {}

File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\_qt\dialogs\qt_reader_dialog.py:199, in handle_gui_reading(paths=[r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr'], qt_viewer=&lt;napari._qt.qt_viewer.QtViewer object&gt;, stack=False, plugin_name=None, error=None, plugin_override=False, **kwargs={})
    197 display_name, persist = readerDialog.get_user_choices()
    198 if display_name:
--&gt; 199     open_with_dialog_choices(
        display_name = 'napari-ome-zarr'
        persist = True
        readerDialog = &lt;napari._qt.dialogs.qt_reader_dialog.QtReaderDialog object at 0x000000007F4728B0&gt;
        readerDialog._extension = 'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr\\'
        readers = {'napari': 'napari builtins', 'napari-ome-zarr': 'napari-ome-zarr'}
        paths = ['F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr']
        stack = False
        qt_viewer = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;
        kwargs = {}
    200         display_name,
    201         persist,
    202         readerDialog._extension,
    203         readers,
    204         paths,
    205         stack,
    206         qt_viewer,
    207         **kwargs,
    208     )

File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\_qt\dialogs\qt_reader_dialog.py:292, in open_with_dialog_choices(display_name='napari-ome-zarr', persist=True, extension=r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr\', readers={'napari': 'napari builtins', 'napari-ome-zarr': 'napari-ome-zarr'}, paths=[r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr'], stack=False, qt_viewer=&lt;napari._qt.qt_viewer.QtViewer object&gt;, **kwargs={})
    288 plugin_name = [
    289     p_name for p_name, d_name in readers.items() if d_name == display_name
    290 ][0]
    291 # may throw error, but we let it this time
--&gt; 292 qt_viewer.viewer.open(paths, stack=stack, plugin=plugin_name, **kwargs)
        plugin_name = 'napari-ome-zarr'
        paths = ['F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr']
        qt_viewer.viewer = Viewer(axes=Axes(visible=False, labels=True, colored=True, dashed=False, arrows=True), camera=Camera(center=(0.0, 0.0, 0.0), zoom=1.0, angles=(0.0, 0.0, 90.0), perspective=0.0, interactive=True), cursor=Cursor(position=(1.0, 1.0), scaled=True, size=1, style=&lt;CursorStyle.STANDARD: 'standard'&gt;), dims=Dims(ndim=2, ndisplay=2, last_used=0, range=((0, 2, 1), (0, 2, 1)), current_step=(0, 0), order=(0, 1), axis_labels=('0', '1')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[], scale_bar=ScaleBar(visible=False, colored=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, ticks=True, position=&lt;Position.BOTTOM_RIGHT: 'bottom_right'&gt;, font_size=10.0, box=False, box_color=&lt;class 'numpy.ndarray'&gt; (4,) float32, unit=None), text_overlay=TextOverlay(visible=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, font_size=10.0, position=&lt;TextOverlayPosition.TOP_LEFT: 'top_left'&gt;, text=''), overlays=Overlays(interaction_box=InteractionBox(points=None, show=False, show_handle=False, show_vertices=False, selection_box_drag=None, selection_box_final=None, transform_start=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684790&gt;, transform_drag=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526847F0&gt;, transform_final=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684850&gt;, transform=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526848B0&gt;, allow_new_selection=True, selected_vertex=None)), help='', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=True, mouse_move_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_move at 0x00000000617C1700&gt;], mouse_drag_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_drag at 0x00000000617AE8B0&gt;], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[&lt;function dims_scroll at 0x000000004F253F70&gt;], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={'Shift': &lt;function InteractionBoxMouseBindings.initialize_key_events.&lt;locals&gt;.hold_to_lock_aspect_ratio at 0x00000000617AE700&gt;, 'Control-Shift-R': &lt;function InteractionBoxMouseBindings._reset_active_layer_affine at 0x00000000610CB1F0&gt;, 'Control-Shift-A': &lt;function InteractionBoxMouseBindings._transform_active_layer at 0x00000000610CB280&gt;})
        stack = False
        kwargs = {}
        qt_viewer = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;
    294 if persist:
    295     if not extension.endswith(os.sep):

File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\components\viewer_model.py:1014, in ViewerModel.open(self=Viewer(axes=Axes(visible=False, labels=True, col...._transform_active_layer at 0x00000000610CB280&gt;}), path=[r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr'], stack=False, plugin='napari-ome-zarr', layer_type=None, **kwargs={})   1011 _path = [_path] if not isinstance(_path, list) else _path
   1012 if plugin:
   1013     added.extend(
-&gt; 1014         self._add_layers_with_plugins(
        added = []
        self = Viewer(axes=Axes(visible=False, labels=True, colored=True, dashed=False, arrows=True), camera=Camera(center=(0.0, 0.0, 0.0), zoom=1.0, angles=(0.0, 0.0, 90.0), perspective=0.0, interactive=True), cursor=Cursor(position=(1.0, 1.0), scaled=True, size=1, style=&lt;CursorStyle.STANDARD: 'standard'&gt;), dims=Dims(ndim=2, ndisplay=2, last_used=0, range=((0, 2, 1), (0, 2, 1)), current_step=(0, 0), order=(0, 1), axis_labels=('0', '1')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[], scale_bar=ScaleBar(visible=False, colored=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, ticks=True, position=&lt;Position.BOTTOM_RIGHT: 'bottom_right'&gt;, font_size=10.0, box=False, box_color=&lt;class 'numpy.ndarray'&gt; (4,) float32, unit=None), text_overlay=TextOverlay(visible=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, font_size=10.0, position=&lt;TextOverlayPosition.TOP_LEFT: 'top_left'&gt;, text=''), overlays=Overlays(interaction_box=InteractionBox(points=None, show=False, show_handle=False, show_vertices=False, selection_box_drag=None, selection_box_final=None, transform_start=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684790&gt;, transform_drag=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526847F0&gt;, transform_final=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684850&gt;, transform=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526848B0&gt;, allow_new_selection=True, selected_vertex=None)), help='', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=True, mouse_move_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_move at 0x00000000617C1700&gt;], mouse_drag_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_drag at 0x00000000617AE8B0&gt;], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[&lt;function dims_scroll at 0x000000004F253F70&gt;], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={'Shift': &lt;function InteractionBoxMouseBindings.initialize_key_events.&lt;locals&gt;.hold_to_lock_aspect_ratio at 0x00000000617AE700&gt;, 'Control-Shift-R': &lt;function InteractionBoxMouseBindings._reset_active_layer_affine at 0x00000000610CB1F0&gt;, 'Control-Shift-A': &lt;function InteractionBoxMouseBindings._transform_active_layer at 0x00000000610CB280&gt;})
        _path = ['F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr']
        kwargs = {}
        plugin = 'napari-ome-zarr'
        layer_type = None
        _stack = False
   1015             _path,
   1016             kwargs=kwargs,
   1017             plugin=plugin,
   1018             layer_type=layer_type,
   1019             stack=_stack,
   1020         )
   1021     )
   1022 # no plugin choice was made
   1023 else:
   1024     layers = self._open_or_raise_error(
   1025         _path, kwargs, layer_type, _stack
   1026     )

File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\components\viewer_model.py:1216, in ViewerModel._add_layers_with_plugins(self=Viewer(axes=Axes(visible=False, labels=True, col...._transform_active_layer at 0x00000000610CB280&gt;}), paths=[r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr'], stack=False, kwargs={}, plugin='napari-ome-zarr', layer_type=None)
   1214 else:
   1215     assert len(paths) == 1
-&gt; 1216     layer_data, hookimpl = read_data_with_plugins(
        paths = ['F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr']
        stack = False
        plugin = 'napari-ome-zarr'
   1217         paths, plugin=plugin, stack=stack
   1218     )
   1220 # glean layer names from filename. These will be used as *fallback*
   1221 # names, if the plugin does not return a name kwarg in their meta dict.
   1222 filenames = []

File F:\Documents\miniconda3\envs\ia39\lib\site-packages\napari\plugins\io.py:104, in read_data_with_plugins(paths=[r'F:\Testdata_Zeiss\OME_ZARR_Testfiles\w96_A1+A2_test_zarr'], plugin='napari-ome-zarr', stack=False)
    102 if plugin not in plugin_manager.plugins:
    103     names = {i.plugin_name for i in hook_caller.get_hookimpls()}
--&gt; 104     raise ValueError(
        trans = &lt;napari.utils.translations.TranslationBundle object at 0x00000000257266D0&gt;
        plugin = 'napari-ome-zarr'
        names = {'ome-types', 'bfio'}
    105         trans._(
    106             "There is no registered plugin named '{plugin}'.\nNames of plugins offering readers are: {names}",
    107             deferred=True,
    108             plugin=plugin,
    109             names=names,
    110         )
    111     )
    112 reader = hook_caller._call_plugin(plugin, path=npe1_path)
    113 if not callable(reader):

ValueError: There is no registered plugin named 'napari-ome-zarr'.
Names of plugins offering readers are: {'ome-types', 'bfio'}
</code></pre> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf.jpeg" data-download-href="/uploads/short-url/sC9qOuXvMU11b5QP8psrhqo01vV.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_375x500.jpeg" alt="image" data-base62-sha1="sC9qOuXvMU11b5QP8psrhqo01vV" width="375" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_750x1000.jpeg 2x" data-dominant-color="627D98"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3024×4032 3.46 MB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Hi Konrad,</p>
<p>I thought I would send you a photo of the files I have in the video folder for a project.</p>
<p>Can you please confirm which files I should be using for stitch_tracklets and refine_tracklets?</p>
<p>Maybe this might help</p>
<p>Thanks</p>
<p>Grant</p> ;;;; <p>Hello,</p>
<p>I get an error message like Chris although I used the full window name(“C1-initialcleaning_1_z.oir Group:1 Level:1 Area:1”).</p>
<pre><code class="lang-auto">//Folder setting
showMessage("Select Open Folder");
openDir = getDirectory("Choose a Directory");
showMessage("Select Save Folder");
saveDir = getDirectory("Choose a Directory");
list = getFileList(openDir);

//connection a name
//Array.show(list);
//print(openDir);
name = openDir + list[0];

//Open Oir file
run("Viewer", "open=[name]");

//Split channels
run("Split Channels");
close();
close();
redname = "C1-"+list[0]+" Group:1 Level:1 Area:1"+".oir";

//Enter min and Max
min = getNumber("Min (0-255):", 0);
max = getNumber("Max (0-255):", 1000);

//拡張子よりも前側のファイル名を取得
name = getTitle();
dotIndex = lastIndexOf(name,".");
title = substring(name,0,dotIndex);
print(title);

//Contrast Adjust
setMinAndMax(min, max);
run("Apply LUT", "stack");

//手動でROIを選択する
setTool("rectangle")
waitForUser("ROI selection", "Select ROI using rectangle tool then click \"OK\".");
run("Crop");

//3D Viewer
run("3D Viewer");
call("ij3d.ImageJ3DViewer.setCoordinateSystem", "false");
call("ij3d.ImageJ3DViewer.add", "C1-initialcleaning_1_z.oir Group:1 Level:1 Area:1", "None", "C1-initialcleaning_1_z.oir Group:1 Level:1 Area:1", "0", "true", "true", "true", "2", "0");//**error**
</code></pre>
<p><strong>error message</strong></p>
<pre><code class="lang-auto">(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_202 [64-bit]; Mac OS X 10.16; 198MB of 7915MB (2%)
Macro line number: 46
 
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at ij.macro.Functions.call(Functions.java:4620)
	at ij.macro.Functions.getStringFunction(Functions.java:277)
	at ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)
	at ij.macro.Interpreter.getString(Interpreter.java:1498)
	at ij.macro.Interpreter.doStatement(Interpreter.java:336)
	at ij.macro.Interpreter.doStatements(Interpreter.java:267)
	at ij.macro.Interpreter.run(Interpreter.java:163)
	at ij.macro.Interpreter.run(Interpreter.java:93)
	at ij.macro.Interpreter.run(Interpreter.java:107)
	at ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)
	at ij.IJ.runMacro(IJ.java:158)
	at ij.IJ.runMacro(IJ.java:147)
	at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)
	at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)
	at net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)
	at net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)
	at net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)
	at org.scijava.script.ScriptModule.run(ScriptModule.java:164)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -32048
	at vib.NaiveResampler$MaxLikelihood.add(NaiveResampler.java:72)
	at vib.NaiveResampler.resample(NaiveResampler.java:233)
	at vib.NaiveResampler.resample(NaiveResampler.java:155)
	at vib.NaiveResampler.resample(NaiveResampler.java:159)
	at voltex.VoltexGroup.&lt;init&gt;(VoltexGroup.java:102)
	at ij3d.ContentInstant.displayAs(ContentInstant.java:153)
	at ij3d.ContentCreator.createContent(ContentCreator.java:106)
	at ij3d.ContentCreator.createContent(ContentCreator.java:74)
	at ij3d.Image3DUniverse.addContent(Image3DUniverse.java:831)
	at ij3d.ImageJ3DViewer.add(ImageJ3DViewer.java:164)
	... 30 more

</code></pre>
<p>I’m so sorry to revive this topic. I need your help.</p> ;;;; <p>oh I see! No, you need to run the script from a terminal shell, not in a ipython/notebook environment (in that case you can only get a static snapshot image with no interactivity).<br>
An independent window must pop up.</p>
<hr>
<p>An other option is to add</p>
<pre><code class="lang-auto">settings.default_backend = "vtk"
</code></pre> ;;;; <p>Use the makeSelection(“polyline”, xpoints, ypoints) macro function to convert the calculated x-y values into a line selection.</p>
<p>The following example creates a segmented line, saves it in an overlay, uses the “Fit Spline” and “Interpolate” commands to generate a set of x-y values, and creates a line selection from the values.</p>
<pre><code class="lang-auto">  newImage("Untitled", "8-bit black", 350, 300, 1);
  makeLine(61,43,165,111,239,54,272,172,197,224);
  run("Add Selection...");
  run("Fit Spline");
  run("Interpolate", "interval=75 smooth adjust");
  getSelectionCoordinates(xpoints, ypoints);
  makeSelection("polyline", xpoints, ypoints)
  run("Add Selection...");
</code></pre>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cbb9890c67eb01c5b9df97fb8bbad9e060bfbdea.png" alt="Screenshot" data-base62-sha1="t4eqUR4UwhaiQebNSXepa8yvv5g" width="377" height="368"></p> ;;;; <p><a class="mention" href="/u/mmusy">@mmusy</a> Thank you for the quick reply, actually, by clicking on pictures, no points show up. I think I might run the code incorrectly. just to clarify what I mean, I recorded my screen:<br>
          <iframe class="vimeo-onebox" src="https://player.vimeo.com/video/807347893?h=75eecd8b7c&amp;app_id=122963" data-original-href="https://vimeo.com/807347893" frameborder="0" allowfullscreen="" seamless="seamless" sandbox="allow-same-origin allow-scripts allow-forms allow-popups allow-popups-to-escape-sandbox allow-presentation"></iframe>
<br>
Thank you for your patience <img src="https://emoji.discourse-cdn.com/twitter/rose.png?v=12" title=":rose:" class="emoji" alt=":rose:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi, by “drawing” I meant “clicking points” which are then connected by a spline (right-clicking removes last point, pressing <code>c</code> clears all points).<br>
Once you are done with one line you press <code>q</code> to go to the next…<br>
You can also save your clicked points to file for later analysis with eg:</p>
<pre><code class="lang-python">np.save("verhoeff_membrane.npy", line1.points())
</code></pre>
<p>let me know if it’s not clear.</p> ;;;; <p>Hi <a class="mention" href="/u/brisvag">@brisvag</a>, I posted a side by side comparison above. I find that with napari I don’t get nearly the same level of detail that I get with imageJ volume renders. Maybe they’re just different.<br>
I asked a similar question to the PyVista discussion area, and someone speculated that ImageJ isn’t doing a volume render in the same way, but instead extracting and plotting many isosurfaces.</p> ;;;; <p><a class="mention" href="/u/mathew">@Mathew</a> <a class="mention" href="/u/mmusy">@mmusy</a> My main goal is to measure the retinal thickness manually, by drawing lines manually and then get the max, min, and mean. Pardon me if I seem to be very noobie since I am pretty new to image processing subject and using ImageJ or other programs.<br>
So retina has 11 layers and I need to measure each layer separately:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10.png" data-download-href="/uploads/short-url/zTtiiG8AfvaBt8hTHYl1vU7iyTm.png?dl=1" title="Segmented" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10_2_690x270.png" alt="Segmented" data-base62-sha1="zTtiiG8AfvaBt8hTHYl1vU7iyTm" width="690" height="270" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10_2_690x270.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10.png 2x" data-dominant-color="4F5255"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Segmented</span><span class="informations">850×333 196 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
and this is an unannotated version of the image:<br>
<a class="attachment" href="/uploads/short-url/7AQ6ZLmR8ySskVI7uMBSaX3vUfq.tif">Retina.tif</a> (976.0 KB)</p>
<p>Thank you very much,</p>
<p>Mehdi</p> ;;;; <p>Hi Konrad,</p>
<p>I downgraded matplotlib to 3.5.1 (and 3.6.1) and I get the same error when I perform the ‘flag’ function in the refine_tracklets GUI.</p>
<p>Please found output below.</p>
<p>What else should I try to get it to work?</p>
<p>Thanks for your help</p>
<p>(base) C:\WINDOWS\system32&gt;conda activate deeplabcut</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;ipython<br>
Python 3.8.16 | packaged by co(deeplabcut) C:\WINDOWS\system32&gt;ipython<br>
Python 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>
Type ‘copyright’, ‘credits’ or ‘license’ for more information<br>
IPython 8.10.0 – An enhanced Interactive Python. Type ‘?’ for help.</p>
<p>In [1]: pip install matplotlib==3.6.1<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Collecting matplotlib==3.6.1<br>
Downloading matplotlib-3.6.1-cp38-cp38-win_amd64.whl (7.2 MB)<br>
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 2.7 MB/s eta 0:00:00<br>
Requirement already satisfied: packaging&gt;=20.0 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (23.0)<br>
Requirement already satisfied: python-dateutil&gt;=2.7 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (2.8.2)<br>
Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (3.0.9)<br>
Requirement already satisfied: contourpy&gt;=1.0.1 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (1.0.7)<br>
Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (1.4.4)<br>
Requirement already satisfied: numpy&gt;=1.19 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (1.23.5)<br>
Requirement already satisfied: cycler&gt;=0.10 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (0.11.0)<br>
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (4.38.0)<br>
Requirement already satisfied: pillow&gt;=6.2.0 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.6.1) (9.4.0)<br>
Requirement already satisfied: six&gt;=1.5 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib==3.6.1) (1.16.0)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Installing collected packages: matplotlib<br>
Attempting uninstall: matplotlib<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Found existing installation: matplotlib 3.7.1<br>
Uninstalling matplotlib-3.7.1:<br>
Successfully uninstalled matplotlib-3.7.1<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Successfully installed matplotlib-3.6.1<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Note: you may need to restart the kernel to use updated packages.</p>
<p>In [2]: import os</p>
<p>In [3]: os._exit(00)</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;conda activate deeplabcut</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;ipython<br>
Python 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>
Type ‘copyright’, ‘credits’ or ‘license’ for more information<br>
IPython 8.10.0 – An enhanced Interactive Python. Type ‘?’ for help.</p>
<p>In [1]: import deeplabcut<br>
Loading DLC 2.3.0…</p>
<p>In [2]: config_path = r’C:\Users\WSA20210206\Desktop\assembly3-grant080323-2023-03-08\config.yaml’</p>
<p>In [3]: h5_pickle = r’C:\Users\WSA20210206\Desktop\assembly3-grant080323-2023-03-08\videos\assembly3DLC_dlcrnetms5_asse<br>
…: mbly3Mar8shuffle1_30000_el.h5’</p>
<p>In [4]: analysed_video = r’C:\Users\WSA20210206\Desktop\assembly3-grant080323-2023-03-08\videos\assembly3DLC_dlcrnetms5<br>
…: _assembly3Mar8shuffle1_30000_full.mp4’</p>
<p>In [5]: deeplabcut.refine_tracklets(<br>
…: config_path,<br>
…: h5_pickle,<br>
…: analysed_video,<br>
…: )<br>
Traceback (most recent call last):<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\cbook_<em>init</em>_.py”, line 307, in process<br>
func(*args, **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\widgets.py”, line 220, in <br>
return self.<em>observers.connect(‘clicked’, lambda event: func(event))<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\deeplabcut\gui\tracklet_toolbox.py”, line 532, in flag_frame<br>
ax.fill_between(<br>
File "C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib_<em>init</em></em>.py", line 1423, in inner<br>
return func(ax, *map(sanitize_sequence, args), **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5337, in fill_between<br>
return self._fill_between_x_or_y(<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5326, in _fill_between_x_or_y<br>
pts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\numpy\ma\core.py”, line 3254, in <strong>getitem</strong><br>
mout = _mask[indx]<br>
<strong>IndexError: invalid index to scalar variable</strong>.<br>
Out[5]:<br>
(&lt;deeplabcut.refine_training_dataset.tracklets.TrackletManager at 0x15109171820&gt;,<br>
&lt;deeplabcut.gui.tracklet_toolbox.TrackletVisualizer at 0x151791d4fa0&gt;)</p>
<p>In [6]:<br>
Do you really want to exit ([y]/n)? y</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;conda activate deeplabcut</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;ipython<br>
Python 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>
Type ‘copyright’, ‘credits’ or ‘license’ for more information<br>
IPython 8.10.0 – An enhanced Interactive Python. Type ‘?’ for help.</p>
<p>In [1]: pip install matplotlib==3.5.1<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Collecting matplotlib==3.5.1<br>
Using cached matplotlib-3.5.1-cp38-cp38-win_amd64.whl (7.2 MB)<br>
Requirement already satisfied: numpy&gt;=1.17 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (1.23.5)<br>
Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (1.4.4)<br>
Requirement already satisfied: cycler&gt;=0.10 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (0.11.0)<br>
Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (3.0.9)<br>
Requirement already satisfied: pillow&gt;=6.2.0 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (9.4.0)<br>
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (4.38.0)<br>
Requirement already satisfied: python-dateutil&gt;=2.7 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (2.8.2)<br>
Requirement already satisfied: packaging&gt;=20.0 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from matplotlib==3.5.1) (23.0)<br>
Requirement already satisfied: six&gt;=1.5 in c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib==3.5.1) (1.16.0)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Installing collected packages: matplotlib<br>
Attempting uninstall: matplotlib<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Found existing installation: matplotlib 3.6.1<br>
Uninstalling matplotlib-3.6.1:<br>
Successfully uninstalled matplotlib-3.6.1<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Successfully installed matplotlib-3.5.1<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Note: you may need to restart the kernel to use updated packages.</p>
<p>In [2]: import os</p>
<p>In [3]: os._exit(00)</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;conda activate deeplabcut</p>
<p>(deeplabcut) C:\WINDOWS\system32&gt;ipython<br>
Python 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>
Type ‘copyright’, ‘credits’ or ‘license’ for more information<br>
IPython 8.10.0 – An enhanced Interactive Python. Type ‘?’ for help.</p>
<p>In [1]: import deeplabcut<br>
Loading DLC 2.3.0…</p>
<p>In [2]: config_path = r’C:\Users\WSA20210206\Desktop\assembly3-grant080323-2023-03-08\config.yaml’</p>
<p>In [3]: h5_pickle = r’C:\Users\WSA20210206\Desktop\assembly3-grant080323-2023-03-08\videos\assembly3DLC_dlcrnetms5_asse<br>
…: mbly3Mar8shuffle1_30000_el.h5’</p>
<p>In [4]: analysed_video = r’C:\Users\WSA20210206\Desktop\assembly3-grant080323-2023-03-08\videos\assembly3DLC_dlcrnetms5<br>
…: _assembly3Mar8shuffle1_30000_full.mp4’</p>
<p>In [5]: deeplabcut.refine_tracklets(<br>
…: config_path,<br>
…: h5_pickle,<br>
…: analysed_video,<br>
…: )<br>
Traceback (most recent call last):<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\cbook_<em>init</em>_.py”, line 287, in process<br>
func(*args, **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\widgets.py”, line 227, in <br>
return self.<em>observers.connect(‘clicked’, lambda event: func(event))<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\deeplabcut\gui\tracklet_toolbox.py”, line 532, in flag_frame<br>
ax.fill_between(<br>
File "C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib_<em>init</em></em>.py", line 1412, in inner<br>
return func(ax, *map(sanitize_sequence, args), **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5252, in fill_between<br>
return self._fill_between_x_or_y(<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5241, in _fill_between_x_or_y<br>
pts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\numpy\ma\core.py”, line 3254, in <strong>getitem</strong><br>
mout = _mask[indx]<br>
<strong>IndexError: invalid index to scalar variable.</strong></p> ;;;; <p><a class="mention" href="/u/mmusy">@mmusy</a> Hello,<br>
Thanks for your help. I tried to use this code in python, but my problem is that I can not draw a line manually. It perfectly shows me two pictures, but I can’t draw lines.<br>
It would be great if you could help me.</p>
<p>Thanks</p> ;;;; <p>…if you like python:</p>
<pre><code class="lang-python">from vedo import *
from vedo.applications import SplinePlotter

pic = Picture("your_image.jpeg")

plt1 = SplinePlotter(pic, size=(1000,500))
plt1.lcolor = 'yellow'
plt1.show(pic, "draw one line", zoom=2, mode='image')
plt1.close()
line1 = plt1.line

plt2 = SplinePlotter(pic, size=(1000,500))
plt2.lcolor = 'light blue'
plt2.show(pic, "now draw the other line", zoom=2, mode='image')
plt2.close()
line2 = plt2.line

d = line1.distance_to(line2)
print("min, mean, max =", np.min(d), np.mean(d), np.max(d))

# show the result
plt = Plotter(size=(1000,500))
show(pic, line1, line2, axes=1, zoom=2)
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097.png" data-download-href="/uploads/short-url/cZ5hJXfcwlFwmSeDH50LEIRTT4H.png?dl=1" title="Screenshot from 2023-03-12 21-48-36"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097_2_690x310.png" alt="Screenshot from 2023-03-12 21-48-36" data-base62-sha1="cZ5hJXfcwlFwmSeDH50LEIRTT4H" width="690" height="310" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097_2_690x310.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097.png 2x" data-dominant-color="6D6E6D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-12 21-48-36</span><span class="informations">807×363 164 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-plaintext">min, mean, max = 133.05594 168.6623 203.39598
</code></pre> ;;;; <p>Hi Jaime,</p>
<p>Is the issue happening only when opening a sub-resolution level ? If yes, then your issue could be related to <a href="https://forum.image.sc/t/qupath-omero-weird-pyramid-levels/65484/7" class="inline-onebox">QuPath - OMERO - Weird Pyramid levels - #7 by NicoKiaru</a>.</p>
<p>If that’s the case, the error will show up when trying to load the right edge of the image - and only for certain vsi files.</p> ;;;; <p>Hi,</p>
<p>I am trying to solve a curious problem with some VSI files I have from an Olympus slide scanner. When I try to open certain regions of certain series I get an error (attached as <code>Exception.txt</code>). So for example, I can open Series <span class="hashtag">#10</span> without a problem, but not Series <span class="hashtag">#9</span>. And when I feed in XYWH coordinates for Series <span class="hashtag">#9</span> I can open some regions but not other regions.<br>
<a class="attachment" href="/uploads/short-url/u4jY0JpdkPNHmsUyDPaDVaho6UA.txt">Exception.txt</a> (3.4 KB)</p>
<p>This makes me think the problem might be corrupted data. But another possibly helpful clue, all the VSI files that have this problem were acquired in the same scanning session. Files from different scanning sessions on different days are not affected.</p>
<p>I have the same problem either when using Bio-Formats Import in ImageJ or <code>python-bioformats</code>.</p>
<p>It is tricky to share the data as the VSI files are considerable (~5GB) but I have attached XML data that I read out using python-bioformats package.<br>
<a class="attachment" href="/uploads/short-url/AfuL6dfJV017GClZZwagvyPcKJD.xml">FT155_metadata.xml</a> (221.0 KB)</p>
<p>Any ideas would be greatly appreciated!</p>
<p>Jaime</p> ;;;; <p>This looks really nice, thanks for sharing – and documenting!</p> ;;;; <p>Dear all,</p>
<p>I am Isaac Vieco-Martí, PhD student at the University of València, Spain. Along the last year I had to do a lot of image analysis with QuPath for my PhD and fortunately I find this community. Thanks to you all I have been able to learn a lot and I discovered this amazing field of Image Analysis. So first, I would thank you all for your help!</p>
<p>This week I faced myself with a new challenge: to create a hexagonal grid to crop an image.<br>
The purpose of doing this trimming is to perform a topological analysis like the one that appears in this <a href="https://pubmed.ncbi.nlm.nih.gov/31173338/" rel="noopener nofollow ugc">paper</a>. I find this <a href="https://forum.image.sc/t/hexagonal-grid-roi-macro/31465/2">post</a> by <a class="mention" href="/u/mountain_man">@mountain_man</a> in which he designs a script to do it in Image J. However, I didn’t see anything about hexagonal grids in QuPath. So, I developed some scripts to create hexagonal grids in QuPath. Here is the link to the <a href="https://github.com/iviecomarti/Hexagons_QuPath" rel="noopener nofollow ugc">repository</a> (so exciting to attach the link for the first time).</p>
<p>I do not have a lot of background in Computer Science (I am a Biotechnologist), so maybe the scripts are not as perfect as they could be, but they work.  For sure we all can improve them <img src="https://emoji.discourse-cdn.com/twitter/blush.png?v=12" title=":blush:" class="emoji" alt=":blush:" loading="lazy" width="20" height="20"></p>
<p>The first I have done is to adapt the script of <a class="mention" href="/u/mountain_man">@mountain_man</a> to make it work in QuPath. You can find the script <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagonal_grid.groovy" rel="noopener nofollow ugc">here</a>.  Then, I  realised that the hexagons are arranged horizontally, and perhaps one needs the grid in a vertical arrangement. Understanding the hexagon geometry, I modified the script to obtain the <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagonal_grid.groovy" rel="noopener nofollow ugc">vertical</a> arrangement. Here you can see the results of both grids.<br>
<strong>Figure 1</strong></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7.png" data-download-href="/uploads/short-url/1YSvLk2suqAoVSxsjwEGMUf7zYb.png?dl=1" title="Diapositiva1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7_2_690x388.png" alt="Diapositiva1" data-base62-sha1="1YSvLk2suqAoVSxsjwEGMUf7zYb" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7.png 2x" data-dominant-color="D4B3B3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Diapositiva1</span><span class="informations">1280×720 125 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Since we usually work with Whole Slide Images, create such large hexagonal grid could be demanding for our pc. It would be easier to create an hexagonal grid overlaying just the annotation.</p>
<p>This <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagons_multiple_annotations.groovy" rel="noopener nofollow ugc">script</a> to gets the width and the height of the bounds of the annotation, and create a hexagonal grid of those dimensions.  Since we can obtain the xCoord and yCoord of the bounds of the original annotation, we can translate the grid to that position to have the overlay (1st column of the figure 2).  With the Geometry tools (this <a href="https://www.imagescientist.com/editing-object-shapes-or-types" rel="noopener nofollow ugc">link</a> is incredible for learning them) we can decide if we want the “intersecting” hexagons (2nd column of the figure 2) or the “within” hexagons (3rd column of the figure 2). We can also obtain the same with the <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagons_multiple_annotations.groovy" rel="noopener nofollow ugc">vertical</a> arrangement.</p>
<p><strong>Figure 2</strong></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a.png" data-download-href="/uploads/short-url/agYvMH3yKIOfNWBq1aXymXdIluO.png?dl=1" title="Diapositiva2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a_2_690x388.png" alt="Diapositiva2" data-base62-sha1="agYvMH3yKIOfNWBq1aXymXdIluO" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a.png 2x" data-dominant-color="C8C0BB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Diapositiva2</span><span class="informations">1280×720 182 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Once you learn the geometry of the hexagon, it is easy to develop other tools. For example, these scripts can create a <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagons_from_centroids.groovy" rel="noopener nofollow ugc">horizonal</a> or <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagon_from_centroids.groovy" rel="noopener nofollow ugc">vertical</a> hexagon of the desired size taking the reference of the centroids of the green annotations (1st column of figure 3).  This perhaps can be useful to create hexagons from a coords list in a csv file, as discussed in this <a href="https://forum.image.sc/t/contruct-voronoi-polygons-from-a-set-of-points-groovy-script/70998/4">post</a>. We can also take a circle and create a hexagon which fits <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagon_inside_outside_circle.groovy" rel="noopener nofollow ugc">inside</a> or <a href="https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagons_inside_outside_circle.groovy" rel="noopener nofollow ugc">outside</a> the circle (2nd column of figure 3).</p>
<p><strong>Figure 3</strong></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2.png" data-download-href="/uploads/short-url/uZkvBAiN3PgJG8pQzbI3vCSEQWS.png?dl=1" title="Diapositiva3" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2_2_690x388.png" alt="Diapositiva3" data-base62-sha1="uZkvBAiN3PgJG8pQzbI3vCSEQWS" width="690" height="388" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2.png 2x" data-dominant-color="B7B3B1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Diapositiva3</span><span class="informations">1280×720 101 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I hope that these scripts can be useful for you.</p>
<p>Thank you again for all your help!</p>
<p>Best,</p>
<p>Isaac</p> ;;;; <p>I opened a new <a href="https://github.com/ome/ngff/issues/174" rel="noopener nofollow ugc">github issue</a> proposing that we look to the <a href="http://cfconventions.org/Data/cf-conventions/cf-conventions-1.8/cf-conventions.html" rel="noopener nofollow ugc">Climate and Forecast (CF) Metadata Conventions</a> as a direct model for subsequent versions of OME-NGFF.</p>
<p>Basically, the climate science field has many of the same metadata problems we have, and they have pretty good solutions for them, and it would probably be wise to use those solutions as much as possible.</p>
<p>I’m happy to discuss the proposal further here or on github.</p> ;;;; <p>Hello everyone!<br>
A new user of imageJ here wish to get your help. I am conducting research on CT angiography images, but I have encountered some problems due to my lack of knowledge of image processing and my unfamiliar use of imageJ.  First of all, my CT image is compressed, and there is no negative HU threshold when it opened through plugins <strong>BIO-formats</strong>. I converted it to carry minus sign through plugins <strong>Convert to signed 16</strong>, and the HU threshold range was still too large after passing process-math-sbtracted 1024. I think it is very important to solve this problem, so I first put forward, The processed threshold distribution range can be seen in the attachment. After solving the first question, I would like to analyze the volume occupied by different components of vascular plaques by ROI, as Professor Erik Meijering has done, but since I am new to imageJ, could someone please guide me on how to process the image afterwards?<br>
thank you very much!<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/2274cb002376175c2a908a6005dabcead4c0f9a1.png" alt="threshold" data-base62-sha1="4UOsD5pIJpzrxjO3JpbxiwEthQJ" width="390" height="409"></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5.png" data-download-href="/uploads/short-url/qUXyDKFWW3MprOGjukBfgHCjiMB.png?dl=1" title="analysis" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5_2_690x345.png" alt="analysis" data-base62-sha1="qUXyDKFWW3MprOGjukBfgHCjiMB" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5.png 2x" data-dominant-color="B8B693"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">analysis</span><span class="informations">1170×585 313 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d79bc6aaf29dd547510f826a3406435a572480c.png" data-download-href="/uploads/short-url/1Vd1zfua2W9xS3DI3wM98wxEtWY.png?dl=1" title="ways" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d79bc6aaf29dd547510f826a3406435a572480c.png" alt="ways" data-base62-sha1="1Vd1zfua2W9xS3DI3wM98wxEtWY" width="690" height="225" data-dominant-color="EFECE1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">ways</span><span class="informations">1059×346 43.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p><a class="mention" href="/u/mweigert">@mweigert</a>, thanks again for your help! I based my sequence class on one of the comments in that issue and previously had mixed it with the class in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence" rel="noopener nofollow ugc"><code>tf.keras.utils.Sequence</code></a> docs thinking that it would be possible to get batches directly from the <code>Sequence</code>.</p>
<p>Now I updated my <code>StardistSequence</code> class to look like this:</p>
<pre><code class="lang-auto">class StardistSequence(tf.keras.utils.Sequence):

    def __init__(self, csv_path, name, batch_size=BATCH_SIZE, key="raw", type_="train"):
        super().__init__()
        df = pd.read_csv(csv_path, index_col=False)
        self._filenames = df[key]
        self._name = name
        self._key = key
        self._batch_size = batch_size
        print(f"{name} {type_} {key} - Batch size: {batch_size} dataset created!")

    def __len__(self):
        return np.ceil(len(self._filenames) / self._batch_size).astype(np.uint8)

    def preprocess_x(self, path):
        axis_norm = (0, 1)
        im = io.imread(path, as_gray=True)
        im = exposure.adjust_gamma(im, .5)
        im = normalize(im, 1, 99.8, axis=axis_norm)
        return im
    
    def preprocess_y(self, path):
        im = io.imread(path, as_gray=True)
        return fill_label_holes(im)
    
    @lru_cache(100)
    def __getitem__(self, idx):
        file_name = self._filenames.iloc[idx]
        preprocess_fn = self.preprocess_x if self._key == "raw" else self.preprocess_y
        im = preprocess_fn(file_name) 
        return im

</code></pre>
<p>And create my sequences like this:</p>
<pre><code class="lang-auto">name = "test"
X_train_seq = StardistSequence(train_csv_path, name, key="raw", type_="train")
y_train_seq = StardistSequence(train_csv_path,  name, key="mask", type_="train")
X_val_seq = StardistSequence(val_csv_path, name, key="raw", type_="val")
y_val_seq = StardistSequence(val_csv_path, name, key="mask", type_="val")
</code></pre>
<p>Finally, I’m able to start training without issues. I’m leaving the updated code for future reference. Once again, thank you so much for your help <a class="mention" href="/u/mweigert">@mweigert</a>!</p> ;;;; <p>I set a reference system on the image. Drew a segmented line and extracted the x-y coordinates of the points on the segmented line by a macro code. I used these x-y coordinates to tune the parameters of my equation. Then by these parameters and the equation, I calculated the x-y values, which should be close to the initial x-y coordinate.<br>
Now, I want to draw a curve by the calculated x-y values on the calibrated initial image to find how fit the curve is with the drawn segmented line. Please help me in this matter.</p> ;;;; <p>I did some test with more CZI images and the one above is not the only one that fails. But all of them open just fine within Fiji using the latest BioFormats w/o any complaints about missing metadata. Any help is greatly appreciated.</p> ;;;; <p>Am l correct that you want to analyze the properties of each of the 18 colonies in this image?</p>
<p>Did you set you define what you want to measure in <code>Analyze &gt; Set Measurements</code>?</p>
<p>And when you want to obtain the result, do you choose <code>Analyze &gt; Measure</code> or <code>Analyze &gt; Analyze Particles</code>? I think the latter option should give you want you want (provided you did set the desired measurements in the <code>Set Measurements</code> menu).</p>
<p>Optionally, you can record your actions to keep track of your steps, this may help in communicating what you have tried and where it may go wrong (<code>Plugins &gt; Macros &gt; Record...</code>, a sort of text window will pop up where each action is recorded).</p> ;;;; <p>In this case, creating a Sequence for X and Y separately should work. See <a href="https://github.com/stardist/stardist/issues/107" class="inline-onebox">CPU memory usage keeps on increasing during training · Issue #107 · stardist/stardist · GitHub</a></p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/8892f61dc3e7afb680e18e3d589b1e356e591709.png" data-download-href="/uploads/short-url/jubNyy6dyCZt4XpDRlHGwEc2SHn.png?dl=1" title="Screenshot from 2023-03-12 08-01-57" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/8892f61dc3e7afb680e18e3d589b1e356e591709.png" alt="Screenshot from 2023-03-12 08-01-57" data-base62-sha1="jubNyy6dyCZt4XpDRlHGwEc2SHn" width="593" height="500" data-dominant-color="898988"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-12 08-01-57</span><span class="informations">798×672 10.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I am trying to use ImageJ to automatically count and measure the area of bacterial colonies. I followed <a href="https://www.youtube.com/watch?v=zwjXbeiviH0" rel="noopener nofollow ugc">this video</a>.</p>
<p>The procedure, in essence, works, and the results are shown above. I converted the selection to 16 bits, applied the threshold, and counted the particles.</p>
<p>But I have two problems:</p>
<ol>
<li>How to properly set the dimensions of the colonies? Here I get either no counts for well-defined spots or multiple counts on the same spot.</li>
<li>How can I compare two different plates? I imagine the images should be exactly the same dimension otherwise the area (in pixel, I imagine) will differ one from another.</li>
</ol>
<p>Thank you</p>
<h3>
<a name="analysis-goals-1" class="anchor" href="#analysis-goals-1"></a>Analysis goals</h3>
<ul>
<li>I am looking to automatically count the colonies and measure their area.</li>
</ul>
<h3>
<a name="challenges-2" class="anchor" href="#challenges-2"></a>Challenges</h3>
<ul>
<li>What stops you from proceeding? Counts of wrong items</li>
<li>What have you tried already? Different size/pixel limits</li>
<li>What software packages and/or plugins have you tried? only ImageJ</li>
</ul> ;;;; <p>Dear <a class="mention" href="/u/zeratoss">@zeratoss</a>,<br>
Could I suggest the newest plugin from Cedric Messaoudi:</p>
<p><strong>MIC plugin Fiji :</strong><br>
menu &gt;Help&gt;Update…</p>
<p>click the button “Manage update sites”</p>
<p>select MiC mask comparator</p>
<p>if it is not available directly you can add it (button add update site) with the folowing URL <a href="https://sites.imagej.net/MiC-mask-comparator/" class="inline-onebox" rel="noopener nofollow ugc">Index of /MiC-mask-comparator</a></p>
<p>MiC is an ImageJ plugin to compare segmentation masks.</p>
<ul>
<li>It computes the number of <strong>true positive (TP)</strong>, <strong>false positives (FP)</strong> and <strong>false negatives (FN)</strong>
<ul>
<li>at pixel level</li>
<li>at object level with an overlap (or intersection over union - IoU) of 0.5
<ul>
<li>possibility of varying IoU</li>
</ul>
</li>
</ul>
</li>
<li>It computes metrics
<ul>
<li>
<strong>Precision</strong> defined as</li>
</ul>
</li>
</ul>
<p><strong>Recall</strong> (or sensitivity) defined as <strong>Jaccard index</strong> (or global perecision) defined as <strong>F1-measure</strong> (or Sorensen Dice Coefficient - DSC) defined as * It displays the <strong>superposition of the two masks</strong> with the ground truth in green and the mask to evaluate in red for pixel level, for Object level the truth is in green, the mask to evaluate in red, TP are thus yellow, FP blue, FN dark green.</p>
<ul>
<li>It displays the <strong>plots</strong> corresponding to metrics as function of IoU</li>
<li>All computed values are stored in <strong>result tables</strong> that can be exported in excel or csv format</li>
<li>possibility to work on <strong>stacks</strong>. It works only in <strong>2D</strong> for now, each slice will be compared to corresponding slice. If varying IoU, an additional plot is displayed with metrics corresponding to the sum of TPs, FNs and FPs on all images.</li>
<li>the plugin is <strong>macro recordable</strong>
</li>
</ul>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/Multimodal-Imaging-Center/MiC">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/Multimodal-Imaging-Center/MiC" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314.png 2x" data-dominant-color="F5F1F1"></div>

<h3><a href="https://github.com/Multimodal-Imaging-Center/MiC" target="_blank" rel="noopener nofollow ugc">GitHub - Multimodal-Imaging-Center/MiC: MiC is an ImageJ plugin to compare...</a></h3>

  <p>MiC is an ImageJ plugin to compare segmentation masks  - GitHub - Multimodal-Imaging-Center/MiC: MiC is an ImageJ plugin to compare segmentation masks</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Good Afternoon,</p>
<p>I am currently engaged in the task of quantifying the number of fluorescent markers present in my images, along with determining the degree of overlap between these markers. While the summary window for the overlap analysis is functioning efficiently and presenting results stacked with filenames, the individual “Analyze Particles” summary tables are generated for each image separately. I would prefer combining the “Analyze Particles” summary tables for each individual image into a single, comprehensive table that includes the respective image names as the first column. This would greatly facilitate downstream analysis and interpretation of my findings.</p>
<p>Here’s what my “Analyze Particles” output looks like right now: <a href="https://www.dropbox.com/s/b2cvl8n6jsy0onm/ImageJ_bDM1TPc3qp%20-%20Copy.png?dl=0" class="inline-onebox" rel="noopener nofollow ugc">Dropbox - ImageJ_bDM1TPc3qp - Copy.png - Simplify your life</a></p>
<p>Here is my core code:</p>
<pre><code class="lang-auto">//markers in each channel
run("Z Project...", "projection=[Max Intensity]");
run("Auto Threshold", "method=Otsu white");
run("Analyze Particles...", "size=3-Infinity circularity=0.20-0.8 show=[Overlay Masks] summarize stack");

//overlapping markers
run("Split Channels");
imageCalculator("AND create", "c1","c2");
run("Auto Threshold", "method=Otsu white");
run("Analyze Particles...", "size=3.00-Infinity circularity=0.20-0.80 show=[Overlay Masks] summarize stack");
</code></pre>
<p>I tried following this tutorial (<a href="https://forum.image.sc/t/combining-several-summary-of-image-title-into-a-single-results-table/40152/6" class="inline-onebox">Combining several "Summary of [image title]" into a single results table - #6 by smith6jt</a>), specifically Antinos’s comments in Nov 2020. The “Analysis” table generated by the script is yielding unexpected results. Specifically, it only reports the number of markers detected in one of the channels, and occasionally shows a count of zero. In order to confirm the presence of markers, I reviewed the original images and verified their presence.</p>
<p>I really appreciate any help you can provide.</p> ;;;; <p>Thanks for your suggestion <a class="mention" href="/u/mweigert">@mweigert</a>, I’ve updated my <code>StardistSequence</code> to look like this:</p>
<pre><code class="lang-auto">class StardistSequence(tf.keras.utils.Sequence):

    def __init__(self, csv_path, name, batch_size=BATCH_SIZE, type_="train"):
        super().__init__()
        df = pd.read_csv(csv_path, index_col=False)
        self._imgs = df["raw"]
        self._masks = df["mask"]
        self._name = name
        self._batch_size = batch_size
        print(f"{name} {type_} - Batch size: {batch_size} dataset created!")

    def __len__(self):
        return np.ceil(len(self._masks) / self._batch_size).astype(np.uint8)

    def preprocess_x(self, path):
        axis_norm = (0, 1)
        im = io.imread(path, as_gray=True)
        im = exposure.adjust_gamma(im, .5)
        im = normalize(im, 1, 99.8, axis=axis_norm)
        return im
    
    def preprocess_y(self, path):
        im = io.imread(path, as_gray=True)
        im = fill_label_holes(im)
        im = (im / 255).astype(np.float32)
        return im
    
    @lru_cache(100)
    def __getitem__(self, idx):
        img_path = self._imgs.iloc[idx]
        mask_path = self._masks.iloc[idx]
        img = self.preprocess_x(img_path)
        mask = self.preprocess_y(mask_path)
        return img, mask
</code></pre>
<p>So every time I call <code>__getitem__()</code> through indexation I get a tuple <code>(img, mask)</code>.</p>
<p>Then my sequence instance creation looks like this:</p>
<pre><code class="lang-auto">train_seq = StardistSequence(train_csv_path, name, type_="train")
val_seq = StardistSequence(val_csv_path, name, type_="val")
</code></pre>
<p>But how can I separate <code>X_train</code> and <code>Y_train</code> from <code>train_seq</code>  to pass them to <code>model.train()</code> as <code>X</code> and <code>Y</code>? (and similar for <code>val_seq</code>).</p> ;;;; <p>Hi,</p>
<p><code>StardistSequence</code> should return a single <code>image, mask</code> pair (not a batch).</p>
<p>Hope that helps,</p>
<p>M</p> ;;;; <p>For those stumbling upon this topic and wondering if <a class="hashtag" href="/tag/napari">#<span>napari</span></a> has something similar to offer: the pull request linked below seems to offer what I was looking to achieve here (and beyond that):</p>
<aside class="onebox githubpullrequest" data-onebox-src="https://github.com/napari/napari/pull/5522">
  <header class="source">

      <a href="https://github.com/napari/napari/pull/5522" target="_blank" rel="noopener">github.com/napari/napari</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">



    <div class="github-icon-container" title="Pull Request">
      <svg width="60" height="60" class="github-icon" viewbox="0 0 12 16" aria-hidden="true"><path fill-rule="evenodd" d="M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"></path></svg>
    </div>

  <div class="github-info-container">



      <h4>
        <a href="https://github.com/napari/napari/pull/5522" target="_blank" rel="noopener">Thick Slices (Dims  as nD-box instead of nD-point)</a>
      </h4>

    <div class="branches">
      <code>napari:main</code> ← <code>brisvag:feature/dims-span</code>
    </div>

      <div class="github-info">
        <div class="date">
          opened <span class="discourse-local-date" data-format="ll" data-date="2023-02-01" data-time="14:26:45" data-timezone="UTC">02:26PM - 01 Feb 23 UTC</span>
        </div>

        <div class="user">
          <a href="https://github.com/brisvag" target="_blank" rel="noopener">
            <img alt="brisvag" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e07ffe67a07bb19c3fcff9219a44ea3c49f4acec.jpeg" class="onebox-avatar-inline" width="20" height="20">
            brisvag
          </a>
        </div>

        <div class="lines" title="41 commits changed 20 files with 690 additions and 319 deletions">
          <a href="https://github.com/napari/napari/pull/5522/files" target="_blank" rel="noopener">
            <span class="added">+690</span>
            <span class="removed">-319</span>
          </a>
        </div>
      </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container"># Description
Finally reviving this old code to upgrade our `Dims` model from a<span class="show-more-container"><a href="https://github.com/napari/napari/pull/5522" target="_blank" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden"> single point to an n-dimensional bounding box.

*NOTE: currently includes changes from #5528, so refer to that one for the changes to `EventedModel` itself!*

The basic change is that instead of defining the current `Dims` "position" by an nD point (n-tuple), it is instead defined by a `span`, which is an n-tuple of 2-tuples, which describes the range of world coordinates that are considered "visible". Everything is defined in world coordinates.

This results in a visible space with higher dimensionality than `ndisplay`, and thus needs to be projected onto the actual canvas space as preferred.

Example functionality once layer are hooked up (from https://github.com/napari/napari/pull/4012#issuecomment-1049850169):

https://user-images.githubusercontent.com/23482191/155531043-644cfa4c-e784-4d01-9d2c-89f7c3235b72.mp4

---

With this PR, the main "primary" fields for `Dims` look like this:

Same as before:
- `ndim`
- `ndisplay`
- `order`
- `axis_labels`

Changed:
- `range`: the last component (the `step`) was split out into its own field
- `step`: used to be in `range` (indicates how much to move in world space with 1 tick of the slider/one arrow click)

Removed:
- `current_step`: indicated the current position in "slider coordinates" (index of the step)

Added:
- `span`: indicates the current bounding box as a `Tuple[Tuple[float, float], ...]` in *world space*

There are also several additions to properties and methods that help with dealing with the setters:

Changed properties:
- `nsteps`: now settable (will change `step` to match the requested number of steps)
- `current_step`: is now deprecated in favor of `point_step` (see below)
- `displayed`: can now be set (will affect `self.order`)

Added properties:
- `thickness`: returns the thickness of the bounding box in each dimension. Can be set, which will set new values for `span` by extending the limits symmetrically.
- `point`: returns the middle point of `span` for each dimension. If set, will move both limits in `step` to match the new center.

For most of the above, there is also a `_step` version (e.g: `point_step`, `_thickness_step`, which returns/sets the same values as the normal counterpart, but in "slider space" rather than world space. So the equivalent of the old `current_step` is now `point_step`. Please feel free to suggest another naming scheme if you have better ideas!

Finally, I added `set_X` utility methods just like we had for the existing fields. They basically allow to set a single component of dims without having to do this all the time:
```
new_range = list(dims.range)
new_range[2] = new_value
dims.range = new_range
```

---

Other relevant notes:
- now everything is in world space, and anything which is not gets computed as a property (differently from main, where `current_step` is in "slider space" and is the primary source of the dims state).
- one critique I received the first time around whas that this was increasing the surface of the public API too much, so I kept most of the new stuff private, when possible. Does the current state look reasonable, or should I privatize more/less?
- one thing that I changed but was not *strictly* necessary is that `range` is now just a 2-tuple, and the `step` is held separately. They are often needed together, but just as often separately, and to me they made no sense in the same object. This simplified a lot of the work in `Dims`, but is not backward compatible and required some changes elsewhere in the codebase.
- there were many previous iterations and a lot of discussion around the implementation (particularly around using evented containers for a more ergonomic interface). Please refer to the Reference section below for a complete history, but **TL;DR: nested evented containers with non-buggy validation are impossible or unreasonably slow with our current `EventedModel`**.

## Type of change

- [ ] Bug-fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [x] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] This change requires a documentation update

# References
- original implementation (which ultimately is very close to this :P): #4012
- example of how layers could be hooked up to it: https://github.com/napari/napari/pull/4012#issuecomment-1049850169 and https://github.com/napari/napari/pull/4012#issuecomment-1057098537
- broken version with evented containers: #4334 
- some extra stuff that didn't work out to make working with evented objects and property better:
	- #4417 
	- #4457
- ... and the "Thick slices" PR that came out of these: #4448, #4522
- attempts at getting nested evented models to work:
	- #4474 
	- #4597 
	- #4609 
	- #4804 

# How has this been tested?

- [ ] example: the test suite for my feature covers cases x, y, and z
- [ ] example: all tests pass with my change
- [ ] example: I check if my changes works with both PySide and PyQt backends
      as there are small differences between the two Qt bindings.  

## Final checklist:
- [ ] My PR is the minimum possible work for the desired functionality
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] If I included new strings, I have used `trans.` to make them localizable.
      For more information see our [translations guide](https://napari.org/developers/translations.html).</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>I would like to train the Stardist model using a custom dataset, I’ve installed the packages needed using the documentation from the <a href="https://github.com/stardist/stardist" rel="noopener nofollow ugc">repo’s README file</a>.<br>
I’ve gathered all the data and created 3 independent datasets (train/validation/test) each of these contain patches of images that I would like to use as input data for the model.<br>
As the size of these files would be too big to fit in memory all at once, I was thinking if it was possible to train using batches as I’ve done previously using <code>pytorch</code>. I’ve read in the <a href="https://stardist.net/docs/faq.html#what-if-my-training-dataset-does-not-fit-into-cpu-memory" rel="noopener nofollow ugc">FAQs</a> that it is possible and that I need to create my own <code>tf.keras.utils.Sequence</code> class for each of my datasets so this is what my code looks like right now:</p>
<p>My <code>Sequence</code> class:</p>
<pre><code class="lang-auto">class StardistSequence(tf.keras.utils.Sequence):

    def __init__(self, csv_path, name, batch_size=BATCH_SIZE, key="raw", type_="train"):
        super().__init__()
        df = pd.read_csv(csv_path, index_col=False)
        self._filenames = df[key]
        self._key = key
        self._name = name
        self._batch_size = batch_size
        print(f"{name} {type_} - {key} Batch size: {batch_size} dataset created!")

    def __len__(self):
        return np.ceil(len(self._filenames) / self._batch_size).astype(np.uint8)

    def preprocess_x(self, path):
        axis_norm = (0, 1)
        im = io.imread(path, as_gray=True)
        im = exposure.adjust_gamma(im, .5)
        im = normalize(im, 1, 99.8, axis=axis_norm)
        return im
    
    def preprocess_y(self, path):
        im = io.imread(path, as_gray=True)
        return fill_label_holes(im)
    
    @lru_cache(100)
    def __getitem__(self, idx):
        batch = self._filenames.iloc[idx * self._batch_size:(idx + 1) * self._batch_size]
        preprocess_fn = self.preprocess_x if self._key == "raw" else self.preprocess_y
        batch = np.array([preprocess_fn(file_name) for file_name in batch])
        return batch
</code></pre>
<p>Sequence creation:</p>
<pre><code class="lang-auto">name = "stardist-test"
X_train_seq = StardistSequence(train_csv_path, name, key="raw", type_="train")
y_train_seq = StardistSequence(train_csv_path, name, key="mask", type_="train")
X_val_seq = StardistSequence(val_csv_path, name, key="raw", type_="val")
y_val_seq = StardistSequence(val_csv_path, name, key="mask", type_="val")
</code></pre>
<p>All my input images are grayscale images with values between <code>0.</code> and <code>1.</code> and all my masks are binary with values between <code>0</code> or <code>255</code>. For example, here we can see that I have 47 batches and each contains 64 patches of 256x256.</p>
<pre><code class="lang-auto">print(len(X_train_seq), X_train_seq[0].shape)
&gt;&gt;&gt; (47, (64, 256, 256))
</code></pre>
<p>Configuring the model:</p>
<pre><code class="lang-auto">n_rays = 32  #Number of radial directions for the star-convex polygon.

# Use OpenCL-based computations for data generator during training (requires 'gputools')
use_gpu = True #False and gputools_available()

# Predict on subsampled grid for increased efficiency and larger field of view
grid = (2,2)

conf = Config2D (
    n_rays       = n_rays,
    grid         = grid,
    use_gpu      = use_gpu,
    n_channel_in = n_channel,
    train_sample_cache = False,
    train_batch_size= BATCH_SIZE, # BATCH_SIZE = 64
)

vars(conf)
&gt;&gt;&gt; {'n_dim': 2,
 'axes': 'YXC',
 'n_channel_in': 1,
 'n_channel_out': 33,
 'train_checkpoint': 'weights_best.h5',
 'train_checkpoint_last': 'weights_last.h5',
 'train_checkpoint_epoch': 'weights_now.h5',
 'n_rays': 32,
 'grid': (2, 2),
 'backbone': 'unet',
 'n_classes': None,
 'unet_n_depth': 3,
 'unet_kernel_size': (3, 3),
 'unet_n_filter_base': 32,
 'unet_n_conv_per_depth': 2,
 'unet_pool': (2, 2),
 'unet_activation': 'relu',
 'unet_last_activation': 'relu',
 'unet_batch_norm': False,
 'unet_dropout': 0.0,
 'unet_prefix': '',
 'net_conv_after_unet': 128,
 'net_input_shape': (None, None, 1),
 'net_mask_shape': (None, None, 1),
 'train_shape_completion': False,
 'train_completion_crop': 32,
 'train_patch_size': (256, 256),
 'train_background_reg': 0.0001,
 'train_foreground_only': 0.9,
 'train_sample_cache': False,
 'train_dist_loss': 'mae',
 'train_loss_weights': (1, 0.2),
 'train_class_weights': (1, 1),
 'train_epochs': 400,
 'train_steps_per_epoch': 100,
 'train_learning_rate': 0.0003,
 'train_batch_size': 64,
 'train_n_val_patches': None,
 'train_tensorboard': True,
 'train_reduce_lr': {'factor': 0.5, 'patience': 40, 'min_delta': 0},
 'use_gpu': True}
</code></pre>
<p>Creating a model instance using such configuration and attempting to train (I’m also using an augmenter function):</p>
<pre><code class="lang-auto">model = StarDist2D(conf, name='stardist_test', basedir='stardist_models')
model.train(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), augmenter=augmenter, epochs=10, steps_per_epoch=len(X_val_seq))
</code></pre>
<p>I get the following error:</p>
<pre><code class="lang-auto">---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [75], in &lt;cell line: 1&gt;()
----&gt; 1 model.train(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), augmenter=augmenter, epochs=10, steps_per_epoch=len(X_val_seq))

File ~/.local/lib/python3.10/site-packages/stardist/models/model2d.py:423, in StarDist2D.train(self, X, Y, validation_data, classes, augmenter, seed, epochs, steps_per_epoch, workers)
    421 n_take = self.config.train_n_val_patches if self.config.train_n_val_patches is not None else n_data_val
    422 _data_val = StarDistData2D(validation_data[0],validation_data[1], classes=classes_val, batch_size=n_take, length=1, **data_kwargs)
--&gt; 423 data_val = _data_val[0]
    425 # expose data generator as member for general diagnostics
    426 self.data_train = StarDistData2D(X, Y, classes=classes, batch_size=self.config.train_batch_size,
    427                                  augmenter=augmenter, length=epochs*steps_per_epoch, **data_kwargs)

File ~/.local/lib/python3.10/site-packages/stardist/models/model2d.py:51, in StarDistData2D.__getitem__(self, i)
     49 def __getitem__(self, i):
     50     idx = self.batch(i)
---&gt; 51     arrays = [sample_patches((self.Y[k],) + self.channels_as_tuple(self.X[k]),
     52                              patch_size=self.patch_size, n_samples=1,
     53                              valid_inds=self.get_valid_inds(k)) for k in idx]
     55     if self.n_channel is None:
     56         X, Y = list(zip(*[(x[0][self.b],y[0]) for y,x in arrays]))

File ~/.local/lib/python3.10/site-packages/stardist/models/model2d.py:53, in &lt;listcomp&gt;(.0)
     49 def __getitem__(self, i):
     50     idx = self.batch(i)
     51     arrays = [sample_patches((self.Y[k],) + self.channels_as_tuple(self.X[k]),
     52                              patch_size=self.patch_size, n_samples=1,
---&gt; 53                              valid_inds=self.get_valid_inds(k)) for k in idx]
     55     if self.n_channel is None:
     56         X, Y = list(zip(*[(x[0][self.b],y[0]) for y,x in arrays]))

File ~/.local/lib/python3.10/site-packages/stardist/models/base.py:202, in StarDistDataBase.get_valid_inds(self, k, foreground_prob)
    200 else:
    201     patch_filter = (lambda y,p: self.max_filter(y, self.maxfilter_patch_size) &gt; 0) if foreground_only else None
--&gt; 202     inds = get_valid_inds(self.Y[k], self.patch_size, patch_filter=patch_filter)
    203     if self.sample_ind_cache:
    204         with self.lock:

File ~/.local/lib/python3.10/site-packages/stardist/sample_patches.py:47, in get_valid_inds(img, patch_size, patch_filter)
     34 def get_valid_inds(img, patch_size, patch_filter=None):
     35     """
     36     Returns all indices of an image that 
     37     - can be used as center points for sampling patches of a given patch_size, and
   (...)
     44         a function with signature patch_filter(img, patch_size) returning a boolean mask 
     45     """
---&gt; 47     len(patch_size)==img.ndim or _raise(ValueError())
     49     if not all(( 0 &lt; s &lt;= d for s,d in zip(patch_size,img.shape))):
     50         raise ValueError("patch_size %s negative or larger than image shape %s along some dimensions" % (str(patch_size), str(img.shape)))

File ~/.local/lib/python3.10/site-packages/csbdeep/utils/utils.py:91, in _raise(e)
     89 def _raise(e):
     90     if isinstance(e, BaseException):
---&gt; 91         raise e
     92     else:
     93         raise ValueError(e)

ValueError: 
</code></pre>
<p>Clearly we don’t have a message for it but this comparison seems to be the problem: <code>len(patch_size)==img.ndim</code> where <code>len(patch_size)</code> is <code>2</code> as it is <code>(256, 256)</code> but <code>img.ndim</code> is <code>3</code> as it is <code>(64, 256, 256)</code>.</p>
<p>Unfortunately I couldn’t find an example that suited my needs nor an issue created in the repo. I was about to create one but it showed me an option to find the answer in this forum. I’ve searched for something similar here but was unable to find anything useful so that is why I created this thread.<br>
The questions I would like to ask would be: what am I missing?, did I forgot to configure something?, is my <code>StardistSequence</code> class well defined? or is it something else?</p>
<p>Thanks in advance!</p> ;;;; <p>I use this script, made by a colleague, to convert my IF images from the proprietary Leica .LIF file type to .TIF files, while preserving all the information from each image. Meaning it exports all of the images out of a single .lif project file into separate .tif files, and also a montage of each channel side-by-side in a jpg.</p>
<p>For some reason however, if a .lif file has too many images in it, it only converts the first handful of images and then ignores the rest. Splitting up the .lif file solves the issue, but is rather inconvenient. Any ideas as to why this might be happening and how to address it?</p>
<pre><code class="lang-javascript">// "lif2tif"
// Saves all open image windows as tif file, in the specified directory

setBatchMode(true); 
input = getDirectory("Input directory, folder where your .lif file is");
output = getDirectory("Output directory, where you'd like your adjusted .tif files to go");

//Dialog.create("File type");
//Dialog.addString("File suffix: ", ".tif", 5);
//Dialog.show();
suffix = ".lif";//Dialog.getString();

processFolder(input);

function processFolder(input) {
	list = getFileList(input);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(input + list[i]))
			processFolder("" + input + list[i]);
		if(endsWith(list[i], suffix))
			processFile(input, output, list[i]);
	}
}

function processFile(input, output, file) {
	// do the processing here by replacing
	// the following two lines by your own code
	print("Processing: " + input + file);
	openLif(input+file);
	print("Saving to: " + output);
	listImages();
}

function openLif(input){
	aa = seriesN(80);
	print(aa);
	print(input+" color_mode=Default view=[Standard ImageJ] stack_order=XYZCT "+aa);
	run("Bio-Formats Importer", "open="+input+" color_mode=Default view=[Standard ImageJ] stack_order=XYZCT "+aa);
}

//Create string "series_1 series_2 series_3 series_4...."
function seriesN(num){
	str = "";
	for (i=0; i&lt;num; i++){
		ii = i+1;
        str = str+"series_"+ii+" ";
     }
     return str;
}

//Save
function listImages(){
	imageList = getList("image.titles");
	if (imageList.length==0)
	     print("No image windows are open");
	else {
    	print("Image windows:");
     	for (i=0; i&lt;imageList.length; i++){
        	print("   "+imageList[i]);
        	saveTiff(imageList[i]);
     	}
	}
	print("");
}


function saveTiff(winName){
	selectWindow(winName); 
	saveAs("Tiff", output + winName);
	close();
}
</code></pre> ;;;; <p>One more note: option 2 actually does not work for ome.zarr yet, only for bdv fileformats.<br>
I have started a PR to work on this: <a href="https://github.com/mobie/mobie-utils-python/pull/94" class="inline-onebox">Start implementing transformation support for ome.zarr by constantinpape · Pull Request #94 · mobie/mobie-utils-python · GitHub</a></p> ;;;; <p>Hey <a class="mention" href="/u/camachodejay">@CamachoDejay</a>,<br>
your understanding of what happens is pretty much correct, but let me add a bit more context.<br>
When you add a new image source to MoBIE, you have two options for how you can specify a transformation that should be applied to it:</p>
<ol>
<li>by adding it to the <code>view</code> for the source (what you do in the example notebook you have linked). In this case, the transformation is written to the view that is being created for the source, but (as you correctly realized), it will not be automatically used if you create other subsequent views that include the source you have added.</li>
<li>by passing the transformation as value for the <code>transformation</code> argument (see <a href="https://github.com/mobie/mobie-utils-python/blob/977120d6b90154211e6c1c121a77e1d9e13e4186/mobie/image_data.py#L165" class="inline-onebox">mobie-utils-python/image_data.py at 977120d6b90154211e6c1c121a77e1d9e13e4186 · mobie/mobie-utils-python · GitHub</a>). In this case the transformation will be written to the underlying image metadata (i.e. to the bdv xml file or the ome.zarr metadata). In this case the transformation will always be applied when the source is loaded, and any transformations specified in a <code>view</code> are applied on top of the transformation in the image metadata.</li>
</ol>
<p>So in practice this means: if you have a transformation that should always be applied (i.e. also in subsequent views you would want to create), use option 2. If you potentially want to create views without the transformation then use option 1.</p>
<p>Note that this is how things should work in theory, if you try it and run into any unexpected behavior please let me know; since there might be some cases I haven’t run into and that don’t fully work yet.<br>
Also, all of this is clearly not documented well enough. Any contribution to document it better (for example through example notebooks that show different ways how to add transformations and views) would be highly welcome ;).</p> ;;;; <p>The releases are available now, you need to update</p>
<ul>
<li>elf to version 0.4.6</li>
<li>mobie-python 0.4.3</li>
</ul>
<p>e.g. by running <code>conda install -c conda-forge "python-elf&gt;=0.4.6 mobie_utils&gt;=0.4.3"</code> in your conda env.</p>
<p>This should fix all issues; let me know if not.</p> ;;;; <p><a class="mention" href="/u/fengzhiheng">@FengZhiheng</a> ,</p>
<aside class="quote no-group" data-username="FengZhiheng" data-post="5" data-topic="22967">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fengzhiheng/40/42465_2.png" class="avatar"> FengZhiheng:</div>
<blockquote>
<p>what shape of one images? Is it the same as input image shape?</p>
</blockquote>
</aside>
<p>Yes, usually the output images are the same size / shape as the input image.</p> ;;;; <p>Hi <a class="mention" href="/u/librethinker">@librethinker</a>,</p>
<p>This bug is fixed in the ImageJ 1.54d5 daily build.</p> ;;;; <p>Try using the software MIPAR. I suppose it can give you decent results.<br>
But it is a proprietary software.</p> ;;;; <p>Thanks Biovoxxel, that looks quite helpful</p> ;;;; <p>Hello to everyone.I have a problem to create a new project with deeplabcut gui. I can’t add my video when creating.<br>
cant use the choose button.<br>
I’m using win10 and deeplabcut 2.3.0<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2.png" data-download-href="/uploads/short-url/lVt5ELjtJyUR4rXFIt8Rhc2q1Kq.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2_2_690x468.png" alt="image" data-base62-sha1="lVt5ELjtJyUR4rXFIt8Rhc2q1Kq" width="690" height="468" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2_2_690x468.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2.png 2x" data-dominant-color="2B3640"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">995×675 37.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
I’m wondering if someone can help me. <img src="https://emoji.discourse-cdn.com/twitter/sob.png?v=12" title=":sob:" class="emoji" alt=":sob:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/sob.png?v=12" title=":sob:" class="emoji" alt=":sob:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/sob.png?v=12" title=":sob:" class="emoji" alt=":sob:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/sob.png?v=12" title=":sob:" class="emoji" alt=":sob:" loading="lazy" width="20" height="20"></p> ;;;; <p>Beside the previous suggestions have also a look at this simple solution if it fits your purpose:</p>
<aside class="quote" data-post="1" data-topic="26065">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/p/3ab097/40.png" class="avatar">
    <a href="https://forum.image.sc/t/distance-between-polygons/26065">Distance between polygons</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hello, 
I would like to measure the distances between the two membranes of bacteria. 
I can do it manually but this is fastidious as it results in 400-500 measures per bacterium. 
I figured that I can easily delimits the membranes with the polygon function but I cannot find a way to measure distances between the two polygons at defined intervals. 
Sorry if it is obvious, but I am only starting to learn how to use ImageJ 
Thank you for your help, 
Pierre
  </blockquote>
</aside>
 ;;;; <p>Hi.</p>
<p>I have been trying to generate analysed videos with DeepLabCut using trained models I’ve received from my Lab. I’m trying to process one video at a time. The video gets processed and plot-poses, pickle, and h5 files are created. The csv is generated when I try to run the script manually but not for some reason when I run the script through a process spawned by NodeJS.</p>
<p>Python Code:</p>
<pre><code class="lang-auto">import os
os.environ["DLClight"]="True"

import deeplabcut
import argparse

import argparse
parser = argparse.ArgumentParser(description='MobileNet 5 marker models')
parser.add_argument('dir', type=str, help='absolute path for the directory for this analysis. must end with a /')
parser.add_argument('video', type=str, help='name of the video file. tested only with mp4, supply that.')
parser.add_argument('dirpath', type=str, help='name of the dir path to the config file')
args = parser.parse_args()
print(args.__dict__)

videofile_path = [args.dir  + args.video]
videofile = args.dir 
path_config_file = args.dirpath +'/config.yaml'

print('NEW project_path: ' + args.dir)
print('NEW path_config_file: ' + path_config_file)
print('NEW videofile_path: ' + videofile_path[0])

if __name__ == "__main__":
    deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='mp4')

    deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype='mp4')

    deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype='mp4')

    deeplabcut.analyze_videos_converth5_to_csv(videofile,'.mp4')
</code></pre>
<p>NodeJS code:</p>
<pre><code class="lang-auto">const model = spawn('python', [
      `models/model.py`, // refers to the above python code
      `uploads/${fileDir}/`, // dir where the video is supposed to be located
      `${fileName}`, // the name of the video file
      `${modelDir}` // the dir where the model's config file is present
    ])
</code></pre>
<p>Things I’ve tried:</p>
<ol>
<li>I thought it could be an issue with the file name containing <code>-</code> but removing that did no change, the video was still not generated.</li>
<li>I tried putting the code and the model in a separate folder to test and remove arguments and statically provided values via variables. The frames are again analysed successfully but no video is rendered or csv file is generated. Code below.</li>
</ol>
<p>Another thing that happens always after the video is analysed, I get the following error:</p>
<pre><code class="lang-auto">Starting to process video: videos/v.mp4
Loading videos/v.mp4 and data.
[Errno 2] No such file or directory: 'videos'
Traceback (most recent call last):
  File "/path/to/models/model.py", line 32, in &lt;module&gt;
    deeplabcut.analyze_videos_converth5_to_csv(videofile,'.mp4')
  File "/Users/mk/Downloads/DeepLabCut/deeplabcut/utils/conversioncode.py", line 130, in analyze_videos_converth5_to_csv
    h5_files = list(
  File "/Users/mk/Downloads/DeepLabCut/deeplabcut/utils/auxiliaryfunctions.py", line 436, in grab_files_in_folder
    for file in os.listdir(folder):
FileNotFoundError: [Errno 2] No such file or directory: 'v.mp4'
</code></pre>
<p>in this case, v.mp4 is the video it just analysed and is trying to analyse again and failing with this error.</p>
<p>Code for the second case of what I tried</p>
<pre><code class="lang-auto">import os
os.environ["DLClight"]="True"

import deeplabcut
# import argparse

# import argparse
# parser = argparse.ArgumentParser(description='MobileNet 5 marker models')
# parser.add_argument('dir', type=str, help='absolute path for the directory for this analysis. must end with a /')
# parser.add_argument('video', type=str, help='name of the video file. tested only with mp4, supply that.')
# parser.add_argument('dirpath', type=str, help='name of the dir path to the config file')
# args = parser.parse_args()
# print(args.__dict__)

videofile_path = ['videos/' + 'v.mp4']
videofile = 'v.mp4'
path_config_file = 'config.yaml'

# print('NEW project_path: ' + args.dir)
print('NEW path_config_file: ' + path_config_file)
print('NEW videofile_path: ' + videofile_path[0])

if __name__ == "__main__":
    deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='mp4')

    deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype='mp4')

    deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype='mp4')

    deeplabcut.analyze_videos_converth5_to_csv(videofile,'.mp4')
</code></pre>
<p>Thank you reading the question. All help is highly appreciated.<br>
Thank you.</p> ;;;; <p>Hey, thanks a lot for the file. I am still very new to a lot of this and wanted to ask how you could go about placing the json file in the modelzoo.py?</p> ;;;; <p>And if it is convenient for you, could you please share figures to show how you nominate these tiffs images and how you write you path while running cellfinder??</p>
<p>Thanks a lot!</p> ;;;; <p>Hello,</p>
<p>I sincerely appreciate all your help!</p>
<p>It is really strange, could you please share you envs and pkgs in your .conda file through email?</p>
<p>I am really doubting there are some wrongs for my installation but I should do it according to your website… I have retried it on another computer but just met the same issues…</p>
<p>Thanks again！</p> ;;;; <p>Hi <a class="mention" href="/u/moreauem">@MoreauEM</a> I’ve moved this to its own topic since it seems a different question.</p>
<p>It’s not clear what you mean by ‘half the screen’. Do you mean over half the image? And with which orientation (top/bottom, left/right)?</p>
<p>(Explaining why you want this would also help understand it better, and reduce the risk of anyone writing scripts that don’t actually solve the real problem.)</p> ;;;; <p>Hi <a class="mention" href="/u/zeratoss">@zeratoss</a>,</p>
<p>General info and overview:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://arxiv.org/abs/2206.01653">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bac99e0da013a3e1e7e363ea82e1aff26e8bf11b.png" class="site-icon" width="16" height="16">

      <a href="https://arxiv.org/abs/2206.01653" target="_blank" rel="noopener">arXiv.org</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_500x500.png" class="thumbnail onebox-avatar" width="500" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_1000x1000.png 2x" data-dominant-color="865F5C">

<h3><a href="https://arxiv.org/abs/2206.01653" target="_blank" rel="noopener">Metrics reloaded: Pitfalls and recommendations for image analysis validation</a></h3>

  <p>Increasing evidence shows that flaws in machine learning (ML) algorithm
validation are an underestimated global problem. Particularly in automatic
biomedical image analysis, chosen performance metrics often do not reflect the
domain interest, thus...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>An IJ script (disclaimer: I have not tested it):</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://imagej.net/tutorials/segmentation-evaluation-metrics">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/17cd920cc3af5597be55618d6d6ea4a54809d2eb.png" class="site-icon" width="32" height="32">

      <a href="https://imagej.net/tutorials/segmentation-evaluation-metrics" target="_blank" rel="noopener">ImageJ Wiki</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d255a5176a70445402413f516b0a0776a37504cd.png" class="thumbnail onebox-avatar" width="256" height="256">

<h3><a href="https://imagej.net/tutorials/segmentation-evaluation-metrics" target="_blank" rel="noopener">Segmentation evaluation metrics - Script</a></h3>

  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/sima">@Sima</a><br>
Have you checked out this theme?</p><aside class="quote quote-modified" data-post="3" data-topic="78386">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/5daacb/40.png" class="avatar">
    <a href="https://forum.image.sc/t/measuring-the-distance-between-two-varying-contours/78386/3">Measuring the distance between two varying contours</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hi <a class="mention" href="/u/carsten2023">@Carsten2023</a> 
Please provide feedback. 
Thanks in advance. 
Here is the result of using the macro below. 
 <a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed.png" data-download-href="/uploads/short-url/yj2ShLVEtmbUrFX5M0CjL9vuSsJ.png?dl=1" title="Image capturée-10-03-2023 19-29-24" rel="noopener nofollow ugc">[Image capturée-10-03-2023 19-29-24]</a> 
macro "distance between contours"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
img=getImageID();
//---------------------------
// Start batch mode
setBatchMode(true);
selectImage(img);
run("Duplicate...", "title=1");
close("\\Others")
//-------------------------------
// Start image processing
h=getHeight();
w=ge…
  </blockquote>
</aside>
<p>
Such an approach is possible, if you define how to measure the thickness. That is to say: how should the segments be?<br>
For a good understanding, we would need:</p>
<ol>
<li>cleanly annotated image</li>
<li>an un-annotated image for testing</li>
</ol>
<p><a class="mention" href="/u/demehdix">@Demehdix</a><br>
(see comment on your other request)<br>
Is it acceptable to work (find the thicknesses on the horizontal lines) on this extraction?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/00b0f77f1df5c9e8e50cf0a814caf846070f768d.jpeg" data-download-href="/uploads/short-url/679h0fyptYQR6IYvnKDiMRBixv.jpeg?dl=1" title="extraction" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00b0f77f1df5c9e8e50cf0a814caf846070f768d_2_50x250.jpeg" alt="extraction" data-base62-sha1="679h0fyptYQR6IYvnKDiMRBixv" width="50" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00b0f77f1df5c9e8e50cf0a814caf846070f768d_2_50x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00b0f77f1df5c9e8e50cf0a814caf846070f768d_2_75x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00b0f77f1df5c9e8e50cf0a814caf846070f768d_2_100x500.jpeg 2x" data-dominant-color="2D2D2D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">extraction</span><span class="informations">220×1100 11.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/demehdix">@Demehdix</a><br>
Have you checked out this theme?</p><aside class="quote quote-modified" data-post="3" data-topic="78386">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/5daacb/40.png" class="avatar">
    <a href="https://forum.image.sc/t/measuring-the-distance-between-two-varying-contours/78386/3">Measuring the distance between two varying contours</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hi <a class="mention" href="/u/carsten2023">@Carsten2023</a> 
Please provide feedback. 
Thanks in advance. 
Here is the result of using the macro below. 
 <a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed.png" data-download-href="/uploads/short-url/yj2ShLVEtmbUrFX5M0CjL9vuSsJ.png?dl=1" title="Image capturée-10-03-2023 19-29-24" rel="noopener nofollow ugc">[Image capturée-10-03-2023 19-29-24]</a> 
macro "distance between contours"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
img=getImageID();
//---------------------------
// Start batch mode
setBatchMode(true);
selectImage(img);
run("Duplicate...", "title=1");
close("\\Others")
//-------------------------------
// Start image processing
h=getHeight();
w=ge…
  </blockquote>
</aside>
<p>
Such an approach is possible, if you define how to measure the thickness. That is to say: how should the segments be?<br>
You have a nice discussion here on the definition of thickness measurements.</p><aside class="quote quote-modified" data-post="1" data-topic="9883">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/joost/40/57507_2.png" class="avatar">
    <a href="https://forum.image.sc/t/determine-average-width-of-irregular-line/9883">Determine average width of irregular line</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    Hey there! 
I have two (newbie) questions about ImageJ and analysis/math in general (if this is the right place to ask). 
For analysis of some of my data I need to obtain the average width of lines of my extruded material. Now I could just measure the width at several points and take the average, but I was wondering if it is possible to select the whole area and then calculate the average width of the entire line. I have tried changing the brightness and threshold to be able to automatically sel…
  </blockquote>
</aside>
 ;;;; <p>Great explanation, Thanks a lot for the help !</p>
<p>I was definitely not familiar enough with Generators and the yield keyword it seems.</p>
<p>Cheers</p> ;;;; <p>Thank your very much <a class="mention" href="/u/bogovicj">@bogovicj</a> . I understand that the output of the network will be N images, where N is the number of markers. But I have a small question, what shape of one images? Is it the same as input image shape?</p> ;;;; <p>Around 2011/2012, scikit-image was going in this direction. We even had a compatibility table, and was working to cover the same API. Except, we soon realized that this took the joy out of it: we were stuck with APIs that made no sense in Python, and were chasing someone else’s roadmap. Letting that go allowed scikit-image to evolve in a more natural way, while greatly improving developer morale.</p>
<p>That said, there is no reason not to have a document that explains a mapping from MATLAB features to skimage; we just don’t want to use it as a feature selection strategy.</p> ;;;; <p>It’s the size of one side of the pixel, and I believe that it snaps to even pixel amounts, regardless of what you put. So you are probably at either the exact pixel resolution or double the pixel resolution, or something like that. Depends on whether your pixel size metadata for the image is accurate though.</p> ;;;; <p>We try! <img src="https://emoji.discourse-cdn.com/twitter/joy.png?v=12" title=":joy:" class="emoji" alt=":joy:" loading="lazy" width="20" height="20"> But scikit-image is designed to <em>supplement</em> NumPy and SciPy for image processing. We typically don’t reimplement/rename things in those libraries. Come to think of it, one thing we would definitely welcome is a documentation webpage, “scikit-image for Matlab Image Processing Toolbox users”, where we would document the different ways to implement common toolbox functionality using scientific Python tools.</p> ;;;; <p>This might be a noob question, but I am struggling to find area distribution based on colors.<br>
I am attaching the original image and a sketch of what I intend to have in the processed image, the enclosed region of a specific color, and its area.<br>
<a href="https://shorturl.at/ryPVY" class="onebox" target="_blank" rel="noopener nofollow ugc">https://shorturl.at/ryPVY</a></p>
<p>I would appreciate any help regarding this.</p> ;;;; <p>Greetings,<br>
I am trying to calculate the mean, max, min, and Sd numbers of the distance between these two lines in an OCT image. Is there any method to help me calculate the thickness of this layer?<br>
P.S.: I used your excellent Micro, but my problem is that the drawn lines between two proposed lines seem to be few and I need the macro to draw more lines to get a more accurate measure of thickness between two lines.<br>
Raw Image:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2.jpeg" data-download-href="/uploads/short-url/w3KM4Nr4Otsi0XgBc0BhGhtBowq.jpeg?dl=1" title="Retinaa" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_690x301.jpeg" alt="Retinaa" data-base62-sha1="w3KM4Nr4Otsi0XgBc0BhGhtBowq" width="690" height="301" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2.jpeg 2x" data-dominant-color="4C4A4A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Retinaa</span><span class="informations">1162×508 76.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Using Macro:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/0416799e707040dca74a9962f6d09417718910f5.jpeg" data-download-href="/uploads/short-url/Aa3SgN7im7C396383asuctOmi1.jpeg?dl=1" title="Retina Post Micro" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/0416799e707040dca74a9962f6d09417718910f5_2_690x301.jpeg" alt="Retina Post Micro" data-base62-sha1="Aa3SgN7im7C396383asuctOmi1" width="690" height="301" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/0416799e707040dca74a9962f6d09417718910f5_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/0416799e707040dca74a9962f6d09417718910f5_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/0416799e707040dca74a9962f6d09417718910f5.jpeg 2x" data-dominant-color="4D4C4A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Retina Post Micro</span><span class="informations">1162×508 88.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Thanks</p> ;;;; <p>Hi I have segmentation labels I created with cellpose and labels a tissue expert created.</p>
<p>What would be a quick and dirty way to score the accuracy of the machine generated labels?</p>
<p>Something like b) in this figure from the cellpose paper:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441.webp" data-download-href="/uploads/short-url/spOh1Z3gG2o5wbDlXH8HKkrl1fj.webp?dl=1" title="" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441_2_645x499.webp" alt="" data-base62-sha1="spOh1Z3gG2o5wbDlXH8HKkrl1fj" role="presentation" width="645" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441_2_645x499.webp, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441.webp 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441.webp 2x" data-dominant-color="CBD3CF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename"></span><span class="informations">685×531 90.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
Thank you!</p>
<p>EDIT: Found this <a href="https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics/notebook" class="inline-onebox" rel="noopener nofollow ugc">All the segmentation metrics! | Kaggle</a> which contains exactly what I want</p> ;;;; <p>Hey <a class="mention" href="/u/melvingelbard">@melvingelbard</a>,</p>
<p>Would you know how to create two annotations each taking half the screen? Would be really helpful, thank you!</p> ;;;; <p>Let me rephrase:</p>
<ul>
<li>Bright parts of the nucleus contain DNA</li>
<li>DNA is distributed around the edges of a nucleus and as filaments within the nucleus</li>
<li>You’d like to exclude anything below a certain threshold, which might change over time due to photobleaching</li>
</ul>
<p>Right?</p>
<p>If so, the two-step process I proposed above should work. By segmenting the nuclei, you can calibrate a threshold for each frame such that it captures the feature you’re interested in. Using the mask you get from setting a threshold for each frame, you can measure what you need on the other image.</p>
<p>Unfortunately, I’m not familiar enough with CellProfiler to offer anything other than conceptual tips.</p>
<p>Leo</p> ;;;; <p>A new <a href="https://forum.image.sc/t/imagej-macro-to-measure-distance-between-two-lines-edges/42019">macro</a> came out recently that should do as you ask. Scroll to near the bottom to download the latest version of the macro and the instructions.</p> ;;;; <p>Perfect, that solved it <a class="mention" href="/u/petebankhead">@petebankhead</a> !</p>
<p>I put my work-in-progress code here for others’ reference. I also tried my hand at setting up an alternative approach with <code>pixelCalibration</code> but wasn’t able to get any output yet. I will update if/when I get there.</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy">
  <header class="source">

      <a href="https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy" target="_blank" rel="noopener nofollow ugc">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy" target="_blank" rel="noopener nofollow ugc">neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy</a></h4>


      <pre><code class="lang-groovy">// based on discussion here: https://forum.image.sc/t/scripted-densitymaps-and-exporting-to-image-file-with-imagej/78306/6
import qupath.lib.images.servers.PixelCalibration

// SET OUTPUT
def out_dir = buildFilePath(PROJECT_BASE_DIR, 'results_cell_counts')
mkdirs(out_dir)
// ************************************************//

def imageData = getCurrentImageData()
runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImageBrightfield": "Optical density sum",  "requestedPixelSizeMicrons": 0.0,  "backgroundRadiusMicrons": 0.0,  "medianRadiusMicrons": 0.0,  "sigmaMicrons": 1.5,  "minAreaMicrons": 10.0,  "maxAreaMicrons": 100.0,  "threshold": 0.23,  "maxBackground": 2.0,  "watershedPostProcess": true,  "excludeDAB": false,  "cellExpansionMicrons": 5.0,  "includeNuclei": true,  "smoothBoundaries": true,  "makeMeasurements": true}');

// // PIXEL SIZE CALCULATIONS FOR DOWNSAMPLING
// // this appears to slow processing down extremely (killed the process after &gt;20 mins when running on small ROI)
// // define target resolution for ouptut pixels, in calibrated units (um if possible!)
// // note that using the builder.buildClassifier(imageData) approach does not allow you to set your pixel sizes directly, so comparability outside of QuPath can be an issue across multiple images  
// double requestedPixelSize = 10
// // determine the downsampling factor, once we know what the pixel sizes are for our x and y in the original image
// double pixelSize = imageData.getServer().getPixelCalibration().getAveragedPixelSize()
// double scale_factor = requestedPixelSize / pixelSize
// // ***************************************************//
</code></pre>



  This file has been truncated. <a href="https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy" target="_blank" rel="noopener nofollow ugc">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Greetings,<br>
I am trying to calculate mean, max, min and Sd numbers of distance between these two lines in an OCT image. Is there any method to help me calculate the thickness of this layer?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2.jpeg" data-download-href="/uploads/short-url/w3KM4Nr4Otsi0XgBc0BhGhtBowq.jpeg?dl=1" title="Retinaa" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_690x301.jpeg" alt="Retinaa" data-base62-sha1="w3KM4Nr4Otsi0XgBc0BhGhtBowq" width="690" height="301" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2.jpeg 2x" data-dominant-color="4C4A4A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Retinaa</span><span class="informations">1162×508 76.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello everyone,</p>
<p>I also have questions regarding this topic and did not want to create another thread so soon after the creation of this one; I am looking at data for a 4x and 10x scan of the same wells and have used the default 2um mean intensity measurements for both. However, both the 4x and 10x have different pixel sizes (1.6252um by 1.6252um for 4x and .65um by .65um for 10 x) and I am wondering if I should go ahead and use 1.6252 and .65 for the 4x and 10x pixel sizes respectively to accurately compare their data (I am also unsure whether 1.6252 and .65 are the pixel size or whether their square is the pixel size).</p>
<p>Thank you</p> ;;;; <p>Try this:</p>
<pre><code class="lang-auto">def annots=getAnnotationObjects()
def sorted = annots.toSorted{it.getROI().getBoundsY()}

sorted.eachWithIndex{obj, i-&gt;
    obj.setName('Annot '+i)
}
</code></pre> ;;;; <aside class="quote no-group" data-username="BiologyTools" data-post="3" data-topic="78384">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/biologytools/40/66366_2.png" class="avatar"> Erik Repo:</div>
<blockquote>
<p>Is there a way to determine this using Bioformats?</p>
</blockquote>
</aside>
<p>Yes, see <a href="https://bio-formats.readthedocs.io/en/stable/developers/wsi.html" class="inline-onebox">Working with whole slide images — Bio-Formats 6.12.0 documentation</a></p>
<p>QuPath uses <code>setFlattenedResolutions(false)</code> when Bio-Formats is the reader – but the code used is quite involved, as it has evolved a lot over the years:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java">
  <header class="source">

      <a href="https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java" target="_blank" rel="noopener">qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java</a></h4>


      <pre><code class="lang-java">/*-
 * #%L
 * This file is part of QuPath.
 * %%
 * Copyright (C) 2014 - 2016 The Queen's University of Belfast, Northern Ireland
 * Contact: IP Management (ipmanagement@qub.ac.uk)
 * Copyright (C) 2018 - 2020 QuPath developers, The University of Edinburgh
 * %%
 * QuPath is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as
 * published by the Free Software Foundation, either version 3 of the
 * License, or (at your option) any later version.
 * 
 * QuPath is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License 
 * along with QuPath.  If not, see &lt;https://www.gnu.org/licenses/&gt;.
</code></pre>



  This file has been truncated. <a href="https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Is there a way to determine this using Bioformats? Otherwise I will need to read the tiff tags and determine it that way.</p> ;;;; <p>Hey, I was wondering if it is possible to change the name given depending on the position of my annotations (such as left or right of a certain X coordinate)? Also, is it possible to automate using the name of the image to set an annotation name?</p>
<p>This is the script I am using right now (from this thread)<br>
int counter = 1<br>
for (var ann: getAnnotationObjects()) {<br>
ann.setName(“211_CD20_Pre_0” + counter++)<br>
}</p> ;;;; <p>Hi <a class="mention" href="/u/jni">@jni</a> ! Thank you for the numpy equivalent. That should suffice for now.</p>
<p>It would be great if <code>scikit-image</code> could serve as a drop-in replacement for MATLAB’s Image Processing Library.</p> ;;;; <p>Dear <a class="mention" href="/u/hroberts49">@HRoberts49</a> ,<br>
please make sure that you selected the cp features according to the orbit documentation and use cp 2.x.x .<br>
I recommend to start with the cp pipeline provided at the orbit doc.</p>
<p>Regards,<br>
Manuel</p> ;;;; <p>I wanted to merge two stacks, one is the red channel and the other is the result of the Z-stack depth color code plugin, meaning that it’s color coded according to the depth. How can I do this without losing the colors? the idea is then visualize it in 3D viewer.</p>
<p>I tried merge both stacks in Image&gt;Color&gt;Merge Channels (Which is not ideal because I have to chose a color for the color coded stack). First, the program shows this warning saying that both channels need to have the same bit-depth. After I subjected the red channel to the same process, Z-stack depth colo, it accepts the merging but when it does it merges only one z plane.</p> ;;;; <p>Does anyone have any experience setting up Cellprofiler pipelines to use in Orbit? I am trying to analyze intensity of a slide with 3 fluorescence channels. I’m not entirely sure what I am doing wrong, as I am not even getting errors. I have followed the documentation on Orbit’s website. My measurements should output to a csv file, and it never appears. The tiles are created by Orbit and 6 folders are created with various sections of the ROI. Alternatively, does anyone have experience processing wholes slides using Cellprofiler? Thanks in advance!</p> ;;;; <p>Thank you for your replay.</p>
<p>In my work, I want to show a .czi WSI in my web-based front-end, so I need to cut the WSI into tiles. I know that the Low&amp;High value is stored in the display setting and I also used them. I used opencv and numpy to process the image like the following pseudo code.</p>
<pre><code class="lang-auto">tiles = []
for ch in range(channels):
    tile = crop_roi[ch]
    tile = np.clip(tile,low[ch],high[ch])
    tile = (tile-tile.min())/(tile.max()-tile.min())*255
    tile = tile.astype(np.uint8)
    tiles.append(tile)
apply_color_map(tiles)
combined_rgb = combine_tiles(tiles)
</code></pre>
<p>Using this method, the color of each combined tile will be different. However, I want a WSI with the same color. Maybe I should try to use cztile and matplotlib to process these images.</p>
<p>Thank you very much.</p> ;;;; <p>Hi <a class="mention" href="/u/carsten2023">@Carsten2023</a><br>
Please provide feedback.<br>
Thanks in advance.</p>
<p>Here is the result of using the macro below.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed.png" data-download-href="/uploads/short-url/yj2ShLVEtmbUrFX5M0CjL9vuSsJ.png?dl=1" title="Image capturée-10-03-2023 19-29-24" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_345x220.png" alt="Image capturée-10-03-2023 19-29-24" data-base62-sha1="yj2ShLVEtmbUrFX5M0CjL9vuSsJ" width="345" height="220" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_345x220.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_517x330.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_690x440.png 2x" data-dominant-color="F3F3F4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Image capturée-10-03-2023 19-29-24</span><span class="informations">971×620 56.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-auto">macro "distance between contours"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
img=getImageID();
//---------------------------
// Start batch mode
setBatchMode(true);
selectImage(img);
run("Duplicate...", "title=1");
close("\\Others")
//-------------------------------
// Start image processing
h=getHeight();
w=getWidth();
//-------------------------------
// scan vertical
for(i=0;i&lt;=h-1;i++){
w=getWidth();
//-------------------------------
// scan horizontal
n=0;
for(j=0;j&lt;=w-1;j++){
value=getValue(j,i);
if(value==0)
n++;
setResult("thickness",i,n);}
updateResults();
}
//-------------------------------
// Graphic representation of thickness
Plot.create("Plot of Results", "x", "thickness");
Plot.add("Line", Table.getColumn("thickness", "Results"));
Plot.setStyle(0, "blue,#a0a0ff,1.0,Line");
print("ended");
//-------------------------------
// End of processing
// End of batch mode
setBatchMode(false);
close();
exit();
}

</code></pre> ;;;; <p>By the way, another option is the windowing plugin from H. Glünder here:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://www.gluender.de/Miscellanea/MiscTexts/UtilitiesText.html#Gl-2019-4">
  <header class="source">

      <a href="https://www.gluender.de/Miscellanea/MiscTexts/UtilitiesText.html#Gl-2019-4" target="_blank" rel="noopener">gluender.de</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://www.gluender.de/Miscellanea/MiscTexts/UtilitiesText.html#Gl-2019-4" target="_blank" rel="noopener">ImageJ-PlugIns and -Macros as well as Classic MacOS™ Utilities</a></h3>

  <p>H. Gluender's  List of Selected Macintosh™ Utilities and Java ImageJ-PlugIns with Links to the ReadMe Documents and Downloads.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/cindy_zhu">@Cindy_Zhu</a>,</p>
<p>You can actually work with 5D data in 3Dscript (while only 4 dimensions, including channels and volume will be displayed at first).</p>
<p>Just in case there is some confusion:</p>
<p>The amount of channels does not have any influence on the bit depth of those.<br>
If you imaged with a specific bit-depth and save the raw data correctly (preferable in the original file format of the imaging software) then all channels are kept physically separate and in the original bit-depth. Just if you export images as Tiff files, some software combines the channels in an RGB color image and then your data is in most cases messed up and does not serve for analysis or reconstruction in 3Dscript.</p>
<p>The z-slices are independent of the channels, so you will not loose anything.</p>
<p>Premise is, that your data are existing as Hyperstack showing 1 separate slider for each individual dimension (C=channels, Z=volume, ►=time)</p> ;;;; <p>Hi <a class="mention" href="/u/steelec">@steelec</a>  you should create the density map builder with</p>
<pre><code class="lang-groovy">def builder = DensityMaps.builder(predicate)
</code></pre>
<p>In Java, it wouldn’t be possible to do it with <code>new DensityMaps.DensityMapParameters()</code> because the construction is private – unfortunately Groovy is very lax about this kind of thing, so permits it and the exception only occurs later.</p>
<p>The awkward thing is then to determine the predicate, which determines which objects will contribute to the map. It’s cumbersome because it needs to be JSON-serializable.</p>
<p>From your description, I think you want all detections - so this may work:</p>
<pre><code class="lang-groovy">def predicate = PathObjectPredicates.filter(PathObjectFilter.DETECTIONS_ALL)
DensityMaps.builder(predicate)
</code></pre> ;;;; <p>Use of pixelcalibration here <a href="https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579" class="inline-onebox">Script for generating double threshold classifier</a></p> ;;;; <p>In the script provided it looks like you only set the radius but not the pixel size or anything else, so might be the expected result?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7b669db015b139315c2dd6d98f1f25c21a54542.png" data-download-href="/uploads/short-url/uMhtsUCq2Nhdh784HG4ZoNPQCfo.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7b669db015b139315c2dd6d98f1f25c21a54542.png" alt="image" data-base62-sha1="uMhtsUCq2Nhdh784HG4ZoNPQCfo" width="608" height="500" data-dominant-color="3C3F41"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">865×711 21.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<aside class="quote no-group" data-username="steelec" data-post="6" data-topic="78306">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/steelec/40/68762_2.png" class="avatar"> Christopher J. Steele:</div>
<blockquote>
<p>Is it possible that I need to set additional parameters here?</p>
</blockquote>
</aside>
<p>This</p> ;;;; <p>Hi <a class="mention" href="/u/tia_sanders">@Tia_Sanders</a>,</p>
<p>Please post the original image so we can reproduce the problem.</p> ;;;; <p>Not sure if it matters, but it doesn’t look like you are closing the ImageJ images.<br>
You also might look into scripts that clear the java buffer between images, it has come up before.<br>
Not sure it’s needed or relevant but <a href="https://forum.image.sc/t/clear-memory-should-be-part-of-all-batch-scripts/46211" class="inline-onebox">Clear memory - should be part of all batch scripts</a></p> ;;;; <p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/77a6b824a46bdcf1686a9214df208226e05f37da.gif" alt="inverse anno" data-base62-sha1="h4u0PIUfYdLDEubxMEHib9Ub7Jg" width="690" height="492" class="animated"><br>
GUI first using two annotations that are not associated and inverting the selected annotation,<br>
then inserting into hierarchy and inverting.</p>
<p>You can also subtract, but you will need to duplicate the interior annotation.</p>
<p>All of this can be scripted if you look up examples of hierarchy posts and/or annotation subtraction.</p> ;;;; <p>Thanks <a class="mention" href="/u/petebankhead">@petebankhead</a>, that certainly helped - I had not realized that the computation occurs when <code>writeDensityMapImage</code> is called. I wrote the following small script to test this but ran into an issue with <code>writeDensityMapImage</code>. Is it possible that I need to set additional parameters here? For reference, we are doing some simple cell identifications (2nd line of script) on nissl data and then looking to map the density (total number of cell identification centroids in each pixel) to an image for further processing.</p>
<p>I left all parameters at their defaults except the radius, which I set to <code>100</code>. Doing the same thing in the gui (cell identification, then create density map and adjusting only the radius) works well. I am assuming here that the GUI is using the buildClassifier method to determine an optimal pixel size, so I do the same thing in the script below.</p>
<p>Thanks ahead of time for any pointers!</p>
<pre><code class="lang-auto">def imageData = getCurrentImageData()
runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImageBrightfield": "Optical density sum",  "requestedPixelSizeMicrons": 0.0,  "backgroundRadiusMicrons": 0.0,  "medianRadiusMicrons": 0.0,  "sigmaMicrons": 1.5,  "minAreaMicrons": 10.0,  "maxAreaMicrons": 100.0,  "threshold": 0.23,  "maxBackground": 2.0,  "watershedPostProcess": true,  "excludeDAB": false,  "cellExpansionMicrons": 5.0,  "includeNuclei": true,  "smoothBoundaries": true,  "makeMeasurements": true}');

def params = new DensityMaps.DensityMapParameters()
def builder = new DensityMaps.DensityMapBuilder(params)
builder.radius(100) //this sets the value (essentially a setRadius)
println builder.buildParameters().getRadius() // you can build the params and then see what the radius is, this is the only way to view (copy)

println builder.buildParameters().getPixelSize()
builder.buildClassifier(imageData) // to allow pixel size to be set according to input data
println builder.buildParameters().getPixelSize() //check if this changes the pixelSize? (no)

fileName = buildFilePath('./dMap.tif')
writeDensityMapImage(imageData, builder, fileName)
</code></pre>
<p>output / error:</p>
<pre><code class="lang-auto">INFO: 389 nuclei detected (processing time: 1.19 seconds)
INFO: 1596 nuclei detected (processing time: 3.03 seconds)
INFO: 2539 nuclei detected (processing time: 3.06 seconds)
INFO: 2097 nuclei detected (processing time: 3.24 seconds)
INFO: 3636 nuclei detected (processing time: 3.44 seconds)
INFO: 2544 nuclei detected (processing time: 3.57 seconds)
INFO: 3046 nuclei detected (processing time: 3.76 seconds)
INFO: 2822 nuclei detected (processing time: 3.78 seconds)
INFO: 2782 nuclei detected (processing time: 4.03 seconds)
INFO: 4749 nuclei detected (processing time: 4.70 seconds)
INFO: 5154 nuclei detected (processing time: 4.92 seconds)
INFO: 5057 nuclei detected (processing time: 5.24 seconds)
INFO: Processing complete in 5.32 seconds
INFO: Tasks completed!
INFO: 100.0
INFO: null
INFO: null
WARN: Unable to read tile: java.lang.NullPointerException
ERROR: null in QuPathScript at line number 14

ERROR: java.base/java.util.Objects.requireNonNull(Unknown Source)
    java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Unknown Source)
    java.base/java.util.stream.ReduceOps$3ReducingSink.accept(Unknown Source)
    java.base/java.util.stream.ReferencePipeline$2$1.accept(Unknown Source)
    java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source)
    java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)
    java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)
    java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(Unknown Source)
    java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(Unknown Source)
    java.base/java.util.stream.AbstractTask.compute(Unknown Source)
    java.base/java.util.concurrent.CountedCompleter.exec(Unknown Source)
    java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
    java.base/java.util.concurrent.ForkJoinTask.invoke(Unknown Source)
    java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(Unknown Source)
    java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)
    java.base/java.util.stream.ReferencePipeline.collect(Unknown Source)
    qupath.lib.classifiers.pixel.PixelClassificationImageServer.readAllTiles(PixelClassificationImageServer.java:192)
    qupath.opencv.ml.pixel.PixelClassifierTools.createPixelClassificationServer(PixelClassifierTools.java:494)
    qupath.lib.analysis.heatmaps.DensityMaps$DensityMapBuilder.buildServer(DensityMaps.java:395)
    qupath.lib.scripting.QP.writeDensityMapImage(QP.java:3341)
    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
    QuPathScript.run(QuPathScript:14)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)
    qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)
    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)
    qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)
    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    java.base/java.lang.Thread.run(Unknown Source)

</code></pre> ;;;; <p>Hi Julia,</p>
<p>Thanks for helping.</p>
<p>I’ve also compared your macro along with the one proposed by Mathew, and found that it worked ok after adding a threshold at the end to avoid measuring the whole image. Your macro also had some deviations, up to 36 % compared to a manual polygon approximation of the shape and deviations in the range 10-20 % were common, so I’ll be proceeding with another proposal.</p>
<p>It seemed like it had some challenges removing all of the substrate, so I would typically see lines of the substrate remaining in the measured image.</p>
<p>Kind regards,<br>
Trond</p> ;;;; <p>Not a dev, but as a user and from other posts, it’s usually one image per image in the original file. Macro images and slide labels are shown/selectable in the Image tab. I <code>think</code> most of that is organized by BioFormats or Openslide which is what QuPath uses to open the images, but I also recall there being posts about recognizing the “macro” label etc. when deciding which image/images to show.</p>
<p>Post bout the metadata needed to get those labels read correctly: <a href="https://forum.image.sc/t/create-associatedimagemap-in-svs-file/77387/5" class="inline-onebox">Create AssociatedImageMap in SVS file - #5 by major1819</a><br>
You can find others if you search for macro, label image etc.</p> ;;;; <p>Thanks for the proposal.</p>
<p>I’ve done a bit further testing with some other images and found a span from very good performance, to not so good (worst in a normal image was 58 % deviation and it did not handle images without a substrate at all). I’ve also gotten another proposal that seemed to perform better in general, so I probably won’t proceed with this one.</p>
<p>I’m rather inexperienced when it comes to image analysis, so do you mind explaining briefly how this macro avoids adding the substrate to the area count?</p>
<p>Kind regards,<br>
Trond</p> ;;;; <p>Hi <a class="mention" href="/u/suica46">@Suica46</a></p>
<p>I am not sure I understand the question correctly.</p>
<ul>
<li>you read a large CZI using pylibCZIrw</li>
<li>tile can be read by defining an ROI and in addition tile objects can be created by using the <a href="https://pypi.org/project/cztile/" class="inline-onebox" rel="noopener nofollow ugc">cztile · PyPI</a> library</li>
<li>the display settings are store inside the metadata of the CZI</li>
</ul>
<p>AFAIK in ZEN the display settings are created by checking the whole image. If you “calculate” your own min&amp;max etc. why should this look exactly as in ZEN lite then?</p> ;;;; <p>Thank you for the suggestion! Watershed was okay for this image, but did not work as well for some of the other images I analyzed.</p> ;;;; <p>This is really great, thank you!</p> ;;;; <p>Both CellPose and and Stardist seem to be working fairly well, thank you! StarDist seems a bit easier for me at least because it can be run directly in ImageJ instead of Python (I don’t have as much experience in Python as ImageJ).</p> ;;;; <p>Hello again!<br>
Im very sorry to revive the thread, but I was wondering if there was any update about all this.<br>
Thank you so much in advance.</p>
<p>Best regards</p> ;;;; <p>I’ve gotten it to work on one image at a time with the following script (I renamed the annotations first with a separate script):</p>
<pre><code class="lang-auto">import qupath.imagej.gui.ImageJMacroRunner

params = new ImageJMacroRunner(getQuPath()).getParameterList()

// Change the value of a parameter, using the JSON to identify the key
params.getParameters().get('downsampleFactor').setValue(1)
params.getParameters().get('sendROI').setValue(true)
params.getParameters().get('sendOverlay').setValue(true)
params.getParameters().get('getOverlay').setValue(true)
if (!getQuPath().getClass().getPackage()?.getImplementationVersion()){
    params.getParameters().get('getOverlayAs').setValue('Annotations')
}
params.getParameters().get('getROI').setValue(false)
params.getParameters().get('clearObjects').setValue(false)

macro =
'originalName = getTitle();'+
'run("Colors...", "foreground=white background=white selection=yellow");'+
'setBackgroundColor(255, 255, 255);'+
'run("Clear Outside");'+
'saveAs("Tiff", "/Users/waldmanad/Desktop/Tiff_Images/" + originalName);'

def imageData = getCurrentImageData()
def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}

// Loop through the annotations and run the macro
for (annotation in annotations) {
    ImageJMacroRunner.runMacro(params, imageData, null, annotation, macro)
    ij.IJ.run("Close All", "");
}

</code></pre>
<p>However, if I run in batch for 20 or so images in the project which represents around 600 boxes. After about the 2nd or 3rd image, I get the following error:</p>
<p>ERROR: Java heap space<br>
java.lang.OutOfMemoryError: Java heap space<br>
at ij.process.ByteProcessor.(ByteProcessor.java:53)<br>
at ij.process.PolygonFiller.getMask(PolygonFiller.java:165)<br>
at ij.gui.ShapeRoi.getMask(ShapeRoi.java:1052)<br>
at ij.process.ImageProcessor.setRoi(ImageProcessor.java:881)<br>
at qupathj.QuPath_Send_Overlay_to_QuPath.createObjectsFromROIs(QuPath_Send_Overlay_to_QuPath.java:197)<br>
at qupath.imagej.gui.ImageJMacroRunner.runMacro(ImageJMacroRunner.java:341)<br>
at java.base/java.lang.invoke.LambdaForm$DMH/0x0000000800270c00.invokeStatic(LambdaForm$DMH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a01c00.invoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a02400.guardWithCatch(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a06400.invoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x00000008009bc000.invokeExact_MT(LambdaForm$MH)<br>
at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>
at java.base/java.lang.invoke.LambdaForm$DMH/0x00000008009d6800.invokeStatic(LambdaForm$DMH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x00000008009fb400.invoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x00000008009fb800.linkToCallSite(LambdaForm$MH)<br>
at QuPathScript.run(QuPathScript:29)<br>
at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>
at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>
at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>
ERROR: Java heap space<br>
java.lang.OutOfMemoryError: Java heap space<br>
at ij.process.ByteProcessor.(ByteProcessor.java:53)<br>
at ij.process.PolygonFiller.getMask(PolygonFiller.java:165)<br>
at ij.gui.ShapeRoi.getMask(ShapeRoi.java:1052)<br>
at ij.process.ImageProcessor.setRoi(ImageProcessor.java:881)<br>
at qupathj.QuPath_Send_Overlay_to_QuPath.createObjectsFromROIs(QuPath_Send_Overlay_to_QuPath.java:197)<br>
at qupath.imagej.gui.ImageJMacroRunner.runMacro(ImageJMacroRunner.java:341)<br>
at java.base/java.lang.invoke.LambdaForm$DMH/0x0000000800270c00.invokeStatic(LambdaForm$DMH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800088c00.invoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a02400.guardWithCatch(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03400.guard(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a03000.reinvoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x0000000800a06400.invoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x00000008009bc000.invokeExact_MT(LambdaForm$MH)<br>
at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>
at java.base/java.lang.invoke.LambdaForm$DMH/0x00000008009d6800.invokeStatic(LambdaForm$DMH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x000000080020d400.invoke(LambdaForm$MH)<br>
at java.base/java.lang.invoke.LambdaForm$MH/0x00000008009fb800.linkToCallSite(LambdaForm$MH)<br>
at QuPathScript.run(QuPathScript:29)<br>
at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>
at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>
at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)</p>
<p>Is there anything in the script I can adjust to make this work without running out of memory? I didn’t anticipate this would be so computationally intensive. I’ve given QuPath 16GB of RAM on my 1.4 GHz Quad-Core Intel Core i5 MacBook Pro.</p> ;;;; <p>Sorry to resurrect an old thread, but here is a QuPath script to write the stain vectors into a JSON file and to read them back. So yes, “passing stain vectors to a third-party application” or in my case, <a class="mention" href="/u/cdgatenbee">@cdgatenbee</a>’s <a href="https://github.com/MathOnco/valis" rel="noopener nofollow ugc">VALIS</a> automated registration scripts.</p>
<p>After I get my stain vectors into Python from the JSON file (I’ll write this bit next), I will apply colour deconvolution as a pre-processing step to VALIS. Most likely, the Haematoxylin channel between two slides stained with non-overlapping markers, or maybe DAPI in a fluorescent slide vs Haematoxylin in an H&amp;E slide.</p>
<p>The JSON file is saved in the same folder as the current image, with the same name but with <code>"_stains.json"</code> appended to it instead of the original extension.</p>
<pre><code class="lang-java">import qupath.lib.scripting.QP
import java.nio.charset.StandardCharsets

import java.nio.file.Files 
import java.nio.file.Paths
import org.apache.commons.io.IOUtils

import qupath.lib.io.GsonTools
import com.google.gson.Gson

import qupath.lib.color.ColorDeconvolutionStains
import qupath.lib.color.StainVector

def write_stains(stains) {
    def server = QP.getCurrentImageData().getServer()

    //*********Get a JSON filename automatically based on naming scheme 
    def path = GeneralTools.toPath(server.getURIs()[0]).toString()
    path = path[0..&lt;path.lastIndexOf('.')]+"_stains.json";
    println path;
    
    boolean prettyPrint = true
    def gson = GsonTools.getInstance(prettyPrint)

    // write (save) the json file
    try (Writer writer = new FileWriter(path)) {
        gson.toJson(stains, writer);
    }
}    

def read_stains() {
    def server = QP.getCurrentImageData().getServer()

    //*********Get a JSON filename automatically based on naming scheme 
    def path = GeneralTools.toPath(server.getURIs()[0]).toString()
    path = path[0..&lt;path.lastIndexOf('.')]+"_stains.json";
    println path;

    def JSONfile = new File(path)
    if (!JSONfile.exists()) {
        println "No stains file for this image..."
        return
    }

    Gson gson = new Gson(); 
    map = gson.fromJson(JSONfile.text, Map.class);
    StainVector stain1 = StainVector.createStainVector(map.stain1.name, map.stain1.r, map.stain1.g, map.stain1.b)
    StainVector stain2 = StainVector.createStainVector(map.stain2.name, map.stain2.r, map.stain2.g, map.stain2.b)
    StainVector stain3 = StainVector.createStainVector(map.stain3.name, map.stain3.r, map.stain3.g, map.stain3.b)
    
    ColorDeconvolutionStains stains = new ColorDeconvolutionStains(map.name, stain1, stain2, stain3, map.maxRed, map.maxGreen, map.maxBlue);
    
    return stains
}

//Here we write the stains
def imageData = getCurrentImageData();
def stains = imageData.getColorDeconvolutionStains()
write_stains(stains)

//Here we read the stains
stains = read_stains()
println stains
</code></pre>
<p>I’ll add some Python code to this thread when I’m done writing it.</p>
<p>Cheers,<br>
Egor</p> ;;;; <p>I have tried viewing an image on Fiji and the channel tools function just ticks all 3 channels even though I have only cfos and DAPI images. When I set the image type to RGB, it manages to put colours but produces 3 sets of DAPI images with different colours.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c2b0658a0ae757e4783e6ef5194581c2b015c0d.jpeg" data-download-href="/uploads/short-url/aROt0LvbdXmxbyq64S4RV26Grp3.jpeg?dl=1" title="Screenshot 2023-03-10 at 4.32.14 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c2b0658a0ae757e4783e6ef5194581c2b015c0d.jpeg" alt="Screenshot 2023-03-10 at 4.32.14 PM" data-base62-sha1="aROt0LvbdXmxbyq64S4RV26Grp3" width="690" height="392" data-dominant-color="AEAEAE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-10 at 4.32.14 PM</span><span class="informations">1108×630 64.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d.jpeg" data-download-href="/uploads/short-url/iXsMeKVyLugZAL4EB9puJAHwL93.jpeg?dl=1" title="Screenshot 2023-03-10 at 4.32.43 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_642x500.jpeg" alt="Screenshot 2023-03-10 at 4.32.43 PM" data-base62-sha1="iXsMeKVyLugZAL4EB9puJAHwL93" width="642" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_642x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_963x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_1284x1000.jpeg 2x" data-dominant-color="AEAE15"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-10 at 4.32.43 PM</span><span class="informations">1322×1028 147 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I’m currently working on a large image set, where I have 24 counting annotations (24 different sets of points). I enter them all from “Create points from all classes” and they’re in order of how I created them (which is also how I will count them). As soon as I start counting and placing points, the list re-orders randomly.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e.png" data-download-href="/uploads/short-url/hWGUuAR0IGrokd6DIykxoRTKo7Y.png?dl=1" title="Screenshot 2023-03-10 at 11.29.13 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_239x500.png" alt="Screenshot 2023-03-10 at 11.29.13 AM" data-base62-sha1="hWGUuAR0IGrokd6DIykxoRTKo7Y" width="239" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_239x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_358x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_478x1000.png 2x" data-dominant-color="EAEAEB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-10 at 11.29.13 AM</span><span class="informations">770×1606 136 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10.png" data-download-href="/uploads/short-url/zuEKDOhgTQhfqy0VsyFMsQHvk6k.png?dl=1" title="Screenshot 2023-03-10 at 11.29.40 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_241x499.png" alt="Screenshot 2023-03-10 at 11.29.40 AM" data-base62-sha1="zuEKDOhgTQhfqy0VsyFMsQHvk6k" width="241" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_241x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_361x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_482x998.png 2x" data-dominant-color="ECEBEB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-10 at 11.29.40 AM</span><span class="informations">776×1608 120 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>It’s just super time consuming to have to search through a list of 24 similar names to identify which annotation category I want to use next. Is there any way around this? I can’t figure out a way to re-organize them while counting.</p>
<p>Thanks,<br>
Claudia</p> ;;;; <p>Hi all,</p>
<p>I’m trying to use pylibczirw and some other related packages to read large .czi files. I want to read them as tiles with 1024*1024 shapes. However, I should colorize multi-dimensional images. But if I use the ‘max’ value of each channel, different tiles will have different colors.<br>
Also, I guess a max-min value of each channel and normalize them after I converted the float16 grayscale images into RGB24 images. Then I added the RGB channels and use the guessed max-min value to normalize the merged image. The contrast will become lower than the performance in Zen Lite.<br>
Is there a method to generate tiled images the same as the performance in Zen Lite?</p> ;;;; <p>Hi all,</p>
<p>I am currently using OrientationJ to determine the alignment of cells and matrix in my cell cultures. I use the analysis and distribution funtions in OrientationJ and this works generally quite okay. However, in some of my images I have aspecific spots, and/or larger areas (staining artifacts) which disturb the image analysis. Do you know of a way to remove these from my images, so that I can get more accurate determination of the angles an representation in the analysis HSB function?</p>
<p>I attached HSB OrientationJ analysis images of the spots (indicated in blue circles). The red lines are the structures that I try to analyze.</p>
<p>Kind regards,<br>
Judith<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/922959794b0e5107a9912817f7026042087a1a4a.jpeg" data-download-href="/uploads/short-url/kR0iHkJ3j6fBgliIicECKvSDahY.jpeg?dl=1" title="OriJ_Large.PNG" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/922959794b0e5107a9912817f7026042087a1a4a.jpeg" alt="OriJ_Large.PNG" data-base62-sha1="kR0iHkJ3j6fBgliIicECKvSDahY" width="492" height="499" data-dominant-color="332727"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">OriJ_Large.PNG</span><span class="informations">611×620 53.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/8/68a67a8075882c64c4eac96024bb0db6dda0408d.jpeg" data-download-href="/uploads/short-url/eVMkNIXQtRAZeUEcb51rbIyIbg1.jpeg?dl=1" title="OriJ_Mixed.PNG" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/8/68a67a8075882c64c4eac96024bb0db6dda0408d.jpeg" alt="OriJ_Mixed.PNG" data-base62-sha1="eVMkNIXQtRAZeUEcb51rbIyIbg1" width="466" height="500" data-dominant-color="221818"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">OriJ_Mixed.PNG</span><span class="informations">590×633 42.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi all,</p>
<p>I just tried to convert a CZI file using the OME-NGFF converter tool, but it fails immediately complaing that “series 96 is not present”. But when opening in Fiji via BioFomats one can clearly see, that there are 96 series.</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.dropbox.com/s/uuh8ktgk5lz7e7d/testwell96_test.czi?dl=0">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d0b5830ec317206a6bb1fa759b18c97832111733.png" class="site-icon" width="32" height="32">

      <a href="https://www.dropbox.com/s/uuh8ktgk5lz7e7d/testwell96_test.czi?dl=0" target="_blank" rel="noopener nofollow ugc">Dropbox</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png" class="thumbnail onebox-avatar" width="160" height="160">

<h3><a href="https://www.dropbox.com/s/uuh8ktgk5lz7e7d/testwell96_test.czi?dl=0" target="_blank" rel="noopener nofollow ugc">testwell96_test.czi</a></h3>

  <p>Shared with Dropbox</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>The metadata of this CZI are fine and I see no issues when running itthrough our CZIChekcer.Any ideas?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/a/5a3605477d1c96c51ee6c367e16999f7b4986b78.png" data-download-href="/uploads/short-url/cS2I52GTSyrHQN0bnqX88PkPc00.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/a/5a3605477d1c96c51ee6c367e16999f7b4986b78.png" alt="image" data-base62-sha1="cS2I52GTSyrHQN0bnqX88PkPc00" width="690" height="288" data-dominant-color="232424"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1583×662 41.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Github Issue: <a href="https://github.com/glencoesoftware/NGFF-Converter/issues/42" rel="noopener nofollow ugc">NGFF-Converter 1.1.4 fails to convert CZI file which opens normally in Fiji using BioFormats · Issue #42 · glencoesoftware/NGFF-Converter (github.com)</a></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c.jpeg" data-download-href="/uploads/short-url/1wmMKMr6g5FBk5Z0zD3zl7S4wLG.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_690x431.jpeg" alt="image" data-base62-sha1="1wmMKMr6g5FBk5Z0zD3zl7S4wLG" width="690" height="431" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_1380x862.jpeg 2x" data-dominant-color="E4E5E4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1201 215 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94f4db372486c4909caba42d98b8e1d57c1f55a1.png" alt="image" data-base62-sha1="lfJgBCwlfgDWILiSn7NTS2WJrWN" width="351" height="497"></p>
<pre><code class="lang-plaintext">17:11:36 DEBUG l.f.FormatHandler - loci.formats.in.ZeissCZIReader.initFile(image.czi)
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getParent()
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1460572002 OPEN
17:11:36 TRACE l.f.FormatHandler - plane #0 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 3296, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=2645, size=421, startCoordinate=0.0, storedSize=421; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=0, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #1 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 858240, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=2267, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=1, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #2 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 1717248, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=1889, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=2, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #3 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 2576256, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=1511, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=3, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #4 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 3435264, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=1133, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=4, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #5 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 4294272, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=755, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=5, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #6 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 5153280, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=377, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=6, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.f.FormatHandler - plane #7 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 6012288, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=0, size=422, startCoordinate=0.0, storedSize=422; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=7, size=1, startCoordinate=0.0, storedSize=1]
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 858450849 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 858450849 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1303715745 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1303715745 CLOSE
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, C:\Program Files\NGFF-Converter\image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - getParent()
17:11:36 TRACE l.c.Location - Location(null, C:\Program Files\NGFF-Converter)
17:11:36 TRACE l.c.Location - list(true)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(C:\Program Files\NGFF-Converter, app)
17:11:36 TRACE l.c.Location - isHidden()
17:11:36 TRACE l.c.Location - Location(C:\Program Files\NGFF-Converter, NGFF-Converter.exe)
17:11:36 TRACE l.c.Location - isHidden()
17:11:36 TRACE l.c.Location - Location(C:\Program Files\NGFF-Converter, NGFF-Converter.ico)
17:11:36 TRACE l.c.Location - isHidden()
17:11:36 TRACE l.c.Location - Location(C:\Program Files\NGFF-Converter, runtime)
17:11:36 TRACE l.c.Location - isHidden()
17:11:36 TRACE l.c.Location -   returning 4 files
17:11:36 TRACE l.f.FormatHandler - rotations = 1
17:11:36 TRACE l.f.FormatHandler - illuminations = 1
17:11:36 TRACE l.f.FormatHandler - phases = 1
17:11:36 TRACE l.f.FormatHandler -     assigned plane index = 0; series index = 0; coreIndex = 0
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1341850139 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1341850139 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1814946073 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1814946073 CLOSE
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, image.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 146696414 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 146696414 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1838364287 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1838364287 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2062486028 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2062486028 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2034833761 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2034833761 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 38434318 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 38434318 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1990101238 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1990101238 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1956860739 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1956860739 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)
17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1158733479 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1158733479 CLOSE
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1460572002 CLOSE
17:11:36 DEBUG l.c.Location - Location.mapFile: image.czi -&gt; null
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getName()

...

17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@143b3a0d
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@143b3a0d
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 375445828 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 375445828 CLOSE
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@78b7b980
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@78b7b980
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 991016407 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 991016407 CLOSE
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()

...

17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@76370bea
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@76370bea
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1712146065 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1712146065 CLOSE
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 598968425 OPEN
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 598968425 CLOSE
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()

...

17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getAbsolutePath()
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - getName()
17:11:36 DEBUG l.c.Location - Location.mapFile: embedded-stream.raw -&gt; null
17:11:36 DEBUG l.c.Location - Location.mapFile: embedded-stream.raw -&gt; null
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 43023822 CLOSE
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - lastModified()
17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\Testdata_Zeiss\CZI_Testfiles\.testwell96_test.czi.bfmemo (8817839 bytes)
17:11:36 DEBUG l.f.Memoizer - start[1678464696498] time[34] tag[loci.formats.Memoizer.loadMemo]
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@5136de40
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@5136de40
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1634768657 OPEN
17:11:36 DEBUG l.f.Memoizer - start[1678464696498] time[34] tag[loci.formats.Memoizer.setId]
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - lastModified()
17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\Testdata_Zeiss\CZI_Testfiles\.testwell96_test.czi.bfmemo (8817839 bytes)
17:11:36 DEBUG l.f.Memoizer - start[1678464696534] time[33] tag[loci.formats.Memoizer.loadMemo]
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@7d75a378
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@7d75a378
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 773527809 OPEN
17:11:36 DEBUG l.f.Memoizer - start[1678464696534] time[34] tag[loci.formats.Memoizer.setId]
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - lastModified()
17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\Testdata_Zeiss\CZI_Testfiles\.testwell96_test.czi.bfmemo (8817839 bytes)
17:11:36 DEBUG l.f.Memoizer - start[1678464696568] time[32] tag[loci.formats.Memoizer.loadMemo]
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@6e0c015c
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@6e0c015c
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1499170649 OPEN
17:11:36 DEBUG l.f.Memoizer - start[1678464696568] time[33] tag[loci.formats.Memoizer.setId]
17:11:36 TRACE l.c.Location - Location(null, F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi)
17:11:36 TRACE l.c.Location - lastModified()
17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\Testdata_Zeiss\CZI_Testfiles\.testwell96_test.czi.bfmemo (8817839 bytes)
17:11:36 DEBUG l.f.Memoizer - start[1678464696602] time[30] tag[loci.formats.Memoizer.loadMemo]
17:11:36 TRACE l.c.Location - getHandle(id = F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi, writable = false)
17:11:36 TRACE l.c.Location - no handle was mapped for this ID
17:11:36 TRACE l.c.Location - Created new handle F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@32aff37f
17:11:36 TRACE l.c.Location - Location.getHandle: F:\Testdata_Zeiss\CZI_Testfiles\testwell96_test.czi -&gt; loci.common.NIOFileHandle@32aff37f
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 280616683 OPEN
17:11:36 DEBUG l.f.Memoizer - start[1678464696601] time[31] tag[loci.formats.Memoizer.setId]
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.DichroicRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.DichroicRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef
17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 773527809 CLOSE
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1499170649 CLOSE
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 280616683 CLOSE
17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1634768657 CLOSE
17:11:36 ERROR c.g.c.ConverterTask - java.lang.IllegalArgumentException: Series 96 not present in metadata!
17:11:36 INFO  c.g.c.ConverterTask - Failed with Exit Code 1 : testwell96_test.zarr

17:11:36 INFO  c.g.c.ConverterTask - Completed conversion of 0 files.
</code></pre> ;;;; <p>Hi all,</p>
<p>I am studying cell alignment (using OrientationJ on fluorescence images) in a uniaxially fixed 3D scaffold. I would like to compare the angle of the cell signal to the angle of the scaffold. In most cases, my scaffold is fixed to 2 points which are facing each other (left drawing), so that the angle of my scaffold is 0 degrees (compared to a vertical line). However, it turned out that not all attachment points are facing each other perfectly, which causes that some scaffolds are fixed in the well under an angle (right drawing). To correct for this, I would like to subtract the angle of the scaffold from the angle of the cells that I get from OrientationJ, however I struggle to determine the angle of this scaffold in an unbiased way. Could you help me to find a way to (automate) the detection of the angle of the scaffold, to reduce the bias compared to doing this manually with the angle tool? The outline of the scaffold is visible in the fluorescence images that I use to run orientation J (second upload).</p>
<p>Best,<br>
Judith</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41.jpeg" data-download-href="/uploads/short-url/kk8CnzHZDtMAjxr8Wl8jS4sLUHL.jpeg?dl=1" title="Velcro angle" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_690x349.jpeg" alt="Velcro angle" data-base62-sha1="kk8CnzHZDtMAjxr8Wl8jS4sLUHL" width="690" height="349" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_690x349.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_1035x523.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_1380x698.jpeg 2x" data-dominant-color="E9EAEB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Velcro angle</span><span class="informations">1862×943 58.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d.jpeg" data-download-href="/uploads/short-url/mbMOQRN2yTL55iPvMc6vAi0WPoN.jpeg?dl=1" title="Velcro angle_ex" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_132x499.jpeg" alt="Velcro angle_ex" data-base62-sha1="mbMOQRN2yTL55iPvMc6vAi0WPoN" width="132" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_132x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_198x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_264x998.jpeg 2x" data-dominant-color="340305"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Velcro angle_ex</span><span class="informations">307×1160 35.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>So I have switched directions but made progress. I have come up with a way to export regions with the masks to ImageJ and make the area outside the mask white using the following code:</p>
<pre><code class="lang-auto">import qupath.imagej.gui.ImageJMacroRunner

params = new ImageJMacroRunner(getQuPath()).getParameterList()

// Change the value of a parameter, using the JSON to identify the key
params.getParameters().get('downsampleFactor').setValue(1)
params.getParameters().get('sendROI').setValue(true)
params.getParameters().get('sendOverlay').setValue(true)
params.getParameters().get('getOverlay').setValue(true)
if (!getQuPath().getClass().getPackage()?.getImplementationVersion()){
    params.getParameters().get('getOverlayAs').setValue('Annotations')
}
params.getParameters().get('getROI').setValue(false)
params.getParameters().get('clearObjects').setValue(false)

macro =
'originalName = getTitle();'+
'run("Colors...", "foreground=white background=white selection=yellow");'+
'setBackgroundColor(255, 255, 255);'+
'run("Clear Outside");'+
'saveAs("Tiff", "/Users/waldmanad/Desktop/Tiff_Images/" + originalName);'

def imageData = getCurrentImageData()
def server = getCurrentServer()
def name = GeneralTools.getNameWithoutExtension(server.getMetadata().getName())
def annotations = getAnnotationObjects().findAll()

// Loop through the annotations and run the macro
for (annotation in annotations) {
    ImageJMacroRunner.runMacro(params, imageData, null, annotation, macro)
    ij.IJ.run("Close All", "");
}

</code></pre>
<p>However, I am having trouble with 2 things:</p>
<ol>
<li>I would like the annotations being sent to ImageJ to be named as the image name + the annotation name. I have the following two lines of code inserted to define the name parameter but not sure how to actually use it:</li>
</ol>
<pre><code class="lang-auto">def server = getCurrentServer()
def name = GeneralTools.getNameWithoutExtension(server.getMetadata().getName())

</code></pre>
<ol start="2">
<li>I have other large regions annotated in the image that I don’t want sent. I tried using the following getAnnotationObjects().findAll(a → a.getROI().getNumPoints() == 4)) in the script above but kept getting the following error:</li>
</ol>
<p>ERROR: startup failed:<br>
QuPathScript: 27: Unexpected input: ‘(a → a.getROI().getNumPoints() == 4))’ @ line 26, column 86.<br>
.getROI().getNumPoints() == 4))<br>
^</p>
<p>1 error<br>
in QuPathScript at line number 26</p>
<p>ERROR: org.codehaus.groovy.control.ErrorCollector.failIfErrors(ErrorCollector.java:292)<br>
org.codehaus.groovy.control.ErrorCollector.addFatalError(ErrorCollector.java:148)<br>
org.apache.groovy.parser.antlr4.AstBuilder.collectSyntaxError(AstBuilder.java:4792)<br>
org.apache.groovy.parser.antlr4.AstBuilder.access$100(AstBuilder.java:169)<br>
org.apache.groovy.parser.antlr4.AstBuilder$3.syntaxError(AstBuilder.java:4803)<br>
groovyjarjarantlr4.v4.runtime.ProxyErrorListener.syntaxError(ProxyErrorListener.java:44)<br>
groovyjarjarantlr4.v4.runtime.Parser.notifyErrorListeners(Parser.java:543)<br>
groovyjarjarantlr4.v4.runtime.DefaultErrorStrategy.notifyErrorListeners(DefaultErrorStrategy.java:154)<br>
org.apache.groovy.parser.antlr4.internal.DescriptiveErrorStrategy.reportNoViableAlternative(DescriptiveErrorStrategy.java:92)<br>
groovyjarjarantlr4.v4.runtime.DefaultErrorStrategy.reportError(DefaultErrorStrategy.java:139)<br>
org.apache.groovy.parser.antlr4.GroovyParser.pathExpression(GroovyParser.java:9977)<br>
org.apache.groovy.parser.antlr4.GroovyParser.postfixExpression(GroovyParser.java:8325)<br>
org.apache.groovy.parser.antlr4.GroovyParser.expression(GroovyParser.java:9115)<br>
org.apache.groovy.parser.antlr4.GroovyParser.commandExpression(GroovyParser.java:9776)<br>
org.apache.groovy.parser.antlr4.GroovyParser.statementExpression(GroovyParser.java:8284)<br>
org.apache.groovy.parser.antlr4.GroovyParser.enhancedStatementExpression(GroovyParser.java:8228)<br>
org.apache.groovy.parser.antlr4.GroovyParser.variableInitializer(GroovyParser.java:2799)<br>
org.apache.groovy.parser.antlr4.GroovyParser.variableDeclarator(GroovyParser.java:2722)<br>
org.apache.groovy.parser.antlr4.GroovyParser.variableDeclarators(GroovyParser.java:2642)<br>
org.apache.groovy.parser.antlr4.GroovyParser.variableDeclaration(GroovyParser.java:5838)<br>
org.apache.groovy.parser.antlr4.GroovyParser.localVariableDeclaration(GroovyParser.java:5739)<br>
org.apache.groovy.parser.antlr4.GroovyParser.statement(GroovyParser.java:7115)<br>
org.apache.groovy.parser.antlr4.GroovyParser.scriptStatement(GroovyParser.java:520)<br>
org.apache.groovy.parser.antlr4.GroovyParser.scriptStatements(GroovyParser.java:427)<br>
org.apache.groovy.parser.antlr4.GroovyParser.compilationUnit(GroovyParser.java:363)<br>
org.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:243)<br>
org.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:221)<br>
org.apache.groovy.parser.antlr4.AstBuilder.buildAST(AstBuilder.java:262)<br>
org.apache.groovy.parser.antlr4.Antlr4ParserPlugin.buildAST(Antlr4ParserPlugin.java:58)<br>
org.codehaus.groovy.control.SourceUnit.buildAST(SourceUnit.java:255)<br>
java.base/java.util.Iterator.forEachRemaining(Unknown Source)<br>
java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Unknown Source)<br>
java.base/java.util.stream.ReferencePipeline$Head.forEach(Unknown Source)<br>
org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:663)<br>
groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:373)<br>
groovy.lang.GroovyClassLoader.lambda$parseClass$2(GroovyClassLoader.java:316)<br>
org.codehaus.groovy.runtime.memoize.StampedCommonCache.compute(StampedCommonCache.java:163)<br>
org.codehaus.groovy.runtime.memoize.StampedCommonCache.getAndPut(StampedCommonCache.java:154)<br>
groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:314)<br>
groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:298)<br>
groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:258)<br>
org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getScriptClass(GroovyScriptEngineImpl.java:350)<br>
org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:159)<br>
qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>
qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>
qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>
java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>
java.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>
java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>
java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>
java.base/java.lang.Thread.run(Unknown Source)</p>
<p>As always, thanks for your help!</p> ;;;; <aside class="quote no-group" data-username="GaussGap" data-post="4" data-topic="78346">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/gaussgap/40/68130_2.png" class="avatar"> GaussGap:</div>
<blockquote>
<p>downsampling w</p>
</blockquote>
</aside>
<p>Thank you very much for your contribution. global_scale seems hamless unless it is very small. I will train with a decreased global_scale, if it doesn’t work well then I will go for downsampling.</p> ;;;; <p>Hi <a class="mention" href="/u/swa">@Swa</a> ,</p>
<p>Interestingly, I needed something like this for <a href="https://github.com/saalfeldlab/bigwarp/blob/b396576bf1ec66b5069818b54b9d6fd746168d6b/src/main/java/bigwarp/source/PlateauSphericalMaskRealRandomAccessible.java">bigwarp recently</a>, so I adapted that code to this ImageJ2 script:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy">
  <header class="source">

      <a href="https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy" target="_blank" rel="noopener">bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy</a></h4>


      <pre><code class="lang-groovy">#@ Integer sizeX
#@ Integer sizeY
#@ Double centerX
#@ Double centerY
#@ Double innerRadius
#@ Double outerRadius
#@ UIService ui

/**
 * Creates a circular gradient
 * 
 * see:
 * https://forum.image.sc/t/circular-gradient/78365
 * 
 * John Bogovic
 */

// the size of the image
itvl = new FinalInterval( [sizeX, sizeY ] as long[] );

</code></pre>



  This file has been truncated. <a href="https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Here are some of the results I get (with the parameters shown):<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0a2e95283d73ef7d6b4b6c2f7c4f696dbaed2c17.png" data-download-href="/uploads/short-url/1s4zO97uZoxcRkurob1M520ryCP.png?dl=1" title="Screenshot from 2023-03-10 10-56-42"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0a2e95283d73ef7d6b4b6c2f7c4f696dbaed2c17.png" alt="Screenshot from 2023-03-10 10-56-42" data-base62-sha1="1s4zO97uZoxcRkurob1M520ryCP" width="517" height="315" data-dominant-color="868084"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-10 10-56-42</span><span class="informations">746×456 27.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>It uses a cosine-shaped falloff between the center and the outside, but feel free to take my code for a <a href="https://github.com/saalfeldlab/bigwarp/blob/b396576bf1ec66b5069818b54b9d6fd746168d6b/src/main/java/bigwarp/source/PlateauSphericalMaskRealRandomAccessible.java#L352-L380">Gaussian</a> or <a href="https://github.com/saalfeldlab/bigwarp/blob/b396576bf1ec66b5069818b54b9d6fd746168d6b/src/main/java/bigwarp/source/PlateauSphericalMaskRealRandomAccessible.java#L412-L438">linear</a> shape falloff if you’d like. Adapting this code will mean learning a little ImageJ2 / imglib2, but please post back with questions, I and others will be happy to help.</p>
<p>John</p> ;;;; <p>Hello Volker,</p>
<p>thanks for the quick reply, i will test your suggestion.</p>
<p>Marcus</p> ;;;; <p>Works really well for most</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e89ee6375099000cb5546f8de5715c3a1d50580.png" alt="image" data-base62-sha1="4m9Q6kIxoK5RbrHJE5ihWvmXzCo" width="278" height="254"></p>
<p>Here the parameter recording for the screenshot you posted. If the original is different, that needs adaption.</p>
<pre><code class="lang-auto">run("Command From Macro", "command=[de.csbdresden.stardist.StarDist2D], args=['input':'Clipboard', 'modelChoice':'Versatile (fluorescent nuclei)', 'normalizeInput':'true', 'percentileBottom':'1.0', 'percentileTop':'99.8', 'probThresh':'0.85', 'nmsThresh':'0.1', 'outputType':'ROI Manager', 'nTiles':'1', 'excludeBoundary':'2', 'roiPosition':'Automatic', 'verbose':'false', 'showCsbdeepProgress':'false', 'showProbAndDist':'false'], process=[false]");
</code></pre> ;;;; <p>Oooh right … sorry for the noise (I <em>plan</em> on using websockets mind you, but not now)</p> ;;;; <p>I think those are the ports if you are using websockets. ~J</p> ;;;; <p>A new paper, <a href="https://t.co/kOJBPfw54o">“Toward scalable reuse of vEM data: OMEZarr to the rescue”</a> from <a class="mention" href="/u/normanrz">@normanrz</a> , <a class="mention" href="/u/joshmoore">@joshmoore</a> and me is out now!  Apologies for lack of open access. For the next 50 days (until 2023-04-28) you can access the chapter on using <span class="hashtag">#OMEZarr</span> in <span class="hashtag">#VolumeEM</span> <a href="https://t.co/4npp6iHztS">here</a>.</p>
<p>Special thanks to volume editors Lucy Collinson, Kedar Narayan, and Paul Verkade, and artist <a href="https://twitter.com/DrHenningFalk">Henning Falk</a> for the excellent illustrations! <a href="https://github.com/zarr-developers/zarr-illustrations-falk-2022">All art from the paper</a> can be <a href="https://creativecommons.org/licenses/by/4.0/">shared and adapted with attribution</a>.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63.jpeg" data-download-href="/uploads/short-url/aebHQvcMlzJASfTaIvPFEW7w8eL.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_420x375.jpeg" alt="image" data-base62-sha1="aebHQvcMlzJASfTaIvPFEW7w8eL" width="420" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_420x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_630x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_840x750.jpeg 2x" data-dominant-color="F1F0F1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1711 151 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div>  <div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735.jpeg" data-download-href="/uploads/short-url/4iuXn56BWnikDw6rfCZmhBGR2LP.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_256x375.jpeg" alt="image" data-base62-sha1="4iuXn56BWnikDw6rfCZmhBGR2LP" width="256" height="375" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_256x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_384x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_512x750.jpeg 2x" data-dominant-color="E7E7E7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×2806 403 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><strong>Abstract</strong>: The growing size of EM volumes is a significant barrier to findable, accessible, interoperable, and reusable (FAIR) sharing. Storage, sharing, visualization and processing are challenging for large datasets. Here we discuss a recent development toward the standardized storage of volume electron microscopy (vEM) data which addresses many of the issues that researchers face. The OME-Zarr format splits data into more manageable, performant chunks enabling streaming-based access, and unifies important metadata such as multiresolution pyramid descriptions. The file format is designed for centralized and remote storage (e.g., cloud storage or file system) and is therefore ideal for sharing large data. By coalescing on a common, community-wide format, these benefits will expand as ever more data is made available to the scientific community.</p> ;;;; <p>I guess 2048x2048 might be a bit too much and downsampling would be a good idea.<br>
Here’s a thread on it. From my understanding it’s a training parameter for the network to downscale the size while it’s training. I guess Konrad could give you a good idea of what it is and does.</p>
<aside class="quote" data-post="1" data-topic="66290">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/aendrs/40/56358_2.png" class="avatar">
    <a href="https://forum.image.sc/t/regarding-the-global-scale-parameter-and-its-use-during-inference/66290">Regarding the global_scale parameter and its use during inference</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    I have a trained model with a global_scale=0.25 parameter in the pose_cfg.yaml files. 
My question is, when I use the function deeplabcut.analyze_videos to process new videos, should I resize them to match the global_scale parameter? 
The analyze_videos function gets as an argument the config.yaml file, where no global_scale parameter is present. 
Thanks
  </blockquote>
</aside>
 ;;;; <p>Hi Gauss,</p>
<p>I am using ResNet50. The smoke test you provided recognizes my GPU however, it shows 7427 MB memmory eventhough I have 10000 MB. Now, I decreased global_scale from 0.8 to 0.6 and it is working now. But the problem is I do not know what global_scale do exactly. Every spot in my images are important for me, do you know what does global_scale do and decreasing it is harmful for my data?</p> ;;;; <p>So the error was the wrong exposed ports in the <code>docker-compose.yml</code> file. Using <code>encrypted=true</code> bypasses the unencrypted 4063 port (IIUC).</p>
<p>I don’t know where I found the 4065 and 4066 ports in that docker-compose file I’ve been copy/pasting, it’s 63 and 64 everywhere else …</p> ;;;; <p>Hi <a class="mention" href="/u/marcus_imagej">@Marcus_Imagej</a>,<br>
as far as I know (I could be wrong) you can not use it from a macro, but you could use the <a href="https://weka.sourceforge.io/doc.dev/weka/clusterers/SimpleKMeans.html" rel="noopener nofollow ugc">weka api</a> from a script (for example jython), if that’s an option for you.</p>
<p>For our <a href="https://montpellierressourcesimagerie.github.io/mri-workshop-machine-learning/" rel="noopener nofollow ugc">DL/ML workshop</a></p>
<p>I wrote something similar, but based on the apache clustering library (which also comes with FIJI).</p>
<p>If you want to try that:<br>
Put the two files <a href="https://raw.githubusercontent.com/MontpellierRessourcesImagerie/mri-workshop-machine-learning/master/k-means_experiment/k-means_segmentation.py" rel="noopener nofollow ugc">k-means_segmentation.py</a> and  <a href="https://raw.githubusercontent.com/MontpellierRessourcesImagerie/mri-workshop-machine-learning/master/k-means_experiment/normalize_feature_stack.py" rel="noopener nofollow ugc">normalize_feature_stack.py</a> into <code>plugins</code> or a subfolder and restart FIJI.<br>
You can now run it from the ImageJ GUI and it is macro recordable. The input is a feature stack. So you need to call <code>Image&gt;Type&gt;??? Stack</code> on your RGB image, or use another way to create the components in the color space you want to use.</p>
<p>Best,<br>
Volker</p> ;;;; <p>It might be because port 4063 is not also exposed. Does <code>omero import --encrypted=true</code> also fail?</p> ;;;; <p>Hi Baris,<br>
I’m running a similar setup like you. If you use DLC 2.3, I encountered some issues with napari-gui so could not label my data.<br>
What I did is I created a specific environment with DLC 2.2.3:</p>
<pre><code class="lang-auto">conda create --name DLC python=3.8
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
conda install -c nvidia cuda-nvcc
pip install --upgrade pip
pip install "tensorflow&lt;2.11"
pip install --upgrade matplotlib==3.5.2
pip install deeplabcut[gui]==2.2.3
pip install torch
pip install -U wxPython
</code></pre>
<p>Check if you have cuda-nvcc in your environment.<br>
Try these smoke tests for tensorflow:</p>
<pre><code class="lang-auto">python -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
</code></pre>
<p>should return a tensor;</p>
<pre><code class="lang-auto">python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
</code></pre>
<p>should recognize your GPU;</p>
<p>I ran into some errors when trying to use EfficiNet b6 or ResNet 152 where they ran out of memory. Went down to ResNet 50 and it’s working.<br>
Hope this helps!</p> ;;;; <p>Hey <a class="mention" href="/u/leroyadrien">@LeroyAdrien</a>,<br>
The problem here is the order of execution. When the generator yields the value, the <code>on_yielded</code> function is triggered. However, the execution of the worker does not stop, but rather moves on to the next loop, while the <code>on_yielded</code> function <strong>runs in parallel</strong>. It is similar with <code>worker.pause</code>. Here we ask the worker to stop, but we don’t wait for it to happen, but continue with adding the layers to the viewer. Pausing will happen only between yields. The loop runs so fast, that when the worker should pause, all the images were already generated, so the worker quits.</p>
<p>In the official napari tutorial it is handled by the <code>worker.send</code> method:</p>
<pre><code class="lang-auto">def send_next_value():
    worker.send(float(line_edit.text())) # &lt;--
    worker.resume()
</code></pre>
<p>Here we send some data for the worker object. This data is processed on yielding:</p>
<pre><code class="lang-auto">@thread_worker
def multiplier():
    total = 1
    while True:
        time.sleep(0.1)
        new = yield total # &lt;-- here we get the data from send()
        total *= new if new is not None else 1
        if total == 0:
            return "Game Over!"
</code></pre>
<p>Also here it is true, that <code>yield</code> does not wait for receiving data. When there’s no data to process, <code>yield</code> returns with <code>None</code>. In the next line the value of <code>new</code> is checked whether it contains a value other than <code>None</code>.</p>
<p>In your case, it can be fixed with a similar trick:</p>
<pre><code class="lang-auto">import napari
import numpy as np
from napari.qt.threading import thread_worker
from magicgui import magicgui
import time

list_dimensions = [1,2,4,8,16]
@thread_worker
def image_generator(list_dimensions):
    for dim in list_dimensions:
        a = np.random.rand(dim, dim)
        b = np.random.rand(dim, dim)
        
        b = b &gt; 0.5
        yielded = yield (a, b)  #|
        while not yielded:      #|=&gt; waiting for data from 'worker.send', meaning 'on_yielded' finished
            yielded = yield     #|

        
viewer = napari.Viewer()
worker = image_generator(list_dimensions)
        
        
def on_yielded(value):
    if value is None:
        return
    print("Yielded smtg")
    worker.pause()
    print(f"Worker paused : {worker.is_paused}")
    im_layer = viewer.add_image(value[0])
    l_layer = viewer.add_labels(value[1])
    worker.send(True)

    
def fetch_new_image():
    worker.resume()
    print(worker.is_running)
    
@magicgui()
def next_image():
    fetch_new_image()


worker.yielded.connect(on_yielded)
viewer.window.add_dock_widget(next_image)


worker.start()
napari.run()
</code></pre>
<p>I’m not sure if I’m correct about all parts, maybe someone from the napari team can correct me if not. But the code seems to work, try it!</p> ;;;; <p>Hi,</p>
<p>I do not manage to import data in a fresh omero docker deployment.</p>
<p>The server runs, login works fine, importing inplace inside the server works, but I have a IceConnectException when trying to “clasically” import data.</p>
<p>Here is a minimal repository reproducing the issue:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/glyg/minimal_import_fail-">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/glyg/minimal_import_fail-" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9.png 2x" data-dominant-color="EBEEEA"></div>

<h3><a href="https://github.com/glyg/minimal_import_fail-" target="_blank" rel="noopener nofollow ugc">GitHub - glyg/minimal_import_fail-: a minimal example of import fail in omero...</a></h3>

  <p>a minimal example of import fail in omero CLI. Contribute to glyg/minimal_import_fail- development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>The docker-compose file:</p>
<pre><code class="lang-yaml">version: "3"

services:
  db_omero:
    image: "postgres:11"
    environment:
      POSTGRES_USER: omero
      POSTGRES_DB: omero
      POSTGRES_PASSWORD: omero
    networks:
      - omero

  omeroserver:
    image: openmicroscopy/omero-server:latest
    container_name: omeroserver
    environment:
      CONFIG_omero_db_host: db_omero
      # This is the postgres service name in docker-compose
      CONFIG_omero_db_user: omero
      CONFIG_omero_db_pass: omero
      CONFIG_omero_db_name: omero
      # This is the omeroserver root password
      ROOTPASS: omero
      OMERO_ROOT_PASSWORD: omero
    networks:
      - omero
    ports:
      - "4064:4064"
      - "4065:4065"
      - "4066:4066"

networks:
  omero:
</code></pre>
<p>The script:</p>
<pre><code class="lang-sh">#!/usr/bin/bash

# needs omero-py
set -eu
docker-compose up -d

OMERO_USER=root
OMERO_PASS=omero
OMERO=/opt/omero/server/OMERO.server/bin/omero

# Wait up to 2 mins
docker-compose exec -T omeroserver $OMERO login -C -s localhost -u "$OMERO_USER" -q -w "$OMERO_PASS" --retry 120
echo "OMERO.server connection established"
omero import -u root -w omero -s localhost -p 4064 img0.tif

</code></pre>
<p>The error:</p>
<details>
<pre><code class="lang-auto">2023-03-10 13:54:45,084 105        [      main] INFO          ome.formats.importer.ImportConfig - OMERO.blitz Version: 5.6.0
2023-03-10 13:54:45,093 114        [      main] INFO          ome.formats.importer.ImportConfig - Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022
2023-03-10 13:54:45,133 154        [      main] INFO   formats.importer.cli.CommandLineImporter - Log levels -- Bio-Formats: ERROR OMERO.importer: INFO
2023-03-10 13:54:45,300 321        [      main] INFO      ome.formats.importer.ImportCandidates - Depth: 4 Metadata Level: MINIMUM
2023-03-10 13:54:45,390 411        [      main] INFO      ome.formats.importer.ImportCandidates - 1 file(s) parsed into 1 group(s) with 1 call(s) to setId in 88ms. (90ms total) [0 unknowns]
2023-03-10 13:54:45,409 430        [      main] INFO       ome.formats.OMEROMetadataStoreClient - Attempting initial SSL connection to localhost:4064
2023-03-10 13:54:45,595 616        [      main] INFO       ome.formats.OMEROMetadataStoreClient - Insecure connection requested, falling back
2023-03-10 13:54:45,629 650        [      main] ERROR  formats.importer.cli.CommandLineImporter - Error during import process.
Ice.ConnectFailedException: java.net.ConnectException: Connexion refusée
	at IceInternal.AsyncResultI.__wait(AsyncResultI.java:276)
	at Ice.ObjectPrxHelperBase.end_ice_isA(ObjectPrxHelperBase.java:310)
	at Ice.ObjectPrxHelperBase.ice_isA(ObjectPrxHelperBase.java:92)
	at Ice.ObjectPrxHelperBase.ice_isA(ObjectPrxHelperBase.java:69)
	at Ice.ObjectPrxHelperBase.checkedCastImpl(ObjectPrxHelperBase.java:2810)
	at Ice.ObjectPrxHelperBase.checkedCastImpl(ObjectPrxHelperBase.java:2770)
	at Glacier2.RouterPrxHelper.checkedCast(RouterPrxHelper.java:1787)
	at omero.client.getRouter(client.java:889)
	at omero.client.createSession(client.java:810)
	at omero.client.joinSession(client.java:745)
	at omero.client.createClient(client.java:544)
	at ome.formats.OMEROMetadataStoreClient.unsecure(OMEROMetadataStoreClient.java:810)
	at ome.formats.OMEROMetadataStoreClient.initialize(OMEROMetadataStoreClient.java:770)
	at ome.formats.importer.ImportConfig.createStore(ImportConfig.java:381)
	at ome.formats.importer.cli.CommandLineImporter.&lt;init&gt;(CommandLineImporter.java:158)
	at ome.formats.importer.cli.CommandLineImporter.main(CommandLineImporter.java:1021)
Caused by: java.net.ConnectException: Connexion refusée
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at IceInternal.Network.doFinishConnect(Network.java:437)
	at IceInternal.StreamSocket.connect(StreamSocket.java:96)
	at IceInternal.TcpTransceiver.initialize(TcpTransceiver.java:24)
	at Ice.ConnectionI.initialize(ConnectionI.java:1921)
	at Ice.ConnectionI.message(ConnectionI.java:940)
	at IceInternal.ThreadPool.run(ThreadPool.java:395)
	at IceInternal.ThreadPool.access$300(ThreadPool.java:12)
	at IceInternal.ThreadPool$EventHandlerThread.run(ThreadPool.java:832)
	at java.base/java.lang.Thread.run(Thread.java:833)
</code></pre>
<details></details></details> ;;;; <p>Hello everybody,</p>
<p>facing the same problem, I’ve made fast <a href="https://github.com/ekatrukha/Metamorph_nD_5D" rel="noopener nofollow ugc">Metamorph nd 5D reader</a>/loader  plugin (opening data as virtual hyperstack), maybe somebody will find it useful.<br>
Basically using one TIF configuration to read the rest.<br>
Found this topic a bit too late <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> Still, it can be helpful for “non-coding” people.</p>
<p>Cheers,<br>
Eugene</p> ;;;; <p>Hi folks,<br>
I have a two channel color composite.<br>
<a class="attachment" href="/uploads/short-url/tuqjbf2HtXD5J7QwN7CGNUIj4o3.tif">composite.tif</a> (18.6 KB)</p>
<p>Lets say I want the same Pixel value in Channel 2 to appear 10 times darker than in Channel 1, without altering the actual pixel values or changing the maximum Intensity value of Channel 1. Just altering the LUT for Channel 2.<br>
It is a 16-bit Image and while raising the maximum intensity in the Brightness&amp;Contrast tab lowers the brightness, it only has an effect till the maximum 16bit value of 65535. I can set the maximum intensity value to let’s say 100k, but it has no further effect, unless I convert the image to 32bit. The maximum window width to have an effect on the LUT seems to be 65535. So my question is: Do you know any possibility to have a bigger effective window width than 65535 for visualization (some kind of stretching of the LUT?) without converting to 32bit which would double the needed disk space?</p>
<p>Thank you for your input!</p> ;;;; <p>No, 3.8, 3.9 should be fine</p> ;;;; <p><a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a> <a class="mention" href="/u/jluethi">@jluethi</a> Thanks a ton guys, I can’t wait to have cellpose on GPU! Fingers crossed</p> ;;;; <p>Hi! Did someone find out how it could be solved? I have the same problem.</p> ;;;; <p>Hi <a class="mention" href="/u/carsten2023">@Carsten2023</a>. Welcome to the forum!</p>
<p>There’s a great search functionality here that will let you see similar questions that have been asked. Here is an example, searching for “distance between lines”:<br>
<a href="https://forum.image.sc/search?expanded=true&amp;q=distance%20between%20lines">https://forum.image.sc/search?expanded=true&amp;q=distance%20between%20lines</a></p>
<p>Perhaps some of these approaches would help?</p> ;;;; <p>It works now. Activating the conda environment with filepath to the specific python version seems to work fine. The notebook then executes without any issues.</p>
<p>Thanks <a class="mention" href="/u/nicokiaru">@NicoKiaru</a> for all the help.</p> ;;;; <p>I’m going to be away from my office computer for the weekend, but I’ll downgrade the matplotlib version first thing Monday morning and let you know what happens</p>
<p>My Python version is 3.8.16</p>
<p>Is there something I should do with the Python version as well?</p> ;;;; <p>Hello i works with the plugin " segmentation / “clor clustering” on ImageJ.</p>
<p>The plugin works perfect to analyse the cover of crops on the field.</p>
<p>But the record a macro does not work!</p>
<p>Can anyone help me make it automatic?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81.jpeg" data-download-href="/uploads/short-url/28eUTo1wgC9GQ7TQV8bTWShH225.jpeg?dl=1" title="color4" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81_2_435x500.jpeg" alt="color4" data-base62-sha1="28eUTo1wgC9GQ7TQV8bTWShH225" width="435" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81_2_435x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81.jpeg 2x" data-dominant-color="E1E5E8"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">color4</span><span class="informations">606×695 87.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce.jpeg" data-download-href="/uploads/short-url/f5BNZ4s16YPFKagXAYLqypYK0Sa.jpeg?dl=1" title="color3" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce_2_690x431.jpeg" alt="color3" data-base62-sha1="f5BNZ4s16YPFKagXAYLqypYK0Sa" width="690" height="431" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce.jpeg 2x" data-dominant-color="9D9776"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">color3</span><span class="informations">1211×758 290 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee.jpeg" data-download-href="/uploads/short-url/9RYSO89QYwO2WvbkQqFmMg49aEu.jpeg?dl=1" title="Color" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee_2_690x416.jpeg" alt="Color" data-base62-sha1="9RYSO89QYwO2WvbkQqFmMg49aEu" width="690" height="416" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee_2_690x416.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee_2_1035x624.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee.jpeg 2x" data-dominant-color="7D7D7D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Color</span><span class="informations">1259×760 309 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thanks<br>
Marcus<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4ce98020f59eaa28e420f3e35860b325f0838365.jpeg" alt="color2" data-base62-sha1="aYoyJH4RdXA1UtdYU72zKjXAVSZ" width="506" height="168"></p> ;;;; <p>Can you downgrade to matplotlib == 3.5.1 or 3.6.1 and see if this helps. What is your python version?</p> ;;;; <p>Hello guys,</p>
<p>I have another question because I cannot simply find the answer.<br>
I created one annotation called “tumor” and a rectangle which includes the tumor area. Now I want to inverse the tumor area within the rectangle to get a new area called stroma (but within in the stroma). If I only inverse the tumor area, QuPath creates an annotation which includes the whole slide which is not demandend.<br>
How can i do that?</p>
<p>E.g.<br>
I want to substract the violet area from the red to get a new area which includes everything else inside the rectangle</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bac8f176dac7474a809cb451216cffa68db215d3.jpeg" data-download-href="/uploads/short-url/qEnkmeVk5zAXdkbJVKTesZAzq7N.jpeg?dl=1" title="grafik" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bac8f176dac7474a809cb451216cffa68db215d3.jpeg" alt="grafik" data-base62-sha1="qEnkmeVk5zAXdkbJVKTesZAzq7N" width="656" height="500" data-dominant-color="BB67A5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">grafik</span><span class="informations">872×664 92.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p><a class="mention" href="/u/ilya_belevich">@Ilya_Belevich</a> : Computer Vision Toolbox has been installed.</p> ;;;; <p>Thanks <a class="mention" href="/u/folterj">@folterj</a> <a class="mention" href="/u/ken.ho">@ken.ho</a> , created an issue <a href="https://github.com/ome/omero-py/issues/366" class="inline-onebox">getResolutionDescriptions pyramids order · Issue #366 · ome/omero-py · GitHub</a> for the problem</p> ;;;; <p>Hey <a class="mention" href="/u/kleski">@kleski</a> the first thing to check is the error posted. Are all your images exactly the same size and shape? This would be both:</p>
<ul>
<li>Your signal and autofluroesence images are the same (only the voxel intensity varies)</li>
<li>If your data is a directory of 2D tiffs, then each one is exactly the same size and shape.</li>
</ul>
<p>If either of these assumptions are not true, then cellfinder won’t be able to run.</p> ;;;; <p><a class="mention" href="/u/ilya_belevich">@Ilya_Belevich</a> We started with a selection of toolboxes we thought would cover most uses. However if you want the computer vision toolbox, we can install that in BAND. I can let you know once that is done</p> ;;;; <p>Hi Pete,</p>
<p>Thanks for the quick reply. Perfect, have managed to get it to run!</p> ;;;; <p>Hi <a class="mention" href="/u/li1234">@Li1234</a> your script provides the image, but it needs to give the path to the model instead.</p>
<p>The documentation was also updated for v0.4.3, so it would be better to use the scripts from there: <a href="https://qupath.readthedocs.io/en/0.4/docs/deep/stardist.html" class="inline-onebox">StarDist — QuPath 0.4.3 documentation</a></p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b.jpeg" data-download-href="/uploads/short-url/9gA5RL2ZKcE3iCzeM74PjMaZ5ub.jpeg?dl=1" title="Contour" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_192x500.jpeg" alt="Contour" data-base62-sha1="9gA5RL2ZKcE3iCzeM74PjMaZ5ub" width="192" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_192x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_288x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_384x1000.jpeg 2x" data-dominant-color="161616"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Contour</span><span class="informations">1554×4026 96.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236.jpeg" data-download-href="/uploads/short-url/3El57GmjAJDFw7N9m0aFv5j7CBg.jpeg?dl=1" title="Contour_distances" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_204x500.jpeg" alt="Contour_distances" data-base62-sha1="3El57GmjAJDFw7N9m0aFv5j7CBg" width="204" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_204x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_306x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_408x1000.jpeg 2x" data-dominant-color="232323"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Contour_distances</span><span class="informations">461×1125 27.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Hello everyone,</p>
<p>I would have to measure the contour as automatically as possible (macro). Specifically, the following values would be important for me:</p>
<ul>
<li>mean value of the distance between contour left and right</li>
<li>smallest distance between contour left and right (xmin)</li>
<li>largest distance between contour left and right (xmax)</li>
<li>distance between contour left and right measured exactly in the middle top to bottom (x at ymiddle)</li>
</ul>
<p>The same in y-direction (contour from top to bottom).</p>
<p>Does anyone have a solution for me?</p>
<p>Thank you a lot in advance!</p> ;;;; <p>Hi all,</p>
<p>I’m completely new to coding/deep learning and I will admit most of this thread has flown over my head. I was wondering whether someone could clarify - can I use stardist with Qupath Version: 0.4.3 on an M1 Mac? I can’t seem to open <a href="https://github.com/qupath/models/raw/main/stardist/he_heavy_augment.pb" rel="noopener nofollow ugc">he_heavy_augment.pb</a>  in qupath, and when I try running this script (from qupath v0.3.0 docs) using one of my mrxs files I get the error message that tensoflow is required</p>
<p>import qupath.ext.stardist.StarDist2D</p>
<p>// Specify the model file (you will need to change this!)<br>
var pathModel = ‘/Volumes/VERBATIM HD/Slide Scanner/Flow through gels/CD68/FT31/slide-2023-02-24T09-20-06-R9-S10.mrxs’</p>
<p>var stardist = StarDist2D.builder(pathModel)<br>
.threshold(0.5)              // Prediction threshold<br>
.normalizePercentiles(1, 99) // Percentile normalization<br>
.pixelSize(0.5)              // Resolution for detection<br>
.build()</p>
<p>// Run detection for the selected objects<br>
var imageData = getCurrentImageData()<br>
var pathObjects = getSelectedObjects()<br>
if (pathObjects.isEmpty()) {<br>
Dialogs.showErrorMessage(“StarDist”, “Please select a parent object!”)<br>
return<br>
}<br>
stardist.detectObjects(imageData, pathObjects)<br>
println ‘Done!’</p>
<p>INFO: Cannot build model with qupath.opencv.dnn.OpenCVDnnModelBuilder@537857b6<br>
ERROR: Unable to load TensorFlow with reflection - are you sure it is available and on the classpath?<br>
ERROR: qupath.ext.tensorflow.TensorFlowTools<br>
java.lang.ClassNotFoundException: qupath.ext.tensorflow.TensorFlowTools<br>
at java.base/java.net.URLClassLoader.findClass(Unknown Source)<br>
at java.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>
at java.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>
at java.base/java.lang.Class.forName0(Native Method)<br>
at java.base/java.lang.Class.forName(Unknown Source)<br>
at qupath.ext.stardist.StarDist2D$Builder.build(StarDist2D.java:676)<br>
at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)<br>
at QuPathScript.run(QuPathScript:7)<br>
at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>
at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>
at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>
at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>
at qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>
at java.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>
at java.base/java.lang.Thread.run(Unknown Source)<br>
ERROR: qupath.ext.tensorflow.TensorFlowTools in QuPathScript at line number 6</p>
<p>ERROR: java.base/java.net.URLClassLoader.findClass(Unknown Source)<br>
java.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>
java.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>
java.base/java.lang.Class.forName0(Native Method)<br>
java.base/java.lang.Class.forName(Unknown Source)<br>
qupath.ext.stardist.StarDist2D$Builder.build(StarDist2D.java:676)<br>
org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)<br>
QuPathScript.run(QuPathScript:6)<br>
org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>
org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>
qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>
qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>
qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>
java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>
java.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>
java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>
java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>
java.base/java.lang.Thread.run(Unknown Source)</p> ;;;; <p>Hey, I also have a very similar problem. Does your GPU memmory usage increase during the training? What is the size of your images?</p> ;;;; <p>Does anyone know how QuPath determines which resolutions to show to the user? I have noticed many whole slide images contain resolutions which include the entire slide including the areas that should not be shown to the user like the label area on a slide. I’m creating a whole slide image viewer control for .NET and I want it to be like QuPath.</p> ;;;; <p>Awesome that it works now <a class="mention" href="/u/judith_pineau">@Judith_Pineau</a> - i didn’t know unzipping on windows could have this kind of side-effects - good to know.<br>
And thanks <a class="mention" href="/u/btbest">@btbest</a> for the <code>--logfile</code>… idk how I hallucinated the underscore <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi,</p>
<p>I try to launch Fiji with Java 8 on ubuntu (Ubuntu 22.04.1) but it does not work when using:</p>
<pre><code class="lang-auto">$HOME/Programs/Fiji/Fiji.app/ImageJ-linux64 --java-home \  
/usr/lib/jvm/java-8-openjdk-amd64
</code></pre>
<p>I receive the following error message: <code>Could not launch system-wide Java (No such file or directory)</code><br>
However, when I launch<code> $HOME/Programs/Fiji/Fiji.app/ImageJ-linux64</code> , Fiji opens correctly.</p>
<p>Any ideas on how I could fix my problem and launch Fiji  with a different version of Java?</p>
<p>Cheers</p> ;;;; <p>Thanks <a class="mention" href="/u/shubo_chakrabarti">@Shubo_Chakrabarti</a>,<br>
re-linking to the individual license solved the issue!</p>
<p>What defines the list of toolboxes available on BAND?<br>
For example, I am missing “Computer Vision Toolbox”…</p>
<p>Best regards,<br>
Ilya</p> ;;;; <p>Hi I am trying to use this BeanShell script for the trainable weka segmentation.</p>
<p><a class="attachment" href="/uploads/short-url/8kgZRopGZg1V9UWfGm1QRXT7JsA.txt">skriptAuto.txt</a> (2.1 KB)</p>
<p>I would like to apply a created classifier to a new image. Unfortunately i get this error message and the statement could not apply classifier.</p>
<p>WARNING: core mtj jar files are not available as resources to this classloader (sun.misc.Launcher$AppClassLoader@764c12b6)<br>
java.lang.NullPointerException<br>
java.util.Arrays.fill(Arrays.java:3021)<br>
trainableSegmentation.WekaSegmentation.setFeaturesDirty(WekaSegmentation.java:7435)<br>
trainableSegmentation.WekaSegmentation.adjustSegmentationStateToData(WekaSegmentation.java:4983)<br>
trainableSegmentation.WekaSegmentation.loadClassifier(WekaSegmentation.java:814)<br>
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>
java.lang.reflect.Method.invoke(Method.java:498)<br>
bsh.Reflect.invokeMethod(Reflect.java:160)<br>
bsh.Reflect.invokeObjectMethod(Reflect.java:93)<br>
bsh.Name.invokeMethod(Name.java:854)<br>
bsh.BSHMethodInvocation.eval(BSHMethodInvocation.java:67)<br>
bsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:94)<br>
bsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:39)<br>
bsh.Interpreter.eval(Interpreter.java:673)<br>
bsh.Interpreter.eval(Interpreter.java:764)<br>
bsh.Interpreter.eval(Interpreter.java:753)<br>
bsh.BshScriptEngine.evalSource(BshScriptEngine.java:89)<br>
bsh.BshScriptEngine.eval(BshScriptEngine.java:61)<br>
javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)<br>
org.scijava.script.ScriptModule.run(ScriptModule.java:164)<br>
org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)<br>
org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>
org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)<br>
org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)<br>
java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>
java.lang.Thread.run(Thread.java:750)<br>
at java.util.Arrays.fill(Arrays.java:3021)<br>
at trainableSegmentation.WekaSegmentation.setFeaturesDirty(WekaSegmentation.java:7435)<br>
at trainableSegmentation.WekaSegmentation.adjustSegmentationStateToData(WekaSegmentation.java:4983)<br>
at trainableSegmentation.WekaSegmentation.loadClassifier(WekaSegmentation.java:814)<br>
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>
at java.lang.reflect.Method.invoke(Method.java:498)<br>
at bsh.Reflect.invokeMethod(Reflect.java:160)<br>
at bsh.Reflect.invokeObjectMethod(Reflect.java:93)<br>
at bsh.Name.invokeMethod(Name.java:854)<br>
at bsh.BSHMethodInvocation.eval(BSHMethodInvocation.java:67)<br>
at bsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:94)<br>
at bsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:39)<br>
at bsh.Interpreter.eval(Interpreter.java:673)<br>
at bsh.Interpreter.eval(Interpreter.java:764)<br>
at bsh.Interpreter.eval(Interpreter.java:753)<br>
at bsh.BshScriptEngine.evalSource(BshScriptEngine.java:89)<br>
at bsh.BshScriptEngine.eval(BshScriptEngine.java:61)<br>
at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)<br>
at org.scijava.script.ScriptModule.run(ScriptModule.java:164)<br>
at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)<br>
at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>
at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)<br>
at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)<br>
at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>
at java.lang.Thread.run(Thread.java:750)</p>
<p>Can someone help me ? I am at a loss.<br>
Thank you!</p> ;;;; <p>Hi <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a>,<br>
this is indeed it! The data that I intend to work with is RGB, so I will explore how to work with an image where I have split the channels.</p>
<p>Hi <a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a>,<br>
thanks for these instructions on how to obtain the sizes of the different layers. And also, I now know that I can run commands in the notebook while napari is running.</p>
<p>I am quite new to napari (started yesterday <img src="https://emoji.discourse-cdn.com/twitter/see_no_evil.png?v=12" title=":see_no_evil:" class="emoji" alt=":see_no_evil:" loading="lazy" width="20" height="20">), there seem to be so many possibilities. I hope to master more soon and get my head around the different options and comands I can use. Thanks for getting me unstuck here.</p> ;;;; <aside class="quote no-group" data-username="Mike_Nelson" data-post="18" data-topic="77554">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mike_nelson/40/26532_2.png" class="avatar"> Mike Nelson:</div>
<blockquote>
<p>if <code>null in cell.classifications</code> does not work, maybe <a class="mention" href="/u/petebankhead">@petebankhead</a> will have an idea for how to frame that. Or it might be the same, where <code>cell -&gt; cell.classifications == null</code></p>
</blockquote>
</aside>
<p>I haven’t followed all this, but all of these should work to find cells without classifications:</p>
<pre><code class="lang-groovy">def unclassifiedCells = cells.findAll {!it.classifications}
def unclassifiedCells2 = cells.findAll {it.classifications.isEmpty()}
def unclassifiedCells3 = cells.findAll {!it.pathClass}
def unclassifiedCells4 = cells.findAll {it.pathClass == null}
</code></pre>
<p>Although I prefer to write it like this (which works too…)</p>
<pre><code class="lang-groovy">def unclassifiedCells = cells.findAll(c -&gt; c.classifications)
def unclassifiedCells2 = cells.findAll(c -&gt; c.classifications.isEmpty())
def unclassifiedCells = cells.findAll(c -&gt; !c.pathClass)
def unclassifiedCells2 = cells.findAll(c -&gt; c.pathClass == null)
</code></pre>
<p>since I think it looks a (little) bit less confusing.</p> ;;;; <p>Hi Konrad,</p>
<p>Thanks for your help yesterday.</p>
<p>I was able to activate the refine tracklets GUI <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>However, I quickly noticed that the 3 bodyparts were identified on the wrong individual throughout the whole video.</p>
<p>I watched the tutorial <a href="https://www.youtube.com/watch?v=bEuBKB7eqmk" rel="noopener nofollow ugc">DeepLabCut 2.2: how to use the refine tracklets GUI! - YouTube</a> about ‘flagging’ the section of the video where the swap occurs, but because the individual labels are wrong throughout the video, I first tried to create a flag window from the start to the end. However, I kept getting this error:</p>
<p>deeplabcut.refine_tracklets(<br>
…: config_path,<br>
…: h5_pickle,<br>
…: analysed_video,<br>
…: )<br>
Traceback (most recent call last):<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\cbook_<em>init</em><em>.py”, line 304, in process<br>
func(*args, **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\widgets.py”, line 247, in<br>
return self.<em>observers.connect(‘clicked’, lambda event: func(event))<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\deeplabcut\gui\tracklet_toolbox.py”, line 532, in flag_frame<br>
ax.fill_between(<br>
File "C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib</em><em>init</em></em>.py", line 1472, in inner<br>
return func(ax, *map(sanitize_sequence, args), **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5431, in fill_between<br>
return self._fill_between_x_or_y(<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5420, in _fill_between_x_or_y<br>
pts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\numpy\ma\core.py”, line 3254, in <strong>getitem</strong><br>
mout = _mask[indx]<br>
IndexError: invalid index to scalar variable.</p>
<p>I also tried with a smaller window just to see if I could get it to work, but I kept getting the same error.</p>
<p>I also pressed the flag button twice at the end of my defined window, and a second red vertical line popped up, but no other shaded area defining the flag window popped up.</p>
<p>Anyway, I lasso the 3 bodyparts of a particular individual following the tutorial and selected the other individual name on the top of the GUI and I got this error:</p>
<p>Traceback (most recent call last):<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\cbook_<em>init</em>_.py”, line 304, in process<br>
func(*args, **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\deeplabcut\gui\tracklet_toolbox.py”, line 684, in on_pick<br>
self.ax_slider.lines.clear()<br>
AttributeError: ‘ArtistList’ object has no attribute ‘clear’</p>
<p>I saw in a previous post a similar issue: <a href="https://forum.image.sc/t/how-to-use-refine-tracklets-gui/65589">How to use refine tracklets GUI</a></p>
<p>which you suggested updating matplotlib version. I did this and my current version is 3.7.1 (below) but I keep encountering the same error</p>
<p>pip show matplotlib<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Name: matplotlib<br>
Version: 3.7.1<br>
Summary: Python plotting package<br>
Home-page: <a href="https://matplotlib.org/" rel="noopener nofollow ugc">https://matplotlib.org</a><br>
Author: John D. Hunter, Michael Droettboom<br>
Author-email: <a href="mailto:matplotlib-users@python.org">matplotlib-users@python.org</a><br>
License: PSF<br>
Location: c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages<br>
Requires: contourpy, cycler, fonttools, importlib-resources, kiwisolver, numpy, packaging, pillow, pyparsing, python-dateutil<br>
Required-by: deeplabcut, filterpy, imgaug<br>
Note: you may need to restart the kernel to use updated packages.</p>
<p>I also checked how to restart kernel and used the following command which was suggested here <a href="https://stackoverflow.com/questions/37751120/restart-ipython-kernel-with-a-command-from-a-cell:" rel="noopener nofollow ugc">jupyter - Restart ipython Kernel with a command from a cell - Stack Overflow</a></p>
<p>import os<br>
os._exit(00)</p>
<p>Any further help will be greatly appreciated <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Thanks</p> ;;;; <p>Hi Konrad,</p>
<p>Thanks for your help yesterday.</p>
<p>I was able to activate the refine tracklets GUI <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>However, I quickly noticed that the 3 bodyparts were identified on the wrong individual throughout the whole video.</p>
<p>I watched the tutorial <a href="https://www.youtube.com/watch?v=bEuBKB7eqmk" class="inline-onebox" rel="noopener nofollow ugc">DeepLabCut 2.2: how to use the refine tracklets GUI! - YouTube</a> about ‘flagging’ the section of the video where the swap occurs, but because the individual labels are wrong throughout the video, I first tried to create a flag window from the start to the end. However, I kept getting this error:</p>
<p>deeplabcut.refine_tracklets(<br>
…: config_path,<br>
…: h5_pickle,<br>
…: analysed_video,<br>
…: )<br>
Traceback (most recent call last):<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\cbook_<em>init</em>_.py”, line 304, in process<br>
func(*args, **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\widgets.py”, line 247, in <br>
return self.<em>observers.connect(‘clicked’, lambda event: func(event))<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\deeplabcut\gui\tracklet_toolbox.py”, line 532, in flag_frame<br>
ax.fill_between(<br>
File "C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib_<em>init</em></em>.py", line 1472, in inner<br>
return func(ax, *map(sanitize_sequence, args), **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5431, in fill_between<br>
return self._fill_between_x_or_y(<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\axes_axes.py”, line 5420, in _fill_between_x_or_y<br>
pts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\numpy\ma\core.py”, line 3254, in <strong>getitem</strong><br>
mout = _mask[indx]<br>
IndexError: invalid index to scalar variable.</p>
<p>I also tried with a smaller window just to see if I could get it to work, but I kept getting the same error.</p>
<p>I also pressed the flag button twice at the end of my defined window, and a second red vertical line popped up, but no other shaded area defining the flag window popped up.</p>
<p>Anyway, I lasso the 3 bodyparts on the individuals following the tutorial and selected the other individual name on the top of the GUI and I got this error:</p>
<p>Traceback (most recent call last):<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\matplotlib\cbook_<em>init</em>_.py”, line 304, in process<br>
func(*args, **kwargs)<br>
File “C:\ProgramData\Anaconda3\envs\deeplabcut\lib\site-packages\deeplabcut\gui\tracklet_toolbox.py”, line 684, in on_pick<br>
self.ax_slider.lines.clear()<br>
AttributeError: ‘ArtistList’ object has no attribute ‘clear’</p>
<p>I saw in a previous post a similar issue: <a href="https://forum.image.sc/t/how-to-use-refine-tracklets-gui/65589" class="inline-onebox">How to use refine tracklets GUI</a></p>
<p>which you suggested updating matplotlib version. I did this and my current version is 3.7.1 (below) but I keep encountering the same error</p>
<p>pip show matplotlib<br>
WARNING: Ignoring invalid distribution -atplotlib (c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages)<br>
Name: matplotlib<br>
Version: 3.7.1<br>
Summary: Python plotting package<br>
Home-page: <a href="https://matplotlib.org" rel="noopener nofollow ugc">https://matplotlib.org</a><br>
Author: John D. Hunter, Michael Droettboom<br>
Author-email: <a href="mailto:matplotlib-users@python.org">matplotlib-users@python.org</a><br>
License: PSF<br>
Location: c:\programdata\anaconda3\envs\deeplabcut\lib\site-packages<br>
Requires: contourpy, cycler, fonttools, importlib-resources, kiwisolver, numpy, packaging, pillow, pyparsing, python-dateutil<br>
Required-by: deeplabcut, filterpy, imgaug<br>
Note: you may need to restart the kernel to use updated packages.</p>
<p>I also checked how to restart kernel and used the following command which was suggested here <a href="https://stackoverflow.com/questions/37751120/restart-ipython-kernel-with-a-command-from-a-cell:" class="inline-onebox" rel="noopener nofollow ugc">jupyter - Restart ipython Kernel with a command from a cell - Stack Overflow</a></p>
<p>import os<br>
os._exit(00)</p>
<p>Any further help will be greatly appreciated <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Thanks</p> ;;;; <p>I was playing around with the script to get it to export the bounding box image and mask but the mask doesn’t seem to save. Here is what I was using:</p>
<pre><code class="lang-auto">import ij.IJ
import qupath.imagej.gui.IJExtension 

def server = getCurrentServer()
 
// Define output path (relative to project)
def name = GeneralTools.getNameWithoutExtension(server.getMetadata().getName())
def pathOutput = buildFilePath(PROJECT_BASE_DIR, 'Tiff_Images', name)
mkdirs(pathOutput)
 
// Don't downsample
double downsample = 1.0
 
// Export each region
int i = 0
for (annotation in getAnnotationObjects().findAll(a -&gt; a.getROI().getNumPoints() == 4)) {
    def annoRoi = annotation.getROI()
    def region = RegionRequest.createInstance(server.getPath(), 1, annoRoi)
    i++
    def label = annotation.getPathClass()
def imp = IJExtension.extractROIWithOverlay(server, annotation, null, region, false, null).getImage()
def outputPath = buildFilePath(pathOutput, name + '_' + label + '.tif')
    IJ.save(imp, outputPath)
}

</code></pre>
<p>Any insight into where I may have gone wrong?</p>
<p>I’ve also been scouring the web to figure out how to adjust the angle of the annotations to avoid the export of the bounding box image with mask and subsequent removal of the area surrounding the mask but haven’t come up with anything as of yet.</p> ;;;; <p>I have thousands of ROIs with overlays and would like to use the edit &gt; clear outside command in batch on all of these to make the area outside the overlay white. I’m new to scripting in imageJ/FIJI and was wondering if anybody had any insight into how I may do this?</p> ;;;; <p>Hi <a class="mention" href="/u/adamerickson">@adamerickson</a>! It looks like there’s no obvious support right now for this. If performance is not a huge issue, you can do it with:</p>
<pre><code class="lang-auto">import numpy as np
from scipy import ndimage as ndi

# I -&gt; image, h -&gt; footprint
filtered = ndi.correlate(image, footprint)

# BW -&gt; mask
roi_filtered = np.where(mask, filtered, image)
</code></pre>
<p>This means that you are computing the filtered image everywhere but only keeping it where the mask is True, so much of the computation is wasted if the rois themselves are small. But, it’ll work!</p> ;;;; <p>Thank you for the updated h5 file; the training process is working! Thank you very much for all the help!</p> ;;;; <p>I’ll go ahead and link this here since the article opens with chatGPT <a href="https://forum.image.sc/t/ai-in-pathology/78368" class="inline-onebox">AI in Pathology</a></p> ;;;; <p>In that case maybe it would work. The standard workflow is to export the bounding box image AND the mask of the annotation, which also uses the bounding box. Then multiply the two out in Python or MATLAB or whatever downstream to generate the final image (annotation 1, background 0, then do whatever to pixels in the original image that are equal to 0, like make them white).<br>
You wouldn’t perform that step in QuPath generally.</p> ;;;; <p>Hello Everyone,</p>
<p>I am wondering what the best way is to implement MATLAB’s roifilt2 function using scikit-image?</p>
<p>From their docs, " <code>roifilt2(h</code>,<a href="https://www.mathworks.com/help/images/ref/roifilt2.html#d124e292852" rel="noopener nofollow ugc"><code>I</code></a>,<a href="https://www.mathworks.com/help/images/ref/roifilt2.html#d124e292909" rel="noopener nofollow ugc"><code>BW</code></a>) filters regions of interest (ROIs) in the 2-D image <code>I</code> using the 2-D linear filter <code>h</code>. <code>BW</code> is a binary mask, the same size as <code>I</code>, that defines the ROIs in <code>I</code>. <code>roifilt2</code> returns an image that consists of filtered values for pixels in locations where <code>BW</code> contains <code>1</code>s, and unfiltered values for pixels in locations where <code>BW</code> contains <code>0</code>s. If you specify a filter, <a href="https://www.mathworks.com/help/images/ref/roifilt2.html#d124e292828" rel="noopener nofollow ugc"><code>h</code></a>, then <code>roifilt2</code> calls <a href="https://www.mathworks.com/help/images/ref/imfilter.html" rel="noopener nofollow ugc"><code>imfilter</code></a> to implement the filter."</p>
<p>Thanks!</p>
<p>Adam</p> ;;;; <p>Yeah, rareCellFetcher if just if you want to fine tune the results of the cell detection, otherwise, you have to chose improved settings or use a different type of cell detection. It is intended for nuclear staining. If you don’t have that, you don’t generally use cell detection.</p>
<p>All annotations have cell counts automatically. How you generate the regions is up to you, automatic through ABBA or something similar, or manual.</p>
<p>Example <a href="https://forum.image.sc/t/cell-detections-in-overlapping-annotations-produces-multiple-cell-counts-per-actual-cell/61836/5" class="inline-onebox">Cell Detections in Overlapping Annotations Produces Multiple Cell Counts per Actual cell - #5 by oburri</a><br>
<a href="https://forum.image.sc/t/abba-qupath-script-to-measure-fluorescence-in-atlas-regions/76320" class="inline-onebox">ABBA -&gt; Qupath: script to Measure Fluorescence in Atlas Regions</a><br>
Etc.</p> ;;;; <aside class="quote no-group quote-modified" data-username="Research_Associate" data-post="26" data-topic="77460" data-full="true">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<aside class="quote no-group" data-username="ADW123" data-post="25" data-topic="77460">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>Make the angle of all annotations 90 degrees and then I could run the scale script to make each annotation the area of interest?</p>
</blockquote>
</aside>
<p>That sounds like <a href="https://forum.image.sc/t/batch-resize-annotations-qupath/77460/16" class="inline-onebox">Batch Resize Annotations QuPath - #16 by ADW123</a> ?</p>
<p>I was hoping that the orientation would stay the same which is why I was thinking of an angle adjustment rather removing the annotation and then adding again based on the centroid. However, I wasn’t sure if there was an easy way to extract annotation angles and then adjust them all to 90 degrees?</p>
<aside class="quote no-group" data-username="ADW123" data-post="25" data-topic="77460">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>Export the current polygon annotations with white space outside the boundaries of the annotation so that the whole annotation is exported but anything beyond it isn’t confounding the downstream analysis?</p>
</blockquote>
</aside>
<p>This generally doesn’t work because image analysis methods will read white or black as something or nothing depending on the context, and not ignore it.</p>
</blockquote>
</aside>
<p>I believe our downstream analysis only picks out black round objects (objects of interest) so I anticipated that some white background around the annotation wouldn’t lead to a negative impact in the results?</p> ;;;; <p>Thank you so much for the reply!</p>
<p>I looked into RareCellFetcher, which seems to be a function to label detected cells. It will be useful if I want to delete false positive cells, but it doesn’t seem to tackle the problem of false negative or offer ways to split when multiple cells are detected as one. So far, I am not interested in training a cell detection model. That might be something to consider in the future after I get a semi-automation pipeline work to feel confident about the cell labeling and have accumulated enough labeled images.</p>
<p>I will explore the cell count function more. What I have hoped for is after mapping images to the atlas and have the images segregated into regions, I will be able to get cell counts in dozens of different regions automatically. I noticed there is a function to count cells in a user-defined ROI, but it doesn’t seem to streamline the process of counting cells in multiple regions.</p>
<p>Please let me know if there are already solutions to these problems. I’m new to this software and any help is appreciated!</p> ;;;; <p>Hey Ananya,<br>
Just wanted to follow up and say that we’re working on raising the image cap size. This won’t be enough to fully process WSI, but it will allow larger image crops, which will result in fewer borders/stitching artifacts.</p>
<p>The issue is that the underlying module sends the image data to our web server. So for very large images, it won’t ever be feasible to copy the whole thing over. A longer-term project is adding more direct support for large images within deepcell, to handle these issues. For now, we’ll see about raising the size of crops that can be processed.</p>
<p>Thanks!</p> ;;;; <p>Hello ! I’m trying to learn Napari and wanted to try multithreading with Generator functions as presented in the napari documentation at <a href="https://napari.org/stable/guides/threading.html" class="inline-onebox" rel="noopener nofollow ugc">Multithreading in napari — napari</a></p>
<p>Unfortunately after having written something to understand it, I don’t see why this simple example that I wrote is not working :</p>
<pre><code class="lang-python">import napari
import numpy as np
from napari.qt.threading import thread_worker
from magicgui import magicgui

list_dimensions = [1,2,4,8,16]

@thread_worker
def image_generator(list_dimensions):
    
    for dim in list_dimensions:
        a = np.random.rand(dim, dim)
        b = np.random.rand(dim, dim)
        
        b = b &gt; 0.5
        
        yield (a, b)
        
viewer = napari.Viewer()
worker = image_generator(list_dimensions)
        

        
def on_yielded(value):
    print("Yielded smtg")
    worker.pause()
    print(f"Worker paused : {worker.is_paused}")
    viewer.add_image(value[0])
    viewer.add_labels(value[1])
    
    
def fetch_new_image():
    worker.resume()
    print(worker.is_running)
    
@magicgui()
def next_image():
    fetch_new_image()


worker.yielded.connect(on_yielded)
viewer.window.add_dock_widget(next_image)


worker.start()
napari.run()
</code></pre>
<p>When I run this code, all images are added in the beginning and it seem that I cannot pause the worker in the on_yielded function</p>
<p>If anyone has an idea as to why it is the case ?</p>
<p>Thank you very much for the help !</p> ;;;; <p><strong>Postdoctoral position in Computerized Image Processing with focus on Applications in Biomedicine</strong></p>
<p><strong>Details at:</strong><br>
<strong><a href="https://www.uu.se/en/about-uu/join-us/details/?positionId=601303" class="inline-onebox" rel="noopener nofollow ugc">Postdoctoral position in Computerized Image Processing with focus on Applications in Biomedicine - Uppsala University, Sweden</a></strong></p>
<p><strong>Application deadline: April 6, 2023</strong></p>
<hr>
<p><strong>Research project: Interpretable AI-based multispectral 3D-analysis of cell interrelations in the cancer microenvironment</strong></p>
<p>Immunotherapy has become a life-saving option for advanced cancer patients. However, only a minority of patients develop a durable response. Despite great efforts to explain the variable responses to immunotherapy and to optimize patient selection, current diagnostic tools cannot sufficiently guide clinical practice. This project will combine state-of-the-art multiplexed microscopy with the latest techniques of image processing and deep learning to radically advance the understanding of how cell interrelations in the tumor microenvironment affect the disease progression and treatment efficacy, ultimately leading to improved treatments and saved lives.</p>
<p>Starting from a large collection of acquired multispectral histology images, the project aims to develop advanced interpretable AI-driven approaches for image data analysis, for characterization of the structural 3D organization and interrelations of different cell types, enabling reliable and explainable prediction of patient disease progression.</p>
<p>This project heavily relies on interdisciplinary competences and will be conducted in close collaboration with the researchers at the <a href="https://www.igp.uu.se/" rel="noopener nofollow ugc">Department of Immunology, Genetics and Pathology (IGP)</a> at Uppsala University. IGP has a broad research profile with strong research groups focused on cancer, autoimmune and genetic diseases, and promotes translational research, with close interactions between medical research and health care. (Notably, Nobel Laureate Svante Pääbo was a guest researcher at IGP 2003–2015.)</p>
<p>The 2 year position is funded through research projects conducted by the <a href="https://www.it.uu.se/about_us/divisions/vi3/research/mida" rel="noopener nofollow ugc"><strong>MIDA</strong></a> - Methods for Image Data Analysis - group and financed by <a href="https://www.cancerfonden.se/" rel="noopener nofollow ugc">Cancerfonden</a>(Swedish Cancer Society) and <a href="https://www.vr.se/english.html" rel="noopener nofollow ugc">Swedish Research Council</a>(Vetenskapsrådet).</p>
<p><strong>For further information about the position, please contact:</strong><br>
Prof. Nataša Sladoje, natasa.sladoje@it.uu.se<br>
Prof. Joakim Lindblad, joakim.lindblad@it.uu.se</p> ;;;; <p>I am not sure about the new scripting methods, so going back to the old ones you would use:</p>
<pre><code class="lang-auto">subsetOfCells= cells.findAll{it.getPathClass() == null}
</code></pre>
<p>if <code>null in cell.classifications</code> does not work, maybe <a class="mention" href="/u/petebankhead">@petebankhead</a> will have an idea for how to frame that.  Or it might be the same, where <code>cell -&gt; cell.classifications == null</code></p>
<p>In retrospect, there would not be a <code>null</code> “within” the classifications. <code>null</code> indicates that there is no classification, at all.</p> ;;;; <aside class="quote no-group" data-username="ADW123" data-post="25" data-topic="77460">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>Make the angle of all annotations 90 degrees and then I could run the scale script to make each annotation the area of interest?</p>
</blockquote>
</aside>
<p>That sounds like <a href="https://forum.image.sc/t/batch-resize-annotations-qupath/77460/16" class="inline-onebox">Batch Resize Annotations QuPath - #16 by ADW123</a> ?</p>
<aside class="quote no-group" data-username="ADW123" data-post="25" data-topic="77460">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png" class="avatar"> ADW123:</div>
<blockquote>
<p>Export the current polygon annotations with white space outside the boundaries of the annotation so that the whole annotation is exported but anything beyond it isn’t confounding the downstream analysis?</p>
</blockquote>
</aside>
<p>This generally doesn’t work because image analysis methods will read white or black as something or nothing depending on the context, and not ignore it.</p> ;;;; <aside class="quote no-group" data-username="congconghu" data-post="1" data-topic="78355">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/congconghu/40/68831_2.png" class="avatar"> Congcong Hu:</div>
<blockquote>
<p>The automatic cell detection is error-prone. This is possibly due to the fact that neuron shapes are usually not that regular and there are neurofilaments crossing cell bodies. I would like a function to manually inspact and curate the auto-detection result.</p>
</blockquote>
</aside>
<p>If you classify the cells as good or bad, you can remove the bad ones, but there’s no way to “train” the cell detection method. Unless you mean StarDist or CellPose, in which case retrain away, there is documentation for each on the forum.<br>
Using standard cell detection and classification, you can go through and validate your classifications using  <a href="https://forum.image.sc/t/rarecellfetcher-a-tool-for-annotating-rare-cells-in-qupath/33654" class="inline-onebox">RareCellFetcher- a tool for annotating rare cells in QuPath</a></p>
<aside class="quote no-group" data-username="congconghu" data-post="1" data-topic="78355">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/congconghu/40/68831_2.png" class="avatar"> Congcong Hu:</div>
<blockquote>
<p>The cell count function only exists for brightfield image and I haven’t found any function to count cells per brain region.</p>
</blockquote>
</aside>
<p>Cell detection works for brightfield, IF, etc, anything where there are roughly circular objects to detect as nuclei, either lighter or darker than the background.</p>
<p>If you have the brain regions imported from ABBA, then cell detection of various kinds can be run within those regions.</p> ;;;; <p>All of those findAlls should use curly braces {} , not parentheses ()</p>
<p>EDIT: I’m wrong about this apparently?</p> ;;;; <p>Here is an excellent recent article on AI in Pathology. The issues raised are equally applicable to other medical disciplines.<br>
The LinkedIn link has a link to a pre-print in case you can’t access the journal online but you will need a LinkedIn longin:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://www.sciencedirect.com/science/article/abs/pii/S0740257023000175">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/000a88b211fc700d3929bb1e37ab763b322cfe10.png" class="site-icon" width="16" height="16">

      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0740257023000175" target="_blank" rel="noopener nofollow ugc">sciencedirect.com</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:113/150;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57abd04ecab695c1253298623c39678584e4e958.gif" class="thumbnail" width="113" height="150"></div>

<h3><a href="https://www.sciencedirect.com/science/article/abs/pii/S0740257023000175" target="_blank" rel="noopener nofollow ugc">AI in Pathology: What could possibly go wrong?</a></h3>

  <p>The field of medicine is undergoing rapid digital transformation. Pathologists are now striving to digitize their data, workflows, and interpretations…</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<aside class="onebox allowlistedgeneric" data-onebox-src="https://www.linkedin.com/posts/richardlevenson_ai-in-pathology2023-in-presspdf-activity-7039629015137062912-co5s?utm_source=share&amp;utm_medium=member_desktop">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/5/f525ddc94e173c36457abea5945e26fa22323293.png" class="site-icon" width="64" height="64">

      <a href="https://www.linkedin.com/posts/richardlevenson_ai-in-pathology2023-in-presspdf-activity-7039629015137062912-co5s?utm_source=share&amp;utm_medium=member_desktop" target="_blank" rel="noopener nofollow ugc">linkedin.com</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:375/500;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_375x500.jpeg" class="thumbnail" width="375" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_750x1000.jpeg 2x" data-dominant-color="EAEAEA"></div>

<h3><a href="https://www.linkedin.com/posts/richardlevenson_ai-in-pathology2023-in-presspdf-activity-7039629015137062912-co5s?utm_source=share&amp;utm_medium=member_desktop" target="_blank" rel="noopener nofollow ugc">Richard Levenson, MD, FCAP on LinkedIn: AI in Pathology_2023 in press.pdf</a></h3>

  <p>Trying again to post our new article on AI in Pathology: What could possibly go wrong. This time with images and poem! Hope this works. Thanks.…</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>I’m having an issue where the macro does not give me the option which analysis to run in the beginning, specifically the entire image. How can I resolve this?</p> ;;;; <p><a class="mention" href="/u/k-dominik">@k-dominik</a>,  why is the recommendation to export the probabilities and then threshold? His segmentation so clearly identifies his petals and his anther.  Is there not a way from the segmentation export to directly measure the area of each where the % area of the background, petals, and anther adds up to 100.  I am trying to something similar, following the tutorials I have exported the segmentation and loaded them back in for object classification but end up having to do “resegment” in step 3 Object Classification.  I basically use the same labels and come up with the same segmentation.<br>
I think I am understanding that if you do you segmentation and have a background and a foreground and that puts the backgroud in channel 1 and the foreground in channel 2 how can I call up the foreground channel only and get an area for that.  Ideally, I could get the same for the background and the totals would be 100.  I don’t want to have to reclassify after I have done the pixel classification. I have attached my segmentation and original image.</p>
<p><a class="attachment" href="/uploads/short-url/4ifpeIOfQT2akxdjehBWP6JApyN.tiff">1_2_L_x5_Simple Segmentation.tiff</a> (46.1 KB)</p>
<p>Thanks for your help.<br>
Kristin<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg" data-download-href="/uploads/short-url/Ef8Wlhap0RPA0mhugiuF1GmMQt.jpeg?dl=1" title="1_2_L_x5" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285_2_666x499.jpeg" alt="1_2_L_x5" data-base62-sha1="Ef8Wlhap0RPA0mhugiuF1GmMQt" width="666" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285_2_666x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg 2x" data-dominant-color="ADADAD"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">1_2_L_x5</span><span class="informations">990×743 227 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Update:I tried to threshold on the probabilities map. 1) I don’t exactly know how to best use the smooth feature. 2) I played around with the threshold and could not get it exactly how I want it…which was already done in the pixel classification…the blue is background and the yellow is foreground. In this image you can see that large areas of the yellow are not being captured, if I adjust the threshold to get all the yellow it gets too much of the blue… Without thresholding or going through featue selection is there a way to get total area of the foreground and background from the original segmentation?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44.jpeg" data-download-href="/uploads/short-url/9Q3go8KJu0W0njNd5AZ3SWO8TEo.jpeg?dl=1" title="probailitiesthreshold" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44_2_690x405.jpeg" alt="probailitiesthreshold" data-base62-sha1="9Q3go8KJu0W0njNd5AZ3SWO8TEo" width="690" height="405" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44_2_690x405.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44_2_1035x607.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44.jpeg 2x" data-dominant-color="ACCE8D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">probailitiesthreshold</span><span class="informations">1364×801 156 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hey Leo,</p>
<p>Thank you for your response, that is a very interesting solution. Unfortunately, I have already spent significant time training my pipeline in CellProfiler to work with a variety of cell lines. I am hoping to tweak it just a smidge (but keep processing the same in case there are inherent biases in different softwares) to allow for this new modification.</p>
<p>I also may be not fully understanding your answer, but I don’t explicitly want just the internal part of the nucleus, I want *only the DNA containing regions of the nucleus, which are typically near the nuclear membrane, but as with some of these cells I highlighted above, can have DNA floating somewhere within the middle of the nucleus. So I would literally just want a tiny thick line tracing where the DNA has been lit up with one channel, and then measure that same location in another channel.</p>
<p>I’m sorry if my wording is confusing; if it is, please let me know!</p>
<p>Thanks,</p>
<p>Nick</p> ;;;; <p>Hi,</p>
<p>I am trying to create macro for a circular gradient that goes from 1 in the centre to 0, like below.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578.png" data-download-href="/uploads/short-url/3eUTpU5NKzMsq2DkKinOmU8Gq5G.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_247x250.png" alt="image" data-base62-sha1="3eUTpU5NKzMsq2DkKinOmU8Gq5G" width="247" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_247x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_370x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_494x500.png 2x" data-dominant-color="2D2B2D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">509×514 49.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The inner circle with a radius A (filled with 1) and the larger circle with a radius B (with the gradient); 32-bit image.<br>
(red circles are only for demo)</p>
<p>I can only think of excessively convoluted ways of doing this; If anyone has a suggestion I’d be happy to have it!</p>
<p>Thanks a lot!</p> ;;;; <p>Is it possible for multi-channels to be added, such that the final composite is still grey scale 8- or 16-bit, and have this stack be imported into the 3D script? Or would adding them cause loss of depth data that would alter the addition of scale bars?</p> ;;;; <p>Hello napari community, I’m coming to Europe 3/27-4/5 and I want to meet you!</p>
<p>I’m an Application Scientist at Chan Zuckerberg Initiative, and I’ll be traveling with my colleague Lucy Obus <a class="mention" href="/u/lco">@lco</a>, a user researcher at CZI, to Europe for conferences. We’ll be in Munich, Germany March 27/28, VizBi in Heidelberg, Germany March 28-31, and Focus on Microscopy in Porto, Portugal April 2-5. We’d love to connect with fellow napari users at the conference or come visit your lab or institution in the Munich, Heidelberg, or Porto area. Reply here or reach out at <a href="mailto:dmccarthy@chanzuckerberg.com">dmccarthy@chanzuckerberg.com</a> - we’d love to meet new folks and visit familiar faces. <img src="https://emoji.discourse-cdn.com/twitter/grinning_face_with_smiling_eyes.png?v=12" title=":grinning_face_with_smiling_eyes:" class="emoji" alt=":grinning_face_with_smiling_eyes:" loading="lazy" width="20" height="20"></p> ;;;; <p>So <a class="mention" href="/u/petebankhead">@petebankhead</a> and <a class="mention" href="/u/research_associate">@Research_Associate</a>, this may be a dumb question but I’m exporting the shapes that I adjusted the area for so that I can run them through a MATLAB/ImageJ analysis pipeline using the following script:</p>
<pre><code class="lang-auto">// Export all annotated regions in an image as ImageJ TIFFs named using the label category
 
def server = getCurrentServer()
 
// Define output path (relative to project)
def name = GeneralTools.getNameWithoutExtension(server.getMetadata().getName())
def pathOutput = buildFilePath(PROJECT_BASE_DIR, 'Tiff_Images', name)
mkdirs(pathOutput)
 
// Define output resolution &amp; convert to downsample (or downsample directly)
//double requestedPixelSize = 2.0
//double downsample = requestedPixelSize / server.getPixelCalibration().getAveragedPixelSize()
 
// Don't downsample
double downsample = 1.0
 
// Export each region
int i = 0
for (annotation in getAnnotationObjects().findAll(a -&gt; a.getROI().getNumPoints() == 4)) {
    def region = RegionRequest.createInstance(
        server.getPath(), downsample, annotation.getROI())
    i++
    def label = annotation.getPathClass()
    def outputPath = buildFilePath(pathOutput, name + '_' + label + '.tif')
    writeImageRegion(server, region, outputPath)
}

</code></pre>
<p>For example, I exported the yellow polygon (top) and green polygon (bottom):</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a131ae4531c29aa93f5aad8984f101941ffdb3e.jpeg" data-download-href="/uploads/short-url/hpVbop4kDIB3GSX51xJ7pQ0mG06.jpeg?dl=1" title="Screen Shot 2023-03-09 at 9.09.39 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a131ae4531c29aa93f5aad8984f101941ffdb3e_2_265x500.jpeg" alt="Screen Shot 2023-03-09 at 9.09.39 PM" data-base62-sha1="hpVbop4kDIB3GSX51xJ7pQ0mG06" width="265" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a131ae4531c29aa93f5aad8984f101941ffdb3e_2_265x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a131ae4531c29aa93f5aad8984f101941ffdb3e_2_397x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a131ae4531c29aa93f5aad8984f101941ffdb3e.jpeg 2x" data-dominant-color="C1A48C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-09 at 9.09.39 PM</span><span class="informations">498×938 84.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I got the following outputs:</p>
<p><a class="attachment" href="/uploads/short-url/eeAR45NMLkmcayd97NumBKNi5lZ.tif">MS274_SC_L_Palmgren_40X_LCST_R_3.tif</a> (5.1 MB)</p>
<p><a class="attachment" href="/uploads/short-url/h4sFiAxgJ4gOyiUi9o8QqgXxHaE.tif">MS274_SC_L_Palmgren_40X_LCST_R_2.tif</a> (5.5 MB)</p>
<p>Will the dimensions of these polygons be respected in the export since the annotations are not perfect squares/rectangles? It looks like not? I want to ensure that matched areas are being exported and put through the analysis pipeline.</p>
<p>UPDATE: I found the following post (<a href="https://forum.image.sc/t/exporting-image-within-roi/74192/7" class="inline-onebox">Exporting image within ROI - #7 by Delice_Lumamba</a>) which highlighted that the exported images will only be squares or rectangles which would indicate that the dimensions would not be respected.</p>
<p>Therefore, to address this, would there be a straightforward way to either:</p>
<ol>
<li>
<p>Make the angle of all annotations 90 degrees and then I could run the scale script to make each annotation the area of interest?</p>
</li>
<li>
<p>Export the current polygon annotations with white space outside the boundaries of the annotation so that the whole annotation is exported but anything beyond it isn’t confounding the downstream analysis?</p>
</li>
</ol>
<p>Option <span class="hashtag">#1</span> would be most preferred as I don’t know how the white space would impact downstream analysis (hopefully not at all since we are just trying to identify black dots.</p> ;;;; <p>Hi</p>
<p>I am using Deeplabcut with the GUI to analyze videos by adding markers at 500 frames of a video in which a mouse moves in its cage. Currently the neural network has already been trained and evaluated. I proceed to the analyze videos tab and yet it shows me an error line which says:<br>
AttributeError: ‘AnalyzeVideos’ object has no attribute ‘logger’</p>
<p>Does anyone know what causes this problem and how it can be solved?</p> ;;;; <p>I am trying to create a rudimentary z-stack sequence as below:</p>
<p>sequence = MDASequence(<br>
time_plan={“interval”: 2, “loops”:1},<br>
z_plan={“above”:4 , “below”:4, “step”:0.25},<br>
)</p>
<p>It runs successfully but not as intended. Instead of acquiring 4 steps above and below the current position, it moves all the way back to zero and acquires from -4 to 4. Is there a way to pass the starting z position as reference?</p> ;;;; <p>I am having issues running AxonTracer - does anyone have a tutorial or a stepwise process (other than the direct literature paper from the author) that can explain the steps ? Starting with image preparation (I am starting with confocal images with 2 channels)</p>
<p>I have never used Fiji/ImageJ before and I want to see if this macro can work for our research.</p>
<p>Thanks!</p> ;;;; <p>Hi I had a somewhat similar question, but instead of a different atlas I just want to update the annotations. Specifically I have edited the labels of the ccf2017 atlas to divide a region into subregions, giving them unique id values. In order to use these labels in abba what alterations need to be made?</p>
<p>I assume I need to update the ontology to include my new regions as children of the parent region. When setting the id value (and other parameters) is enough that it is unique or do they need to be inserted in order such that all subsequent ids are incremented? It would be easier if I can just make the ids (40,000, 40,001 …) something far outside the range.<br>
e.g. in the file “1.json” in ./abba_atlases</p>
<pre><code class="lang-auto">{
                     "id": 1085,
                     "atlas_id": 1125,
                     "ontology_id": 1,
                     "acronym": "MOs6b",
                     "name": "Secondary motor area, layer 6b",
                     "color_hex_triplet": "1F9D5A",
                     "graph_order": 29,
                     "st_level": 11,
                     "hemisphere_id": 3,
                     "parent_structure_id": 993,
                     "children": [
                        # proposed subdivisions
                        {"id": 1086, "atlas_id":1126, ...}, 
                        {"id":1087, "atlas_id":1127, ...}, 
]
                    }
</code></pre>
<p>As for the atlas images can I just “swap out” the existing labels for my own, update the region outlines (maybe unnecessary if only utilized for visualization), and recompile the images into a new .h5 with multiple resolutions? Then replace the atlas and ontologies located in the abba_atlases folder?</p>
<p>Or is there a simpler way such as just exporting the transformations applied by abba and applying to my label image? If I can get this transformed image I can apply it to my section images to extract the regions in python.</p> ;;;; <p>I’m almost certain my needs are a subset of the union of all others, but I would call out things like:</p>
<ul>
<li>In my day to day work I build visualization tools.  Channel names are a minimal piece of semantically important data that are not found in the core spec right now.  (granted it was covered in the <code>omero</code> metadata, which is considered “transitional”).  Channel names are something that is relatively important to surface early - for example, letting users select channels to load based on reading their names.</li>
<li>We also have invested in work to convert from Zeiss CZI metadata format to OME-XML (<a href="https://github.com/AllenCellModeling/czi-to-ome-xslt" class="inline-onebox" rel="noopener nofollow ugc">GitHub - AllenCellModeling/czi-to-ome-xslt: A repository of XSL transform sheets to map CZI metadata to OME.</a>).  Getting possibly arbitrarily detailed microscopy settings does seem to imply the need for a catchall thing like the StructuredAnnotation concept.  I think the basic ome-xml model is nice, where you try to have a reasonable coverage of consensus important stuff, and then allow extensibility.</li>
<li>Furthermore on the above point, we will still be in a world where we want to convert from proprietary file format to open standard file format, while maintaining metadata as losslessly as possible.</li>
<li>Notwithstanding the XSLT library I linked above, (and the excellent ome-types library) we aren’t especially tied to continuing to use XML.</li>
<li>It would seem reasonable if .zattrs just held some other url link to the “extended” metadata.</li>
</ul> ;;;; <p>I see. I tried without quotation marks but it still doesn’t work. I also tried using the old <code>getPathClass()</code> command but get a “method does not exist error”.</p> ;;;; <aside class="quote no-group" data-username="Christian_Tischer" data-post="11" data-topic="55987">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png" class="avatar"> Christian Tischer:</div>
<blockquote>
<p><a class="mention" href="/u/joshmoore">@joshmoore</a> is there by now some other way of writing an ImagePlus to OME-Zarr? I thought we started discussing this somewhere and maybe <a class="mention" href="/u/melissa">@melissa</a> had some code, but maybe I am mixing things up…</p>
</blockquote>
</aside>
<p>I’m not aware of anything else that does this either; I definitely don’t have code to write from an ImagePlus.</p> ;;;; <p>Hi <a class="mention" href="/u/cindy_zhu">@Cindy_Zhu</a>,</p>
<p>Yes that is the correct one and also yes, the image needs to be 8- or 16-bit.<br>
How are your channes or your complete image saved?<br>
Since it is fluorescence it should be a grayscale image of a bit-depth between 8 and 32-bit.<br>
I would discourage from dsaving those data as color images (meaning RGB, as labelled at the top of the images when opened in InageJ).<br>
In case of multi-channel images, they are best saved as composite images with all channels being kept physically separate.<br>
In such cases you can convert the channels for display reasons to 8 or 16 bit using <code>&gt;Image &gt;Type</code></p> ;;;; <p>In addition, <a href="https://github.com/stardist/stardist">StarDist</a> should do a pretty good job in such a case.</p>
<p>EDIT: sorry <a class="mention" href="/u/research_associate">@Research_Associate</a>, didn’t see that you had it included already in your post. I just saw the CellPose examples.</p> ;;;; <p>HI <a class="mention" href="/u/biovoxxel">@biovoxxel</a><br>
I just downloaded the 3D script plugin, but it’s asking me to convert it to either an 8- or 16-bit image, and the plugin looks like this:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08.png" data-download-href="/uploads/short-url/qVhpJSfQlbrLVVADigNkeJjO8Tm.png?dl=1" title="Screenshot 2023-03-09 at 3.01.04 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_244x500.png" alt="Screenshot 2023-03-09 at 3.01.04 PM" data-base62-sha1="qVhpJSfQlbrLVVADigNkeJjO8Tm" width="244" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_244x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_366x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_488x1000.png 2x" data-dominant-color="D4DAE2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-09 at 3.01.04 PM</span><span class="informations">794×1624 79.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Is this the plugin that you are working off of as well?</p> ;;;; <p>Are these cyclic bleached/stripped images of the same slide, or images of serial sections on separate slides? Ashlar would not be helpful in any case for serial sections, but my final answer below would still apply.</p>
<p>GeoMX output images have already been stitched, so Ashlar is unable to help here. The error you saw is Ashlar being conservative about how out-of-square the pixels are allowed to be, since the internal math currently assumes they are perfectly square. But even if that were addressed you would not get a useful output from Ashlar for these images. Given enough time and RAM it would produce an image, but this would just be the rigid co-registration of your three stitched images which is not accurate enough for single-cell analysis.</p>
<p>To align your images you would need a non-rigid whole-slide registration tool, of which several have been published recently. Here are some that we have our eye on, but we haven’t evaluated them rigorously for single-cell-level accuracy:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/NHPatterson/wsireg">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/NHPatterson/wsireg" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137.png 2x" data-dominant-color="ECEEED"></div>

<h3><a href="https://github.com/NHPatterson/wsireg" target="_blank" rel="noopener nofollow ugc">GitHub - NHPatterson/wsireg: multimodal whole slide image registration in a...</a></h3>

  <p>multimodal whole slide image registration in a graph structure - GitHub - NHPatterson/wsireg: multimodal whole slide image registration in a graph structure</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/ChristianMarzahl/WsiRegistration">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/ChristianMarzahl/WsiRegistration" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3.png 2x" data-dominant-color="ECEDEE"></div>

<h3><a href="https://github.com/ChristianMarzahl/WsiRegistration" target="_blank" rel="noopener nofollow ugc">GitHub - ChristianMarzahl/WsiRegistration: Robust quad-tree based...</a></h3>

  <p>Robust quad-tree based registration on whole slide images - GitHub - ChristianMarzahl/WsiRegistration: Robust quad-tree based registration on whole slide images</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p><a href="https://valis.readthedocs.io/en/latest/" class="onebox" target="_blank" rel="noopener nofollow ugc">https://valis.readthedocs.io/en/latest/</a></p> ;;;; <p>Hi!<br>
I just installed cellfinder and on our first run we encountered this error:</p>
<p><code>File failed to load with imio. Ensure all image files contain the same number of pixels. Full traceback above.</code></p>
<p>Here is the output log if it helps:<br>
<a class="attachment" href="/uploads/short-url/lbIhexDuOVIrkKxb9SVBbbF3agq.txt">cellfinder_log_2023-03-09_14-19-33.txt</a> (3.5 KB)<br>
And here is our setup steps:<br>
<a class="attachment" href="/uploads/short-url/xZwAtPHT2ufZpwgrmCUKOyQRZ7H.txt">Brainglobe setup notes.txt</a> (287 Bytes)</p> ;;;; <p>I’m trying to get cell counts in different brain regions. qupath provides the function of mapping brain slice images to atlas and identifying cell borders, which I highly appreciate. However, there are a few issues or lack of function so I’m not able to accomplish what I want.</p>
<ol>
<li>The automatic cell detection is error-prone. This is possibly due to the fact that neuron shapes are usually not that regular and there are neurofilaments crossing cell bodies. I would like a function to manually inspact and curate the auto-detection result.</li>
<li>The cell count function only exists for brightfield image and I haven’t found any function to count cells per brain region.</li>
</ol>
<p>These functionalities will be greatly helpful to me and, I believe, also the community. I’m not sure if these are something the developers have in mind or something exist but I am not aware of. I’m thinking about adding these functions myselves but would like suggestions about alternatives or better approaches before I start working on it.</p> ;;;; <p>Glad it’s faster! You can get everything at <a href="https://qupath.github.io/javadoc/docs/" class="inline-onebox">Overview (QuPath 0.4.0)</a></p>
<p>The main ‘default’ scripting functions are at <a href="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html" class="inline-onebox">QP (QuPath 0.4.0)</a></p>
<p>This is also available in v0.4.x directly via <em>Help → Show javadocs</em>.</p> ;;;; <p>Just to followup on <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a> you can check the sizes:</p>
<pre><code class="lang-auto">viewer.layers["Labels"].data.shape   # this will return a 2D tuple, like (256, 256)
viewer.layers["myimg"].data.shape    # this will return a 3D tuple, ending in 3, like (256, 256, 3)
</code></pre> ;;;; <p>Hi Pete,</p>
<p>I tested out the script you shared for putting the tiles in parallel. While it is still slower than using the pixel classifier through the GUI, it’s so much faster on the very large images. My CPU usage actually got up to 70% with that script!</p>
<p>One other question for you. Where is the best place I can find definitions of the functions/variables used for scripting in QuPath? I’ve found examples here and there, like from Image Scientist and here on the Image Forum, but I’m trying to find something more complete, kind of like ImageJ’s built-in macro function page.</p>
<p>Thank you again for your help!</p> ;;;; <p>You could use that or Image &gt; Adjust &gt; Threshold &gt; set a threshold with the Auto button &gt; click Apply. The latter also creates a mask after you’ve set a threshold.</p> ;;;; <p>I’m just using the 3D viewer plugin once the composite has been generated.</p> ;;;; <p>Hi <a class="mention" href="/u/danielle_z">@Danielle_Z</a> ,</p>
<p>I presume APOC is confused about the image format. It doesn’t support RGB images. Could you try with a greyscale image such as the one provided with APOC?</p><aside class="onebox githubblob" data-onebox-src="https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif">
  <header class="source">

      <a href="https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif" target="_blank" rel="noopener">haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif</a></h4>


  This file is binary. <a href="https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif" target="_blank" rel="noopener">show original</a>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Alternatively, you could split the RGB image into three channels (right-click on the image layer in the layer list) and then train on one channel or all three if they are not identical.</p>
<p>Let us know if this helps!</p>
<p>Best,<br>
Robert</p> ;;;; <p>Dear folks,</p>
<p>I am trying to follow along these instructions (goes wrong for me at item 8 in the list under <code>Usage: Object and Semantic Segmentation</code>):</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://pypi.org/project/napari-accelerated-pixel-and-object-classification/">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/8/08f9ca89433aef1933c7c6d5755a75dbf2fafaaa.png" class="site-icon" width="32" height="30">

      <a href="https://pypi.org/project/napari-accelerated-pixel-and-object-classification/" target="_blank" rel="noopener nofollow ugc">PyPI</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f97026709e67b2111b465be6427519ead928642.webp" class="thumbnail onebox-avatar" width="300" height="300">

<h3><a href="https://pypi.org/project/napari-accelerated-pixel-and-object-classification/" target="_blank" rel="noopener nofollow ugc">napari-accelerated-pixel-and-object-classification</a></h3>

  <p>Pixel and label classification using OpenCL-based Random Forest Classifiers</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>It is launched in a jupyter-notebook (venv). I successfully open an image in napari via:<br>
<code>viewer = napari.view_image("path/blobs.jpg")</code></p>
<p>After running the above command I do the following in napari that just opened:</p>
<ol>
<li>Add a labels layer</li>
<li>Draw annotations for 2 layers</li>
<li><code>Tools &gt; Segmentation / labeling &gt; Object Segmentation (APOC)</code></li>
<li>Select options as stated in the instructions (link above).</li>
</ol>
<br>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3.png" data-download-href="/uploads/short-url/hsBWjmTXBHum8xB2W80DuqTsT6z.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3_2_690x447.png" alt="image" data-base62-sha1="hsBWjmTXBHum8xB2W80DuqTsT6z" width="690" height="447" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3_2_690x447.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3_2_1035x670.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3.png 2x" data-dominant-color="404348"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1088×706 108 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<br>
<p>Now when I press <code>Train</code> I receive the following error. It seems that the label layer does not have the same dimensions as the blob image. <code>Selected images and annotation must have the same dimensionality and size!</code><br>
I cannot work out how to solve this and ensure the annotation has the same size as the image (to which I added a Labels layer). I am not sure how to check or adjust the dimensions.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f422c750459b5d949abf44af78a8cfdd98b67df.png" data-download-href="/uploads/short-url/6K4duahCJ9MGzhzkwC9tw7gOx5l.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f422c750459b5d949abf44af78a8cfdd98b67df.png" alt="image" data-base62-sha1="6K4duahCJ9MGzhzkwC9tw7gOx5l" width="690" height="114" data-dominant-color="F0E4E5"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1009×168 17.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Many thanks for your help <img src="https://emoji.discourse-cdn.com/twitter/crossed_fingers/5.png?v=12" title=":crossed_fingers:t5:" class="emoji" alt=":crossed_fingers:t5:" loading="lazy" width="20" height="20"></p> ;;;; <aside class="quote no-group" data-username="Christian_Tischer" data-post="11" data-topic="55987">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png" class="avatar"> Christian Tischer:</div>
<blockquote>
<p>is there by now some other way of writing an ImagePlus to OME-Zarr?</p>
</blockquote>
</aside>
<p>Sorry, I don’t know.</p> ;;;; <p>If you know there will only be one object and want to take the first, you can also use “find” rather than “findAll” and the [0]</p> ;;;; <p>Ah, null is a little bit weird since it is a value, not a class. You might think of it as being the value 0, rather than typing out ‘zero’.<br>
So <code>null</code> does not include any quotation marks as it is not a string or a name. It is just… <code>null</code>.</p>
<p>That also means you cannot use a variable called null since the programming language needs it.</p> ;;;; <p>First of all (<em>very</em> late) and very big to <a class="mention" href="/u/kimberly_meechan">@Kimberly_Meechan</a> for implementing this!!</p>
<p><a class="mention" href="/u/constantinpape">@constantinpape</a> <a class="mention" href="/u/ekaterina_m">@Ekaterina_M</a> do you know how up-to-date this code is?<br>
It seems to write version 0.4… was that still done by <a class="mention" href="/u/kimberly_meechan">@Kimberly_Meechan</a> or some later addition?</p>
<p><a class="mention" href="/u/joshmoore">@joshmoore</a> is there by now some other way of writing an ImagePlus to OME-Zarr? I thought we started discussing this somewhere and maybe <a class="mention" href="/u/melissa">@melissa</a> had some code, but maybe I am mixing things up…</p> ;;;; <p>Nothing to do with the imaging device, just the way it is shown.</p>
<p>What are you using for displaying the 3D image?</p>
<p>I thought the “3D viewer” had the option to switch between orthographic and perspective, but it seems not, sorry.</p> ;;;; <aside class="quote no-group" data-username="petebankhead" data-post="2" data-topic="78349">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar"> Pete:</div>
<blockquote>
<p>getCurrentHierarchy</p>
</blockquote>
</aside>
<p>that has worked out, thanks a lot<br>
just with a little mod, add [0] to take just the single object</p>
<pre><code class="lang-groovy">parentAnnotation = getAnnotationObjects().findAll{it.getPathClass() == getPathClass("Tissue_D")}[0]
print(parentAnnotation)
childDetection = getDetectionObjects().findAll{it.getPathClass() == getPathClass("E-cad-Cleared")}[0]
print(childDetection)

getCurrentHierarchy().addPathObjectBelowParent(parentAnnotation, childDetection, true)
</code></pre>
<p>thanks a lot again</p> ;;;; <p>Is the orthographic camera something through ImageJ, or would changing the imaging device of the sample help with this issue?</p> ;;;; <aside class="quote no-group" data-username="emartini" data-post="1" data-topic="78349">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/emartini/40/292_2.png" class="avatar"> Emanuele Martini:</div>
<blockquote>
<p><code>PathObjectHierarchy()</code></p>
</blockquote>
</aside>
<p>That should be <code>getCurrentHierarchy()</code></p> ;;;; <p>Hey <a class="mention" href="/u/amaranth123">@amaranth123</a> just as an exercise I tried running cellfinder with your data. I converted the data to directories of single-plane tiffs and ran cellfinder with:</p>
<pre><code class="lang-auto">cellfinder -s signal -b background -o cellfinder_output -v 5 5 5 --orientation sal --atlas allen_mouse_25um
</code></pre>
<p>Cell detection seems ok with the default parameters, I’m sure this can be improved with a bit of playing around though. I very quickly (1 minute) generated some training data and re-trained the network (2 epochs), and the results seem fairly promising.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2.png" data-download-href="/uploads/short-url/yuennKmDOdyURxXdOotDcu4Auz0.png?dl=1" title="cf"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2_2_552x500.png" alt="cf" data-base62-sha1="yuennKmDOdyURxXdOotDcu4Auz0" width="552" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2_2_552x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2_2_828x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2.png 2x" data-dominant-color="2F2F2E"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">cf</span><span class="informations">874×791 308 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The registration however isn’t good, I think because there is very little autofluorescence in these images. It’s slightly better using the signal channel than the background though. Could be improved with some tweaking though I expect.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2d5cda4c2f34a80537848f0fa0eb1f01f5af3a.png" data-download-href="/uploads/short-url/t8eALXjwRvmAGxy2OG8V1IMDMTw.png?dl=1" title="bg"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2d5cda4c2f34a80537848f0fa0eb1f01f5af3a.png" alt="bg" data-base62-sha1="t8eALXjwRvmAGxy2OG8V1IMDMTw" width="690" height="414" data-dominant-color="2E2A2B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">bg</span><span class="informations">1530×920 47.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I’m not sure why you’re having so many issues running the tools on your machine, do you have another computer to try?</p> ;;;; <p><a class="mention" href="/u/mike_nelson">@Mike_Nelson</a>, thank you so much for going through each step!</p>
<p>That worked perfectly! Thank you so much!</p>
<p>I added the other phenotype loops (see bellow), and it all worked well except for the ‘null’. This is the correct term, right? Would I need an “else” term in between? or is it beacuse “null” is not in cell.classifications?</p>
<pre><code class="lang-auto">runObjectClassifier("tumor_tcell_macro_null")
def imageData = getCurrentImageData()
def cells = getCellObjects()
subsetOfCells= cells.findAll(cell-&gt; 'macrophage' in cell.classifications)
classifier = loadObjectClassifier("CD68CD163CD11bPDL1PD1")
classifier.classifyObjects(imageData, subsetOfCells, false)
subsetOfCells= cells.findAll(cell-&gt; 'CD3' in cell.classifications)
classifier = loadObjectClassifier("CD8CD11bPDL1PD1")
classifier.classifyObjects(imageData, subsetOfCells, false)
subsetOfCells= cells.findAll(cell-&gt; 'CK' in cell.classifications)
classifier = loadObjectClassifier("PDL1PD1")
classifier.classifyObjects(imageData, subsetOfCells, false)
subsetOfCells= cells.findAll(cell-&gt; 'null' in cell.classifications)
classifier = loadObjectClassifier("CD11bPDL1PD1")
classifier.classifyObjects(imageData, subsetOfCells, false)
fireHierarchyUpdate()
</code></pre> ;;;; <p>Dear all,<br>
I have this scenario:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcada0259649ee306271a191b6d7e703c5853d15.png" alt="image" data-base62-sha1="qV7KQdB20CNd5hlPZSsdTHU3vlX" width="447" height="409"></p>
<p>what I would like to do is to insert programmatically E-cad-Cleared as a child of CleanTumor</p>
<p>I am trying to use this</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://qupath.github.io/javadoc/docs/qupath/lib/objects/hierarchy/PathObjectHierarchy.html">
  <header class="source">

      <a href="https://qupath.github.io/javadoc/docs/qupath/lib/objects/hierarchy/PathObjectHierarchy.html" target="_blank" rel="noopener">qupath.github.io</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://qupath.github.io/javadoc/docs/qupath/lib/objects/hierarchy/PathObjectHierarchy.html" target="_blank" rel="noopener">PathObjectHierarchy (QuPath 0.4.0)</a></h3>

  <p>declaration: package: qupath.lib.objects.hierarchy, class: PathObjectHierarchy</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
but I think I am failing generally to get the PathObjects</p>
<p>so this is my little snippet</p>
<pre><code class="lang-groovy">parentAnnotation = getAnnotationObjects().findAll{it.getPathClass() == getPathClass("Tissue_D")}
print(parentAnnotation)
childDetection = getDetectionObjects().findAll{it.getPathClass() == getPathClass("E-cad-Cleared")}
print(childDetection)


PathObjectHierarchy().addPathObjectBelowParent(parentAnnotation, childDetection, true)
//fireHierarchyUpdate()
</code></pre>
<p>this is the error I get</p>
<pre><code class="lang-auto">INFO: [CleanTumor (Geometry) (Tissue_D) (3 objects)]
INFO: [Detection (Geometry) (E-cad-Cleared)]
ERROR: MissingMethodException at line 7: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.PathObjectHierarchy() is applicable for argument types: () values: []

ERROR: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.callGlobal(GroovyScriptEngineImpl.java:404)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.access$100(GroovyScriptEngineImpl.java:90)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl$3.invokeMethod(GroovyScriptEngineImpl.java:303)
    org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:73)
    org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)
    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)
    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:176)
    Script36.run(Script36.groovy:8)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)
    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:982)
    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:914)
    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:829)
    qupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1345)
    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    java.base/java.lang.Thread.run(Unknown Source)
</code></pre>
<p>I’ve tried also with addObject instead of addPathObject</p>
<p>what am I doing wrong?</p>
<p>thanks<br>
Emanuele</p> ;;;; <p>It is not a web app installed on OMERO.web, so yes, i guess.</p> ;;;; <p>I wrote this:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/petebankhead/imagejts-tools">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/petebankhead/imagejts-tools" target="_blank" rel="noopener">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796.png 2x" data-dominant-color="F4F2F0"></div>

<h3><a href="https://github.com/petebankhead/imagejts-tools" target="_blank" rel="noopener">GitHub - petebankhead/imagejts-tools: Example ImageJ plugins to convert ROIs...</a></h3>

  <p>Example ImageJ plugins to convert ROIs to and from GeoJSON - GitHub - petebankhead/imagejts-tools: Example ImageJ plugins to convert ROIs to and from GeoJSON</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>It kinda works, but there is some awkward stuff around where in the GeoJSON z-stack/timepoint/channel info is stored. If you need these features, exchange with QuPath might not work properly. For 2D it might be ok.</p>
<p>I’ve temporarily abandoned it due to a lack of time, but hope to return one day.</p> ;;;; <p>Hi <a class="mention" href="/u/oeway">@oeway</a>  <a class="mention" href="/u/petebankhead">@petebankhead</a>,</p>
<p>I wonder if there is any progress with ImageJ plugin for importing and exporting GeoJSON files.</p>
<p>Thanks<br>
Ofra</p> ;;;; <aside class="quote no-group" data-username="mmongy" data-post="4" data-topic="78333">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/51bf81/40.png" class="avatar"> Marc Mongy:</div>
<blockquote>
<p>i don’t know how the “user” and “password” are supposed to be obtained. Normally, you should import omero.gateway to get them, isn’t it?</p>
</blockquote>
</aside>
<p>Note that these are the ImageJ-OMERO <a href="https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROCredentials.java">credentials wrapper</a>.</p>
<p>I believe the way credentials work was changed in the ImageJ-OMERO refactoring, so I am guessing/hoping there is just a missing connection (passing the credentials around) that wasn’t hooked up properly.</p>
<p>I have to say that I only have experience running ImageJ-OMERO client-side. You’re using the server-side paradigm, correct?</p> ;;;; <p>I have a sort of odd and a bit cumbersome solution. I’ve only tested it on the first image, but it works for me… (it should work on all images)<br>
The idea is to reslice the image and use a 2D fill to fill the bead, then subtract the images to get only the bead. (For some reason the reslicing doesn’t work on just one image, hence the odd workaround to make a small stack first.)</p>
<pre><code class="lang-auto">titel_orig = getTitle();
run("Duplicate...", " "); //keep the orginal
rename("img1");
run("8-bit");
setAutoThreshold("Default dark");
setOption("BlackBackground", true);
run("Convert to Mask");
run("Fill Holes"); //fill small holes created by thresholding
// make a stack of four images, the first and last black
run("Duplicate...", " ");
rename("img2");
run("Images to Stack", "  title=img ");
setSlice(2);
run("Add Slice"); //adds slice after the active one
run("Reverse"); //black slice at the beginning
setSlice(3);
run("Add Slice"); //add black slice at end
rename("img3");
stack1 = getTitle();

//reslice the stack and fill holes - this fills the bead, but not the substrate, because that goes to the image border
run("Invert", "stack");
run("Reslice [/]...", "output=1.000 start=Top avoid");
rename("img4");
run("Fill Holes", "stack");
run("Reslice [/]...", "output=1.000 start=Top avoid");
rename("img5")
stack2 = getTitle();

//subtract the two images to leave only the bead
imageCalculator("Subtract create stack", stack2,stack1);
//delete the extra images
setSlice(1);
run("Delete Slice"); 
setSlice(2);
run("Delete Slice"); 
setSlice(2);
run("Delete Slice"); 
rename(titel_orig + "_area");

//measure area
run("Set Measurements...", "area display redirect=None decimal=0");
run("Measure");

//tidy-up
close("img*");
</code></pre> ;;;; <p>Thank you ! Should I use the &gt;Convert to mask, right ?</p> ;;;; <p><a class="mention" href="/u/nicokiaru">@NicoKiaru</a>, however, you’re probably right: when trying to get to the source of the error, in OMEROCredentials, here is the problematic function:</p>
<pre><code class="lang-auto">public void validate() {
		if (user == null || password == null) {
			throw new IllegalArgumentException("Invalid credentials: " +
				"must specify either session ID OR username+password");
		}
	}
</code></pre>
<p>and by looking inside the file, i don’t know how the “user” and “password” are supposed to be obtained. Normally, you should import omero.gateway to get them, isn’t it?</p>
<p>Marc.</p> ;;;; <p>Hello everyone,</p>
<p>It is my first time using Deeplabcut and any machine learning program. I have an issue with the training. First of all my system is: RTX3080 10GB memmory, Windows 10, CUDA version 11.6 (I tried different CUDA versions, didnt work either). My image size is 2048X2048 and I do not want to crop them, I have 900 labeled frames in total.</p>
<p>Important config parameters: Bach size: 1, Max iterations: 500000, Max_inputsize: 6000, allow_growth=True. I ran <code>conda install cudnn -c conda-forge</code> after installing DeepLabCut and it installed everything normally.</p>
<p>The training starts normally and I get the “Starting training…” message. However, no new iterations can be seen four hours even when I set display_iterations to 10. No activity on GPU CUDA can be seen in the task menager. I am sure Deeplabcut recognizes my GPU because it occupies the memmory, but I see no activity in CUDA:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/8/b8e88e7e5f459aca16b3c6668a5a6c257f88bb43.png" data-download-href="/uploads/short-url/qnM6uU7v1kxXEASWXLxiGYQ58xd.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/8/b8e88e7e5f459aca16b3c6668a5a6c257f88bb43.png" alt="image" data-base62-sha1="qnM6uU7v1kxXEASWXLxiGYQ58xd" width="690" height="494" data-dominant-color="F5F8F9"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">972×697 23.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a38dcf7941c6770e14e97a4e4fbef04bf07b209d.png" data-download-href="/uploads/short-url/nkRECqxBHNwIdfW5vtIhigwAnGR.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a38dcf7941c6770e14e97a4e4fbef04bf07b209d.png" alt="image" data-base62-sha1="nkRECqxBHNwIdfW5vtIhigwAnGR" width="690" height="272" data-dominant-color="1D1D1B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">707×279 8.62 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>In the Prompt, I get some non-fatal error messages including 2 root errors:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bf1bf68450a5a5dde292b5423e0a1ef001e0557a.png" data-download-href="/uploads/short-url/rgD6E81BascFyPu6rpbkQnslkwO.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bf1bf68450a5a5dde292b5423e0a1ef001e0557a.png" alt="image" data-base62-sha1="rgD6E81BascFyPu6rpbkQnslkwO" width="690" height="59" data-dominant-color="171717"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1601×139 7.85 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And “Allocator ran out of memmory” messages:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c68224777bd796a48d22a0914bb3dc37d610a259.png" data-download-href="/uploads/short-url/sk5mOOzrHQ4kP7RgThcRAIjIBNn.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c68224777bd796a48d22a0914bb3dc37d610a259.png" alt="image" data-base62-sha1="sk5mOOzrHQ4kP7RgThcRAIjIBNn" width="690" height="58" data-dominant-color="1C1D1C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1901×161 15 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I am not sure if the memmory of my GPU is not enough or there is a mismatch between tensorflow, CUDA and Cudnn but I tried almost all 11.X CUDA versions? Any help you could provide is in great value for me. Thank you very much in advance.</p> ;;;; <p>The last error you posted seems to be the same as <a href="https://forum.image.sc/t/cellfinder-issue-exception-in-thread-thread-1/78332/3">this issue</a>, so could you try increasing the <code>Number of free CPUs</code> option?</p>
<p>Would you mind posting all related issues in the same forum thread? It would make it easier for me (and others) to keep track. Thanks!</p> ;;;; <p>Not sure if I’ve ever scripted it myself, but there should be three main steps:</p>
<ol>
<li>Create a <code>DensityMapBuilder</code>
<ul>
<li>Use <a href="https://github.com/qupath/qupath/blob/25a6fde481830f149c4e65fa27a6deff0909c103/qupath-core-processing/src/main/java/qupath/lib/analysis/heatmaps/DensityMaps.java#L151" class="inline-onebox">qupath/DensityMaps.java at 25a6fde481830f149c4e65fa27a6deff0909c103 · qupath/qupath · GitHub</a>
</li>
</ul>
</li>
<li>Customize the builder if you need to
<ul>
<li>See <a href="https://qupath.github.io/javadoc/docs/qupath/lib/analysis/heatmaps/DensityMaps.DensityMapBuilder.html">javadocs here</a>
</li>
</ul>
</li>
<li>Write the density map, using the current <code>ImageData</code>, <code>DensityMapBuilder</code> and export file path
<ul>
<li>See <a href="https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-core-processing/src/main/java/qupath/lib/scripting/QP.java#L3331" class="inline-onebox">qupath/QP.java at 1368912885c1a191beaea32c28d85a3707f657f8 · qupath/qupath · GitHub</a>
</li>
</ul>
</li>
</ol> ;;;; <p>Hmm, I also do not see any way to create the denisty map itself from a script. It seems possible to write out an existing map, or create hotspots or annotations from scratch, but not create the map itself - unless I am missing something <a class="mention" href="/u/petebankhead">@petebankhead</a>?</p> ;;;; <p>Hi,<br>
I generated a classifier model using CPA 3.0.4 based on features from the actin cytoskeleton of cells. When I plug this model into the ClassifyObjects module in CP 4.2.4 and run the pipeline on new images, I get an error message indicating that some measurements don’t exist, but the appropriate modules are present and running. I would appreciate any help or insight I can get for this issue. I am attaching here all the required information for others to look into (error message, CP pipeline, CPA model, sample images).<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png" data-download-href="/uploads/short-url/sAwr8M64Nvc1FyOISchV1Hie36Y.png?dl=1" title="errorCellProfiler" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png" alt="errorCellProfiler" data-base62-sha1="sAwr8M64Nvc1FyOISchV1Hie36Y" width="421" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 2x" data-dominant-color="CBD2DA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">errorCellProfiler</span><span class="informations">430×510 73.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<a class="attachment" href="/uploads/short-url/5v48dYmZdYzze8TvbWMxMGMPWX.zip">CPAmodel_CPpipeline.zip</a> (9.8 MB)</p>
<p>Thank you!</p>
<p>Victoria</p> ;;;; <p>Thank you Ghazi,<br>
I made the macro on my own and I was successful. I didn’t know about these already built-in macros.<br>
Anyhow, your link is very interesting because I can browse in for any need and use some of them for any future need!</p>
<p>Thank you!</p>
<p>Giacomo</p> ;;;; <p>Yeah, converting to OME-TIFF would likely be the easiest solution to get the files imported immediately. You would need to use the bfconvert tool from the latest 6.12.0 command line tools from <a href="https://www.openmicroscopy.org/bio-formats/downloads/" class="inline-onebox">Bio-Formats Downloads | Open Microscopy Environment (OME)</a></p>
<p>The list of options available for bfconvert can be found at <a href="https://bio-formats.readthedocs.io/en/v6.12.0/users/comlinetools/conversion.html" class="inline-onebox">Converting a file to different format — Bio-Formats 6.12.0 documentation</a></p> ;;;; <p>A proposal (valid for this image);<br>
Appreciate any feedback. Thanks in advance.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da.jpeg" data-download-href="/uploads/short-url/8MR7tJUprZj4nOVgXhFNJOFMksO.jpeg?dl=1" title="HWP1_5-0009" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_331x250.jpeg" alt="HWP1_5-0009" data-base62-sha1="8MR7tJUprZj4nOVgXhFNJOFMksO" width="331" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_331x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_496x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_662x500.jpeg 2x" data-dominant-color="838B7C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">HWP1_5-0009</span><span class="informations">1920×1448 164 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-auto">macro "measuring the area of the bead"
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
img=getImageID();
// Start batch mode
setBatchMode(true);
selectImage(img);
run("Duplicate...", "title=1");
close("\\Others");
//---------------------------
// Image processing
run("8-bit");
setOption("BlackBackground", true);
run("Convert to Mask");
//setTool("wand");
doWand(33, 33, 58.0, "Legacy");
setBackgroundColor(255,255, 255);
run("Clear Outside");
setBackgroundColor(0,0, 0);
run("Select None")
//------------------------------
// start of scan
total_number=0;
h=getHeight();
w=getWidth();
//------------------------------
//vertical scan
for(i=0;i&lt;h;i++){
            number=0;
	//horizontal scan	
	for(j=0;j&lt;w;j++){
//makeLine(0,i,w,i);
if(getPixel(j,i)&gt;0)
number+=1;
}
//print("for the row "+i+" the area is: "+  number);
if(number&lt;w)
total_number=total_number+=number;
}
print("The area of the bead =", total_number+ " px");
// end of scan
//------------------------------
// End of processing
// End of batch mode
setBatchMode(false);
close();
exit();
}
type or paste code here
</code></pre> ;;;; <p>The SEM imaging is very surface sensitive, so you’ll see more or less the free surface created by FIB cut under the angle of 36 ° (90 °-54 °).<br>
The information depth depends on electron beam energy and detector used.<br>
Focusing problems (depth of field) can be negligible by setting the imaging parameters.<br>
Tilt correction is often applied during SEM image acquisition.</p>
<ul>
<li>The sample itself may shift during the long-time milling process due to stage drift and thermal drift of the sample.</li>
<li>During FIB cut, the position of objects in the cut surface and the working distance will also change for geometrical reasons (see image).<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3.png" data-download-href="/uploads/short-url/29jh7917k2XCJCiIhmsFvnbO00P.png?dl=1" title="FIB_Cut_Shift" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_666x500.png" alt="FIB_Cut_Shift" data-base62-sha1="29jh7917k2XCJCiIhmsFvnbO00P" width="666" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_666x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_999x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_1332x1000.png 2x" data-dominant-color="FBF7F7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">FIB_Cut_Shift</span><span class="informations">3000×2250 184 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div>
</li>
</ul>
<p>The change in working distance may be (partly) compensated by applying a function like “track working distance” during milling.</p>
<p>The above-mentioned effects will depend on the object size and required resolution.<br>
Drift of objects if the acquired series of SEM images may be compensated by tracking objects in different images or bey tracking fiducial markers. Depending on your software such corrections may be applied in the process of image acquisition.</p>
<p>There are a low of publications (and PhD thesis) about FIB tomography.<br>
Maybe you’ll find something for your system.</p>
<p>The post-processing of the images (image stack) is often done using expensive commercial software (e.g. AVIZO).<br>
If you don’t have access: a lot of work was done suing free software (e.g. ImageJ/Fiji, IMOD) or Matlab add-ons.</p>
<p>Some references:<br>
Image registration (correction of shift): <a href="http://bigwww.epfl.ch/thevenaz/stackreg/" class="inline-onebox" rel="noopener nofollow ugc">StackReg</a></p>
<p>Phyton based:<br>
<a href="https://fibtracking.readthedocs.io/en/latest/intro.html" class="onebox" target="_blank" rel="noopener nofollow ugc">https://fibtracking.readthedocs.io/en/latest/intro.html</a></p> ;;;; <p>Hi,<br>
I am having a similar problem and was wondering if you might have found a solution to this question. I have currently generated the h5 file probability map from Ilastik, but I have not been able to convert it to a multistack tiff nor generate a single cell segmentation map from the .h5 file in Cellprofiler.<br>
Best!</p> ;;;; <p>Thanks <a class="mention" href="/u/mike_nelson">@Mike_Nelson</a> , we are indeed trying to solve the same issue!</p>
<p>I think our main issue is that we want to generate the density map programmatically through the script, and cannot figure out what the command that creates the map is. From there, we should be able to navigate through to saving from your post(s) and the code from Pete.</p>
<p>You are correct, If I manually create the density map with the GUI then everything works as advertised, but for the life of me I cannot figure out how to generate the map with only scripting.</p> ;;;; <p>Thank you, everyone, for your input!</p>
<p>Perhaps the word “junk” was the wrong word. Certainly the composite classifiers can be useful for rare cell discovery and more. I found evidence of a rare cell type in our data set when I was using the composite classifiers earlier on. A better word might be “noise.” There’s only so much time in a work week, and rare cell discovery isn’t the aim of our current study. Hence my need for more targeted phenotyping.</p>
<p>I will try out these different methods and see what works. Thank you again, everyone!</p> ;;;; <p>Hej <a class="mention" href="/u/constantinpape">@constantinpape</a> and other mobie-utils-python fans,</p>
<p>I have been trying to understand how affine transformations and views work during project generation and I have prepared the following minimal example: <a href="https://github.com/CamachoDejay/mobie-python-examples/blob/0860d919f29ff6eb4853790eea01eb3f79c65a22/mobie-project-views_tmatrix.ipynb" class="inline-onebox" rel="noopener nofollow ugc">mobie-python-examples/mobie-project-views_tmatrix.ipynb at 0860d919f29ff6eb4853790eea01eb3f79c65a22 · CamachoDejay/mobie-python-examples · GitHub</a></p>
<p>At the beginning I was surprised that my view <code>"Overlay_A"</code> did not work as I expected:</p>
<pre><code class="lang-auto">source_list = [["original"], ["rot_transformed"]]
settings = [ 
    {"color": "green", "contrastLimits": [0., 255.], "blendingMode": "sum"},
    {"color": "magenta", "contrastLimits": [0., 255.], "blendingMode": "sum"},
]


mobie.create_view(dataset_folder, "Overlay_A",
                  sources=source_list, 
                  display_settings=settings,
                  overwrite=True)
</code></pre>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/a/2af0b8d98c11a1fe0a81c298c0ff1ce476bd31c7.jpeg" alt="Overlay_A" data-base62-sha1="67RNv6sj7V9YcGwybk9KPBBg8UD" width="216" height="210"></p>
<p>This is because I used <code>source_list = [["original"], ["rot_transformed"]]</code>, where <code>"rot_transformed"</code> included a transformation matrix during <code>mobie.add_image</code>.</p>
<pre><code class="lang-auto">transformation = [0.8034685581387304,0.46418531456408085,0.0,17949.15847223811,
                 -0.46418531456408085,0.8034685581387304,0.0,206061.42501785012,
                  0.0,0.0,0.9279168767591658,0.0]

view = mobie.metadata.get_default_view("image", raw_name, color="white", source_transform={"parameters": transformation})

mobie.add_image(...
</code></pre>
<p>I guess that this came from the misunderstanding that this affine matrix was <strong>stored</strong> as a transformation of the data which is always associated to it as default. I guess that the correct interpretation is that upon <code>mobie.add_image</code> a <strong>new view</strong> is created with the name <strong>rot_transformed</strong> and stored in the <strong>lm</strong> menu, but that when I create other views with that source this information is not kept.</p>
<p>With that in mind I then created a <code>"Overlab_B"</code> view via:</p>
<pre><code class="lang-auto">source_list = [["original"], ["rot_transformed"]]

tIdentity =  [1., 0., 0., 0.,
              0., 1., 0., 0.,
              0., 0., 1., 0.]

transformation = [0.8034685581387304,0.46418531456408085,0.0,17949.15847223811,
                 -0.46418531456408085,0.8034685581387304,0.0,206061.42501785012,
                  0.0,0.0,0.9279168767591658,0.0]

transformations = [{'affine':tIdentity}, {'affine':transformation}]
transformations = [{'affine': {'parameters':tIdentity, 'sources': source_list[0]}}, {'affine': {'parameters':transformation, 'sources': source_list[1]}}]
settings = [ 
    {"color": "green", "contrastLimits": [0., 255.], "blendingMode": "sum"},
    {"color": "magenta", "contrastLimits": [0., 255.], "blendingMode": "sum"},
]


mobie.create_view(dataset_folder, "Overlay_B",
                  sources=source_list, 
                  display_settings=settings,
                  source_transforms=transformations,
                  overwrite=True)
</code></pre>
<p>This way I got the desired result:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/4/643af584eea3c63f855ca5fe413fb6cfe1e233af.jpeg" alt="Overlay_B" data-base62-sha1="eiG46EEHzk0ThvWu6PqROyhdWKX" width="210" height="210"></p>
<p>Is this correct <a class="mention" href="/u/constantinpape">@constantinpape</a>?</p>
<p>Best regards,<br>
Rafa</p> ;;;; <p>Personally I find all the angles involved in FIB-SEM the trickiest thing (or well, second trickiest- finding the stupid target is worse), so maybe I’m not fulling understanding the issue, but here’s my take: With respect to the SEM gun, the images should all be aligned in z. The 54 degree angle is the angle between the SEM and FIB guns, but that won’t change the angle at which the SEM is scanning each image. So the z axis would be defined by what the SEM is seeing, not at what you are milling.</p>
<p>If that’s not exactly what you’re describing, could you maybe link to a small reduced dataset (scaled down/cropped so we don’t kill the internet)?</p> ;;;; <p>It should work without the bug fix.</p>
<p>This is the output from the script I get</p>
<pre><code class="lang-auto">1: 10009-17752.560117302055
2: 0-65535
3: 0-65535
4: 0-65535
</code></pre>
<p>when running the script using ImageJ 1.54c (pre bugfix), which is the same as what is shown by the Image&gt;Show Info command:</p>
<pre><code class="lang-auto">Display ranges
  1: 10009-17752.5601
  2: 0-65535
  3: 0-65535
  4: 0-65535
</code></pre> ;;;; <p>what would be your recommendation to do until then? convert to ome.tiff or trying something with zarr? tiffs and bigtiffs are slower? with zarr, i am not sure how would i even proceed.</p> ;;;; <p>Thanks a lot.</p>
<p>I have taken note of the suggestions. Now it raises a:</p>
<blockquote>
<p><strong>ValueError</strong> : Expect same length of resolution as ndim, got: resolution=(454, 454), ndim=3</p>
</blockquote>
<p>Let me know when I can test the new release.</p>
<p>Best regards,<br>
Rafa</p> ;;;; <p>To be a bit more clear, I want to only measure the hatched area as shown in this image:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec.jpeg" data-download-href="/uploads/short-url/J9au7hAkTteOBfqxUeX3E2VLn6.jpeg?dl=1" title="HWP1__5-0000_markup" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_662x500.jpeg" alt="HWP1__5-0000_markup" data-base62-sha1="J9au7hAkTteOBfqxUeX3E2VLn6" width="662" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_1324x1000.jpeg 2x" data-dominant-color="707367"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">HWP1__5-0000_markup</span><span class="informations">1920×1448 189 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Kind regards,<br>
Trond</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da.jpeg" data-download-href="/uploads/short-url/8MR7tJUprZj4nOVgXhFNJOFMksO.jpeg?dl=1" title="HWP1_5-0009" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_662x500.jpeg" alt="HWP1_5-0009" data-base62-sha1="8MR7tJUprZj4nOVgXhFNJOFMksO" width="662" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_1324x1000.jpeg 2x" data-dominant-color="838B7C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">HWP1_5-0009</span><span class="informations">1920×1448 164 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62.jpeg" data-download-href="/uploads/short-url/hyfOgesYpPuUiKp8fhgl7fQc9oK.jpeg?dl=1" title="HWP1_5-0002" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_662x500.jpeg" alt="HWP1_5-0002" data-base62-sha1="hyfOgesYpPuUiKp8fhgl7fQc9oK" width="662" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_1324x1000.jpeg 2x" data-dominant-color="7D8578"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">HWP1_5-0002</span><span class="informations">1920×1448 194 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628.jpeg" data-download-href="/uploads/short-url/sqXR4JzDtxHOUr5xAK0dwb9Fl1K.jpeg?dl=1" title="HWP9_HW65_5-0018" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_662x500.jpeg" alt="HWP9_HW65_5-0018" data-base62-sha1="sqXR4JzDtxHOUr5xAK0dwb9Fl1K" width="662" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_1324x1000.jpeg 2x" data-dominant-color="697962"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">HWP9_HW65_5-0018</span><span class="informations">1920×1448 249 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Hi,</p>
<p>Background:<br>
I’m working with additive manufacturing, and need to calculate cross section area of stringer beads in polished specimens. Please see the example photos showing typical images, if the upload didn’t work, the most relevant example can be found here:</p>
<p><a href="https://sintef-my.sharepoint.com/:i:/g/personal/trond_arne_hassel_sintef_no/EQDGCuom0MxLm_vObIlLbl8BeLiLGts9zo-IoHXW-sH25w?e=8HZ5Za" class="onebox" target="_blank" rel="noopener nofollow ugc">https://sintef-my.sharepoint.com/:i:/g/personal/trond_arne_hassel_sintef_no/EQDGCuom0MxLm_vObIlLbl8BeLiLGts9zo-IoHXW-sH25w?e=8HZ5Za</a></p>
<p>Pitted sample: <a href="https://sintef-my.sharepoint.com/:i:/g/personal/trond_arne_hassel_sintef_no/EQDGCuom0MxLm_vObIlLbl8BeLiLGts9zo-IoHXW-sH25w?e=4pHjKK" class="inline-onebox" rel="noopener nofollow ugc">Sign in to your account</a></p>
<p>Analysis goal:<br>
I have quite a few of these, so I would prefer to batch process them using a macro in ImageJ/Fiji. I’ve figured out how to set scale, convert to 8 bit greyscale and do threshold adjustment, but I’ve yet to find an automated way of measuring only the area of the bead (the rounded deposit on top of the plate), not the substrate (the bright rectangular section in the lower part of the image).</p>
<p>The position of the substrate surface is not in the exact same height in each photo, and neither is it perfectly level. An horizontal assumption could still be acceptable, since manual cropping, which is my current alternative, also requires a horizontal line for crop area selection.</p>
<p>Macro for setting scale, converting to grayscale and thresholding:</p>
<p>run(“Set Scale…”, “distance=715.0313 known=0.5 unit=mm global”);<br>
run(“8-bit”);<br>
setAutoThreshold(“Default”);<br>
//run(“Threshold…”);<br>
//setThreshold(0, 45);<br>
run(“Convert to Mask”);<br>
run(“Close”);<br>
run(“Measure”);</p>
<p>I believe the this topic is close to what I’m trying to achieve: <a href="https://forum.image.sc/t/crop-image-between-two-lines/4405/4">Crop image between two lines - Image Analysis - Image.sc Forum</a>, but I’ve not been successful in modifying the macro presented there to provide a useable result in my case.</p>
<p>Modified from “Crop image between two lines”, that doesn’t work the way it should (either stops the rectangle above the bead, or way into the substrate, instead of the desired top surface of the substrate):</p>
<p>run( “Select All” );<br>
setKeyDown( “alt” );<br>
projection = getProfile();<br>
min = Array.findMinima( projection, 7.3 );<br>
setKeyDown( “none” );<br>
if ( min[0] &lt; min[1] ) { y = min[0]; } else { y = min[1]; }<br>
makeRectangle( 0, 0, getWidth(), abs(min[1] ) );<br>
run( “Crop” );<br>
run( “Select None” );</p>
<ol>
<li>
<p>Do any know of an automated way to crop the image or make a selection so that only the actual stringer area is measured?</p>
</li>
<li>
<p>In addition, I’m also looking for an automated way to include dark spots (pitting) from preparation in the assessed cross section area, shown in the photo of the pitted sample. Any suggestions on a non-manual way to do this?</p>
</li>
</ol> ;;;; <p>Hello <a class="mention" href="/u/uschmidt83">@uschmidt83</a>,</p>
<p>it seems I simply had to increase the tiles even further than what I did before, so not only to 2 or 4 tiles.<br>
Now, when using 8 tiles, the segmentation works fine.<br>
Thank you very much!</p> ;;;; <p>Has anyone managed to run the Columbus software in a CentOS 6 Docker container? I have got it to install in the container, but the database service does not seem to start up properly when I try to start the Columbus service.</p>
<p>Thanks</p>
<p>Sam Braithwaite</p> ;;;; <p>Thank you</p>
<p>I’ll give it a go tomorrow <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi Allan, sorry to bump this topic but did you make any progress with this since then? We also have a similar problem and have been exploring medium-term and long-term options for replacing Columbus.</p>
<p>The main problem which we have is with users being dependent on propriety PerkinElmer analysis pipelines which are not going to be supported in other software. We are investigating use of WIPP long-term for such analysis although think the software needs some user interface enhancements before it will be widely usable by users who are not from a computational background.</p>
<p>In the medium-term we are investigating ways to run Columbus in the best possible way to manage its legacy software requirements such as through containerisation, as we would rather avoid the high cost of SImA if possible.</p>
<p>Regards,</p>
<p>Sam Braithwaite</p> ;;;; <p>Yup, the path to the video you analyzed (not the labeled one)</p> ;;;; <p>Could you attach the log file?</p> ;;;; <p>Hi Fabio you can do it if you create a thresholder:</p>
<p>(If you use scanned slides) Classified the tissue with Analyze&gt;deprecrated&gt;simple tissue detection. And select the threshold useful for your purpose</p>
<p>Select your ROI and create a thresholder that you can run in your images/slides Classify&gt;Pixel classification&gt;create thresholder<br>
Modify the threshold and sigma values for your suitable values select e.g above threshold negative and below threshold positive.<br>
When you run the measure it will provide you with the total area of parenchyma (positive) and air (negative) and their percentages regarding the ROI<br>
For your image you should work from a threshold of 220ish and try what suits you best</p> ;;;; <p>I retried it but just got the same result, then I cancelled the “no-classification”, but met the following issue:<br>
Traceback (most recent call last):<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\runpy.py”, line 194, in _run_module_as_main<br>
return _run_code(code, main_globals, None,<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\runpy.py”, line 87, in <em>run_code<br>
exec(code, run_globals)<br>
File "C:\Users\ps.conda\envs\cellfinder\Scripts\cellfinder.exe_<em>main</em></em>.py", line 7, in <br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder\main.py”, line 96, in main<br>
run_all(args, what_to_run, atlas)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder\main.py”, line 173, in run_all<br>
points = classify.main(<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder_core\classify\classify.py”, line 66, in main<br>
model = get_model(<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder_core\classify\tools.py”, line 46, in get_model<br>
model.load_weights(model_weights)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\keras\utils\traceback_utils.py”, line 70, in error_handler<br>
raise e.with_traceback(filtered_tb) from None<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\h5py_hl\files.py”, line 567, in <strong>init</strong><br>
fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\h5py_hl\files.py”, line 231, in make_fid<br>
fid = h5f.open(name, flags, fapl=fapl)<br>
File “h5py_objects.pyx”, line 54, in h5py._objects.with_phil.wrapper<br>
File “h5py_objects.pyx”, line 55, in h5py._objects.with_phil.wrapper<br>
File “h5py\h5f.pyx”, line 106, in h5py.h5f.open<br>
OSError: Unable to open file (truncated file: eof = 152043520, sblock-&gt;base_addr = 0, stored_eof = 184831728)</p> ;;;; <p>Thank you, Konrad.</p>
<p>I double checked the user manual and I saw the figure highlighting that file ending with ‘el’ was the one I was after</p>
<p>The help also mentions to use the corresponding video file. Is the video file different to the originally uploaded video file?</p>
<p>I’m really sorry to ask basic questions but I’m super eager to get things right, and I’m sure it will be second nature shortly <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Thanks, again</p> ;;;; <p>Hi <a class="mention" href="/u/nicokiaru">@NicoKiaru</a>,</p>
<p>I haven’t written that piece of code. I just tried to follow the track given by the error message to see where it fails, and it lead to this. I didn’t really thought about this.</p>
<p>For me, the credentials are the username and password you type at the web frontend, or on the frontend of an external client like OMERO.insight, or in the command line for more “crude” clients like OMERO.downloader, to connect to OMERO.</p>
<p>When using ImageJ-OMERO, i figured this problem would probably have already been addressed by the developers. For this reason, my first thought was the fact that ImageJ-OMERO couldn’t access to the credentials already entered on the web frontend, for some reason (likely misconfigured permissions, i encountered this problem too often in cases where program can’t be launched or data can’t be accessed).</p>
<p>In fact, i’m probably less familiar than you with the server side of OMERO, in terms of programming. The best people that could answer this are either <a class="mention" href="/u/ctrueden">@ctrueden</a> or <a class="mention" href="/u/hinerm">@hinerm</a>.</p>
<p>Sorry for that, i’m learning as i go along.</p>
<p>Thanks anyway, Marc.</p> ;;;; <p>Strange. Could you try running again but setting a new output directory?</p> ;;;; <p>thanks a lot! This issus is solved by your suggestion, however, it just come with the following problem, but my command line is: cellfinder -s G:/cz/signaltif -b G:/cz/bgtif -o G:/cz/output -v 5 5 5 --orientation sal --atlas allen_mouse_50um --no-classification --n-free-cpus 100 --ball-xy-size 15 --ball-z-size 15, which suggest I don’t want cell classification.</p>
<p>Traceback (most recent call last):<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\runpy.py”, line 194, in _run_module_as_main<br>
return _run_code(code, main_globals, None,<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\runpy.py”, line 87, in <em>run_code<br>
exec(code, run_globals)<br>
File "C:\Users\ps.conda\envs\cellfinder\Scripts\cellfinder.exe_<em>main</em></em>.py", line 7, in <br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder\main.py”, line 96, in main<br>
run_all(args, what_to_run, atlas)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder\main.py”, line 210, in run_all<br>
points = get_cells(args.paths.classified_points, cells_only=True)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\imlib\IO\cells.py”, line 23, in get_cells<br>
return get_cells_xml(cells_file_path, cells_only=cells_only)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\imlib\IO\cells.py”, line 55, in get_cells_xml<br>
with open(xml_file_path, “r”) as xml_file:<br>
FileNotFoundError: [Errno 2] No such file or directory: ‘G:/cz/output\points\cell_classification.xml’</p> ;;;; <p>Also want to delete my account - how can we do this??</p> ;;;; <p>I’m not sure about the exact issue, but I don’t think cellfinder will be able to efficiently use 128 cores. Try setting <code>--n-free-cpus 100</code>.</p> ;;;; <p>Hello <a class="mention" href="/u/mmongy">@mmongy</a>,</p>
<p>I took the liberty to split your message from the rest of the topic.</p>
<p>The null pointer exception refers to this line:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181">
  <header class="source">

      <a href="https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181" target="_blank" rel="noopener">imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181</a></h4>



    <pre class="onebox"><code class="lang-java">
      <ol class="start lines" start="171" style="counter-reset: li-counter 170 ;">
          <li>		final OMEROCredentials credentials) throws OMEROException</li>
          <li>	{</li>
          <li>		this(omeroService, server, credentials, //</li>
          <li>			new omero.client(server.host, server.port));</li>
          <li>	}</li>
          <li>
          </li>
<li>	private OMEROSession(final OMEROService omeroService,</li>
          <li>		final OMEROServer omeroServer, final OMEROCredentials omeroCredentials,</li>
          <li>		final omero.client omeroClient) throws OMEROException</li>
          <li>	{</li>
          <li class="selected">		omeroCredentials.validate();</li>
          <li>
          </li>
<li>		this.omeroService = omeroService;</li>
          <li>		this.server = omeroServer;</li>
          <li>
          </li>
<li>		initializeSession(omeroCredentials, omeroClient);</li>
          <li>	}</li>
          <li>
          </li>
<li>	// -- Data transfer --</li>
          <li>
          </li>
<li>	/**</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>So <code>omeroCredentials</code> may be null.</p>
<p>I’m not really familiar with the server side of OMERO and running script on it, but the error seems to indicate the absence of credentials. How do you collect the credentials ?</p> ;;;; <p>Hello. I am using cellfinder and meet the following issue, does that mean it can’t work because my work station is 128 threading, more than 63??   Does anyone know how to process it? Thanks a lot!</p>
<p>Exception in thread Thread-1:<br>
Traceback (most recent call last):<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\threading.py”, line 932, in _bootstrap_inner<br>
self.run()<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\threading.py”, line 870, in run<br>
self._target(*self._args, **self._kwargs)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\multiprocessing\pool.py”, line 519, in _handle_workers<br>
cls._wait_for_updates(current_sentinels, change_notifier)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\multiprocessing\pool.py”, line 499, in _wait_for_updates<br>
wait(sentinels, timeout=timeout)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\multiprocessing\connection.py”, line 879, in wait<br>
ready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\multiprocessing\connection.py”, line 811, in _exhaustive_wait<br>
res = _winapi.WaitForMultipleObjects(L, False, timeout)<br>
ValueError: need at most 63 handles, got a sequence of length 128</p> ;;;; <p>Hello <a class="mention" href="/u/k-dominik">@k-dominik</a>,</p>
<p>thanks for your advice! It took me some time, but I may solve the issue. Converting is still a pain in 2023 (happy another new year <img src="https://emoji.discourse-cdn.com/twitter/smiley.png?v=12" title=":smiley:" class="emoji" alt=":smiley:" loading="lazy" width="20" height="20"> ), the main problem I found is that even by using <a href="https://www.ffmpeg.org/download.html" rel="noopener nofollow ugc">ffmpg</a>, FIJI and Ilastik were refusing to accept the video. I found that the easiest way is to use ffmpg for converting to .avi format and later the Ilastic plugin. It’s a bit time and CPU-consuming, but I think it may work even with my low-hundreds of videos I wanna track. What is worthy mentioning is that problem is often caused by compression which could be avoided by telling ffmpg to do the jedi trick and say <a href="https://superuser.com/questions/1536000/how-can-you-decompress-a-videofile-in-ffmpeg" rel="noopener nofollow ugc">“there is no compression you are looking for”</a>.</p>
<p>Cheers, Kuba</p> ;;;; <p>Dear <a class="mention" href="/u/uschmidt83">@uschmidt83</a> ,</p>
<p>Thank you so much for updating the Dockerfile.</p>
<p>Annesha</p> ;;;; <p>You have 16 bodyparts, but the h5 contains 18 in the bodyparts level, whitespaces at the end of the bodypart names also for some reason. Try out this corrected h5 <a href="https://drive.google.com/file/d/1YcAfZMcytQC-odkQQH8rKGL-cs_7y-g6/view?usp=sharing" class="inline-onebox" rel="noopener nofollow ugc">CollectedData_nishat.h5 - Google Drive</a></p> ;;;; <p>Definitely agreed, <a class="mention" href="/u/jkh1">@jkh1</a></p>
<p><strong><em>However</em></strong>, I’ll say here (and re-iterate at the start of the meetings) I think we should be careful to limit ourselves to very short explanations (or pitches) of <em>what metadata</em> and <em>why</em> we need it at this stage. Not because it’s not critical, valuable, and interesting, but just because there are some preliminaries we need to work out first.</p>
<p>In fact, I’d already consider it a success if we just have a list of the metadata models<code>[1]</code>, perhaps what relationship they have to one another, e.g. whether one extends another, and how many attendees are interested in each. We can then use that to schedule further discussions.</p>
<p>Instead, I’d suggest focusing more on <em>how</em> we are going to share the metadata. What are the <strong>requirements</strong> for each of the different metadata types that everyone has pitched?<code>[2]</code>  And very importantly, who is interested in actually specifying how this will work? And by when? <img src="https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12" title=":slightly_smiling_face:" class="emoji" alt=":slightly_smiling_face:" loading="lazy" width="20" height="20"></p>
<p>In terms of the mechanics of storage, <a class="mention" href="/u/jkh1">@jkh1</a>’s points already ticked a few of my boxes (identifiers and ontologies <img src="https://emoji.discourse-cdn.com/twitter/+1.png?v=12" title=":+1:" class="emoji" alt=":+1:" loading="lazy" width="20" height="20">) but there are probably various ways to do even just that. Other questions that come to mind are:</p>
<ul>
<li>What existing formats are already in use that people MUST see integrated? (e.g. XML)?</li>
<li>What metadata is so latency-intolerant that it MUST be in the Zarr JSON attribute so that it’s loaded in the first GET?</li>
<li>and what metadata can we store separately and just “register” in the model?</li>
<li>How do we link between concepts in the various models?</li>
<li>Are there related, less metadata-py things that need similar consideration (points, geojson, etc.)?</li>
<li>And then the ever fun topics of versioning, upgrades, plugins (i.e. dynamically finding the necessary code for metadata), etc.?</li>
</ul>
<p>Thoughts / additions?</p>
<p>Looking forward to it.<br>
~Josh</p>
<hr>
<sup>
<code>[1]</code> I specifically mention models here rather than ontologies since NGFF is more the former. Whatever we put in place should have a way to make use of ontologies, but I think discussing which ontologies to use is even <i>less</i> what we want to do next week.
</sup>
<br><br>
<sup>
<code>[2]</code> If others would like to add their pitches above, as @jkh1 did, that might help.
</sup> ;;;; <p>The pickle or h5 files that end with “_el” or whatever tracker you chose.</p> ;;;; <p>The easiest way, I would think, is to train a <a href="https://qupath.readthedocs.io/en/latest/docs/tutorials/pixel_classification.html" rel="noopener nofollow ugc">pixel classifier</a>. Set the image type as brightfield (other) then set the channel colour vectors for your nuclear stain, the picric acid and the red dye. Select areas in the image that represent good examples of the collagen and cytoplasm and the pixel classifier should be able to do the rest…<br>
The documentation is quite thorough with videos to help you. Good luck! <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi <a class="mention" href="/u/quatreulls">@quatreulls</a>,</p>
<p>the tab is actually there, but the log-window is not displaying it. To check, copy the text from the log window and paste it into an editor.</p>
<p>Best,<br>
Volker</p> ;;;; <p>Hello,</p>
<p>My first post, hope I can explain what I mean sufficiently well. I have a dataset acquired using focused ion beam slice and view. This means that my dataset contains a stack of 2D images. I need to align these after aquisition and it is driving me nuts. I believe that it should be a relatively simple task.</p>
<p>To the problem: The ion beam makes a trench in my sample at normal incidence angle. I image the 2D image on the surface created by the ion beam but at a incidence angle of 54 degrees (tilt correction is made “on the fly” so no problem here). Next the ion beam shaves off a layer and I acquire the next image and so on. This means that the location of the image will shift in the z-direction as this process progresses. Aligning the images is easy and works well. But the coordinate system will not be as I like it to be. Due to the geometry of the setup the x and y cordinate system is in the plane of the image but the z-axis will not be aligned with the aligned slides. Instead the image stack will be progressing in a direction inclined to the z-axis direction. The deviation is either 54 degrees or 90-54 degrees (have to think more to be certain). Does anyone know how to correct for this?</p>
<p>I believe that what I wanna do is illustrated correctly in the attached image. Change from coordinate system (x,y,z’)-&gt;(x,y,z).</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2.jpeg" data-download-href="/uploads/short-url/9o3YKihZ29E9lycDEkCUU4Hc4Ei.jpeg?dl=1" title="FIBSEM tilt" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2_2_690x306.jpeg" alt="FIBSEM tilt" data-base62-sha1="9o3YKihZ29E9lycDEkCUU4Hc4Ei" width="690" height="306" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2_2_690x306.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2.jpeg 2x" data-dominant-color="C9D4E7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">FIBSEM tilt</span><span class="informations">1031×458 23.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thanks,<br>
Daniel</p> ;;;; <p>Dear <a class="mention" href="/u/uberchimpy">@uberchimpy</a> ,<br>
you need to loop in the serie of images</p>
<pre><code class="lang-auto">input = getDirectory("Choose folder with lif files ");
list = getFileList(input);
run("Bio-Formats Macro Extensions");
list = getFileList(input);   
for (i=0; i&lt;list.length; i++) {
	if (endsWith(list[i],".lif")){
		inputPath= input +  File.separator + list[i];
		//how many series in this lif file?
		Ext.setId(inputPath);
		Ext.getSeriesCount(seriesCount); //-- Gets the number of image series in the active dataset.
		for (j=1; j&lt;=seriesCount; j++) {
            // open an image with Bio-Format
			run("Bio-Formats", "open=inputPath autoscale color_mode=Default view=Hyperstack stack_order=XYCZT series_"+j);
			
// Continue your code here 
		}
	}
}

</code></pre> ;;;; <p>Great that it works now!</p>
<p>For the log, the argument should be <code>logfile</code> without underscore as far as I can see from the code. On my Windows machine, it also works with just <code>log</code>, though I’m not sure why <img src="https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12" title=":sweat_smile:" class="emoji" alt=":sweat_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Thanks for sharing the file. The problem is also discussed here:</p>
<p><a href="https://github.com/DeepLabCut/DeepLabCut/issues/2169" class="inline-onebox" rel="noopener nofollow ugc">FileNotFoundError thrown in GUI after updating to 2.3.1 · Issue #2169 · DeepLabCut/DeepLabCut · GitHub</a></p> ;;;; <p>Hi Giacomo,</p>
<p>Sorry, a little bit late, but there are some built-in functions that could help you with this task without spending too much effort:<br>
File.getNameWithoutExtension(path) : Returns the name (e.g., “blobs”) from <em>path</em> without the extension. Requires 1.52r.<br>
or<br>
File.nameWithoutExtension** :  The name of the last file opened with the extension removed</p>
<p>For more info , you can have a look on: <a href="https://imagej.nih.gov/ij/developer/macro/functions.html" class="inline-onebox" rel="noopener nofollow ugc">Built-in Macro Functions</a></p>
<p>Best wishes,<br>
Ghazi</p> ;;;; <p>Hi everyone, hi <a class="mention" href="/u/ctrueden">@ctrueden</a>, hi <a class="mention" href="/u/hinerm">@hinerm</a>,</p>
<p>Sorry to perform forum thread necromancy. After some testing, i had to correct 2 lines on GitHub (the classes “ScriptGenerator” and “ScriptRunner” where called with bad paths when calling from “run-script” and "gen-script).</p>
<p>The generated Python scripts launch well (i have a correct front-end window with fields to enter imageID and else), but when i click “Run Script”, i have this error:</p>
<pre><code class="lang-auto">OpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0
OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release
[ERROR] java.lang.NullPointerException
	at net.imagej.omero.OMEROSession.&lt;init&gt;(OMEROSession.java:181)
	at net.imagej.omero.OMEROSession.&lt;init&gt;(OMEROSession.java:157)
	at net.imagej.omero.module.ModuleAdapter.session(ModuleAdapter.java:393)
	at net.imagej.omero.module.ModuleAdapter.launch(ModuleAdapter.java:212)
	at net.imagej.omero.module.ScriptRunner.invoke(ScriptRunner.java:87)
	at net.imagej.omero.module.ScriptRunner.invoke(ScriptRunner.java:70)
	at net.imagej.omero.module.ScriptRunner.main(ScriptRunner.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at net.imagej.launcher.ClassLauncher.launch(ClassLauncher.java:279)
	at net.imagej.launcher.ClassLauncher.run(ClassLauncher.java:186)
	at net.imagej.launcher.ClassLauncher.main(ClassLauncher.java:87)
</code></pre>
<p>It looks like the problem come from the connection with OMERO (OMERO session), so, is it something like a permissions problem? How can i fix it?</p>
<p>Thanks by advance, Marc.</p> ;;;; <p>Yes. If you don’t have the index already (<code>for (int i=0; i&lt;sizeOfWellSamples()...</code>, etc.) then you’ll need to use <code>indexOf</code>-logic.</p>
<p>~J.</p> ;;;; <p>Ok, thanks!</p>
<p>Therefore if a well sample is retrieved from an image, it should be checked against its parent well to get the index?</p> ;;;; <p>Hello.</p>
<p>I might be formatting my message wrong but I am really stuck on something and I am in desperate need for some help.</p>
<p>So I want to call the plugin Linear Stack Alignment with SIFT in python and I do not know what’s wrong… I’ve uploaded my script in order for you to get a better idea.</p>
<p>So basically my arguments are the following, input file is the full path to my 3D tif file and the output file is the path where I want to save the registered 3D tif file (name of the output file included as well). So I have a doubt  that I should include my image like this in the line output = ij.py.run_plugin(plugin, args, input_img) by previously loading it. I’ve read somewhere that I should only input the path but it does not seem to work or there is an error somewhere else. I’ve checked the path and my 3D tif file is loaded correctly (I’m saying this because multiple times I got this error There are no images open                                                                                                                                                                                                                                                                                                     [java.lang.Enum.toString] There are no images open[java.lang.Enum.toString]). SO I’ve checked by printing its dimensions and it’s working. I’ve also checked by printing the version of Fiji is my path for finding it was good and it does print the version.</p>
<p>Currently I am using this version of my code that I’ve uploaded and I still get the error : There are no images open                                                                                                                                                                                                                                                                                                     [java.lang.Enum.toString] There are no images open[java.lang.Enum.toString]<br>
<a class="attachment" href="/uploads/short-url/nXcyyfZWvaj8DQXmghC81wBuGJc.txt">script.txt</a> (301 Bytes)</p>
<p>I would also be grateful if there is any documentation of what arguments are accepted by this plugin and if you could provide a link because I could not find any. I am asking this because I want to exactly reproduce the results that I get using the interface on Windows.</p>
<p>Also I am not sure how to save my output.</p>
<p>Thank you in advance.</p> ;;;; <aside class="quote no-group" data-username="Wayne" data-post="9" data-topic="78219">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/w/0ea827/40.png" class="avatar"> Wayne Rasband:</div>
<blockquote>
<p>even simpler way to get the display ranges of the channels</p>
</blockquote>
</aside>
<p>Thanks a lot! I tried that and did not manage to get the correct results…<br>
Could it be that this did not work before your bug fix?</p> ;;;; <p>Good morning, <a class="mention" href="/u/pierre.pouchin">@pierre.pouchin</a>.</p>
<p>In the OMERO objects, the index is the location in the list/array of well samples held by the Well object, e.g.:</p>
<pre><code class="lang-auto">            List&lt;WellSample&gt; samples = well.copyWellSamples();
            int number = well.sizeOfWellSamples();
            WellSample sample = well.getWellSample(2);
</code></pre>
<p>~J.</p> ;;;; <p>Regarding requirements, we need metadata to give enough context to enable re-use to address biological questions. From my experience trying to re-use public image data in cell biology, we need at least:</p>
<ul>
<li>identification of samples and treatments associated with each image
<ul>
<li>including a standardized way of distinguishing control vs treatment when relevant</li>
</ul>
</li>
<li>identification of entities visible in the images
<ul>
<li>this goes beyond just the dye/fluorophore associated with each channel but something like which protein is targeted with which antibody</li>
</ul>
</li>
</ul>
<p>In all cases, database identifiers and ontologies should be used where possible to avoid ambiguities.</p> ;;;; <p>If you still have trouble, you may want to try StarDist, it tends to handle fairly circular objects well. Though it can bias towards circularity. CellPose is another option.</p>
<p>From a quick test on the cellpose website<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d.jpeg" data-download-href="/uploads/short-url/c8lnd9OiGjODcKmTrgE6kcDBFbT.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_690x160.jpeg" alt="image" data-base62-sha1="c8lnd9OiGjODcKmTrgE6kcDBFbT" width="690" height="160" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_690x160.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_1035x240.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_1380x320.jpeg 2x" data-dominant-color="504C4B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×447 107 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
It would be better if you had a monolayer of cells rather than multiple layers.</p> ;;;; <p>ImagePlus imgPlus = new ImagePlus(maskDir+imgName+‘.png’)<br>
ImageProcessor OrigIP= imgPlus.getProcessor()</p>
<p><strong>FloatProcessor ip = new TypeConverter(OrigIP, true).convertRGBToFloat()</strong></p>
<p>int n = ip.getStatistics().max<br>
def objects = RoiLabeling.labelsToConnectedROIs(ip, n)</p>
<p>def rois = objects.collect {<br>
if(it == null)<br>
return<br>
return IJTools.convertToROI(it, 0, 0, downsample, plane);<br>
}<br>
rois = rois.findAll{null!=it}</p>
<p>def pathObjects = rois.collect{<br>
return PathObjects.createAnnotationObject(it)<br>
}<br>
addObjects(pathObjects)</p>
<p>worked for me. But since there’s something wrong with the RGB to grayscale conversion (small area are ignored and blended with the surrounding), I’m going to create grayscale masks and to apply the script. Thank you for your suggestion!</p> ;;;; <p>When I drop label folder to GUI.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png" data-download-href="/uploads/short-url/ch6Uq9sHI0X6tvhKx2jKUNQKtsZ.png?dl=1" title="Screenshot from 2023-03-09 14-23-41" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png" alt="Screenshot from 2023-03-09 14-23-41" data-base62-sha1="ch6Uq9sHI0X6tvhKx2jKUNQKtsZ" width="390" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png 2x" data-dominant-color="442239"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-09 14-23-41</span><span class="informations">738×944 169 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png" data-download-href="/uploads/short-url/ch6Uq9sHI0X6tvhKx2jKUNQKtsZ.png?dl=1" title="Screenshot from 2023-03-09 14-23-41" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png" alt="Screenshot from 2023-03-09 14-23-41" data-base62-sha1="ch6Uq9sHI0X6tvhKx2jKUNQKtsZ" width="390" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png 2x" data-dominant-color="442239"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-09 14-23-41</span><span class="informations">738×944 169 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8.png" data-download-href="/uploads/short-url/nFTchsFcCFD1QdE6I8azjEEziMo.png?dl=1" title="Screenshot from 2023-03-09 14-23-58" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8_2_390x500.png" alt="Screenshot from 2023-03-09 14-23-58" data-base62-sha1="nFTchsFcCFD1QdE6I8azjEEziMo" width="390" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8.png 2x" data-dominant-color="45233B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-09 14-23-58</span><span class="informations">738×944 173 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3.png" data-download-href="/uploads/short-url/ndLewlbDLUfvaF2RCs9AVbIfTs7.png?dl=1" title="Screenshot from 2023-03-09 14-24-11" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3_2_390x500.png" alt="Screenshot from 2023-03-09 14-24-11" data-base62-sha1="ndLewlbDLUfvaF2RCs9AVbIfTs7" width="390" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3.png 2x" data-dominant-color="44223A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-09 14-24-11</span><span class="informations">738×944 171 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/709ecbde770b3662bfd890bd56ad5192fdfa7b83.png" data-download-href="/uploads/short-url/g4hHaGPR54pbP31RV1awbf9gHRh.png?dl=1" title="Screenshot from 2023-03-09 14-24-21" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/709ecbde770b3662bfd890bd56ad5192fdfa7b83.png" alt="Screenshot from 2023-03-09 14-24-21" data-base62-sha1="g4hHaGPR54pbP31RV1awbf9gHRh" width="690" height="192" data-dominant-color="45243B"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-09 14-24-21</span><span class="informations">744×208 35.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Also I can not use label frames GUI during extract outlier frames…<br>
It seems my folder is not recognized. There are extracted frames in folder.</p>
<p>ValueError: cannot reindex on an axis with duplicate labels</p> ;;;; <p>#@ File (label = “Input directory”, style = “directory”) InputDir<br>
#@ File (label = “Output directory”, style = “directory”) OutputDir<br>
FileList = getFileList(InputDir);<br>
for (f = 0; f &lt; lengthOf(FileList); f++) {<br>
ActiveImage = InputDir + File.separator + FileList[f];<br>
if (!File.isDirectory(ActiveImage));<br>
open(ActiveImage);<br>
ImageName = getTitle();</p>
<pre><code>run("Z Project...", "projection=[Max Intensity]");
//run("Channels Tool...");
Stack.setActiveChannels("111");
Stack.setActiveChannels("100");
run("RGB Color");
saveAs("Tiff", OutputDir + File.separator + ImageName + "_dapi" + ".TIF");
close();
Stack.setActiveChannels("000");
Stack.setActiveChannels("010");
run("RGB Color");
saveAs("Tiff", OutputDir + File.separator + ImageName + "_tra" + ".TIF");
close();
Stack.setActiveChannels("000");
Stack.setActiveChannels("001");
run("RGB Color");
saveAs("Tiff", OutputDir + File.separator + ImageName + "_ints2" + ".TIF");
close();
close();
selectWindow(ImageName);
close();
</code></pre>
<p>}</p> ;;;; <p>Sure. Sorry for the late reply. First of all, the aim of the work is to classify cells into cfos positive cells, Crh positive cells and ESR1 positive cells found in the Pontine micturition center of the brain. I then want to how many cfos positive cells also expresses Crh, Esr1 or both.<br>
<a class="attachment" href="/uploads/short-url/dYZ1yuwdwTcEmzZuOYG7dutdKid.cpproj">Nocturia Brain.cpproj</a> (1.2 MB)<br>
<a class="attachment" href="/uploads/short-url/dpXDG6YciQE11NhI4slxGIM435i.tif">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c1.tif</a> (6.5 MB)<br>
<a class="attachment" href="/uploads/short-url/eSJSu0l3BtNrXWGod6kQser3moh.tif">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c2.tif</a> (6.5 MB)<br>
<a class="attachment" href="/uploads/short-url/sfxj4nDJE5lpnTnIjnY8UxZSLF5.tif">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c3.tif</a> (6.5 MB)<br>
<a class="attachment" href="/uploads/short-url/1wwMMh3EWOBSJTzH5hXeZtreNQb.tif">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c4.tif</a> (6.5 MB)</p>
<p>Also on a related but different inquiries, any suggestion on how to compress tiff files.</p> ;;;; <p>Dear Matthieu,</p>
<p>It worked smoothly. Thanks for the help.<br>
“Find Peaks” is available in the BAR plugin.</p>
<p>Thanks,<br>
Giulia</p> ;;;; <p>I’m hoping to find others doing .NET development with Bioformats &amp; ImageJ searching Github, I’ve only found CellTool which seems to have been abandoned. Also could we create a tag for .NET development or just C#.</p> ;;;; <p>Hey!</p>
<p>I don’t have a quick answer for you, but how about the following:</p>
<ul>
<li>First segment the nuclei. I’ve had great success with <a href="https://www.cellpose.org/" rel="noopener nofollow ugc">CellPose</a>, especially if you’re willing to train your own model. A GPU is practically mandatory, YMMV.</li>
<li>Once you have a mask for each nucleus, shrink the ROI by a % of the total area. The .shape <a href="https://shapely.readthedocs.io/en/stable/reference/shapely.buffer.html?highlight=buffer" rel="noopener nofollow ugc">Shapely</a> function works well for me.</li>
<li>Using the nuceli mask, drop the MGV of anything outside the masks to 0</li>
<li>At this point, you should have an image that includes only the internal part of the nuclei and everything else should be 0</li>
<li>Measure the feature directly, as you described above</li>
</ul>
<p>What do you think?</p>
<p>Leo</p> ;;;; <p>I was just looking this up again for someone who messaged me through my site, might be related, but it seems like it might be rather straightforward. I have not tested this, but the javadocs suggest that it only takes two strings.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#writeDensityMapImage(java.lang.String,java.lang.String)">
  <header class="source">

      <a href="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#writeDensityMapImage(java.lang.String,java.lang.String)" target="_blank" rel="noopener">qupath.github.io</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#writeDensityMapImage(java.lang.String,java.lang.String)" target="_blank" rel="noopener">QP (QuPath 0.4.0)</a></h3>

  <p>declaration: package: qupath.lib.scripting, class: QP</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
Give that a shot, it should only take the name you used to save the density map, and the file location.<br>
<a href="https://gist.github.com/petebankhead/2e7325d8c560677bba9b867f68070300">This script from Pete</a> shows another example of using the saved density map name.</p>
<p>That might look as simple as</p>
<pre><code class="lang-auto">def server = getCurrentServer()

// Define output path (relative to project)
def name = GeneralTools.getNameWithoutExtension(server.getMetadata().getName())
pathOutput = buildFilePath(PROJECT_BASE_DIR, 'densityMaps', name)
mkdirs(pathOutput)
fileName = buildFilePath(pathOutput, 'Density map ' + name + '.tif')
writeDensityMapImage("MyFirstDensityMap", fileName)
</code></pre>
<p>I have not tested it, however.</p> ;;;; <aside class="quote no-group" data-username="paulage" data-post="11" data-topic="77554">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png" class="avatar"> Paula Gonzalez Ericsson:</div>
<blockquote>
<pre><code class="lang-auto">runObjectClassifier("tumor_tcell_macro_null")
def imageData = getCurrentImageData()
def cells = getCellObjects()
{
selectObjectsByClassification("macrophage")
classifier = loadObjectClassifier("CD68CD163CD11bPDL1PD1")
classifiers.classifyObjects(imageData, cells, false)
}
fireHierarchyUpdate()
</code></pre>
</blockquote>
</aside>
<p>Modifying this miniscript as mentioned above, and <a href="https://forum.image.sc/t/hierarchical-phenotyping-with-trained-classifiers/78298/4">borrowing formatting from Pete’s update here</a>, yeilds:</p>
<pre><code class="lang-auto">runObjectClassifier("tumor_tcell_macro_null")
def imageData = getCurrentImageData()
def cells = getCellObjects()
//the brackets do nothing and would cause an error here I think.
//{
subsetOfCells= cells.findAll(cell-&gt; 'macrophages' in cell.classifications)
classifier = loadObjectClassifier("CD68CD163CD11bPDL1PD1")
classifiers.classifyObjects(imageData, subsetOfCells, false)
//}
fireHierarchyUpdate()
</code></pre>
<p>I cannot say whether that is exactly correct since I do not know what the output of the classifier is, or what exactly the names of the classes that currently exist are.</p> ;;;; <aside class="quote no-group" data-username="Christian_Tischer" data-post="8" data-topic="78219">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png" class="avatar"> Christian Tischer:</div>
<blockquote>
<p>Just to be sure, does that also work if the <code>ImagePlus</code> is a <code>CompositeImage</code>?</p>
</blockquote>
</aside>
<p>Yes.</p>
<p>Here is an even simpler way to get the display ranges of the channels in a composite image:</p>
<blockquote>
<p>imp = IJ.openImage(IJ.getDir(“downloads”)+“2023_01_18–Extract_8hr_1-1.tif”);<br>
luts = imp.getLuts();<br>
for (i=0; i&lt;luts.length; i++)<br>
IJ.log((i+1)+“: “+luts[i].min+”-”+luts[i].max);</p>
</blockquote> ;;;; <p><a class="mention" href="/u/meghanrb">@meghanrb</a><br>
<a href="https://imagej.net/imaging/watershed" rel="noopener nofollow ugc">Watershed seperation</a> looks like it will help with your issue.</p>
<p>Christian</p> ;;;; <aside class="quote no-group" data-username="paulage" data-post="11" data-topic="77554">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png" class="avatar"> Paula Gonzalez Ericsson:</div>
<blockquote>
<p>Will running my phenotype classifier not define cells as “macrophage”, “tumor”, “tcell” and “null”?</p>
</blockquote>
</aside>
<p>That depends on how you set it up, you certainly could set it up that way if those were the names you selected for your classes.</p>
<aside class="quote no-group" data-username="paulage" data-post="11" data-topic="77554">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png" class="avatar"> Paula Gonzalez Ericsson:</div>
<blockquote>
<p><code>selectObjectsByClassification("macrophage")</code></p>
<p>Do I still have to add a line defining each category?</p>
</blockquote>
</aside>
<p>That line selects cells that are classified a certain way. If you want to select other cells, you need to use a different string. That function uses the string to create the class without you needing to create a class. Other functions require a class instead of a string of characters.</p>
<aside class="quote no-group" data-username="paulage" data-post="11" data-topic="77554">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png" class="avatar"> Paula Gonzalez Ericsson:</div>
<blockquote>
<p>I tried using the above prompt instead to call the cell phenotypes so as not to have the problem you explained above where “cell” is empty on the second loop.</p>
</blockquote>
</aside>
<p>Based on the script below, that line does nothing. You select the cells but do not use the selected cells for anything. You then pass all of the cells to the <code>classifyObjects</code> function.</p>
<aside class="quote no-group" data-username="paulage" data-post="11" data-topic="77554">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png" class="avatar"> Paula Gonzalez Ericsson:</div>
<blockquote>
<p>However I get a “method does not exist error” that I think is linked to</p>
<p><code>classifiers.classifyObjects(imageData, cells, false)</code></p>
</blockquote>
</aside>
<p>Correct, <code>classifiers</code> does not exist. You loaded the object classifier into a variable named <code>classifier</code>, not a variable named <code>classifiers</code>.</p> ;;;; <p>Hello everyone,</p>
<p>Hereby an additional draft agenda for the calls next week:</p>
<ol>
<li>Recap
<ul>
<li>Paper and NGFF status (Josh 10m)</li>
<li>Update on tables and transforms specs (Josh / Kevin 10m)</li>
</ul>
</li>
<li>Brief introduction to metadata (Wouter-Michiel ~ 10m)
<ul>
<li>Phases of metadata generation</li>
<li>Examples of current metadata standards</li>
</ul>
</li>
<li>Metadata in ome-ngff  (Wouter-Michiel ~ 10m)
<ul>
<li>Choices regarding extent of standardization</li>
<li>Possible design choices</li>
<li>Tooling</li>
</ul>
</li>
<li>Pitch metadata needs (~ 20m)</li>
<li>Open discussion (~ 50m)
<ul>
<li>Opinions on standardization</li>
<li>Opinions on design choices</li>
<li>Implementation in OME-NGFF</li>
</ul>
</li>
<li>Summary and providing outlook (~ 10m)</li>
</ol>
<p>This agenda is still subject to change. For point 4 on the agenda, I would like to ask people willing to provide a brief overview of their metadata requirements to reply to this post and also state your field (Thanks in advance!). Each person will get about 2-3 minutes.</p>
<p>Looking forward to seeing many of you next week!</p> ;;;; <p>Current Error below</p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 76MB of 12097MB (&lt;1%)</p>
<p>java.lang.NoClassDefFoundError: Could not initialize class org.bytedeco.javacpp.opencv_core<br>
at java.lang.Class.forName0(Native Method)<br>
at java.lang.Class.forName(Class.java:348)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1109)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1058)<br>
at TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:232)<br>
at TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:219)<br>
at TemplateMatching.cvMatch_Template.run(cvMatch_Template.java:91)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)</p> ;;;; <aside class="quote no-group" data-username="smcardle" data-post="5" data-topic="78298">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/smcardle/40/17938_2.png" class="avatar"> Sara McArdle:</div>
<blockquote>
<p>[Tangent: it’s worth mentioning that looking at unexpected classifications can yield interesting results, both in terms of finding out there are mistakes in your trained classifiers and in finding undiscovered cell types].</p>
</blockquote>
</aside>
<p>Yep.</p>
<p>Related discussion here and some links. <a href="https://forum.image.sc/t/simplify-subcategories-by-hierarchical-object-classification/77554/2" class="inline-onebox">Simplify subcategories by hierarchical object classification - #2 by Mike_Nelson</a></p> ;;;; <p>Ah, I understand. Well, fundamentally, QuPath doesn’t know biology, so if you’re going to have complicated biology-based rules, it’s going to be a complicated script. I’m going to make up some markers in the text below for clarity, hopefully it translates well enough to what you’re doing.<br>
[Tangent: it’s worth mentioning that looking at unexpected classifications can yield interesting results, both in terms of finding out there are mistakes in your trained classifiers and in finding undiscovered cell types].</p>
<p>How I would approach this is something like this:</p>
<ol>
<li>Run each of your major cell type classifiers (T cells, tumor, other immune, etc) individually. Use a line like<br>
<code>def CD3cells=getDetectionObjects().findAll{'CD3' in it.classifications}</code> to record in a groovy object which are your T cells. Repeat for each major type.</li>
<li>Generate a composite classifier for the cell subsets that can be multiple types (eg, make a single CD4/CD8/PD1 classifier; each T cell will eventually be some combination of all of these). Run the composite classifier. For each T cell, grab the subclassification into a list<br>
<code>def CD3subclasses=CD3cells.collect{it.classifications}</code>
</li>
<li>Repeat steps 1 and 2 for each major cell type</li>
<li>After you’ve grabbed all the subclasses for each major type into an object, you can reset their classes to whatever you have saved as a last step:</li>
</ol>
<pre><code class="lang-auto">CD3cells.eachWithIndex{cell,idx-&gt;
    it.classifications = CD3subclasses[idx]
    it.classifications += 'CD3'
}
</code></pre>
<p>Problems with this method will arise when you inevitably have some combinations of major types that shouldn’t exist (ie CD3+PanCK+ cells). You’ll have to decide how you want to handle those instances (delete the troublesome cells? Have one phenotype always win?)</p> ;;;; <p>Hi <a class="mention" href="/u/theresa_p">@Theresa_P</a>, this reminds me of these previous discussions:</p>
<aside class="quote quote-modified" data-post="1" data-topic="56465">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/niederle/40/7123_2.png" class="avatar">
    <a href="https://forum.image.sc/t/error-with-stardist-plugin-on-a-large-image/56465">Error with stardist plugin on a large image</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hi, 
I’ve already reported the error on the stardist-github and got the recommendation to ask here as it might be a Fiji specific error (<a href="https://github.com/CSBDeep/CSBDeep_website/issues/6" rel="noopener nofollow ugc">this one was mentioned</a> ) 
I tried to run the StarDist 2D plugin on a 5108x4308 16-bit tiff and got the following error: 
[INFO] Reading available sites from https://imagej.net/
[INFO] Successfully retrieved OPTIONS.
[INFO] Successfully called PropFind, directory exists: .
[INFO] Successfully locked db.xml.gz.lock.
[INFO] Successfully uploaded to db.xml.gz.lock
…
  </blockquote>
</aside>

<aside class="quote quote-modified" data-post="1" data-topic="60373">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/lguerard/40/14201_2.png" class="avatar">
    <a href="https://forum.image.sc/t/issue-with-stardist-w-or-w-o-trackmate-with-large-images/60373">Issue with StarDist (w/ or w/o TrackMate) with large images</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hi all, 
Linked to <a href="https://forum.image.sc/t/issue-with-trackmate-and-stardist-on-big-dataset/57976">this topic</a>, I wanted to report that I also encountered a similar issue today without the use of the Imglib2-imaris-bridge. 
As you can see from the screenshot, the detector somehow stopped after a while: 
 <a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b42c46143b7f72f422db78b8468d686970d946d.jpeg" data-download-href="/uploads/short-url/aJMRcKqKkTVSWsexIr3vgTQXd0N.jpeg?dl=1" title="image" rel="noopener nofollow ugc">[image]</a> 
Data is not that big, it’s 3.3Gb… 
I also tried running StarDist alone and it is also showing the top of the picture so I guess it is really linked to the StarDist people (which is why I created this new topic). 
Any ideas why ? Thanks !
  </blockquote>
</aside>

<p>Please try to increase the number of tiles and see if the problem goes away. (Not a great solution, but might be working.)</p> ;;;; <aside class="quote no-group" data-username="Kristin.Gallik" data-post="27" data-topic="49633">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/kristin.gallik/40/60087_2.png" class="avatar"> Krisitn Gallik:</div>
<blockquote>
<p>While I am glad there are no differences between 4.2 and 4.3 in this regard,</p>
</blockquote>
</aside>
<p>Good! I’m relieved, there shouldn’t be a difference <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<aside class="quote no-group" data-username="Kristin.Gallik" data-post="27" data-topic="49633">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/kristin.gallik/40/60087_2.png" class="avatar"> Krisitn Gallik:</div>
<blockquote>
<p>While I am glad there are no differences between 4.2 and 4.3 in this regard, I don’t understand why running addPixelClassifierMeasurements in a script is so much slower compared to running it through the GUI.</p>
</blockquote>
</aside>
<p>The link in my last post considers it a bit, and also links to an earlier discussion about it:</p><aside class="quote quote-modified" data-post="2" data-topic="41579">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png" class="avatar">
    <a href="https://forum.image.sc/t/addpixelclassifiermeasurements-low-performance/41579/2">addPixelClassifierMeasurements - low performance</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    You can easily turn off the preview with the C shortcut key or the ‘C’ key in the toolbar. But it needs to be on by default or there would be more complaints it isn’t doing anything… 
You can also turn off live prediction with the big button in the classifier training window (albeit only when training). 
Do neither of these do what you want? 

The preview overlay is parallelized, but adding measurements is not. Probably it should be, but I hadn’t previously heard of it being such a performance …
  </blockquote>
</aside>

<p>There should already be some major improvements in v0.4.x compared to earlier versions:</p><aside class="onebox githubpullrequest" data-onebox-src="https://github.com/qupath/qupath/pull/1076#issuecomment-1279692584">
  <header class="source">

      <a href="https://github.com/qupath/qupath/pull/1076#issuecomment-1279692584" target="_blank" rel="noopener">github.com/qupath/qupath</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">

    <div class="github-icon-container" title="Comment">
      <svg width="60" height="60" class="github-icon" viewbox="0 0 16 16" aria-hidden="true"><path fill-rule="evenodd" d="M1.5 2.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v5.5a.25.25 0 01-.25.25h-3.5a.75.75 0 00-.53.22L3.5 11.44V9.25a.75.75 0 00-.75-.75h-1a.25.25 0 01-.25-.25v-5.5zM1.75 1A1.75 1.75 0 000 2.75v5.5C0 9.216.784 10 1.75 10H2v1.543a1.457 1.457 0 002.487 1.03L7.061 10h3.189A1.75 1.75 0 0012 8.25v-5.5A1.75 1.75 0 0010.25 1h-8.5zM14.5 4.75a.25.25 0 00-.25-.25h-.5a.75.75 0 110-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0114.25 12H14v1.543a1.457 1.457 0 01-2.487 1.03L9.22 12.28a.75.75 0 111.06-1.06l2.22 2.22v-2.19a.75.75 0 01.75-.75h1a.25.25 0 00.25-.25v-5.5z"></path></svg>
    </div>



  <div class="github-info-container">

      <h4>
        Comment by
        <a href="https://github.com/petebankhead" target="_blank" rel="noopener">
          <img alt="petebankhead" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/bedfaac2750bf058873bc67485ebe9dac1f5ade4.png" class="onebox-avatar-inline" width="20" height="20">
          petebankhead
        </a>
        to
        <a href="https://github.com/qupath/qupath/pull/1076#issuecomment-1279692584" target="_blank" rel="noopener">Improve pixel classifier measurement performance</a>
      </h4>



    <div class="branches">
      <code>qupath:main</code> ← <code>petebankhead:pixel-measurements</code>
    </div>

  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">I tested performance using CMU-1.svs.
I used a very basic thresholder and simpl<span class="show-more-container"><a href="https://github.com/qupath/qupath/pull/1076" target="_blank" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">e classifier trained for 3 classes, saved for both classification and probability output - then ran the script at the bottom.

Using a Mac Studio (2022) with M1 Max and 32 GB RAM the processing time was:

| v0.3.0  | v0.4.0-SNAPSHOT |
| ------------- | ------------- |
| 593.9 s  | 60.1 s  |

Results identical as far as I can tell. So... quite a substantial difference :)

Cell detection took close to 30s, with 326 498 cells detected,.

```groovy
def checkpoints = [:]


setImageType('BRIGHTFIELD_H_E')
setColorDeconvolutionStains('{"Name" : "H&amp;E default", "Stain 1" : "Hematoxylin", "Values 1" : "0.65111 0.70119 0.29049", "Stain 2" : "Eosin", "Values 2" : "0.2159 0.8012 0.5581", "Background" : " 255 255 255"}')

clearAllObjects()

checkpoints &lt;&lt; ['Tissue detection': System.currentTimeMillis()]

createAnnotationsFromPixelClassifier("Tissue detection", 10000.0, 0.0, "INCLUDE_IGNORED")

checkpoints &lt;&lt; ['Cell detection': System.currentTimeMillis()]

selectAnnotations()
runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImageBrightfield": "Hematoxylin OD",  "requestedPixelSizeMicrons": 1.0,  "backgroundRadiusMicrons": 8.0,  "medianRadiusMicrons": 0.0,  "sigmaMicrons": 1.5,  "minAreaMicrons": 10.0,  "maxAreaMicrons": 400.0,  "threshold": 0.1,  "maxBackground": 2.0,  "watershedPostProcess": true,  "cellExpansionMicrons": 5.0,  "includeNuclei": true,  "smoothBoundaries": true,  "makeMeasurements": true}')

for (classifier in ['Some probability', 'Some classification']) {

    // Create annotation measurements
    checkpoints &lt;&lt; ["Annotation measurements for $classifier": System.currentTimeMillis()]
    selectAnnotations()
    addPixelClassifierMeasurements(classifier, classifier)
    
    // Create cell measurements
    checkpoints &lt;&lt; ["Cell measurements for $classifier": System.currentTimeMillis()]
    selectCells()
    addPixelClassifierMeasurements(classifier, classifier)
}
checkpoints &lt;&lt; ["Done": System.currentTimeMillis()]
resetSelection()
println 'Done!'

def entries = checkpoints.entrySet() as List
println "Total time: \t${entries[-1].value - entries[0].value} ms"
for (int i = 0; i &lt; entries.size()-1; i++) {
    println "    ${entries[i].key} \t${entries[i+1].value - entries[i].value}"
}
```</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>I guess there’s still work to do to parallelise the batch processing. I haven’t had time to work on it, but the scripts I posted in the other topics can sometimes help.</p>
<p>It’s an easier task if your pixel classifier doesn’t use enormous amounts of memory, so that all the tiles can be stored in memory at once. Then the <a href="https://forum.image.sc/t/addpixelclassifiermeasurements-low-performance/41579/2">script here</a> might help. But I’m not sure which resolution yours pixel classifier runs at, so I don’t know if it’ll work for you on the 70-80 GB images… if it pre-requests all the classified tiles, then forgets them again because it runs out of memory, the script could make performance much worse.</p> ;;;; <p>Nice, love seeing initiatives that make microscopy accessible to more people. <a class="mention" href="/u/p_tadrous">@P_Tadrous</a> has a <a href="https://www.youtube.com/channel/UCOvBahuVgEmLB5ycQEsgEnQ">Youtube series</a> on how to make his 3D printed PUMA microscope, which supports IF, phase contrast, and I think even augmented reality to overlay grids for manual cell counting through the eyepiece. Definitely worth checking out!</p>
<p>And on the note of the optical clearing of wood, an old acquaintance of mine made this video, using polymerized methyl methacrylate as the medium instead: <a href="https://www.youtube.com/watch?v=uUU3jW7Y9Ak" class="inline-onebox">Making transparent wood - YouTube</a></p> ;;;; <aside class="quote no-group" data-username="igorafsouza" data-post="8" data-topic="73482">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/igorafsouza/40/68590_2.png" class="avatar"> Igor:</div>
<blockquote>
<p>My question is: is it possible to change the n_rays value for a pre-trained model?</p>
</blockquote>
</aside>
<p>No, sorry.</p>
<aside class="quote no-group" data-username="igorafsouza" data-post="8" data-topic="73482">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/igorafsouza/40/68590_2.png" class="avatar"> Igor:</div>
<blockquote>
<p>If not, how should I proceed in order to test different n_rays values?</p>
</blockquote>
</aside>
<p>How do you mean?</p>
<p>One thing you can always do is to use the pre-trained model for prediction on the images that you care about. After fixing potential prediction mistakes, use these results to train a new model with different parameters (e.g. with more rays).</p> ;;;; <p><a href="https://github.com/stardist/stardist/issues/68#issuecomment-1455823190">I recently learned</a> that the library can be found here:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/bioimage-io/model-runner-java">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/bioimage-io/model-runner-java" target="_blank" rel="noopener">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/3/23dadbd95fe8ed3a01b8ff456c94347cb444215d_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/3/23dadbd95fe8ed3a01b8ff456c94347cb444215d_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/3/23dadbd95fe8ed3a01b8ff456c94347cb444215d_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/23dadbd95fe8ed3a01b8ff456c94347cb444215d.png 2x" data-dominant-color="F7F3EE"></div>

<h3><a href="https://github.com/bioimage-io/model-runner-java" target="_blank" rel="noopener">GitHub - bioimage-io/model-runner-java</a></h3>

  <p>Contribute to bioimage-io/model-runner-java development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hi Pete,</p>
<p>Thanks for replying. What information do you need specifically to help in finding a solution? For more context:</p>
<ul>
<li>czi pyramidal files from a whole slide scanner imported into QuPath using BioFormats.</li>
<li>data collected are brightfield and polarized light (QuPath reads both in as their own sets of RGB, so 6 channels total)</li>
<li>I trained a pixel classifier to detect the polarized light signal as “Polarized Light” and the background/absence of signal as “ignore*”</li>
<li>I generated rough outlines of the tissue samples for the pixel classifier to be applied/restricted to</li>
<li>The analysis goal is to use the pixel classifier to measure the total area of polarized light signal across the whole tissue sample; no cell detections involved</li>
<li>Once the pixel classifier was working as intended, I created the script I shared to measure the polarized light signal more efficiently.</li>
</ul>
<p>I ran this script in 4.2 on a single image 68 GB uncompressed in size. It took 5 minutes and 17 seconds (so my previous statement about run time in this case is incorrect) and the CPU usage remained low &lt;15%.<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8d0fc8b1b113d91509f538c2e8a77f201ce3fe9.png" alt="image" data-base62-sha1="zv8ab64UQW4BfBkUNi1XVzneGRb" width="586" height="393"></p>
<p>I then opened a different image from this same data set that is 79.9 GB in size and applied the pixel classifier via Classify &gt; Pixel Classification &gt; Load Pixel Classifier and used the Measure button and had it measure in annotations only. This took less than a minute to run and the CPU usage was very high &gt;70%.</p>
<p>Running the same exact pixel classifier on the same two images in 4.3 as described above:<br>
68 GB image using the script to measure the polarized light signal:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6e1a3929fab525b48ae32fd4012c95b46533e17.png" alt="image" data-base62-sha1="wWtfl9bfV1CMnQtU0r5fH6XczvV" width="586" height="393"><br>
2 seconds less than 4.2!</p>
<p>79.9 GB image with the classifier applied via Classify &gt; Pixel Classification &gt; Load Pixel Classifier took less than a minute (QuPath is the Orange Trace):<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4cb02997696c5351d7853d9b3c122b5e4d8e81a4.png" data-download-href="/uploads/short-url/aWpIgZAuXhvUKgAz1upMKyeNKOU.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4cb02997696c5351d7853d9b3c122b5e4d8e81a4.png" alt="image" data-base62-sha1="aWpIgZAuXhvUKgAz1upMKyeNKOU" width="690" height="242" data-dominant-color="B0BFB2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1197×421 32.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
This trace of CPU usage is pretty much the same as when I ran this same analysis on 4.2.</p>
<p>While I am glad there are no differences between 4.2 and 4.3 in this regard, I don’t understand why running addPixelClassifierMeasurements in a script is so much slower compared to running it through the GUI. I created the script I posted using the create script button in the workflow tab, so I shouldn’t be calling anything differently in the script compared to using the GUI.</p>
<p>I did look over the solution you directed me too. If that can be adjusted to analyze the tiles of the image in parallel, that might do the trick. I’m just not sure how to add that in for QuPath’s language.</p> ;;;; <p>Just downloaded imageJ and I am getting the following error while trying to run template match -cvMatch. Kindly help.</p>
<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 38MB of 12097MB (&lt;1%)<br>
java.lang.UnsatisfiedLinkError: no jniopencv_core in java.library.path<br>
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1875)<br>
at java.lang.Runtime.loadLibrary0(Runtime.java:872)<br>
at java.lang.System.loadLibrary(System.java:1124)<br>
at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:1543)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1192)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1042)<br>
at org.bytedeco.javacpp.opencv_core.(opencv_core.java:10)<br>
at java.lang.Class.forName0(Native Method)<br>
at java.lang.Class.forName(Class.java:348)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1109)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1058)<br>
at TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:232)<br>
at TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:219)<br>
at TemplateMatching.cvMatch_Template.run(cvMatch_Template.java:91)<br>
at ij.IJ.runUserPlugIn(IJ.java:237)<br>
at ij.IJ.runPlugIn(IJ.java:203)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)<br>
Caused by: java.lang.UnsatisfiedLinkError: no opencv_core2410 in java.library.path<br>
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1875)<br>
at java.lang.Runtime.loadLibrary0(Runtime.java:872)<br>
at java.lang.System.loadLibrary(System.java:1124)<br>
at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:1543)<br>
at org.bytedeco.javacpp.Loader.load(Loader.java:1143)<br>
… 14 more</p> ;;;; <h6>
<a name="by-matt-mccormick-orciduploadrobavbggcq0ltwsmonefcecmprygifhttpsorcidorg0000-0001-9475-3756-mary-elise-dedicke-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0001-8848-3235-jean-christophe-fillion-robin-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0002-9688-8950-will-schroeder-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0003-3815-9386-1" class="anchor" href="#by-matt-mccormick-orciduploadrobavbggcq0ltwsmonefcecmprygifhttpsorcidorg0000-0001-9475-3756-mary-elise-dedicke-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0001-8848-3235-jean-christophe-fillion-robin-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0002-9688-8950-will-schroeder-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0003-3815-9386-1"></a>By: Matt McCormick <a href="https://orcid.org/0000-0001-9475-3756"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bff6a0d75375482cfaf128e71b58c428272d88a0.gif" alt="ORCID" data-base62-sha1="robAVbgGcQ0LTwsMOnEfcEcmpry" width="16" height="16"></a>, Mary Elise Dedicke <a href="https://orcid.org/0000-0001-8848-3235"><span alt="ORCID" class="broken-image" title="This image is broken"><svg class="fa d-icon d-icon-unlink svg-icon" aria-hidden="true"><use href="#unlink"></use></svg></span></a>, Jean-Christophe Fillion-Robin <a href="https://orcid.org/0000-0002-9688-8950"><span alt="ORCID" class="broken-image" title="This image is broken"><svg class="fa d-icon d-icon-unlink svg-icon" aria-hidden="true"><use href="#unlink"></use></svg></span></a>, Will Schroeder <a href="https://orcid.org/0000-0003-3815-9386"><span alt="ORCID" class="broken-image" title="This image is broken"><svg class="fa d-icon d-icon-unlink svg-icon" aria-hidden="true"><use href="#unlink"></use></svg></span></a>
</h6>
<p>Did you know that your web browser comes bundled with extremely powerful development tools?</p>
<p>Modern web browsers are the foundation of the <a href="https://en.wikipedia.org/wiki/Web_platform">The Web Platform</a>, a full-featured computing environment. However, <em>full-featured platforms are not useful in and of themselves</em>. A large community of software developers is required to program the applications on a platform that people love. And, <a href="https://www.kitware.com/how-to-debug-wasi-pipelines-with-itk-wasm/">effective debugging results in effective programming</a>.</p>
<p>In this tutorial, we will learn how to debug <a href="https://wasm.itk.org">itk-wasm</a> C++ data processing pipelines built to <a href="https://webassembly.org">WebAssembly (wasm)</a> with the full-featured graphical debugger built into Chromium-based browsers.</p>
<p>In the following sections, we will first explain how to obtain a useful JavaScript backtrace in Node.js, then debug C++ code built to WebAssembly in a Chromium-based web browser. Let’s get started! <img src="https://emoji.discourse-cdn.com/twitter/rocket.png?v=12" title=":rocket:" class="emoji" alt=":rocket:" loading="lazy" width="20" height="20"></p>
<h2>
<a name="h-0-preliminaries-2" class="anchor" href="#h-0-preliminaries-2"></a>0. Preliminaries</h2>
<p>Before starting this tutorial, check out our <a href="https://www.kitware.com/how-to-debug-wasi-pipelines-with-itk-wasm/">WASI WebAssembly command line debugging tutorial</a>.</p>
<p>As with the previous tutorial, we will be debugging the following C++ code:</p>
<pre><code class="lang-cpp">#include &lt;iostream&gt;

int main() {
  std::cout &lt;&lt; "Hello debugger world!" &lt;&lt; std::endl;

  const char * wasmDetails = "are no longer hidden";

  const int a = 1;
  const int b = 2;
  const auto c = a + b;

  // Simulate a crash.
  abort();
  return 0;
}
</code></pre>
<p><a href="https://github.com/InsightSoftwareConsortium/itk-wasm/tree/main/examples/debugging">The tutorial code</a> provides npm scripts as a convenient way to execute debugging commands, which you may also invoke directly in a command line shell.</p>
<h2>
<a name="h-1-nodejs-backtrace-3" class="anchor" href="#h-1-nodejs-backtrace-3"></a>1. Node.js Backtrace</h2>
<p>When debugging WebAssembly built with the itk-wasm Emscripten toolchain, set the <code>CMAKE_BUILD_TYPE</code> to <code>Debug</code> just like with native binary debug builds.</p>
<p>As with native builds, this build configuration adds debugging symbols, the human-readable names of functions, variables, etc., into the binary.  This also adds support for C++ exceptions and retrieving the string name associated with exceptions. Without this itk-wasm instrumentation, a C++ exception will throw an error with an opaque integer value. And, Emscripten JavaScript WebAssembly bindings will not be minified, which facilitates debugging.</p>
<p>When built with the default <code>Release</code> build type:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651.png" data-download-href="/uploads/short-url/mJeSVckwyLmNlqL32UPhPqSjEop.png?dl=1" title="Emscripten build Release"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651_2_690x155.png" alt="Emscripten build Release" data-base62-sha1="mJeSVckwyLmNlqL32UPhPqSjEop" width="690" height="155" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651_2_690x155.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651_2_1035x232.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651.png 2x" data-dominant-color="0A0A0A"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Emscripten build Release</span><span class="informations">1066×241 23.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>the JavaScript support code is minified, and difficult to debug:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c.png" data-download-href="/uploads/short-url/14OQ2Bch3htCzijyrT0JQshKd5i.png?dl=1" title="Run Node Release"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c_2_690x224.png" alt="Run Node Release" data-base62-sha1="14OQ2Bch3htCzijyrT0JQshKd5i" width="690" height="224" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c_2_690x224.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c.png 2x" data-dominant-color="151714"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Run Node Release</span><span class="informations">991×322 77.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>However, when built with the <code>Debug</code> build type:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png" data-download-href="/uploads/short-url/bgYuIjkw1VnTOuRQzsXtkehkf0v.png?dl=1" title="Emscripten build Debug"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png" alt="Emscripten build Debug" data-base62-sha1="bgYuIjkw1VnTOuRQzsXtkehkf0v" width="690" height="192" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_1035x288.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png 2x" data-dominant-color="090909"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Emscripten build Debug</span><span class="informations">1138×318 34.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>you can obtain a useful backtrace:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f.png" data-download-href="/uploads/short-url/nJkIl5oxkJ3W7UuYBEmJDcHxQ1F.png?dl=1" title="Run Node Debug"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f_2_690x273.png" alt="Run Node Debug" data-base62-sha1="nJkIl5oxkJ3W7UuYBEmJDcHxQ1F" width="690" height="273" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f_2_690x273.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f_2_1035x409.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f.png 2x" data-dominant-color="061106"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Run Node Debug</span><span class="informations">1118×443 58.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>This is helpful for debugging issues that occur in the Emscripten JavaScript interface. The next section describes how to debug issues inside the Emscripten-generated WebAssembly.</p>
<h2>
<a name="h-2-debugging-in-chromium-based-browsers-4" class="anchor" href="#h-2-debugging-in-chromium-based-browsers-4"></a>2. Debugging in Chromium-based Browsers</h2>
<p>Recent Chromium-based browsers have support for debugging C++ -based WebAssembly in the browser. With a few extra steps described in this section, it is possible to interactively step through and inspect WebAssembly that is compiled with C++ running in the browser.</p>
<p>WebAssembly debugging in DevTools requires a few extra setup steps compared to a default browser installation.</p>
<p>First, <a href="https://goo.gle/wasm-debugging-extension">install the Chrome WebAssembly Debugging extension</a>.</p>
<p>Next, enable it in DevTools:</p>
<ol>
<li>In DevTools, click the <em>gear (<img src="https://emoji.discourse-cdn.com/twitter/gear.png?v=12" title=":gear:" class="emoji" alt=":gear:" loading="lazy" width="20" height="20">)</em> icon in the top-right corner.</li>
<li>Go to the <em>Experiments</em> panel.</li>
<li>Select <em>WebAssembly Debugging: Enable DWARF support</em>.</li>
</ol>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8.png" data-download-href="/uploads/short-url/nFt0HIS0tSO300OHyK27k2pqCy4.png?dl=1" title="Enable Wasm Debugging"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8_2_582x500.png" alt="Enable Wasm Debugging" data-base62-sha1="nFt0HIS0tSO300OHyK27k2pqCy4" width="582" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8_2_582x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8_2_873x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8.png 2x" data-dominant-color="F7F6F7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Enable Wasm Debugging</span><span class="informations">901×774 75.2 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Exit Settings, then reload DevTools as prompted.</p>
<p>Next, open the options for the Chrome WebAssembly Debugging extension:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b703b7b9cc9803e8cf705673444e2d6060908293.png" alt="Wasm Debugging Options" data-base62-sha1="q71ldlCRIO5znMxwCwBuKvkCleH" width="527" height="383"></p>
<p>Since itk-wasm builds in a clean Docker environment, the debugging source paths in the Docker environment are different from the paths on the host system. The debugging extension has a path substitution system that can account for these differences. In the Docker image, the directory where <code>itk-wasm</code> is invoked is mounted as <code>/work</code>. Substitute <code>/work</code> with the directory where the <code>itk-wasm</code> CLI is invoked. For example, if <code>itk-wasm</code> was invoked at <code>/home/matt/src/itk-wasm/examples/Debugging</code>, then set the path substitution as shown below:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15.png" data-download-href="/uploads/short-url/pkEGg1S9dGlGZWH2cjwWUl68di5.png?dl=1" title="Path substitution"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15_2_690x415.png" alt="Path substitution" data-base62-sha1="pkEGg1S9dGlGZWH2cjwWUl68di5" width="690" height="415" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15_2_690x415.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15_2_1035x622.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15.png 2x" data-dominant-color="F0F0F0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Path substitution</span><span class="informations">1278×770 95.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Build the project with itk-wasm and the <code>Debug</code> <code>CMAKE_BUILD_TYPE</code> to include DWARF debugging information:</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png" data-download-href="/uploads/short-url/bgYuIjkw1VnTOuRQzsXtkehkf0v.png?dl=1" title="Emscripten build Debug"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png" alt="Emscripten build Debug" data-base62-sha1="bgYuIjkw1VnTOuRQzsXtkehkf0v" width="690" height="192" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_1035x288.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png 2x" data-dominant-color="090909"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Emscripten build Debug</span><span class="informations">1138×318 34.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Here we load and run the WebAssembly with a simple HTML file and server:</p>
<pre><code class="lang-html">&lt;html&gt;
  &lt;head&gt;
    &lt;script src="https://cdn.jsdelivr.net/npm/itk-wasm@1.0.0-a.11/dist/umd/itk-wasm.js"&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;This is an example to demonstrate browser-based debugging of
    C++-generated WebAssembly. For more information, please see the
    &lt;a target="_blank" href="https://wasm.itk.org/examples/debugging.html"&gt;associated
      documentation&lt;/a&gt;.&lt;/p&gt;

    &lt;script&gt;
      window.addEventListener('load', (event) =&gt; {
        const pipeline = new URL('emscripten-build-debug/DebugMe', document.location)
        itk.runPipeline(null, pipeline)
      });
    &lt;/script&gt;

  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b.png" data-download-href="/uploads/short-url/cbGXSkN2P2HM3VK7aK3MAL7Eerp.png?dl=1" title="HTTP Server"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b_2_690x472.png" alt="HTTP Server" data-base62-sha1="cbGXSkN2P2HM3VK7aK3MAL7Eerp" width="690" height="472" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b_2_690x472.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b.png 2x" data-dominant-color="0A0B09"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">HTTP Server</span><span class="informations">1010×692 85.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>And we can debug the C++ code in Chrome’s DevTools debugger alongside the executing JavaScript!</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67.png" data-download-href="/uploads/short-url/zFfTJxejJDIk3sRviOi3DMG4vOf.png?dl=1" title="Debug C++ DevTools"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_690x400.png" alt="Debug C++ DevTools" data-base62-sha1="zFfTJxejJDIk3sRviOi3DMG4vOf" width="690" height="400" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_690x400.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_1035x600.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_1380x800.png 2x" data-dominant-color="F3EFF0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Debug C++ DevTools</span><span class="informations">1571×911 202 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<h2>
<a name="whats-next-5" class="anchor" href="#whats-next-5"></a>What’s Next</h2>
<p>In our next post, we will provide a tutorial on how to generate JavaScript and TypeScript bindings for Node.js and the browser.</p>
<p><strong>Enjoy ITK!</strong></p> ;;;; <p>I would first try the raw image data or on background-subtracted image data and you’ll definitely need to split the channels first. I should have said so in my previous post. Please let me know how it goes.</p> ;;;; <p>Hi <a class="mention" href="/u/kristin.gallik">@Kristin.Gallik</a> I don’t really have enough info to know what could be going on here. Your issue sounds more related to <a href="https://forum.image.sc/t/qupath-measure-pixel-classifier-area-per-cell-detection-for-wsis/72701/9" class="inline-onebox">Qupath measure pixel classifier area per cell detection for WSIs - #9 by petebankhead</a></p>
<p>If you can compare and provide timings following the <em>exact</em> same process in v0.4.2 and v0.4.3 I would be interested in the results. I can’t think of any changes between the versions that would be relevant to your script (except possibly the Bio-Formats version being updated, but I don’t know if your image is even being read using Bio-Formats).</p> ;;;; <p>Hi Pete,</p>
<p>I found this solution trying to solve this same issue in batch analysis with QuPath 4.3. I am applying a pixel classifier to some large pyramidal images to get an area measurement. When I run the classifier via the drop-down menu, the measurements are very quick, ~10 second and uses ~70% of my CPU. When I try running on batch, the CPU use only ever gets to ~10% max. I tried increases the RAM allocation for QuPAth to 80% (&gt;400GB on my machine) which did not help increase the speed. I let the batch run for about 30 minutes and it didn’t even get through one image. I have used pixel and object classifiers in a batch run in 4.2 without any issue, so I am puzzled about what’s happening here. I did try the script you suggested here but this unfortunately did not solve the problem either. I got an warning about readBufferedImage being depreciated, so I updated it to the readRegions suggested in the warning. This also didn’t help.</p>
<p>The script I’m running is pretty simple:</p>
<pre><code class="lang-auto">def tempRequest = RegionRequest.createInstance(getCurrentServer().getPath(), 1.0, 0, 0, 10, 10)
def tempImg = getCurrentServer().readRegion(tempRequest)
selectAnnotations();
addPixelClassifierMeasurements("Polarized_Light", "Polarized_Light")
</code></pre>
<p>Any help is much appreciated.</p> ;;;; <p>Thank you Mary! Should I do this on the sum projection? And splitting the channel first?</p> ;;;; <p>So I have a Fluorescent protein molecules spread over ROI separate from each molecule (please see movie frames attached) which I take 50 ms frame rate 40,000 frames. I run thunderstorm and get the table which has all the data for each frame. In the intensity column (integrated photons number under the peak as a definition). But I really not able to understand this intensity meaning, what does it mean? Is it that if I have spot of molecule and the each frame suppose shows 4000 intensity (for one frame and one spot) that means it’s 4000 total number of photons in that spot for that frame alone, so if i have to calculate total photon from that spot I will need to add all the frames till it’s blinking? Please if you can explain.</p>
<p>Table output file screenshot and image sequence 500 frames (For example) attached.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/a/2a91475c5e80fbe9aa563c034ca5fd8f8e60a275.png" data-download-href="/uploads/short-url/64zjlwKa7gfLksBwNokhyWSlnYp.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/a/2a91475c5e80fbe9aa563c034ca5fd8f8e60a275.png" alt="image" data-base62-sha1="64zjlwKa7gfLksBwNokhyWSlnYp" width="690" height="257" data-dominant-color="E7E7E7"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1038×388 20 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<a class="attachment" href="/uploads/short-url/axpXuCxawFIrpK5bs2jaSRCeKaW.tif">mvi-1.tif</a> (14.4 MB)</p> ;;;; <p>Hi <a class="mention" href="/u/meethm">@meethm</a> I edited your post to format the script as code so it is easier to read (you can do it with the <code>&lt;/&gt;</code> button when posting).</p>
<p>There are some changes in v0.4.x that would make creating such a script easier – and you can avoid a lot of the <code>getPathClass()</code> stuff (even if it’s still used internally). There’s an example at <a href="https://github.com/qupath/qupath/pull/1094" class="inline-onebox">Improve working with measurements and classifications by petebankhead · Pull Request #1094 · qupath/qupath · GitHub</a></p>
<p>With the ‘new’ method you could identify all the cells with a particular classification like this:</p>
<pre><code class="lang-groovy">def cells = getCellObjects()
def cd8Pos = cells.findAll(cell -&gt; 'CD8' in cell.classifications)

println "${cd8Pos.size()}/${cells.size()} are CD8+ve"
</code></pre>
<p>That sound find cells with the classification ‘CD8’, ‘CD3: CD8’, ‘CD8: CD3’… basically any set of classifications where ‘CD8’ is included. But it <em>wouldn’t</em> get something like ‘CD8+ve’ because that’s a whole other thing from QuPath’s perspective.</p>
<p>Do be a bit cautious though, and try to check it is behaving as you expect. It’s a very new feature and as such there isn’t really much documentation and I haven’t explored its possibilities fully myself.</p>
<p>For example, as a check you could add this to the end of the script above to count the actual classifications you’ve got, all of which should contain ‘CD8’ somewhere:</p>
<pre><code class="lang-groovy">def countMap = cd8Pos.countBy(p -&gt; p.getPathClass())
for (def entry : countMap.entrySet()) {
    println "${entry.key} -&gt; ${entry.value}"
}
</code></pre> ;;;; <p>Hi Sara,</p>
<p>Thank you for your response! I used the composite classifier before I started scripting. The problem with the composite classifier is that I get a bunch of junk in the output that I don’t want to analyze. A simple example would be CD3e-CD8+, which isn’t something I want to carry forward into neighborhood analysis because it’s not a true CD8 T cell since it’s missing the CD3e marker. With the hierarchical phenotyping, I can specify that that cell is phenotyped as “Other.” You can imagine that with 30 markers and numerous classifiers for one image that the amount of junk produced by applying the classifiers sequentially is astounding. The hierarchical phenotyping helps me avoid that problem, but I want to combine it with an object classifier instead of the single measurement classifier since I think the object classifiers can be a lot more accurate.</p>
<p>I hope that makes sense! Maybe there’s something about applying sequential classifiers that I don’t know too.</p> ;;;; <p><a class="mention" href="/u/shubo_chakrabarti">@Shubo_Chakrabarti</a><br>
ilya.belevich @ <a href="http://helsinki.fi">helsinki.fi</a></p> ;;;; <p>Hello,</p>
<p>I am trying to script the production of cell detection density maps using QuPath’s built in DensityMaps ( <a href="https://qupath.github.io/javadoc/docs/qupath/lib/analysis/heatmaps/DensityMaps.html" class="inline-onebox" rel="noopener nofollow ugc">DensityMaps (QuPath 0.4.0)</a> ), followed by exportation to an image file, which I think will have to go through imageJ</p>
<p>However, I am unable to get anything working. I would really appreciate a short example of how to use DensityMaps with exportation.</p>
<p>Here is all I have so far, I am not even sure if the map created by this contains anything.</p>
<pre><code class="lang-auto">import qupath.lib.analysis.heatmaps.DensityMaps

selectAnnotations();

runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{"detectionImageBrightfield": "Optical density sum",  "requestedPixelSizeMicrons": 0.0,  "backgroundRadiusMicrons": 0.0,  "medianRadiusMicrons": 0.0,  "sigmaMicrons": 1.5,  "minAreaMicrons": 10.0,  "maxAreaMicrons": 100.0,  "threshold": 0.23,  "maxBackground": 2.0,  "watershedPostProcess": true,  "excludeDAB": false,  "cellExpansionMicrons": 5.0,  "includeNuclei": true,  "smoothBoundaries": true,  "makeMeasurements": true}');

def params = new DensityMaps.DensityMapParameters()
def map = new DensityMaps.DensityMapBuilder(params)
</code></pre>
<p>Thank you!</p> ;;;; <p>Analyzing a video that just worked 30 minutes ago on DLC 2.3.0.</p>
<p>Now encountering the following error on analyzing that results in the GUI and DLC crashing:</p>
<p>2023-03-08 13:59:44.949135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13404 MB memory:  → device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9<br>
2023-03-08 13:59:44.971871: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled<br>
0%|                                                                                         | 0/4501 [00:00&lt;?, ?it/s]2023-03-08 13:59:46.652324: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401<br>
100%|██████████████████████████████████████████████████████████████████████████████| 4501/4501 [01:08&lt;00:00, 66.05it/s]<br>
*** Received signal 11 ***<br>
*** BEGIN STACK TRACE POINTERS ***<br>
0x00007ffaec2fd08f<br>
0x00007ffaec2fd277<br>
0x00007ffbdd684090<br>
0x00007ff6a8c327ec<br>
0x00007ffbd031e390<br>
0x00007ffbdfe13eaf<br>
0x00007ffbdfd8eae6<br>
0x00007ffbdfe12e9e<br>
0x00007ffb390c66c8<br>
0x00007ffb2b7110ee<br>
0x00007ffb2b71008f<br>
0x00007ffb2bd03e98<br>
0x00007ffb390831b5<br>
0x00007ffb390859ee<br>
0x00007ffb389f0b3f<br>
0x00007ffb391dcf40<br>
0x00007ffb389f0b19<br>
0x00007ffb3908888f<br>
0x00007ffb39080fcd<br>
0x00007ffb2bd00e15<br>
0x00007ffb76986d82<br>
0x00007ffb76a203f6<br>
0x00007ffb76a28d88<br>
0x00007ffb76a25169<br>
0x00007ffb76940695<br>
0x00007ffb769409d8<br>
0x00007ffb76a203f6<br>
0x00007ffb76a28d88<br>
0x00007ffb76a22df8<br>
0x00007ffb76a2798a<br>
0x00007ffb76a217f4<br>
0x00007ffb76a1ca4f<br>
0x00007ffb76986bf5<br>
0x00007ffb76a203f6<br>
0x00007ffb76a28d88<br>
0x00007ffb76a22df8<br>
0x00007ffb76a2798a<br>
0x00007ffb76940aaf<br>
0x00007ffb76a203f6<br>
0x00007ffb76a28d88<br>
0x00007ffb76a22df8<br>
0x00007ffb76a2798a<br>
0x00007ffb76940aaf<br>
0x00007ffb769403a6<br>
0x00007ffb768a9d3c<br>
0x00007ffb768aaeaa<br>
0x00007ffb768abd23<br>
0x00007ffb768abd96<br>
0x00007ff6a8c314f4<br>
0x00007ffbdedf26bd<br>
0x00007ffbdfdcdfb8<br>
*** END STACK TRACE POINTERS ***</p>
<p>0x00007FFAEC383505      tensorflow::CurrentStackTrace<br>
0x00007FFAEC2FD281      tensorflow::testing::InstallStacktraceHandler<br>
0x00007FFBDD684090      log2f<br>
0x00007FF6A8C327EC      OPENSSL_Applink<br>
0x00007FFBD031E390      _C_specific_handler<br>
0x00007FFBDFE13EAF      _chkstk<br>
0x00007FFBDFD8EAE6      RtlFindCharInUnicodeString<br>
0x00007FFBDFE12E9E      KiUserExceptionDispatcher<br>
0x00007FFB390C66C8      QObject::event<br>
0x00007FFB2B7110EE      QApplicationPrivate::notify_helper<br>
0x00007FFB2B71008F      QApplication::notify<br>
0x00007FFB2BD03E98      (unknown)<br>
0x00007FFB390831B5      QCoreApplication::notifyInternal2<br>
0x00007FFB390859EE      QCoreApplicationPrivate::sendPostedEvents<br>
0x00007FFB389F0B3F      QWindowsGuiEventDispatcher::sendPostedEvents<br>
0x00007FFB391DCF40      QEventDispatcherWin32::processEvents<br>
0x00007FFB389F0B19      QWindowsGuiEventDispatcher::processEvents<br>
0x00007FFB3908888F      QEventLoop::exec<br>
0x00007FFB39080FCD      QCoreApplication::exec<br>
0x00007FFB2BD00E15      (unknown)<br>
0x00007FFB76986D82      PyCFunction_DebugMallocStats<br>
0x00007FFB76A203F6      PyOS_URandomNonblock<br>
0x00007FFB76A28D88      PyEval_GetFuncDesc<br>
0x00007FFB76A25169      PyEval_EvalFrameDefault<br>
0x00007FFB76940695      PyObject_Call<br>
0x00007FFB769409D8      PyFunction_Vectorcall<br>
0x00007FFB76A203F6      PyOS_URandomNonblock<br>
0x00007FFB76A28D88      PyEval_GetFuncDesc<br>
0x00007FFB76A22DF8      PyEval_EvalFrameDefault<br>
0x00007FFB76A2798A      PyEval_EvalCodeWithName<br>
0x00007FFB76A217F4      PyEval_EvalCode<br>
0x00007FFB76A1CA4F      PyAST_Optimize<br>
0x00007FFB76986BF5      PyCFunction_DebugMallocStats<br>
0x00007FFB76A203F6      PyOS_URandomNonblock<br>
0x00007FFB76A28D88      PyEval_GetFuncDesc<br>
0x00007FFB76A22DF8      PyEval_EvalFrameDefault<br>
0x00007FFB76A2798A      PyEval_EvalCodeWithName<br>
0x00007FFB76940AAF      PyFunction_Vectorcall<br>
0x00007FFB76A203F6      PyOS_URandomNonblock<br>
0x00007FFB76A28D88      PyEval_GetFuncDesc<br>
0x00007FFB76A22DF8      PyEval_EvalFrameDefault<br>
0x00007FFB76A2798A      PyEval_EvalCodeWithName<br>
0x00007FFB76940AAF      PyFunction_Vectorcall<br>
0x00007FFB769403A6      PyVectorcall_Call<br>
0x00007FFB768A9D3C      Py_hashtable_copy<br>
0x00007FFB768AAEAA      Py_hashtable_copy<br>
0x00007FFB768ABD23      Py_RunMain<br>
0x00007FFB768ABD96      Py_Main<br>
0x00007FF6A8C314F4      OPENSSL_Applink<br>
0x00007FFBDEDF26BD      BaseThreadInitThunk<br>
0x00007FFBDFDCDFB8      RtlUserThreadStart</p>
<p>In addition, prior to my testing on a shortened video I tried creating a video from a DLC 2.3.0 created .h5 file that showed flawless tracking and it fails in v2.3.1 to output anything.</p>
<p>I very much appreciate all you guys do for us and I’m trying to help fix this but I’m a little disappointed in this release so far.</p> ;;;; <p>Hey <a class="mention" href="/u/meethm">@meethm</a>,<br>
It sounds like what you’re looking for is a composite classifier? <a href="https://qupath.readthedocs.io/en/stable/docs/tutorials/multiplex_analysis.html#combine-the-classifiers" class="inline-onebox">Multiplexed analysis — QuPath 0.4.3 documentation</a></p>
<p>This will do every combination of classifiers in the order you choose. It can become a bit overwhelming. To simplify the interpretation, I recommend not labeling the negatives of each class. When training, mark them as “Ignore*”. That way, there’s simply fewer words to read and you’re less likely to make mistakes.</p> ;;;; <p>Either download the [.json] file from <a href="https://github.com/DeepLabCut/DeepLabCut/tree/main/deeplabcut/modelzoo" rel="noopener nofollow ugc">here (Github)</a> or the attachment to this post and place in the missing directory:</p>
<blockquote>
<p>&lt;C:\your file directory structure&gt; \DEEPLABCUT\lib\site-packages\deeplabcut\modelzoo\</p>
</blockquote>
<p><a class="attachment" href="/uploads/short-url/7Sx5idp0DsxQ63yd9WBEsFEsTxv.json">models.json</a> (108 Bytes)</p> ;;;; <p>That is a closed source device adapter.  Not much I can do for you.  Please ask Cairn for help.  Please also ask them to make the source code open source and contribute to the Micro-Manager source code repository on github.</p> ;;;; <p>Hi <a class="mention" href="/u/ursu">@Ursu</a> . We could help you better if you posted an original image. Is your stack a z stack or is it just a sequence of different images? If you have a z stack, you should use either the 3D Objects Counter or use the 3D Manager of 3D Image Suite. Analyze Particles works more for 2D images.</p> ;;;; <p>I just updated to see if it would correct another error I have and am encountering the same problem. Very frustrating!</p>
<p>The full output is as follows:</p>
<p>(DEEPLABCUT) C:\Users\vamoro1&gt;python -m deeplabcut<br>
Loading DLC 2.3.1…<br>
C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\statsmodels\compat\pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.<br>
from pandas import Int64Index as NumericIndex<br>
Starting GUI…<br>
Traceback (most recent call last):<br>
File “C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\gui\window.py”, line 396, in _open_project<br>
self._update_project_state(<br>
File “C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\gui\window.py”, line 383, in _update_project_state<br>
self.add_tabs()<br>
File “C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\gui\window.py”, line 474, in add_tabs<br>
self.modelzoo = ModelZoo(<br>
File “C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\gui\tabs\modelzoo.py”, line 27, in <strong>init</strong><br>
self._set_page()<br>
File “C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\gui\tabs\modelzoo.py”, line 46, in _set_page<br>
supermodels = parse_available_supermodels()<br>
File “C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\modelzoo\utils.py”, line 10, in parse_available_supermodels<br>
with open(json_path) as file:<br>
FileNotFoundError: [Errno 2] No such file or directory: ‘C:\Users\vamoro1\.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\modelzoo\models.json’</p> ;;;; <p>Thanks Nicolas, using your repo worked!</p> ;;;; <p>So, I’ve been able to get get around the [No optimal solution found] problem and the large gaps in tracking data to be filled by manually setting</p>
<blockquote>
<p>topktoretain: 2</p>
</blockquote>
<p>in inference_cfg.yaml. Which makes me question even more if the values in the GUI map actually to the underlying codebase at runtime.</p>
<p>I still can’t get a video created with the tracklets shown, though.</p>
<p>If I open the .h5 in the tracklet GUI I can see that the network identified both animals flawlessly.</p> ;;;; <p>I think is Cairn_NI</p> ;;;; <p>I see the problems with that script.</p>
<ol>
<li>Will running my phenotype classifier not define cells as “macrophage”, “tumor”, “tcell” and “null”? The specific cell type is selected with:</li>
</ol>
<p><code>selectObjectsByClassification("macrophage")</code></p>
<p>Do I still have to add a line defining each category?</p>
<ol start="2">
<li>
<p>I tried using the above prompt instead to call the cell phenotypes so as not to have the problem you explained above where “cell” is empty on the second loop.</p>
</li>
<li>
<p>runObjectClassifier() will run the classifier on the whole image, not only on the selected cells. So I was hoping to use:</p>
</li>
</ol>
<pre><code class="lang-auto">classifier = loadObjectClassifier("CD68CD163CD11bPDL1PD1")
classifiers.classifyObjects(imageData, cells, false)
</code></pre>
<p>To just apply this classifier on “macrophages”</p>
<p>However I get a “method does not exist error” that I think is linked to</p>
<p><code>classifiers.classifyObjects(imageData, cells, false)</code></p>
<p>This is the whole script I tried:</p>
<pre><code class="lang-auto">runObjectClassifier("tumor_tcell_macro_null")
def imageData = getCurrentImageData()
def cells = getCellObjects()
{
selectObjectsByClassification("macrophage")
classifier = loadObjectClassifier("CD68CD163CD11bPDL1PD1")
classifiers.classifyObjects(imageData, cells, false)
}
fireHierarchyUpdate()
</code></pre> ;;;; <p>You could start by thresholding and making a mask of your lysosomes, do the same with your other channel, then do an AND operation with Process &gt; Image calculator. The resulting image will only contain objects that have both cyan and magenta signal. Then you can use Analyze Particles to count the objects in the resulting image.</p> ;;;; <p>This repository is unfortunately not up to date anymore, please use this remote repository:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/NicoKiaru/ABBA-QuPath-utility-scripts">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/NicoKiaru/ABBA-QuPath-utility-scripts" target="_blank" rel="noopener">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32.png 2x" data-dominant-color="F0EDEA"></div>

<h3><a href="https://github.com/NicoKiaru/ABBA-QuPath-utility-scripts" target="_blank" rel="noopener">GitHub - NicoKiaru/ABBA-QuPath-utility-scripts: A collection of scripts for...</a></h3>

  <p>A collection of scripts for ABBA, Fiji and QuPath. - GitHub - NicoKiaru/ABBA-QuPath-utility-scripts: A collection of scripts for ABBA, Fiji and QuPath.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>There’s a PR opened here: <a href="https://github.com/nickdelgrosso/ABBA-QuPath-utility-scripts/pull/1" class="inline-onebox">Update ABBA scripts by NicoKiaru · Pull Request #1 · nickdelgrosso/ABBA-QuPath-utility-scripts · GitHub</a></p>
<p>I did not test it extensively, so I hope it works, but can’t be sure.</p>
<p>Regarding the error, can you show which extensions are installed in your QuPath ? It looks like you may be missing<a href="https://github.com/BIOP/qupath-extension-warpy"> Warpy</a>, which is a requirement for the ABBA qupath plugin.</p> ;;;; <p>Hi,</p>
<p>I am using a qupath (0.4.3) script (<a href="https://github.com/estellenassar/ABBA-QuPath-utility-scripts/blob/0e8c4a42e99efea0ffe57bee8b610e9ba14226e3/cells_in_regions.groovy" class="inline-onebox" rel="noopener nofollow ugc">ABBA-QuPath-utility-scripts/cells_in_regions.groovy at 0e8c4a42e99efea0ffe57bee8b610e9ba14226e3 · estellenassar/ABBA-QuPath-utility-scripts · GitHub</a>) to extract nuclei in brain regions annotations imported from ABBA and get the following error:</p>
<p>Any help would be greatly appreciated!</p>
<pre><code class="lang-auto">INFO: 1 region detected (processing time: 0.04 seconds)
INFO: Processing complete in 0.13 seconds
INFO: Tasks completed!
WARN: Unable to set parameter detectionImage with value NeuN (Opal 690)
WARN: Unable to set parameter thresholdCompartment with value Cell: DAPI mean
INFO: 97 nuclei detected (processing time: 3.71 seconds)
INFO: 2335 nuclei detected (processing time: 11.89 seconds)
INFO: 1184 nuclei detected (processing time: 12.41 seconds)
INFO: 1286 nuclei detected (processing time: 12.51 seconds)
INFO: 3614 nuclei detected (processing time: 12.61 seconds)
INFO: 4059 nuclei detected (processing time: 12.69 seconds)
INFO: 3480 nuclei detected (processing time: 13.25 seconds)
INFO: 4509 nuclei detected (processing time: 13.28 seconds)
INFO: 4391 nuclei detected (processing time: 13.49 seconds)
INFO: 4273 nuclei detected (processing time: 13.64 seconds)
INFO: 4062 nuclei detected (processing time: 13.82 seconds)
INFO: 1296 nuclei detected (processing time: 14.11 seconds)
INFO: 4327 nuclei detected (processing time: 15.02 seconds)
INFO: 3703 nuclei detected (processing time: 15.09 seconds)
INFO: 1109 nuclei detected (processing time: 15.83 seconds)
INFO: 3928 nuclei detected (processing time: 15.92 seconds)
INFO: 2911 nuclei detected (processing time: 12.53 seconds)
INFO: 1176 nuclei detected (processing time: 16.42 seconds)
INFO: 4059 nuclei detected (processing time: 16.44 seconds)
INFO: 144 nuclei detected (processing time: 4.60 seconds)
INFO: 3209 nuclei detected (processing time: 16.57 seconds)
INFO: 3791 nuclei detected (processing time: 17.46 seconds)
INFO: 1139 nuclei detected (processing time: 18.57 seconds)
INFO: 864 nuclei detected (processing time: 18.63 seconds)
INFO: 1439 nuclei detected (processing time: 19.96 seconds)
INFO: 1718 nuclei detected (processing time: 6.98 seconds)
INFO: 3547 nuclei detected (processing time: 8.42 seconds)
INFO: 3658 nuclei detected (processing time: 8.55 seconds)
INFO: 2154 nuclei detected (processing time: 9.05 seconds)
INFO: 1945 nuclei detected (processing time: 10.13 seconds)
INFO: Processing complete in 24.73 seconds
INFO: Tasks completed!
INFO: Loading 99 Allen Regions for TEL15_1_out_of_3.czi - Scene #01
WARN: Unable to set parameter detection[Channel 4] with value 0.15
WARN: Unable to set parameter detection[Channel 5] with value 0.2
WARN: Unable to set parameter detection[Channel 6] with value -1.0
WARN: Unable to set parameter detection[Channel 7] with value -1.0
INFO: Processing complete in 27.73 seconds
INFO: Tasks completed!
ERROR: com/google/gson/RuntimeTypeAdapterFactory
java.lang.NoClassDefFoundError: com/google/gson/RuntimeTypeAdapterFactory
    at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.getRealTransformAdapter(RealTransformDeSerializer.java:24)
    at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.deserialize(RealTransformDeSerializer.java:49)
    at ch.epfl.biop.qupath.transform.Warpy.getRealTransform(Warpy.java:362)
    at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
    at QuPathScript.run(QuPathScript:66)
    at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)
    at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)
    at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)
    at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)
    at qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
  Caused by com.google.gson.RuntimeTypeAdapterFactory        at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.getRealTransformAdapter(RealTransformDeSerializer.java:24)
        at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.deserialize(RealTransformDeSerializer.java:49)
        at ch.epfl.biop.qupath.transform.Warpy.getRealTransform(Warpy.java:362)
        at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
        at QuPathScript.run(QuPathScript:66)
        at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)
        at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)
        at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)
        at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)
        at qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
</code></pre>
<p>Script:</p>
<pre><code class="lang-auto">import static qupath.lib.gui.scripting.QPEx.* // For intellij editor autocompletion
import static ch.epfl.biop.qupath.atlas.allen.api.AtlasTools.*

import qupath.lib.objects.PathObjects
import qupath.lib.roi.ROIs
import qupath.lib.regions.ImagePlane
import qupath.lib.measurements.MeasurementList
import qupath.lib.objects.PathCellObject

import ch.epfl.biop.qupath.transform.*
import net.imglib2.RealPoint

useSmallArea = false;
clearAllObjects();

// create and select rectangle (code from https://qupath.readthedocs.io/en/stable/docs/scripting/overview.html#creating-rois)

if (useSmallArea) {
    int z = 0
    int t = 0
    def plane = ImagePlane.getPlane(z, t)
    def roi = ROIs.createRectangleROI(7000, 8000, 100, 100, plane)
    def annotation = PathObjects.createAnnotationObject(roi)
    addObject(annotation)
} else {
    runPlugin('qupath.imagej.detect.tissue.SimpleTissueDetection2', '{"threshold": 1,  "requestedPixelSizeMicrons": 20.0,  "minAreaMicrons": 10000.0,  "maxHoleAreaMicrons": 1000000.0,  "darkBackground": true,  "smoothImage": false,  "medianCleanup": false,  "dilateBoundaries": false,  "smoothCoordinates": true,  "excludeOnBoundary": false,  "singleAnnotation": true}');
}

selectAnnotations();

// run Positive Cell Detection
runPlugin('qupath.imagej.detect.cells.PositiveCellDetection', '{"detectionImage": "NeuN (Opal 690)",  "requestedPixelSizeMicrons": 0.5,  "backgroundRadiusMicrons": 35.0,  "medianRadiusMicrons": 1.5,  "sigmaMicrons": 2.0,  "minAreaMicrons": 40.0,  "maxAreaMicrons": 600.0,  "threshold": 0.5,  "watershedPostProcess": true,  "cellExpansionMicrons": 2.0,  "includeNuclei": false,  "smoothBoundaries": true,  "makeMeasurements": true,  "thresholdCompartment": "Cell: DAPI mean", "thresholdPositive1": 0.5,  "thresholdPositive2": 0.4,  "thresholdPositive3": 0.6,  "singleThreshold": true}');

// select and delete rectangle (already selected but just in case)
selectAnnotations();
clearSelectedObjects();

// load warped Allen regions
def imageData = getCurrentImageData();
def splitLeftRight = false; // 3.8.23 changed to false as true gives error
loadWarpedAtlasAnnotations(imageData, splitLeftRight);

// select all cells and insert them into hierarchy
//clearSelectedObjects();
selectCells();

def selectedObjects = getCurrentImageData().getHierarchy().getSelectionModel().getSelectedObjects();
insertObjects(selectedObjects);

// run Subcellular Spot Detection
runPlugin('qupath.imagej.detect.cells.SubcellularDetection', '{"detection[Channel 1]": -1.0,  "detection[Channel 2]": 0.4,  "detection[Channel 3]": 0.3,  "detection[Channel 4]": 0.15,  "detection[Channel 5]": 0.2,  "detection[Channel 6]": -1.0,  "detection[Channel 7]": -1.0,  "doSmoothing": false,  "splitByIntensity": true,  "splitByShape": true,  "spotSizeMicrons": 0.5,  "minSpotSizeMicrons": 0.2,  "maxSpotSizeMicrons": 7.0,  "includeClusters": false}');

// https://github.com/BIOP/qupath-biop-extensions/blob/d6eb0aec6766bd5b1f8aadd5d73a725fe25d77d9/src/test/resources/abba_scripts/importABBAResults.groovy
// Get ABBA transform file located in entry path +
def targetEntry = getProjectEntry()
def targetEntryPath = targetEntry.getEntryPath();

def fTransform = new File (targetEntryPath.toString(),"ABBA-Transform.json")

if (!fTransform.exists()) {
    System.err.println("ABBA transformation file not found for entry "+targetEntry);
    return ;
}

def pixelToCCFTransform = Warpy.getRealTransform(fTransform).inverse(); // Needs the inverse transform

getDetectionObjects().forEach(detection -&gt; {
    RealPoint ccfCoordinates = new RealPoint(3);
    MeasurementList ml = detection.getMeasurementList();
    ccfCoordinates.setPosition([detection.getROI().getCentroidX(),detection.getROI().getCentroidY(),0] as double[]);
    pixelToCCFTransform.apply(ccfCoordinates, ccfCoordinates);
    ml.addMeasurement("Allen CCFv3 X mm", ccfCoordinates.getDoublePosition(0) )
    ml.addMeasurement("Allen CCFv3 Y mm", ccfCoordinates.getDoublePosition(1) )
    ml.addMeasurement("Allen CCFv3 Z mm", ccfCoordinates.getDoublePosition(2) )
})

// change cell name to replace name with unique ID number
counter = 1

selectDetections()
detections = getSelectedObjects()

for (detection in detections) {
    if (detection.class.equals(PathCellObject.class)) {
        detection.setName(counter.toString());
        ++counter
    } else {
        detection.setName('');
    }
}


// save annotations
File directory = new File(buildFilePath(PROJECT_BASE_DIR,'export2'));
directory.mkdirs();
imageName = ServerTools.getDisplayableImageName(imageData.getServer())
saveAnnotationMeasurements(buildFilePath(directory.toString(),imageName+'__annotations.tsv'));
saveDetectionMeasurements(buildFilePath(directory.toString(),imageName+'__detections.tsv'));
</code></pre> ;;;; <p>Hello everyone,</p>
<p>I am working on analysis in QuPath for tumor/immune microenvironment study. I have recently been  phenotyping via scripting of a hierarchy using a simple threshold classifier. For example:</p>
<pre><code class="lang-groovy">import static qupath.lib.gui.scripting.QPEx.*
import qupath.lib.objects.PathObjects
import qupath.lib.objects.classes.PathClassFactory
import qupath.lib.objects.classes.PathClassTools

*// set marker expression location*

*measurement1 = "CD4: Cell: Mean" //CD4*

*// CD4*
*CD4Pos = getPathClass("CD4+")*
*CD4Neg = getPathClass("CD4-")*

*//Layer 2: classify CD4+/- in CD3e+*
*selectObjects { p -&gt; p.getPathClass() == getPathClass("CD3e+")}*
*for (detection in getSelectedObjects()) {*
*    m1 = measurement(detection, measurement1) *
*    if ( m1 &gt;  meas01)*
*        detection.setPathClass(CD4Pos)   *
*  else *
*  detection.setPathClass(CD4Neg)             *
*}*
*fireHierarchyUpdate()*
</code></pre>
<p>That’s just a small portion of the total hierarchy, but hopefully you get the idea. Is it possible to put together a phenotyping hierarchy like this using a trained classifier (e.g. artificial neural network trained) instead of the simple threshold/binary method? I’ve searched the forums but couldn’t find anything about this specifically. Maybe I missed it!</p>
<p>Thank you so much,</p>
<p>Molly</p> ;;;; <p><a class="mention" href="/u/ilya_belevich">@Ilya_Belevich</a> could you share the email address associated with your MathWorks account that you are trying to login with?</p> ;;;; <p>Hi, I have 2 RTX 4090 setted up, v531 driver, 11.8 Cuda, and installed DLC using the conda yaml file. No<br>
error showing when training, however GPU was not used during training.</p>
<p>code after training started:<br>
Starting training…<br>
2023-03-09 02:00:38.669048: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100<br>
2023-03-09 02:00:40.508583: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</p>
<p>Any help will be greatly appreciated!!!</p> ;;;; <p>What is the name of the device adapter you use to control the Optoscan?  Looking at the source code of that device adapter would allow me to give you some more solid advice.</p> ;;;; <p>That file should be in the MGX CNN addon package. When you type locate libc10.so what do you get? It should get installed to /usr/lib/MorphoGraphX.</p>
<p>How are you starting MorphoGraphX? If starting from the command line you need to use “mgx” not “MorphoGraphX” as the startup script sets the library paths.</p> ;;;; <p>Since the only equipment in your config file is the PVCAM camera, both statements seem correct;).  You can try installing various versions of PVCAM, but your best bet is to ask Photometrics support.</p> ;;;; <p>How to quantify collagen staining/ trichome staining in Qupath?<br>
How to set the threshold?</p> ;;;; <p>Hi,</p>
<p>I’ve been trying (and failing) to do this same task in ImageJ. I found the example beanshell script and tried running that, but it popped up with “Error while adjusting data!” and then everything went downhill. The console popped up with a lot of java errors and then another window popped up saying “Classifier could not be applied”. Am I missing a step? Or what have I done wrong?</p>
<p>Thanks so much!<br>
~Katie</p> ;;;; <p>You don’t directly measure stiffness (or extensibility), but rather you would fit those parameters to your growth data. In that model, growth depends on stretch x extensibility, so either might change. One could fit the entire structure, or information by layers if you have it. IN the model it is possible to specify parameter for the different layers, or even different wall layers, for example epidermis-cortex.</p> ;;;; <p>I’m confused, because you mentioned in the <a href="https://forum.image.sc/t/issue-with-loading-system-configuration/58336/6">other thread that the issue is with the config file</a>, but here you’re saying it’s the PVCAM camera. But in any case, attached is the config file that was, again, working not only initially, but for even the old Windows 7 machine we used before the PC broke down.<br>
<a class="attachment" href="/uploads/short-url/ppBjsCyd1Jp4PbX2J9oFQq4C6E5.txt">2023_03_08.cfg.txt</a> (493 Bytes)</p>
<p>Any suggestions besides contacting Photometrics support? Seeking all troubleshooting options.</p> ;;;; <p>I don’t believe there is a release dat as of yet but it will be included in the next release of each component (likely to be OMERO 5.6.7 and Insight 5.7.3).</p> ;;;; <p>Salut <a class="mention" href="/u/mathew">@Mathew</a></p>
<p>Merci pour ton aide, je vais essayer avec ton code.</p> ;;;; <p>Hello,</p>
<p>I have a network that was training on over 2,000 images of 3 individuals. Unfortunately, three of the many videos were taken of just 2 individuals and when I try to analyze them, selecting [2] as the number of animals in the video, I get the following:</p>
<blockquote>
<p>C:\Users\vamoro1.conda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\refine_training_dataset\stitch.py:687: UserWarning: No optimal solution found. Employing black magic…<br>
warnings.warn(“No optimal solution found. Employing black magic…”)</p>
</blockquote>
<p>The .h5 file is sparsely populated and the resulting video has no tracklets identified.</p>
<p>My understanding from the paper and <a href="https://deeplabcut.github.io/DeepLabCut/docs/maDLC_UserGuide.html" rel="noopener nofollow ugc">Userguide</a> is this should be fine as it states:</p>
<blockquote>
<p>“Note, once trained if you have a video with more or less animals, that is fine - you can have more or less animals during video analysis!”</p>
</blockquote>
<p>Any help would be greatly appreciated as I’d hate to train a new network for just a handful of videos.</p>
<p>Thank you!</p> ;;;; <p>Hi Giulia,</p>
<p>The issue is that you are selecting a ROI without first selecting the window, so Fiji selects the window and only sees a ROI that does not change.</p>
<p>Do this:</p>
<pre><code class="lang-auto">t=getTitle;
numROIs = roiManager("count");

for(i=0; i&lt;numROIs;i++) // loop through ROIs
{
	selectWindow(t);
	roiManager("Select", i);
	run("Plot Profile");
}
</code></pre>
<p>Where did you find “Find Peaks”?  I don’t have it on my Fiji?</p>
<p>Sincerely,</p>
<p>Matthieu</p> ;;;; <p>Don’t be sorry, you helped me a lot!</p>
<p>Thank you!</p> ;;;; <p>Hi,</p>
<p>Resizing the pixel size is unwise.  The issue is upstream when the pictures were taken.  Difference is a factor of 4 -either zoom during acquisition or lens…</p>
<p>Best thing I’d do is to retake the pictures with large pixels to match those with smaller pixels.  You could fudge it by rescaling the images with smaller pixels (Image&gt; Scale…) by a factor of 0.25, but you will then lose all the details in these images.</p>
<p>Sorry.</p> ;;;; <p>Awesome <a class="mention" href="/u/wayne">@Wayne</a>! Thanks a lot!<br>
Just to be sure, does that also work if the <code>ImagePlus</code> is a <code>CompositeImage</code>?</p> ;;;; <p>Hi <a class="mention" href="/u/christian_tischer">@Christian_Tischer</a>, <a class="mention" href="/u/biovoxxel">@biovoxxel</a> ,<a class="mention" href="/u/nicokiaru">@NicoKiaru</a>,</p>
<p>Here is a version of the script that gets the display ranges of all the channels:</p>
<pre><code class="lang-auto">imp = IJ.openImage(IJ.getDir("downloads")+"2023_01_18--Extract_8hr_1-1.tif");
for (c=1; c&lt;=imp.getNChannels(); c++) {
     imp.setC(c);
     ip = imp.getProcessor();
     min = ip.getMin();
     max = ip.getMax();
     IJ.log(c+": "+min+"-"+max);
}
</code></pre>
<p>Update to the ImageJ 1.54d2 daily build and you can use this slightly simpler version:</p>
<pre><code class="lang-auto">imp = IJ.openImage(IJ.getDir("downloads")+"2023_01_18--Extract_8hr_1-1.tif");
for (c=1; c&lt;=imp.getNChannels(); c++) {
     imp.setC(c);
     min = imp.getDisplayRangeMin();
     max = imp.getDisplayRangeMax();
     IJ.log(c+": "+min+"-"+max);
}
</code></pre> ;;;; <p>Maybe if you check the tools-options “ask for config at startup” you can avoid the first one, and if the last config loaded was empty than it will default to doing nothing. Not sure if it will then remember the config loaded through PM later and default to that, but worth a try</p> ;;;; <p>Hi everyone,</p>
<p>I would like to quantify how many lysosomes, within my total population of them, have or not have my protein of interest in their lumen. I think I should create a mask of the lysosomes (here in cyan) and then detect if there is signal (in the image in magenta).<br>
I appreciate your help! Thank you</p>
<p>Francesca</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/141e48611c04ba77e0fd75c2bc4bbc1428cf832e.png" alt="Screen Shot 2023-03-08 at 10.51.37 AM" data-base62-sha1="2RYqEohTgTz98IsjruYHUIABDae" width="456" height="274"></p> ;;;; <p>Right, I see.</p>
<p>Thanks for pointing that out! That makes perfect sense now.</p>
<p>And yes, <a class="mention" href="/u/research_associate">@Research_Associate</a>, I do take your point that additional measurements may be necessary.</p>
<p>Thanks again both for your prompt responses!</p> ;;;; <p>Basically just the title! I am classifying objects using CPA Classifer, and for this manual training step it would be helpful to be able to adjust the colour balance of loaded object tiles such that each of the 4 channels being displayed here are nice and balanced, and background can be removed for viewing purposes. The view options only appear to let you adjust overall brightness of all channels together</p>
<p>Using the log contrast stretch option helps to balance channels to some extent (in my case), but background in the images still makes manual viewing and classification fairly difficult. Previously I have run pipelines producing background-subtracted and/or intensity-normalised images which came up quite nicely through CPA, but the images here are just the raw images.</p>
<p>Or would the only way be to have saved adjusted images/thumbnails separately and direct CPA to look at those? Thanks very much!</p> ;;;; <p>Just remembering this, and not particularly relevant, but… cheesoSPIM <a href="https://github.com/PRNicovich/cheesoSPIM" class="inline-onebox">GitHub - PRNicovich/cheesoSPIM: A cheap mesoscopic light sheet and optical projection tomography microscope</a></p> ;;;; <p>I assume you mean that the Haematoxylin mean for the cell you have highlighted is 0.0987 which is less than the threshold of 0.2 you have set.</p>
<p>If you look on your cell detection parameters you are using the OD sum image, not the Haematoxylin OD image. The OD sum is the total of haematoxylin OD plus DAB OD plus residual OD so will probably be above your detection threshold.</p> ;;;; <p>It looks like you are looking at the HTX OD sum on the left, and using the full Optical Density Sum, not HTX, on the right.</p>
<p>To answer the bigger question though, at some point you have to accept that there is a middle ground between false negatives and false positives, and figure out some other method like classification or additional measurements to deal with errors.</p> ;;;; <p>this makes a lot of sense! So, to explain, I have a pool of samples belonging to different categories (A, B, C, D, E…) in condition 1 and condition 2 (so the entire pool is 2x categories) and regardless of the time when they were collected, they tend to have the distance (in micron) = 0.104 circa.<br>
In the same category and treatment (i.e. A, condition 1) I have - let’s say - 16 samples, which are generally consistent [distance (in micron) = 0.104 circa].<br>
In another case, instead, samples from a specific category, regardless the conditions, have distance (in micron) = 0.415 circa.<br>
So when it comes to comparing them, we have a problem and the possible ‘normalizations’ we could do downstream the data acquisition could end up in fabricated and imprecise data.</p>
<p>DO you think that re-setting the pixel size before analyzing the images can help?</p>
<p>Thank you very much</p> ;;;; <p>It’s just that there are already ways of doing that. I’m not sure how easy it is to pick out values in RGB. Although I think there was something a while back about storing millions of labels as an RGB image due to the bit depth.</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906.jpeg" data-download-href="/uploads/short-url/iHsjTdGIhHFDXFXeazSugLMs9zE.jpeg?dl=1" title="Screenshot 2023-03-08 at 14.45.50" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_690x448.jpeg" alt="Screenshot 2023-03-08 at 14.45.50" data-base62-sha1="iHsjTdGIhHFDXFXeazSugLMs9zE" width="690" height="448" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_690x448.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_1035x672.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_1380x896.jpeg 2x" data-dominant-color="E3E1E0"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-08 at 14.45.50</span><span class="informations">1920×1247 218 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a.jpeg" data-download-href="/uploads/short-url/aem0YdPwHy8V4ngsTzJll7nR47M.jpeg?dl=1" title="Screenshot 2023-03-08 at 14.57.07" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_690x448.jpeg" alt="Screenshot 2023-03-08 at 14.57.07" data-base62-sha1="aem0YdPwHy8V4ngsTzJll7nR47M" width="690" height="448" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_690x448.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_1035x672.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_1380x896.jpeg 2x" data-dominant-color="DFD5D4"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-08 at 14.57.07</span><span class="informations">1920×1247 305 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi,</p>
<p>I’m a beginner to image analysis, and have been working through Pete Bankhead’s videos as well as parsing through various topics here in the community which have been really helpful . However, I’ve now run into a problem where I’m having difficulty trying to figure out how qupath is detecting cells. It seems that it is ‘counting’ cells below the intensity threshold set, and I’m a bit at a loss as to how to get around it, as increasing the intensity threshold causes false negatives.</p>
<p>I’m using a Mac, running Qupath Version: 0.4.3</p>
<p>I attach in my next post a screen shot from my visual stain editor, which I run before I run cell detection, as well as the cell detection screenshot and the false positive detection highlighted. Any helpful suggestions would be much appreciated!</p> ;;;; <p>Hello I have a simple problem that is blocking me. I am a real beginner and looking for help.</p>
<p>My simple code was supposed to plot profile for each ROI, however it repeat the plot profile of the first ROI for n times.</p>
<p>I hope that an expert person could easily spot the mistake.</p>
<p>t=getTitle;<br>
numROIs = roiManager(“count”);</p>
<p>for(i=0; i&lt;numROIs;i++) // loop through ROIs<br>
{<br>
roiManager(“Select”, i);<br>
selectWindow(t);<br>
run(“Plot Profile”);<br>
run(“Find Peaks”, “min._peak_amplitude=20 min._peak_distance=10 min._value=<span class="chcklst-box fa fa-square-o fa-fw"></span> max._value=<span class="chcklst-box fa fa-square-o fa-fw"></span>”);<br>
}</p> ;;;; <p>Hello,<br>
I am trying to calculate particle size and their number in a stack of images. I am a total beginner and used macro to create a mini code. I would like to have the results of all images in one csv with unique labels. But currently I am facing the problem, that ImageJ adds the outlines of the old images to the new one (s. attached image, yellow outlines and labels), so that images become more and more cluttered. Has someone an idea what to do?</p>
<p>run(“8-bit”);<br>
setAutoThreshold(“Default dark no-reset”);<br>
//run(“Threshold…”);<br>
setThreshold(114, 128, “raw”);<br>
run(“Analyze Particles…”, “size=200-Infinity display exclude include overlay add composite”);<br>
run(“Flatten”);</p>
<p><a class="attachment" href="/uploads/short-url/hK2f6qOjBG0D1zhK1AfwRrbQuIF.tiff">GOPR0876_19_17_OD-1.tiff</a> (751.6 KB)</p> ;;;; <p>Thanks you! This is exactly what I was after. I suspected that the function wrapped some native ImageJ functions but couldn’t figure out which one. Your second suggestion is perfectly fine.</p> ;;;; <p>Is that for all the images?<br>
Plot profile will plot grey values in Y pixel by pixel, if pixel size changes between images, values in X will reflect this change.</p>
<p>Sincerely,</p>
<p>Matthieu</p> ;;;; <p>thank you! that is what i thought was happening. what version of Omero and Omero.insight will have Bio-Formats 6.12.0?</p> ;;;; <p>Batch mode will suppress the display of any images (including IJ plots), so it won’t work unless you keep including <code>setBatchMode('show')</code> calls <em>before</em> displaying plots. I am not sure that will make the macro run any faster.<br>
I’m not sure what exactly you are trying to do, but once the ‘Find Peaks’ plot is displayed it becomes the frontmost image, and thus the plot profile will no longer be obtained from the initial image parsed by the macro.</p>
<p>I would expect something like this instead:</p>
<pre><code class="lang-javascript">
id = getImageID(); // id of active image
maximaXcoordHolder = newArray() // storing array
maximaYcoordHolder = newArray() // storing array

n = roiManager("count");
for (i = 0; i &lt; n; i++) {
	// we must activate initial image to ensure plot profile runs on it
	selectImage(id);
	// obtain profile
	roiManager("select", i);
	Roi.getBounds(x, y, width, height);
	makeLine(x, y, width, height);
	run("Plot Profile");
	// Plot profile is now frontmost window. Store its title
	profileTitle = getTitle();
	run("Find Peaks", "min._peak_amplitude=1 min._peak_distance=0 min._value=[] max._value=[] exclude list");
	// Find maxima profile is now frontmost window. Store its title
	peaksTitle = getTitle();
	// extract maxima coordinates from the "Plot Values" Table also open
	maximaXcoord = Table.getColumn("X1");
	maximaYcoord = Table.getColumn("Y1");
	// close windows no longer in use
	selectWindow(profileTitle);
	run("Close");
	selectWindow(peaksTitle);
	run("Close");
	// store coordinates in a common array
	maximaXcoordHolder = Array.concat(maximaXcoord, maximaXcoordHolder);
	maximaYcoordHolder = Array.concat(maximaYcoord, maximaYcoordHolder);
}

// close residual windows and show result
selectWindow("Plot Values");
run("Close")
Table.showArrays("Maxima Coords", maximaXcoordHolder, maximaYcoordHolder); 
</code></pre>
<p>That being said, you probably don’t need ‘Find Peaks’ at all for this!? ‘Find Peaks’ was created to extract maxima and minima from plotted data. If you are parsing images, you could use built-in macro functions that will work in batch mode out of the box:</p>
<pre><code class="lang-javascript">(...)
makeLine(x, y, width, height);
profile = getProfile();
maxIndices = Array.findMaxima(profile, 10);
print("Maxima (descending strength):");
for (i= 0; i &lt; maxIndices.length; i++) {
	x = maxIndices[i];
	y = profile[x];
	print("x= ", x, " y= ", y);
}
</code></pre> ;;;; <p>When I open the pic, on the border of the image I have 212.55 x 212.55 microns (512x512) and the pixel is 0.4151329.</p>
<p>Thanks for helping</p> ;;;; <p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/739ad55b634da7a6da8a91c0aa36df4b518467e0.jpeg" alt="sytox_ImageJ" data-base62-sha1="guGDzbFzBvKcGo0A1bgA6SWpoc0" width="278" height="254"></p>
<p>I have stained my tissue with the nuclei dye Sytox green to image cell nuclei (I have also tried DAPI with the same problems as Sytox green). I am trying to measure cell shape (especially interested in circularity and roundness), and because I need it to be fairly exact, drawing the outline around each cell is not possible. I have tried using the measurements plugin in ImageJ, but because the cells are so close together it does not recognize them as different nuclei. I have also tried CellProfiler, but just the example pipes from their website, which has the same problem as ImageJ. I don’t need to be able to recognize every single cell in the image.</p>
<p>I have tried to find posts on here already that answer this question, but because the cells are so close together, none of them seem exactly applicable. I would really appreciate suggestions of any ways that I might be able to get it to work.</p>
<p>I have included an image of what I am trying to analyze. Thank you!</p> ;;;; <p>Thank you, Peter! That was the cause.<br>
I must have caused this situation when I was playing around cellpose and GPU.</p> ;;;; <p>I’m crash-coursing some PyTorch to better understand the issue (and the issue with training). Stay tuned…</p> ;;;; <p>The issue is you are using PySide6 QT backend.<br>
napari does not yet officially support this, so I’m not sure how you ended up in this situation.<br>
Anyhow, the error you have here is in vispy and has actually been fixed. Likewise a number of Qt6 issues in napari have also recently been fixed, so support will be improved in the next release.</p>
<p>As you are on x86 macOS, you can try to uninstall PySide6 and install instead pyside2 or pyqt5.</p> ;;;; <p>I’m using MacBook Pro, running 3.10.9 Python version, x86_64 architecture,  0.4.17 napari version.<br>
When I call <code>napari.Viewer()</code>, napari icon appears on the docker but no window is opened. It also raises this error:</p>
<pre><code class="lang-auto">---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[9], line 1
----&gt; 1 asdf = napari.Viewer()

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/viewer.py:67, in Viewer.__init__(self, title, ndisplay, order, axis_labels, show)
     63 from .window import Window
     65 _initialize_plugins()
---&gt; 67 self._window = Window(self, show=show)
     68 self._instances.add(self)

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_main_window.py:463, in Window.__init__(self, viewer, show)
    460 self._unnamed_dockwidget_count = 1
    462 # Connect the Viewer and create the Main Window
--&gt; 463 self._qt_window = _QtMainWindow(viewer)
    465 # connect theme events before collecting plugin-provided themes
    466 # to ensure icons from the plugins are generated correctly.
    467 _themes.events.added.connect(self._add_theme)

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_main_window.py:88, in _QtMainWindow.__init__(self, viewer, parent)
     86 super().__init__(parent)
     87 self._ev = None
---&gt; 88 self._qt_viewer = QtViewer(viewer, show_welcome_screen=True)
     89 self._quit_app = False
     91 self.setWindowIcon(QIcon(self._window_icon))

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_viewer.py:217, in QtViewer.__init__(self, viewer, show_welcome_screen)
    214 # This dictionary holds the corresponding vispy visual for each layer
    215 self.layer_to_visual = {}
--&gt; 217 self._create_canvas()
    219 # Stacked widget to provide a welcome page
    220 self._canvas_overlay = QtWidgetOverlay(self, self.canvas.native)

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_viewer.py:401, in QtViewer._create_canvas(self)
    399 def _create_canvas(self) -&gt; None:
    400     """Create the canvas and hook up events."""
--&gt; 401     self.canvas = VispyCanvas(
    402         keys=None,
    403         vsync=True,
    404         parent=self,
    405         size=self.viewer._canvas_size[::-1],
    406     )
    407     self.canvas.events.draw.connect(self.dims.enable_play)
    409     self.canvas.events.mouse_double_click.connect(
    410         self.on_mouse_double_click
    411     )

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_vispy/canvas.py:33, in VispyCanvas.__init__(self, *args, **kwargs)
     31 self._last_theme_color = None
     32 self._background_color_override = None
---&gt; 33 super().__init__(*args, **kwargs)
     34 self._instances.add(self)
     36 # Call get_max_texture_sizes() here so that we query OpenGL right
     37 # now while we know a Canvas exists. Later calls to
     38 # get_max_texture_sizes() will return the same results because it's
     39 # using an lru_cache.

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/scene/canvas.py:135, in SceneCanvas.__init__(self, title, size, position, show, autoswap, app, create_native, vsync, resizable, decorate, fullscreen, config, shared, keys, parent, dpi, always_on_top, px_scale, bgcolor)
    130 # Set to True to enable sending mouse events even when no button is
    131 # pressed. Disabled by default because it is very expensive. Also
    132 # private for now because this behavior / API needs more thought.
    133 self._send_hover_events = False
--&gt; 135 super(SceneCanvas, self).__init__(
    136     title, size, position, show, autoswap, app, create_native, vsync,
    137     resizable, decorate, fullscreen, config, shared, keys, parent, dpi,
    138     always_on_top, px_scale)
    139 self.events.mouse_press.connect(self._process_mouse_event)
    140 self.events.mouse_move.connect(self._process_mouse_event)

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/canvas.py:211, in Canvas.__init__(self, title, size, position, show, autoswap, app, create_native, vsync, resizable, decorate, fullscreen, config, shared, keys, parent, dpi, always_on_top, px_scale, backend_kwargs)
    209 # Create widget now (always do this *last*, after all err checks)
    210 if create_native:
--&gt; 211     self.create_native()
    213     # Now we're ready to become current
    214     self.set_current()

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/canvas.py:228, in Canvas.create_native(self)
    226 assert self._app.native
    227 # Instantiate the backend with the right class
--&gt; 228 self._app.backend_module.CanvasBackend(self, **self._backend_kwargs)
    229 # self._backend = set by BaseCanvasBackend
    230 self._backend_kwargs = None  # Clean up

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/backends/_qt.py:372, in QtBaseCanvasBackend.__init__(self, vispy_canvas, **kwargs)
    369 self._initialized = False
    371 # Init in desktop GL or EGL way
--&gt; 372 self._init_specific(p, kwargs)
    373 assert self._initialized
    375 self.setMouseTracking(True)

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/backends/_qt.py:791, in CanvasBackendDesktop._init_specific(self, p, kwargs)
    788 def _init_specific(self, p, kwargs):
    789 
    790     # Deal with config
--&gt; 791     glformat = _set_config(p.context.config)
    792     glformat.setSwapInterval(1 if p.vsync else 0)
    793     # Deal with context

File ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/backends/_qt.py:283, in _set_config(c)
    280 glformat.setAlphaBufferSize(c['alpha_size'])
    281 if QT5_NEW_API or PYSIDE6_API:
    282     # Qt5 &gt;= 5.4.0 - below options automatically enabled if nonzero.
--&gt; 283     glformat.setSwapBehavior(glformat.DoubleBuffer if c['double_buffer']
    284                              else glformat.SingleBuffer)
    285 elif PYQT6_API:
    286     glformat.setSwapBehavior(glformat.SwapBehavior.DoubleBuffer if c['double_buffer']
    287                              else glformat.SwapBehavior.SingleBuffer)

AttributeError: 'PySide6.QtGui.QSurfaceFormat' object has no attribute 'DoubleBuffer'
</code></pre>
<p>Any ideas what could be wrong? Missing a package? I’ve been running napari from this environment before. Thanks!</p> ;;;; <p>If anyone would find it useful, I’ve managed to get it working using some scripts from <a class="mention" href="/u/petebankhead">@petebankhead</a> and <a class="mention" href="/u/romainguiet">@romainGuiet</a>. Also thanks to <a class="mention" href="/u/research_associate">@Research_Associate</a> and <a class="mention" href="/u/ym.lim">@ym.lim</a> !</p>
<p>Here it is:</p>
<pre><code class="lang-auto">import qupath.ext.stardist.StarDist2D
import qupath.ext.biop.cellpose.Cellpose2D
import qupath.lib.objects.PathObjects
import qupath.lib.analysis.features.ObjectMeasurements

clearDetections()

// Set some variables
var imageData = getCurrentImageData()
var server = getCurrentServer()
var pathObjects = getSelectedObjects()
if (pathObjects.isEmpty()) {
    createSelectAllObject(true)
    }
var cal = server.getPixelCalibration()
var downsample = 1.0

// Run cellpose
pathModel = 'cyto'
def cellpose = Cellpose2D.builder(pathModel)
        .channels('Channel 2')
        .normalizePercentiles(1,99)
        .pixelSize( 0.25 )
        .diameter(0.0)
        .build()
cellpose.detectObjects(imageData, pathObjects)

// Save cytoplasm
def cytoplasms = getDetectionObjects()
selectDetections()

// Run stardist
pathModel = 'C:/Program Files/QuPath/dsb2018_heavy_augment.pb'
var stardist = StarDist2D.builder(pathModel)
        .threshold(0.5)             
        .channels('Channel 1')
        .normalizePercentiles(1, 99) 
        .pixelSize(0.5) 
        .cellExpansion(0)
        .measureShape()
        .measureIntensity() 
        .includeProbability(true)
        .build()
stardist.detectObjects(imageData, pathObjects)

clearSelectedObjects()

// Save nuclei
def nuclei = getDetectionObjects()

// Start with a clean slate
clearDetections()

// Create cells
cells = []
cytoplasms.each{ cytoplasm -&gt;
    nuclei.each{ nucleus -&gt;      
        if ( cytoplasm.getROI().contains( nucleus.getROI().getCentroidX() , nucleus.getROI().getCentroidY())){
            cells.add(PathObjects.createCellObject(cytoplasm.getROI(), nucleus.getROI(), getPathClass("Tissue"), null ));
            }
        }
    }
addObjects(cells)

// Add measurements
def measurements = ObjectMeasurements.Measurements.values() as List
def compartments = ObjectMeasurements.Compartments.values() as List
def shape = ObjectMeasurements.ShapeFeatures.values() as List
def cells = getCellObjects()
for ( cell in cells ) {
    ObjectMeasurements.addIntensityMeasurements( server, cell, downsample, measurements, compartments )
    ObjectMeasurements.addCellShapeMeasurements( cell, cal,  shape )
    }

// Finished!
fireHierarchyUpdate()
println("Done!")
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2.jpeg" data-download-href="/uploads/short-url/rgDehKIiTJ5r5nBXdqBxCglM7Au.jpeg?dl=1" title="Capture2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_345x242.jpeg" alt="Capture2" data-base62-sha1="rgDehKIiTJ5r5nBXdqBxCglM7Au" width="345" height="242" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_345x242.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_517x363.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_690x484.jpeg 2x" data-dominant-color="141B19"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Capture2</span><span class="informations">881×618 44.8 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi everyone,</p>
<p>Since I now have a new graphic card (RTX3080), I removed the deeplabcut environment and installed it again. It also automatically updated Deeplabcut to version 2.3.1. When I try to load my existed project I am having an error and the project wont be loaded. When I try to create a new project, I get the same error message again. I tried to re-install Deeplabcut and conda environment several times but did not work. Here is the error message I got when I load the project:</p>
<p>FileNotFoundError: [Errno 2] No such file or directory: ‘C:\Users\Zoology_office_05\Anaconda\envs\DEEPLABCUT\lib\site-packages\deeplabcut\modelzoo\models.json’</p>
<p>Any contribution would be appreciated. Thank you very much in advance</p> ;;;; <p>Thanks Erik, thats great that you got it sorted!</p> ;;;; <p>Thank you for providing the sample files. I was able to reproduce the issue with Bio-Formats 6.11.1 and I can confirm that a fix was put in place for Bio-Formats 6.12.0. With the most recent Bio-Formats the files can be opened and displayed as expected. The latest OMERO release is still using Bio-Formats 6.11.1 but this fix should be included in the next upcoming OMERO release.</p> ;;;; <p>Hi!</p>
<p>I’m having problem with “refine tracklets”…</p>
<p>This is my step.<br>
Max gap of missing data to fill = 0 (this video has 6300 frames)<br>
1.run stitching<br>
2.Launch track refnement GUI and fixed all frames.<br>
3.Filter tracks(+.csv)<br>
4. Merge dataset. Then met error.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a.png" data-download-href="/uploads/short-url/1Fb3z6RQNGbFav2gZ5HqtZI5pqO.png?dl=1" title="Screenshot from 2023-03-08 19-56-26" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a_2_690x112.png" alt="Screenshot from 2023-03-08 19-56-26" data-base62-sha1="1Fb3z6RQNGbFav2gZ5HqtZI5pqO" width="690" height="112" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a_2_690x112.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a.png 2x" data-dominant-color="3F1C34"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-08 19-56-26</span><span class="informations">991×161 29.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Any help is appreciated!!</p>
<p>My PC:<br>
ubuntu 22.04, cuda11.7, driver 525.85.12<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5.png" data-download-href="/uploads/short-url/a8E0hEQbjWdci868k8Orabtcb6l.png?dl=1" title="Screenshot from 2023-03-09 13-08-02" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5_2_547x500.png" alt="Screenshot from 2023-03-09 13-08-02" data-base62-sha1="a8E0hEQbjWdci868k8Orabtcb6l" width="547" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5_2_547x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5.png 2x" data-dominant-color="3B1830"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-09 13-08-02</span><span class="informations">721×658 81.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
conda environment<br>
matplotlib=3.5.1<br>
python=3.8</p>
<p>deeplabcut=2.3.0</p> ;;;; <p>Thanks, I didn’t realise that stardist accepted detections as inputs. After completing cellpose I just <code>selectDetections()</code> and then run stardist. This also means the nuclei belong to the cellpose parent.</p>
<p>I am still struggling to code the <code>createCellObject()</code> to combine the two…</p> ;;;; <p>sure,thanks a lot for your help. I will email my data to you.</p> ;;;; <p><a class="mention" href="/u/steinr">@steinr</a> When I try to run this code on a smaller image, compensating the values. I get a final temperature image with full black and full white values. what could be the issue here?</p>
<p>Original image<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/4791aa1b6ce4f106f631b3aba235761afcd43c4c.jpeg" alt="image" data-base62-sha1="ad7Z4M1yidSPLbYUbuegTsJyl40" width="412" height="364"></p>
<p>B/W convert<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/b/9b5a611a7743f74b6fab55e6caee8bed2d441024.png" alt="image" data-base62-sha1="majDYdGv70rBQeqd4rkvNNQju5K" width="412" height="389"></p>
<p>Temperature image<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/89d23199b792c124af1092997989383f121e0608.png" data-download-href="/uploads/short-url/jFdKAIA1LBI85RHsxWgGq5J6SE0.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/89d23199b792c124af1092997989383f121e0608_2_598x500.png" alt="image" data-base62-sha1="jFdKAIA1LBI85RHsxWgGq5J6SE0" width="598" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/89d23199b792c124af1092997989383f121e0608_2_598x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/89d23199b792c124af1092997989383f121e0608.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/89d23199b792c124af1092997989383f121e0608.png 2x" data-dominant-color="D3D3D3"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">610×510 33.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi ValeEle,</p>
<p>When opening the images, could you look at the image and pixel size?</p>
<p>For image size, look at the top of the image.  For Pixel size, go to Image&gt;Properties.  Are they the same?</p>
<p>Sincerely,</p>
<p>Matthieu</p> ;;;; <p>Dear all,</p>
<p>maybe this topic already popped up somewhere, so I apologize for me being naïve.</p>
<p>I am analyzing a series of images in *.czi format (from Carl Zeiss Confocal microscope, acquired with ZEN software (2012)) and I am interested in collecting the X and Y data of the grayscale intensity, therefore I upload the picture, I draw a line along the region of interest (for the ones who understand, it is the coil series of the protoxylem stained with PI dye) which is a “spiral-like” structure, appearing white on a black background, and I “Plot Profile”. This latter action gives me X= distance in micron and Y= grayscale.</p>
<p>Now, generally, the value of the distance (micron) between 0 and the first point is 0.104, but when I use a specific series of images, though that they are taken in the same manner, in the same conditions, with the same characteristics, the distance becomes 0.415 automatically.</p>
<p>My collaborator and I were wondering if there is any way to adjust this sampling by hand, so that also this latter series of samples have the same distance of the others (all 0.104).</p>
<p>Any comment or tip will be highly appreciated!</p> ;;;; <p>Could you share the data so I can take a look myself?</p> ;;;; <p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c.png" data-download-href="/uploads/short-url/eNOcuzIEaTd7st8ciaA406XY74E.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_690x343.png" alt="image" data-base62-sha1="eNOcuzIEaTd7st8ciaA406XY74E" width="690" height="343" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_690x343.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_1035x514.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_1380x686.png 2x" data-dominant-color="EADFD2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3671×1829 295 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2.jpeg" data-download-href="/uploads/short-url/jhJPxLoqZ3TWyxhQ6wX3OYugQ8i.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_690x364.jpeg" alt="image" data-base62-sha1="jhJPxLoqZ3TWyxhQ6wX3OYugQ8i" width="690" height="364" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_690x364.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_1035x546.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_1380x728.jpeg 2x" data-dominant-color="1F2227"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1013 69.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>yean thanks,</p>
<p>my data are 2 dirs with 1233 .tiff(12.1 GB in total) respectively.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256.jpeg" data-download-href="/uploads/short-url/oT0ELG3gAdaorVSMKLtW2ALnnfw.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_690x363.jpeg" alt="image" data-base62-sha1="oT0ELG3gAdaorVSMKLtW2ALnnfw" width="690" height="363" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_690x363.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_1035x544.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_1380x726.jpeg 2x" data-dominant-color="1F2227"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×1011 71.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello.</p>
<p>No actually. To go beyond the features you mention you need to use Jython to script TrackMate.</p> ;;;; <p>Hi <a class="mention" href="/u/ilya_belevich">@Ilya_Belevich</a></p>
<p>You shouldnt have to activate. Let me check this out and I will get back to you. I might contact you via your email to have you test it again.</p>
<p>Best,<br>
Shubo</p> ;;;; <p>Hello,<br>
I would like to know whether there are more detailed subsets of TrackMate functionalities with macros ?<br>
For example:</p>
<ol>
<li>Setting “Linking max distance” and “Gap-closing max distance” (figure 1)</li>
<li>Export “All spots table” (figure 2)</li>
</ol>
<p>Thank you so much !<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5.jpeg" data-download-href="/uploads/short-url/xRqHgGg2FFnufSb116NEUdA77Xn.jpeg?dl=1" title="figure 1" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5_2_303x499.jpeg" alt="figure 1" data-base62-sha1="xRqHgGg2FFnufSb116NEUdA77Xn" width="303" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5_2_303x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5.jpeg 2x" data-dominant-color="EAEAEA"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">figure 1</span><span class="informations">336×554 32.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f883160e69e24a1f41c4c464d3227c0460f48af.jpeg" data-download-href="/uploads/short-url/fUEN36oZ3IizJN1vzcXsraSxruD.jpeg?dl=1" title="figure 2" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f883160e69e24a1f41c4c464d3227c0460f48af.jpeg" alt="figure 2" data-base62-sha1="fUEN36oZ3IizJN1vzcXsraSxruD" width="457" height="500" data-dominant-color="F2F2F1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">figure 2</span><span class="informations">464×507 60.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello!</p>
<p>Has someone ever accessed the <a href="https://docs.openmicroscopy.org/ome-model/6.3.1/developers/screen-plate-well.html#wellsample" rel="noopener nofollow ugc">“index” field for Well Samples</a> using the Java API?</p>
<p>I cannot find it in the DataObject or the IObject.<br>
I might be blind though!</p> ;;;; <p>Hi Shubo,<br>
when I enter my account info it gives this error:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73bc01de35133e967779e08e3332f674ef34d526.png" alt="image" data-base62-sha1="gvPIciEUHfzZGJTpgA9Vikj4DwW" width="501" height="465"><br>
Shell I go via the activation route?</p>
<p>Best regards,<br>
Ilya</p> ;;;; <p>Alternatively, would not be possible to change the channel to my “shutter” wavelenght but avoiding frame capture in that channel?</p> ;;;; <p>Hi Nico,</p>
<p>I tried to set up State Device Shutter within my configuration but honestly, I´m stuck.  When I include State Device Shutter in a new Group or within the group created to change the wavelenght of my monochromator, all I get within Presets is a “None” option… May be the Optoscan is not a State Device?<br>
I think I don´t understand the StateDeviceShutter function, I tried to find some documentation but I failed.</p>
<p>Thanks for the help</p> ;;;; <p>Thanks, that will do the job. But then the user has to load a config twice; first when starting uManager (empty config) and then via pycromanager (config containing devices) or is there a way to only load a config file once?</p> ;;;; <p>Hi <a class="mention" href="/u/achille1">@Achille1</a></p>
<pre><code class="lang-auto">//---------------------
n=nSlices;
run("Set Measurements...", "feret's display redirect=None decimal=2");
for(i=1;i&lt;=n;i++) {
   setSlice(i);
run("Analyze Particles...", "display exclude slice");
}
Table.deleteColumn("FeretX");
Table.deleteColumn("FeretY");
Table.deleteColumn("MinFeret");
Table.update
//---------------------
</code></pre>
<p>Feedback is appreciated. —&gt; Obviously, my macro does not give any information in the volume.(sorry)</p> ;;;; <p>I guess we could then also just wait for the fix!<br>
Thank you <a class="mention" href="/u/wayne">@Wayne</a>!</p> ;;;; <p>Dear all, I recently received some MMI-format files from a research partner which were taken using MetaSystems’ Metafer and Zeiss microscope on chromosomal metaphase phases. However, I am now faced with the problem of what to do with these files as they cannot be opened by regular image viewers. Our research team does not have access to any equipment related to Metafer.</p>
<p>Please advise on any tools or solutions that could help me convert these files into a more universally compatible format such as JPEG. Any suggestions would be greatly appreciated.</p>
<p>Thank you in advance.</p> ;;;; <p>def imp = IJ.openImage(maskDir+imgName+‘.png’)<br>
def ip = imp.getProcessor()<br>
FloatProcessor foatIP = TypeConverter(ip).convertRGBToFloat()</p>
<p>produces the following error…</p>
<p>ERROR: MissingMethodException at line 27: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.TypeConverter() is applicable for argument types: (ij.process.ColorProcessor) values: [ip[width=1836, height=3307, bits=24, min=0.0, max=255.0]]</p>
<p>would it be much better to convert RGB masks to grayscale before brining them to QuPath?</p> ;;;; <p>Could you describe your data (what is the image, how big is it, what resolution etc), and also attach a screenshot of the cell detection parameters?</p> ;;;; <p>I can confirm. The main version in the cellpose repo (state mid February) has some MPS options, but when I use it from an M1 Mac, it only uses the CPU for me.<br>
Installing <a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a> branch linked above, I can get inference running on the GPU on my Mac, but not training.</p>
<p><a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a> Would be awesome if you find a way to integrate your changes upstream! Super useful that you’ve figured out a way to make this work! <img src="https://emoji.discourse-cdn.com/twitter/clap.png?v=12" title=":clap:" class="emoji" alt=":clap:" loading="lazy" width="20" height="20"></p> ;;;; <p><a class="mention" href="/u/christian_tischer">@Christian_Tischer</a> : thanks to <a class="mention" href="/u/wayne">@Wayne</a> updating ImageJ should be sufficient.</p>
<p>Otherwise I can modify the code, but I’d like to have it working for multiple channels. Currently the code suggested only works for the first image processor. Could you make the code work for multi-channel images ?</p> ;;;; <p>Hi. Adam,<br>
I did just as you said here, but it was still stuck with such error while it was received for 3D filtering on napari :<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54.png" data-download-href="/uploads/short-url/nLwDpxqBfWSjpvuV3F0BXovUQte.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_690x358.png" alt="image" data-base62-sha1="nLwDpxqBfWSjpvuV3F0BXovUQte" width="690" height="358" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_690x358.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_1035x537.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_1380x716.png 2x" data-dominant-color="191A16"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3816×1984 505 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/markkitt">@markkitt</a><br>
Nice documentation. Thank you for these links.</p> ;;;; <p>To make a long story short, the problem is that the orientation detection filters have a hard time resolving the orientations at the junction. The filters need to be longer in order to resolve the orientation.</p>
<p>I developed an adaptive method below, but it may be doing too much.</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/mkitti/AdaptiveResolutionOrientationSpace">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/mkitti/AdaptiveResolutionOrientationSpace" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/5/b5f8d9a55399b13b070327c2f4e0277b60bec701.png" class="thumbnail onebox-avatar" width="256" height="256">

<h3><a href="https://github.com/mkitti/AdaptiveResolutionOrientationSpace" target="_blank" rel="noopener nofollow ugc">GitHub - mkitti/AdaptiveResolutionOrientationSpace: Adaptive Resolution...</a></h3>

  <p>Adaptive Resolution Orientation Space performs multi-orientation analysis across angular resolutions to segment complex filamentous networks - GitHub - mkitti/AdaptiveResolutionOrientationSpace: Ad...</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>See the following article for details:<br>
Kittisopikul M, Vahabikashi A, Shimi T, Goldman RD, Jaqaman K. Adaptive multiorientation resolution analysis of complex filamentous network images. Bioinformatics. 2020 Dec 22;36(20):5093-5103. doi: 10.1093/bioinformatics/btaa627. PMID: 32653917; PMCID: PMC8453242.</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://pubmed.ncbi.nlm.nih.gov/32653917/">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce19e2f5f546bf5a36c511a7b75edc28c9d99197.png" class="site-icon" width="64" height="64">

      <a href="https://pubmed.ncbi.nlm.nih.gov/32653917/" target="_blank" rel="noopener nofollow ugc">PubMed</a>
  </header>

  <article class="onebox-body">
    <img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_500x500.png" class="thumbnail onebox-avatar" width="500" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_1000x1000.png 2x" data-dominant-color="3975AA">

<h3><a href="https://pubmed.ncbi.nlm.nih.gov/32653917/" target="_blank" rel="noopener nofollow ugc">Adaptive multiorientation resolution analysis of complex filamentous network...</a></h3>

  <p>Supplementary information is available at Bioinformatics online.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Maybe look into other examples of importing objects/masks with different pixel values per object.</p><aside class="quote quote-modified" data-post="5" data-topic="75516">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/s/8491ac/40.png" class="avatar">
    <a href="https://forum.image.sc/t/import-external-masks-in-qupath/75516/5">Import external masks in QuPath</a> <a class="badge-wrapper  bullet" href="/c/image-analysis/6"><span class="badge-category-bg" style="background-color: #25AAE2;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for questions about image processing and analysis.">Image Analysis</span></a>
  </div>
  <blockquote>
    ok got it now working and also changed the script so that one can use it for the complete project: 
def server = getCurrentServer()
def name_server = server.getMetadata()["name"]

def directoryPath = '/your_directory_path/HistoQC/histoqc_output_534243234523/'+ name_server + '/' // TO CHANGE

//clearAllObjects()
//double downsample = 32 // TO CHANGE (if needed)
ImagePlane plane = ImagePlane.getDefaultPlane()

File folder = new File(directoryPath);
File[] listOfFiles = folder.listFiles();

current…
  </blockquote>
</aside>
<p>
If your mask images are actually RGB though, and not normal mask images, that could be an issue.</p> ;;;; <p>I’ve constructed a pixel classifier that segments H&amp;E images into four different classes.<br>
Here’s an example of what the classifier produces (it actually is an annotation mask I labelled though…):<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717.png" data-download-href="/uploads/short-url/6btJAj7bZDtL6ZSxonevWRUr9wH.png?dl=1" title="2022S 0280579050101_0_label" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_380x500.png" alt="2022S 0280579050101_0_label" data-base62-sha1="6btJAj7bZDtL6ZSxonevWRUr9wH" width="380" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_380x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_570x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_760x1000.png 2x" data-dominant-color="5F6798"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">2022S 0280579050101_0_label</span><span class="informations">2169×2848 51.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>What I want is to import the masks and to create annotation objects at QuPath, so that I can do something more with the classification result (it actually is a screenshot during annotation process; of course I’m going to create a new QuPath project containing annotated ROIs, not WSIs).<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d.jpeg" data-download-href="/uploads/short-url/a8QAXmnJpCbH5S1hhui10jLlgQJ.jpeg?dl=1" title="캡처.PNG" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d_2_384x499.jpeg" alt="캡처.PNG" data-base62-sha1="a8QAXmnJpCbH5S1hhui10jLlgQJ" width="384" height="499" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d_2_384x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d_2_576x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d.jpeg 2x" data-dominant-color="91669C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">캡처.PNG</span><span class="informations">592×769 138 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I discovered <a href="https://gist.github.com/petebankhead/f807395f5d4f4bf0847584458ab50277" rel="noopener nofollow ugc">Script to import binary masks &amp; create annotations (see also QuPath-Export binary masks.groovy) · GitHub</a>, but the example doesn’t quite seem to be relevant for me… any kind of tip or suggestion would be greately appreciated! thanks!</p>
<h3>
<a name="sample-image-andor-code-1" class="anchor" href="#sample-image-andor-code-1"></a>Sample image and/or code</h3>
<ul>
<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) – (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>
<li>Share a <a href="https://en.wikipedia.org/wiki/Minimal_working_example" rel="noopener nofollow ugc">minimal working example</a> of your macro code.</li>
</ul>

<h3>
<a name="background-2" class="anchor" href="#background-2"></a>Background</h3>
<ul>
<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific “jargon”.</li>
</ul>
<h3>
<a name="analysis-goals-3" class="anchor" href="#analysis-goals-3"></a>Analysis goals</h3>
<ul>
<li>What information are you interested in getting from this image?</li>
</ul>
<h3>
<a name="challenges-4" class="anchor" href="#challenges-4"></a>Challenges</h3>
<ul>
<li>What stops you from proceeding?</li>
<li>What have you tried already?</li>
<li>Have you found any related forum topics? If so, cross-link them.</li>
<li>What software packages and/or plugins have you tried?</li>
</ul> ;;;; <p>Hi again,</p>
<p>I’ve created a new project with a video to track 3 bodyparts on 2 individuals - everything went well with stitches_tracklets. Now, I cant work out the file paths for the video and pickle files for the refine_tracklets commands.</p>
<p>I checked the help and it reads:</p>
<p>config: str<br>
Full path of the config.yaml file.</p>
<p>pickle_or_h5_file: str<br>
Full path of either the pickle file obtained after calling<br>
deeplabcut.convert_detections2tracklets, or the h5 file written after<br>
refining the tracklets a first time. Note that refined tracklets are<br>
always stored in the h5 format.</p>
<p>video: str<br>
Full path of the corresponding video.<br>
If the video duration and the total length of the tracklets disagree<br>
by more than 5%, a message is printed indicating that the selected<br>
video may not be the right one.</p>
<p>But I wasn’t following which files were created when I performed the convert_detections2tracklets command. I can see pickle files and new full-mp4 video were created in the videos folder, but I cant work out what files I should be inputting for the refine_tracklets command.</p>
<p>Thanks, again for your help <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hey, everyone!</p>
<p>I am brand new to everything around this forum and the topics discussed here, so please bear with me. <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Is there one image analysis software that I could have on my mobile device to scan the results of a lateral flow assay, such as the ones we use now for covid?</p>
<p>Maybe this already exists but I was wondering how would you guys approach this issue.<br>
How to normalize for light conditions? How to measure the line intensity? Is that something that can be done with a smart phone camera?</p>
<p>I will be happy to chat with whoever finds this application interesting!</p>
<p>Thank you for your time!</p> ;;;; <p>Hello <a class="mention" href="/u/alpha2zee">@alpha2zee</a><br>
Thanks so much for your effort in establishing this macro. Is there any possibility of just drawing parallel lines, regardless of whether the lines curve or not (as if one line is horizontal,<br>
there is no issue, my issue is when both are curved.)? Shown as pink lines. Thanks!<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2d69984aa4026c1fdd843bc26e97cc8943a5fc60.jpeg" alt="Capture" data-base62-sha1="6tJIM2QhGYuGQGe6d8kZjawnw2s" width="266" height="155"><br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/b/ab94d601811d918afa64da3f151fe253d14390fa.jpeg" alt="Capture2" data-base62-sha1="otSwFRXmCfRFp6A44PztK1T4Wka" width="277" height="111"></p> ;;;; <p>Hi <a class="mention" href="/u/marie-nkaefer">@Marie-nkaefer</a></p>
<p>Sorry I didn’t see this. Thank you so much for the great suggestions!</p> ;;;; <aside class="quote no-group" data-username="vedsharma" data-post="3" data-topic="78234">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/vedsharma/40/43754_2.png" class="avatar"> Ved Sharma:</div>
<blockquote>
<p>BUT, the results I am getting are very different than what I get from <a href="http://bigwww.epfl.ch/thevenaz/stackreg/">stackreg</a>.</p>
</blockquote>
</aside>
<p>Yes, I know. I compared it with stackreg before:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm">
  <header class="source">

      <a href="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm" target="_blank" rel="noopener">clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm</a></h4>


      <pre><code class="lang-ijm">// CLIJ example macro: motionCorrection_compare_stackreg.ijm
//
// This macro compares performance of motion correction
// on the GPU with StackReg. You need to install the
// BIG-EPFL update site in order to test it.
//
// Author: Robert Haase
// June 2019
// ---------------------------------------------
run("Close All");

// define move to correct
file = "https://github.com/clij/clij-docs/raw/master/src/main/resources/motion_correction_Drosophila_DSmanila1.tif";
//file = "C:/structure/code/clij-docs/src/main/resources/motion_correction_Drosophila_DSmanila1.tif";

// define identifiers for intermediate results in the GPU
inputStack = "inputStack";
slice = "slice";
shifted = "shifted";
binary = "binary";
</code></pre>



  This file has been truncated. <a href="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<aside class="quote no-group" data-username="vedsharma" data-post="3" data-topic="78234">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/vedsharma/40/43754_2.png" class="avatar"> Ved Sharma:</div>
<blockquote>
<p>I am looking for an option to run stackreg or a similar algorithm on GPU. Any suggestions?</p>
</blockquote>
</aside>
<p>Reimplementing stackreg for the GPU might be quite complicated. I looked into the code back in the days in 2019. To be honest, I’m afraid I couldn’t do it; or it would take me a substantial amount of time…</p> ;;;; <p>Hi <a class="mention" href="/u/juliafed">@juliafed</a><br>
I get this with the attached interactive macro.<br>
Appreciate any feedback. Thanks in advance.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c.png" data-download-href="/uploads/short-url/3Lg1sIYj8WGQjv7CNIAnasT4wO0.png?dl=1" title="Selectionner_les_ points_de jonctions" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c_2_382x500.png" alt="Selectionner_les_ points_de jonctions" data-base62-sha1="3Lg1sIYj8WGQjv7CNIAnasT4wO0" width="382" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c_2_382x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c_2_573x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c.png 2x" data-dominant-color="141212"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Selectionner_les_ points_de jonctions</span><span class="informations">735×960 70 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<pre><code class="lang-auto">macro "How to capture two cross-cutting lines "
{
requires("1.54b");
setBackgroundColor(0,0,0);
setOption("BlackBackground",true);
//--------------------------------
// Start processing
img=getImageID();
newImage("1", "RGB black", 154, 190, 1);
selectImage(img);
//---------------------------------
//rectify the luminescence and enlarge study image
run("Duplicate...", "title=temp");
run("Polynomial Shading Corrector", "degree_x=2 degree_y=2 regularization=2");
run("Select All");
run("Copy");
close("temp");
selectWindow("1");
run("Paste");
run("Select None");
//---------------------------------
// Locate the object of study
run("Unsharp Mask...", "radius=5 mask=0.60");
run("Minimum...", "radius=4");
run("Maximize");
run("Find Maxima...", "prominence=20 output=[Point Selection]");
roiManager("Add");
roiManager("Show None");
run("8-bit");
run("Ridge Detection", "line_width=3.5 high_contrast=230 low_contrast=87 extend_line method_for_overlap_resolution=NONE sigma=1.51 lower_threshold=3.06 upper_threshold=7.99 minimum_line_length=5 maximum=1000");
roiManager("Show All");
//---------------------------------
run("Set Measurements...", "display redirect=None decimal=2");
roiManager("Measure");
Table.sort("X");
//---------------------------------
for ( i = nResults() - 1; i &gt;= 0; i--) {
selectWindow("1");
Table.setSelection(i,i);
//setTool("multipoint");
run("Point Tool...", "type=Circle color=Red size=XXXL label counter=0");
makePoint(getResult("X",i),getResult("Y",i), "extra large red circle");
msg="Look at the red circle.\nShould the point be taken account?";
if(getBoolean(msg) == 1){
//setTool("multipoint");
run("Point Tool...", "type=Circle color=White size=XXXL label counter=0");
makePoint(getResult("X",i),getResult("Y",i), "extra large white circle");
roiManager("Add");}
else
Table.deleteRows(i, i) ;
}
updateResults();
//---------------------------------
roiManager("Select", 0);
roiManager("Delete");
roiManager("Show None");
roiManager("Show All");
// End of processing
//--------------------------------
exit();
}type or paste code here
</code></pre>
<p>From the results you can draw your lines.</p>
<p>An alternative for your situation is to use openCv:</p><aside class="onebox stackexchange" data-onebox-src="https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc">
  <header class="source">

      <a href="https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc" target="_blank" rel="noopener nofollow ugc">stackoverflow.com</a>
  </header>

  <article class="onebox-body">
      <a href="https://stackoverflow.com/users/3168934/jo%c3%a3o-david" target="_blank" rel="noopener nofollow ugc">
    <img alt="Jo&amp;#227;o David" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/a/da9e578f5f7eaf2f5ffd37624d2279032e5f88f5.png" class="thumbnail onebox-avatar" width="256" height="256">
  </a>

<h4>
  <a href="https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc" target="_blank" rel="noopener nofollow ugc">How to find the junction points or segments in a skeletonized image Python OpenCV?</a>
</h4>

<div class="tags">
  <strong>python, image, opencv, image-processing</strong>
</div>

<div class="date">
  asked by
  
  <a href="https://stackoverflow.com/users/3168934/jo%c3%a3o-david" target="_blank" rel="noopener nofollow ugc">
    Jo&amp;#227;o David
  </a>
  on <a href="https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc" target="_blank" rel="noopener nofollow ugc">08:10PM - 08 May 22 UTC</a>
</div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>I like having feedback.</p> ;;;; <p><a href="https://drive.google.com/drive/folders/1D0bg0LYTWEd_ZHhsEWWkipgCXsrUfjmm?usp=share_link" class="onebox" target="_blank" rel="noopener nofollow ugc">https://drive.google.com/drive/folders/1D0bg0LYTWEd_ZHhsEWWkipgCXsrUfjmm?usp=share_link</a><br>
The link above contains the csv and h5 files. Thank you!</p> ;;;; <p>Hello Pete,</p>
<p>Thank you very much for your response.</p>
<p>I launched a script with just a line as you said:</p>
<pre><code class="lang-auto">qupath.lib.gui.prefs.PathPrefs.userPathProperty().set('/path/to/user/path/')
</code></pre>
<p>with the command :</p>
<pre><code class="lang-auto">declare qupath="$HOME/Program_files/QuPath/bin/QuPath.sh"
declare groovy_script1="$path/QP_Proj/scripts/0-Set_Preferences.groovy"
$qupath script $groovy_script1 --save
</code></pre>
<p>And I got a successful response:</p>
<pre><code class="lang-auto">23:06:13.847 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Refreshing extensions in /gpfs/home/user/qupath/extensions
23:06:13.880 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Added extension: /gpfs/home/user/qupath/extensions/qupath-extension-imagecombiner-0.2.3.jar
23:06:13.892 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Added extension: /gpfs/home/user/qupath/extensions/qupath-extension-stardist-0.4.0.jar
23:06:13.902 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Added extension: /gpfs/home/user/qupath/extensions/qupath-extension-cellpose-0.6.1.jar

</code></pre>
<p>And now when I launch a script my extenstion file is taken in account.</p>
<p>Thank you very much for the fast response!</p> ;;;; <p>Maybe it’s too big. You can upload it to Google drive and send me the link</p> ;;;; <aside class="quote no-group" data-username="Youngolaf" data-post="21" data-topic="70119">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png" class="avatar"> Youngolaf:</div>
<blockquote>
<p>I am trying to figure out what is going wrong with the training areas. It seems like the validationImage line is messing with the hierarchy. When I run the following code:</p>
<pre><code class="lang-auto">print "hier1" +  hierarchy
validationImage = createTrainingAreas(hierarchy, imgName)
print "hier2" +  hierarchy
</code></pre>
</blockquote>
</aside>
<p>The above looks right, assuming you created points for essentially all 900 cells in your image.</p>
<aside class="quote no-group" data-username="Youngolaf" data-post="21" data-topic="70119">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png" class="avatar"> Youngolaf:</div>
<blockquote>
<p>and when I run the following code inside the validationImage block:</p>
<pre><code class="lang-auto">print "hier2" +  hierarchy
print "objects" + hierarchy.getAnnotationObjects()
trainingAreas = hierarchy.getAnnotationObjects().findAll{it.getPathClass() == getPathClass(testClass)}
print "trainingAreas: " + trainingAreas
</code></pre>
<p>I get:</p>
<pre><code class="lang-auto">hier2Hierarchy: 1802 objects
INFO: objects[Test (901 objects)]
INFO: trainingAreas: []
</code></pre>
</blockquote>
</aside>
<p>This part looks wrong in several ways. First, the “objects” print line should only have 1 or a small number of objects. You did not use “getDetectionObjects” which is what the script creates the most of. There should only be one Annotation object per training area. The whole image should only have a very small number of annotations.</p>
<aside class="quote no-group" data-username="Youngolaf" data-post="21" data-topic="70119">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png" class="avatar"> Youngolaf:</div>
<blockquote>
<p>I think the “<code>.findAll{it.getPathClass() == getPathClass(testClass)}</code>” is what is causing the issues.</p>
</blockquote>
</aside>
<p>That is what I <a href="https://forum.image.sc/t/transfer-class-from-points-to-detections/70119/18">mentioned before</a>, it looks like you are not referencing the class that you used for the training annotations.</p>
<p>If you only had one large box for training, your total annotation count during the script should be 2, one for the annotation surrounding the cells, and one for the annotation class that was loaded in to identify the testing area. At any other time, outside the script, getAnnotationObjects should return 1 object.</p> ;;;; <p>I found a workaround that works for RGB tiles.</p>
<pre><code class="lang-auto">byte[] bytesr = b.imRead.openBytes(b.Coords[coord.Z, 0, coord.T], tilex, tiley, sx, sy);
            bool interleaved = b.imRead.isInterleaved();
            if (!interleaved)
            {
                byte[] rb = new byte[strplane * sy];
                byte[] gb = new byte[strplane * sy];
                byte[] bb = new byte[strplane * sy];
                int ind = 0;
                for (int y = 0; y &lt; sy; y++)
                {
                    for (int st = 0; st &lt; strplane; st++)
                    {
                        rb[((strplane) * y) + st] = bytesr[((strplane) * y) + st];
                        ind++;
                    }
                }
                byte[] bytes = new byte[ind];
                Array.Copy(bytesr,ind,bytes,0,ind);
                for (int y = 0; y &lt; sy; y++)
                {
                    int x = 0;
                    for (int st = 0; st &lt; strplane; st++)
                    {
                        int i = ((strplane) * y) + x;
                        gb[i] = bytes[((strplane) * y) + st];
                        x++;
                    }
                }
                Array.Copy(bytesr, ind * 2, bytes, 0, ind);
                for (int y = 0; y &lt; sy; y++)
                {
                    int x = 0;
                    for (int st = 0; st &lt; strplane; st++)
                    {
                        int i = ((strplane) * y) + x;
                        bb[i] = bytes[((strplane) * y) + st];
                        x++;
                    }
                }
                Bitmap[] bms = new Bitmap[3];
                if (b.bitsPerPixel == 8)
                {
                    bms[2] = new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, rb, new ZCT(0, 0, 0), p, littleEndian);
                    bms[1] = new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, gb, new ZCT(0, 0, 0), p, littleEndian);
                    bms[0] = new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, bb, new ZCT(0, 0, 0), p, littleEndian);
                    return Bitmap.RGB8To24(bms);
                }
                else
                {
                    bms[2] = new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, rb, new ZCT(0, 0, 0), p, littleEndian);
                    bms[1] = new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, gb, new ZCT(0, 0, 0), p, littleEndian);
                    bms[0] = new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, bb, new ZCT(0, 0, 0), p, littleEndian);
                    return Bitmap.RGB16To48(bms);
                }
            }
</code></pre> ;;;; <p>I don’t entirely understand the question, but I’m throwing this thread out there in case it’s helpful:</p>
<aside class="quote quote-modified" data-post="1" data-topic="71579">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/smcardle/40/17938_2.png" class="avatar">
    <a href="https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579">Script for generating double threshold classifier</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    Hi All, 
Here’s a script I wrote that will hopefully be useful to some people. 
Normally, if I need to find double-positive regions, I run two pixel classifiers, then use Java geometry functions to find the object intersections. This works, but if there are many small detections, it can be quite slow. The script below uses ImageOps to apply a threshold to two different channels and writes it as a single pixel classifier. Then, the “colocalized” objects can be created directly through normal QuPa…
  </blockquote>
</aside>
 ;;;; <p>Hi <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a></p>
<p>Thank you for your quick reply. I checked the macro and it works without any error, so that’s great!</p>
<p>BUT, the results I am getting are very different than what I get from <a href="http://bigwww.epfl.ch/thevenaz/stackreg/" rel="noopener nofollow ugc">stackreg</a>. Basically, I am happy with the stackreg results but it’s slow for my large time-lapse (many GBs), so I am looking for an option to run stackreg or a similar algorithm on GPU. Any suggestions?</p>
<p>Ved</p> ;;;; <p>I tried uploading the file here but it does not allow me to upload a h5 file. Is there any other way I can share it with you? Thank you.</p> ;;;; <p>Looks like there is something wrong with the bodyparts level. The column names on that level seem to be repeating. Can you upload it here?</p> ;;;; <p>Hi <a class="mention" href="/u/nicokiaru">@NicoKiaru</a>,</p>
<p>Happy to share the data and would appreciate you looking at it.<br>
<a href="https://wormhole.app/1R9qn#H7m_0vO7RUXfVvpwNTEiUg" rel="noopener nofollow ugc">Here is a link</a> to one dataset we already aligned (images downsampled by 10x) it contains images as well as json files from QuickNii (filebuilderoutput_aligned.json) and VisuAlign (…_visualign.json). The sections are not the prettiest.<br>
If you meant a dataset that hasn’t been aligned or if full resolution images would be more helpful I also have a few of those.</p>
<p>The multiresolution file compatibility is one of really nice things about ABBA. I did look at BigWarp, but since I’m already familiar with Visualign I was hesitant to jump in. But may just have to get used to it.</p>
<p>Thanks for your help,<br>
Pascal</p> ;;;; <p><a class="mention" href="/u/dgault">@dgault</a> So far I have found a workaround for this issue which partially works returning a grayscale image when dealing with RGB tiles. By doing the following to the bytes from OpenBytes().</p>
<pre><code class="lang-auto">int strplane = 0;
            if (RGBChannelCount == 1)
            {
                if (b.bitsPerPixel &gt; 8)
                    strplane = sx * 2;
                else
                    strplane = sx;
            }
            bytes = b.imRead.openBytes(b.Coords[coord.Z, coord.C, coord.T], tilex, tiley, sx, sy);
            if (b.file.EndsWith("tif") || b.file.EndsWith("ndpi"))
            {
                byte[] rb = new byte[strplane * sy];
                Bitmap[] bfs = new Bitmap[3];
                for (int y = 0; y &lt; sy; y++)
                {
                    for (int x = 0; x &lt; strplane; x++)
                    {
                        rb[((strplane) * y) + x] = bytes[((strplane) * y) + x];
                    }
                }
                if (b.bitsPerPixel == 8)
                {
                    return new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, rb, new ZCT(0, 0, 0), p, littleEndian);
                }
                else
                {
                    return new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, rb, new ZCT(0, 0, 0), p, littleEndian);
                }
            }
</code></pre> ;;;; <p>Hi <a class="mention" href="/u/christian_tischer">@Christian_Tischer</a>,</p>
<p>This bug will be fixed in the next daily build. In the meantime, you can work around it by using ImageProcessor.getMin() and ImageProcessor.getMax(), as in this example:</p>
<pre><code class="lang-auto">imp = IJ.openImage(IJ.getDir("downloads")+"2023_01_18--Extract_8hr_1-1.tif");
ip = imp.getProcessor();
min = ip.getMin();
max = ip.getMax();
IJ.log("display range: "+min+"-"+max);
</code></pre> ;;;; <p>There’s already a related thread on this topic, so be sure to check out</p><aside class="quote quote-modified" data-post="16" data-topic="77523">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png" class="avatar">
    <a href="https://forum.image.sc/t/not-seeing-mac-gpu-with-cle-available-device-names/77523/16">Not seeing Mac GPU with cle.available_device_names()</a> <a class="badge-wrapper  bullet" href="/c/usage-issues/7"><span class="badge-category-bg" style="background-color: #BF1E2E;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for discussing technical questions and problems with scientific image software.">Usage &amp; Issues</span></a>
  </div>
  <blockquote>
    <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a> 
For me Cell 1 outputs: 
['Apple M1'] 
and cell 3 (the device = cpu one) gives an error: 
IndexError                                Traceback (most recent call last)
Cell In[3], line 1
----&gt; 1 device = cle.select_device(dev_type='cpu')
      2 device

File ~/Dev/miniforge3/envs/cle-test/lib/python3.10/site-packages/pyclesperanto_prototype/_tier0/_device.py:72, in select_device(name, dev_type, score_key)
     68 except:
     69     pass
---&gt; 72 device = filter_devices(name, dev_typ…
  </blockquote>
</aside>

<p>Can you post the output to:</p>
<pre><code class="lang-auto">import pyclesperanto_prototype as cle
cle.available_device_names()
</code></pre>
<p>I get:<br>
<code>['Apple M1']</code><br>
and I can select it using:<br>
<code>cle.select_device('Apple')</code></p> ;;;; <p>Hi Rafa,<br>
there were a few issues (both in your example code and my upstream dependencies).</p>
<ul>
<li>if you want true 2d data the <code>chunks</code>, <code>scale_factors</code> and <code>resolution</code> all need to be 2d instead of 3d. See the changes I have made here: <a href="https://github.com/constantinpape/mobie-python-examples/blob/9a8647de8dc6f13e579dc165faf1b4ed93cfdabf/mobie-project-generation_zarr.ipynb" class="inline-onebox">mobie-python-examples/mobie-project-generation_zarr.ipynb at 9a8647de8dc6f13e579dc165faf1b4ed93cfdabf · constantinpape/mobie-python-examples · GitHub</a>
</li>
<li>However, there was an error that prevented the 2d case to work. I have fixed it, but need to make a new release of that dependency. I will ping you as soon as the release is on conda-forge, and how you can upgrade to it.</li>
<li>When you rerun it, make sure that you remove both the tmp folder and the mobie project folder, otherwise that can lead to errors due to incompatible data.</li>
</ul> ;;;; <p>I’m not sure we need another thread for the same topic.<br>
As discussed in that thread, the released cellpose does not appear to fully use MPS to use GPU on macOS arm64. You can try my fork where I’ve gotten it to work see my recent post:</p><aside class="quote quote-modified" data-post="35" data-topic="68018">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png" class="avatar">
    <a href="https://forum.image.sc/t/cellpose-on-macos-m1-pro-apple-silicon-arm64/68018/35">Cellpose on MacOS M1 Pro (Apple Silicon arm64)</a> <a class="badge-wrapper  bullet" href="/c/development/5"><span class="badge-category-bg" style="background-color: #F7941D;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for development (i.e., programming) questions about scientific image software.">Development</span></a>
  </div>
  <blockquote>
    So I’ve not had any luck with released cellpose to use MPS. <a class="mention" href="/u/jluethi">@jluethi</a> can chime in as well. 
I made a fork that works for me. You can try it, like Joel did. His post has good details: 

For him I think it worked for inference but not training. 
Also you may want to use Nightly PyTorch (see <a href="http://PyTorch.org" rel="noopener nofollow ugc">PyTorch.org</a>) 
Anyhow, it’s on my TODO to figure out/upstream my changes. 
About video tutorials, I’m not sure how to make those—I’m very much a beginner myself, I’ve just had an M1 mac for a while I guess.
  </blockquote>
</aside>
<p>
Otherwise, you need to be patient. Apple is still rapidly iterating MPS support in PyTorch, so it’s totally understandable that the cellpose folks haven’t gotten it all ironed out.</p> ;;;; <p>So I’ve not had any luck with released cellpose to use MPS. <a class="mention" href="/u/jluethi">@jluethi</a> can chime in as well.<br>
I made a fork that works for me. You can try it, like Joel did. His post has good details:</p><aside class="quote quote-modified" data-post="17" data-topic="68018">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jluethi/40/43165_2.png" class="avatar">
    <a href="https://forum.image.sc/t/cellpose-on-macos-m1-pro-apple-silicon-arm64/68018/17">Cellpose on MacOS M1 Pro (Apple Silicon arm64)</a> <a class="badge-wrapper  bullet" href="/c/development/5"><span class="badge-category-bg" style="background-color: #F7941D;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for development (i.e., programming) questions about scientific image software.">Development</span></a>
  </div>
  <blockquote>
    Just came across this thread, this is super cool <a class="mention" href="/u/psobolewskiphd">@psobolewskiPhD</a> ! Very nice to be able to run cellpose models using the GPU on my Mac locally! For some 3D example data, I came down from 20 minutes to below 1 minute inference time <img width="20" height="20" src="https://emoji.discourse-cdn.com/twitter/smile.png?v=12" title="smile" alt="smile" class="emoji"> 
Thanks a lot for sharing this, I hope this makes it back into the main cellpose, this is super useful! 
If anyone else comes across this: 
You can just run python -m cellpose to start the GUI and it automatically uses the GPU then. 
Also, here is a workflow to…
  </blockquote>
</aside>
<p>
For him I think it worked for inference but not training.<br>
Also you may want to use Nightly PyTorch (see <a href="http://PyTorch.org" rel="noopener nofollow ugc">PyTorch.org</a>)</p>
<p>Anyhow, it’s on my TODO to figure out/upstream my changes.</p>
<p>About video tutorials, I’m not sure how to make those—I’m very much a beginner myself, I’ve just had an M1 mac for a while I guess.</p> ;;;; <p>Hi <a class="mention" href="/u/vedsharma">@vedsharma</a> ,</p>
<p>I just quickly checked the problem and I can reproduce the error. The issue not obvious to me unfortunately. As I can’t fix it immediately, I recommend using this macro instead:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm">
  <header class="source">

      <a href="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm" target="_blank" rel="noopener">clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm</a></h4>


      <pre><code class="lang-ijm">// CLIJ example macro: motionCorrection,ijm
//
// This macro shows how to do motion correction 
// image stack where slices might be shifted to 
// each other in the GPU.
//
// Author: Robert Haase
// December 2018
// ---------------------------------------------
run("Close All");

// define move to correct
file = "https://github.com/clij/clij-docs/raw/master/src/main/resources/motion_correction_Drosophila_DSmanila1.tif";
//file = "C:/structure/code/clij-docs/src/main/resources/motion_correction_Drosophila_DSmanila1.tif";

// define identifiers for intermediate results in the GPU
inputStack = "inputStack";
slice = "slice";
shifted = "shifted";
binary = "binary";
</code></pre>



  This file has been truncated. <a href="https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm" target="_blank" rel="noopener">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>It seems to me pretty much the same algorithm, just a macro-version of it.</p>
<p>Let me know if this works for you!</p>
<p>Best,<br>
Robert</p> ;;;; <p><a class="mention" href="/u/kbellve">@kbellve</a></p>
<p>Thanks, I see now. Since I saw it in the pull down menu in MM, I thought it was a module I could install and run. I too have a Piezoelectric focuser flexure working with a Raspberry Pi and controller.</p>
<div class="onebox lazyYT lazyYT-container" data-youtube-id="ID_FFl_-kV8" data-youtube-title="Piezoelectric focuser Flexure Mechanism Test" data-parameters="feature=oembed&amp;wmode=opaque">
  <a href="https://www.youtube.com/watch?v=ID_FFl_-kV8" target="_blank" rel="noopener nofollow ugc">
    <img class="ytp-thumbnail-image" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9ba14e15d3c43e64934311f8f15011ce5579337.jpeg" title="Piezoelectric focuser Flexure Mechanism Test" width="480" height="360">
  </a>
</div>

<p>-JW:</p> ;;;; <p>Hi, I have converted the images to png format and cleaned up the csv but I am still getting the same error when I try using the command deeplabcut.create_training_dataset. I have also taken a look at the .h5 file and it looks alright. I have attached a screenshot of the contents here for your reference. I would appreciate some more guidance regarding this issue. Thank you.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/097b8c1c3757785a014b446a85586becd58b88be.png" data-download-href="/uploads/short-url/1lSZL4bJSSmnH0c10WmBh9bCFB4.png?dl=1" title="Screenshot 2023-03-07 at 2.08.58 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/097b8c1c3757785a014b446a85586becd58b88be_2_690x161.png" alt="Screenshot 2023-03-07 at 2.08.58 PM" data-base62-sha1="1lSZL4bJSSmnH0c10WmBh9bCFB4" width="690" height="161" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/097b8c1c3757785a014b446a85586becd58b88be_2_690x161.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/097b8c1c3757785a014b446a85586becd58b88be_2_1035x241.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/097b8c1c3757785a014b446a85586becd58b88be_2_1380x322.png 2x" data-dominant-color="EEEEEE"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-07 at 2.08.58 PM</span><span class="informations">2209×516 219 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>additional info.<br>
this is log when old version of format is used</p>
<pre><code class="lang-auto">2023-03-07 13:56:01,460 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view 
2023-03-07 13:56:01,460 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] TreeselectEvent: valueChanged 
2023-03-07 13:56:01,461 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] Deselect node: null 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] PRESS import: save changes 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: null 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: C:\Users\User1\Downloads\OS-1\OS-1.vsi 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) 	 mapAnnotation is null 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] clean up 
2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] load setup: Universal 
2023-03-07 13:56:02,641 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) OMERO.blitz Version: 5.6.0 
2023-03-07 13:56:02,641 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 
2023-03-07 13:56:02,641 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view 
2023-03-07 13:56:02,643 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) Depth: 4 Metadata Level: MINIMUM 
2023-03-07 13:56:02,682 INFO  [                loci.formats.ImageReader] ( Thread-18) CellSensReader initializing C:\Users\User1\Downloads\OS-1\OS-1.vsi 
2023-03-07 13:56:02,887 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) 1 file(s) parsed into 1 group(s) with 1 call(s) to setId in 240ms. (244ms total) [0 unknowns] 
2023-03-07 13:56:03,634 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) OMERO.blitz Version: 5.6.0 
2023-03-07 13:56:03,634 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 
2023-03-07 13:56:03,636 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) Depth: 4 Metadata Level: MINIMUM 
2023-03-07 13:56:03,648 INFO  [                loci.formats.ImageReader] ( Thread-18) CellSensReader initializing C:\Users\User1\Downloads\OS-1\OS-1.vsi 
2023-03-07 13:56:03,675 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) 1 file(s) parsed into 1 group(s) with 1 call(s) to setId in 38ms. (40ms total) [0 unknowns] 
2023-03-07 13:56:03,676 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) OMERO.blitz Version: 5.6.0 
2023-03-07 13:56:03,676 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 
2023-03-07 13:56:04,055 INFO  [    ome.formats.OMEROMetadataStoreClient] ( Thread-18) Pinging session every 300s. 
2023-03-07 13:56:04,173 DEBUG [ ome.services.blitz.util.CurrentPlatform] ( Thread-18) recognized current operating system as being Microsoft Windows 
2023-03-07 13:56:04,932 INFO  [      o.f.i.transfers.UploadFileTransfer] ( Thread-18) Transferring C:\Users\User1\Downloads\OS-1\OS-1.vsi... 
2023-03-07 13:56:05,153 INFO  [      o.f.i.transfers.UploadFileTransfer] ( Thread-18) Transferring C:\Users\User1\Downloads\OS-1\_OS-1_\stack1\frame_t.ets... 
2023-03-07 13:56:05,703 INFO  [      o.f.i.transfers.UploadFileTransfer] ( Thread-18) Transferring C:\Users\User1\Downloads\OS-1\_OS-1_\stack10001\frame_t.ets... 
</code></pre>
<p>this is new file format</p>
<pre><code class="lang-auto">2023-03-07 13:56:34,189 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view 
2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] PRESS import: save changes 
2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: null 
2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: \\server\OM\MIHOVIL_FILL\test\20230307_Image_smaller\20230307_Image_smaller.vsi 
2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP 
2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) 	 mapAnnotation is null 
2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] clean up 
2023-03-07 13:56:35,236 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] load setup: Universal 
2023-03-07 13:56:35,259 INFO  [       ome.formats.importer.ImportConfig] ( Thread-23) OMERO.blitz Version: 5.6.0 
2023-03-07 13:56:35,259 INFO  [       ome.formats.importer.ImportConfig] ( Thread-23) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 
2023-03-07 13:56:35,259 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view 
2023-03-07 13:56:35,260 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-23) Depth: 4 Metadata Level: MINIMUM 
2023-03-07 13:56:35,356 INFO  [                loci.formats.ImageReader] ( Thread-23) CellSensReader initializing \\server\OM\MIHOVIL_FILL\test\20230307_Image_smaller\20230307_Image_smaller.vsi 
2023-03-07 13:56:35,534 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-23) 1 file(s) parsed into 0 group(s) with 1 call(s) to setId in 273ms. (274ms total) [0 unknowns] 
</code></pre> ;;;; <p>Hi there,</p>
<p>I’m using Batch counting plugin to count cfos. It was working pretty good for couple batch counting and now it shows an error</p>
<p>Error:		Empty array in line 221:<br>
(called from line 369)<br>
(called from line 158)<br>
(called from line 80)</p>
<pre><code>	lastx = updatedX [ updatedX . length - 1 &lt;]&gt; ; 
</code></pre>
<p>Line 221 from the source is</p>
<p>lastx = updatedX[updatedX.length - 1];<br>
lasty = updatedY[updatedY.length - 1];<br>
for (i = updatedX.length; i &lt; 250; i++) {<br>
updatedX = append(updatedX, lastx);<br>
updatedY = append(updatedY, lasty);</p>
<p>Any suggestions to resolve this would be appreciated. Thanks</p> ;;;; <p>Not sure about the rest but the test project works. That’s just indicating that you need to update the location of the image file.</p> ;;;; <p>Hi <a class="mention" href="/u/haesleinhuepf">@haesleinhuepf</a></p>
<p>I was trying to run the <a href="https://clij.github.io/clij2-docs/reference_translationTimelapseRegistration" rel="noopener nofollow ugc">time lapse registration</a> in the CLIJ2 library, but it’s giving me an error. I was wondering if you know what the problem could be.</p>
<p>Here is the macro code I am running:</p>
<pre><code class="lang-auto">run ("Close All");
open("D:/image.tif");
run("CLIJ2 Macro Extensions", "cl_device=");
Ext.CLIJ2_clear();
input = getTitle();
Ext.CLIJ2_push(input);
close();
 
Ext.CLIJx_translationTimelapseRegistration(input, output);

Ext.CLIJ2_pull(output);
Ext.CLIJ2_clear();
</code></pre>
<p>And here is the error:</p>
<pre><code class="lang-auto">java.lang.ClassCastException: java.lang.String cannot be cast to net.haesleinhuepf.clij.clearcl.ClearCLBuffer
	at net.haesleinhuepf.clijx.registration.TranslationTimelapseRegistration.executeCL(TranslationTimelapseRegistration.java:23)
	at net.haesleinhuepf.clij.macro.CLIJHandler.lambda$handleExtension$0(CLIJHandler.java:163)
	at net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:97)
	at net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:28)
	at net.haesleinhuepf.clij.macro.CLIJHandler.handleExtension(CLIJHandler.java:53)
	at ij.macro.ExtensionDescriptor.dispatch(ExtensionDescriptor.java:288)
	at ij.macro.Functions.doExt(Functions.java:5096)
	at ij.macro.Functions.getStringFunction(Functions.java:279)
	at ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)
	at ij.macro.Interpreter.getString(Interpreter.java:1498)
	at ij.macro.Interpreter.doStatement(Interpreter.java:336)
	at ij.macro.Interpreter.doStatements(Interpreter.java:267)
	at ij.macro.Interpreter.run(Interpreter.java:163)
	at ij.macro.Interpreter.run(Interpreter.java:93)
	at ij.macro.Interpreter.run(Interpreter.java:107)
	at ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)
	at ij.IJ.runMacro(IJ.java:160)
	at ij.IJ.runMacro(IJ.java:149)
	at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)
	at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)
	at net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)
	at net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)
	at net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)
	at org.scijava.script.ScriptModule.run(ScriptModule.java:164)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
java.lang.NullPointerException
	at net.haesleinhuepf.clij.CLIJ.convert(CLIJ.java:461)
	at net.haesleinhuepf.clij.CLIJ.show(CLIJ.java:358)
	at net.haesleinhuepf.clij.macro.CLIJHandler.pullFromGPU(CLIJHandler.java:253)
	at net.haesleinhuepf.clij2.plugins.Pull.executeCL(Pull.java:24)
	at net.haesleinhuepf.clij.macro.CLIJHandler.lambda$handleExtension$0(CLIJHandler.java:163)
	at net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:97)
	at net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:28)
	at net.haesleinhuepf.clij.macro.CLIJHandler.handleExtension(CLIJHandler.java:53)
	at ij.macro.ExtensionDescriptor.dispatch(ExtensionDescriptor.java:288)
	at ij.macro.Functions.doExt(Functions.java:5096)
	at ij.macro.Functions.getStringFunction(Functions.java:279)
	at ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)
	at ij.macro.Interpreter.getString(Interpreter.java:1498)
	at ij.macro.Interpreter.doStatement(Interpreter.java:336)
	at ij.macro.Interpreter.doStatements(Interpreter.java:267)
	at ij.macro.Interpreter.run(Interpreter.java:163)
	at ij.macro.Interpreter.run(Interpreter.java:93)
	at ij.macro.Interpreter.run(Interpreter.java:107)
	at ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)
	at ij.IJ.runMacro(IJ.java:160)
	at ij.IJ.runMacro(IJ.java:149)
	at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)
	at net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)
	at net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)
	at net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)
	at net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)
	at org.scijava.script.ScriptModule.run(ScriptModule.java:164)
	at org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)
	at org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)
	at org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
</code></pre>
<p>Thank you!<br>
Ved</p> ;;;; <p>Yes I’m still having trouble. I found the example at <a href="https://github.com/melissalinkert/bioformats-examples/blob/4a07150286ef810ebdd1976942cf232890520bb3/src/main/java/TiledReaderWriter.java" class="inline-onebox" rel="noopener nofollow ugc">bioformats-examples/TiledReaderWriter.java at 4a07150286ef810ebdd1976942cf232890520bb3 · melissalinkert/bioformats-examples · GitHub</a> however it doesn’t seem to work for non-interleaved RGB images. Giving the resulting image I posted. Here is the C# code I’m using which works for interleaved images.</p>
<pre><code class="lang-auto">byte[] bytesr = b.imRead.openBytes(b.Coords[coord.Z, 0, coord.T], tilex, tiley, sx, sy);
Bitmap bm = new Bitmap(b.file, sx, sy, PixelFormat, bytesr, coord, p, littleEndian);
</code></pre> ;;;; <p>Je suis débutant en traitement d’images et je travaille actuellement sur l’analyse de l’écoulement de matériaux hétérogènes dans une buse et je souhaite fournir dans mon rapport des informations sur la longueur et l’orientation (tenseur d’orientation) des fibres. J’ai donc effectué une segmentation des images µCT avec les plugins <strong>Trainable Weka</strong> afin de ne conserver que la phase fibreuse de mon composite.<br>
Est-ce que quelqu’un pourrait m’expliquer comment calculer la distribution de longueur et la distribution d’orientation des fibres pour des images 3D avec Fiji ?<br>
De plus, comment puis-je obtenir les composantes du tenseur d’orientation des fibres ? Je joins une image .gif.</p>
<p>Merci beaucoup pour votre temps, Cordialement,</p>
<p>Achille<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/14cdd5b740c62fc4b7da7baa4bdd22c81f059f4c.gif" alt="Fibrous" data-base62-sha1="2Y2y0peo3ZIWiUdjRwsYIKPRQba" width="210" height="210" class="animated"><a href="https://drive.google.com/file/d/1wDNhbyom7A0QWp9RXQsvzypytHcte_Wg/view?usp=share_link" rel="noopener nofollow ugc">https://drive.google.com/file/d/1wDNhbyom7A0QWp9RXQsvzypytHcte_Wg/view?usp=share_link</a></p> ;;;; <p><a class="mention" href="/u/chris82">@chris82</a> Sorry, for the very late reply. I have been busy with other projects and somehow I missed the original tagging. Sadly, I haven’t done anything with the shield version, nor do I have an alternative to the TSL1401 sensor.  My schedule might be opening up in a 3-4 months, and if there is still interest, I could build and test the shield version.</p>
<p><a class="mention" href="/u/jww">@JWW</a> As <a class="mention" href="/u/nicost">@nicost</a> wrote, pgFocus is not a focus drive itself but modifies voltage to a piezo positioner, which changes Z position.  Good luck with micromanager setup.</p> ;;;; <p>Some CC’s that I’ve promised: <a class="mention" href="/u/andy-sweet">@andy-sweet</a> <a class="mention" href="/u/nclack">@nclack</a> and of course <a class="mention-group notify" href="/groups/ngff">@ngff</a></p> ;;;; <p>Welcome to this forum <a class="mention" href="/u/peoples_biologist">@peoples_biologist</a>!</p>
<p>You will need to write a script to do this.  There is a tutorial here: <a href="https://nicost.github.io/I2K-MM/mm_api_b.html" class="inline-onebox">MM2API</a>. If you prefer Python, you can use PycroManager to talk to the MM API: <a href="https://github.com/micro-manager/pycro-manager/blob/92b752b604695746099eb52080d4b601122448bf/tutorial/Pycro-Manager%20acquisitions%20tutorial.ipynb" class="inline-onebox">pycro-manager/Pycro-Manager acquisitions tutorial.ipynb at 92b752b604695746099eb52080d4b601122448bf · micro-manager/pycro-manager · GitHub</a></p> ;;;; <p>Yes, you would need to use a supported camera (plenty to choose from).</p> ;;;; <p>I am trying to figure out what is going wrong with the training areas. It seems like the validationImage line is messing with the hierarchy. When I run the following code:</p>
<pre><code class="lang-auto">print "hier1" +  hierarchy
validationImage = createTrainingAreas(hierarchy, imgName)
print "hier2" +  hierarchy
</code></pre>
<p>I get the following output:</p>
<pre><code class="lang-auto">INFO: hier1Hierarchy: 902 objects
INFO: hier2Hierarchy: 1802 objects
</code></pre>
<p>and when I run the following code inside the validationImage block:</p>
<pre><code class="lang-auto">print "hier2" +  hierarchy
print "objects" + hierarchy.getAnnotationObjects()
trainingAreas = hierarchy.getAnnotationObjects().findAll{it.getPathClass() == getPathClass(testClass)}
print "trainingAreas: " + trainingAreas
</code></pre>
<p>I get:</p>
<pre><code class="lang-auto">hier2Hierarchy: 1802 objects
INFO: objects[Test (901 objects)]
INFO: trainingAreas: []
</code></pre>
<p>I think the “<code>.findAll{it.getPathClass() == getPathClass(testClass)}</code>” is what is causing the issues. But since I am not entirely sure of the syntax, I am not sure how to fix it.</p>
<p>Regarding the test project, I was not able to open it, I get errors and cannot open it:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71c19bf0a5103ef119777ddc65cdb110ff480b83.png" data-download-href="/uploads/short-url/gekL6N9ov6EhAJkYMd5vaJasC3h.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71c19bf0a5103ef119777ddc65cdb110ff480b83.png" alt="image" data-base62-sha1="gekL6N9ov6EhAJkYMd5vaJasC3h" width="549" height="500" data-dominant-color="383C3E"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">621×565 14.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <aside class="quote no-group" data-username="ChrisStarling" data-post="1" data-topic="78225">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png" class="avatar"> Chris Starling:</div>
<blockquote>
<p>These are converted to annotations. Then stardist is run on these annotations before converting them back to detections to create “cells”.</p>
</blockquote>
</aside>
<p>I believe you don’t have to convert the CellPose detection objects into annotation objects, although I have not verified this.</p>
<p>StarDist should accept any PathObjects, including detections when you run <code>stardist.detectObjects(imageData, pathObjects)</code></p>
<p>This should save you a lot of time.</p> ;;;; <p>Thanks Will.</p>
<p>Yeah, did some checking, it seems that one needs to reverse the list return from getResolutionDescriptions()</p>
<pre><code class="lang-auto">lst_resol = [(r.sizeX, r.sizeY) for r in pixels_store.getResolutionDescriptions()]
</code></pre>
<pre><code class="lang-python">len(lst_resol)
</code></pre>
<pre><code>7
</code></pre>
<pre><code class="lang-python">lst_resol.reverse()
print(lst_resol)
</code></pre>
<pre><code>[(538, 520), (1077, 1041), (2155, 2083), (4310, 4166), (8621, 8333), (17243, 16667), (34486, 33335)]
</code></pre>
<p>``python<br>
pixels_store.setResolutionLevel(3)<br>
print(“Tile size:”, pixels_store.getTileSize())<br>
resol_lvl = pixels_store.getResolutionLevel()<br>
print(“resol_lvl:”, resol_lvl, “resolution:”, lst_resol[resol_lvl])<br>
plane = pixels_store.getPlane(0,0,0)<br>
print(“plane size:”, len(plane))<br>
print(“plane sizeY:”, len(plane)/lst_resol[resol_lvl][0])</p>
<pre><code class="lang-auto">    Tile size: [1024, 1024]
    resol_lvl: 3 resolution: (4310, 4166)
    plane size: 17955460
    plane sizeY: 4166.0</code></pre> ;;;; <p><a class="mention" href="/u/nicokiaru">@NicoKiaru</a> would you have time to change the code accordingly in your repo?</p> ;;;; <p>Hi <a class="mention" href="/u/cindy_zhu">@Cindy_Zhu</a>,</p>
<p>How do you render the image?</p>
<p>If you want quite some flexibility for 3D reconstruction I would use <a href="https://github.com/bene51/3Dscript/wiki">3Dscript</a> for the rendering which allows you even animation and it adds a scale bar by default automatically. Because one problem in your different zoom versions seems to be the actual quality of the image, since the number of pixel changes. In 3Dscript you can zoom freely as needed and the scale bar even adapts to the zoom automatically.</p>
<p>I just think it does not show the text in addition, but that could be typed on top using an overlay.  This is how I did it in this example image.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42.jpeg" data-download-href="/uploads/short-url/lV9EXoEMMZd7PXVkZjDE6cSPEsi.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_500x500.jpeg" alt="image" data-base62-sha1="lV9EXoEMMZd7PXVkZjDE6cSPEsi" width="500" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_1000x1000.jpeg 2x" data-dominant-color="0B0B02"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1000×1000 72.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Okay, so:<br>
I re-downloaded Ilastik 1.4.0 for Linux, and un-compressed it from the terminal on the cluster server (which is linux-based).<br>
Then I:</p>
<ul>
<li>gave access (execution rights) to run_ilastik.sh</li>
<li>gave access (execution rights) to the file <code>ilastik-1.4.0-Linux/bin/python</code> because there was an output saying that it was denied permission<br>
This is the bash code I tried to run (doing <code>sbatch name_of_my_code.sh</code>) :</li>
</ul>
<pre><code class="lang-python">#!/bin/bash

#SBATCH --job-name=imageanalysisgast
#SBATCH --time=5:00:00
#SBATCH --cpus-per-task=5
#SBATCH --mail-type=end
#SBATCH --array=1-1%5
#SBATCH --error first_batch.err
#SBATCH --output output_file.out

srun ./Python_Ilastik/ilastik-1.4.0-Linux/run_ilastik.sh --headless \
													--readonly \
													--project="./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp" \
													--export_source="Probabilities Stage 2" \
													--ouput_format="hdf5" \
													--output_filename_format="{nickname}_{result_type}.h5" \
													--log_file="log.txt" \
													./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA1-1_1stitch.tif_z.tif.h5


echo "This is array task ${SLURM_ARRAY_TASK_ID}, the sample name is ${nickname}." &gt;&gt; output.txt
exit 0
</code></pre>
<p>And now… it is running ! I am pretty sure it was coming from something stupid - I did the unzipping on my windows desktop, then copied to the server, and I guess there were some glitch when you do it using WinZip on a Windows computer that made some files unusable.</p>
<p>Anyway, I now have my probability output <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"> !!!<br>
However, I still do not have a log.txt file, and in my .err file it’s written</p>
<pre><code class="lang-python">WARNING 2023-03-07 17:20:57,922 opConservationTracking 2050886 140187458860864 Could not find any ILP solver
WARNING 2023-03-07 17:20:57,984 opStructuredTracking 2050886 140187458860864 Could not find any ILP solver
WARNING 2023-03-07 17:20:57,986 structuredTrackingWorkflow 2050886 140187458860864 Could not find any learning solver. Tracking will use flow-based solver (DPCT). Learning for tracking will be disabled!
WARNING 2023-03-07 17:21:03,210 newAutocontextWorkflow 2050886 140187458860864 Unused command-line args: ['--ouput_format=hdf5', '--log_file=log.txt']
WARNING arraytypes.py(1271): FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
WARNING arraytypes.py(1277): FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
</code></pre>
<p>so for some reason, it doesn’t want to use my log_file argument.<br>
But now that I’m able to get this to run, maybe it’s not important <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>So, turns out my problem was likely mostly coming from a stupid unzipping bug, but thank you so so much for the help and pointers and advices to improve my coding !<br>
Hopefully I won’t run into any more trouble <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"><br>
Thanks !!!</p> ;;;; <p>I am trying to run the Find Peaks function of the BAR plugin on a profile plot, in batch mode.</p>
<p>For example with a code like this:</p>
<pre><code class="lang-auto">setBatchMode(true);

n = roiManager("count");
for (i = 0; i &lt; n; i++) {
    	roiManager("select", i);
        Roi.getBounds(x, y, width, height);
        makeLine(x, y, width, height);
        run("Plot Profile");
        run("Find Peaks", "min._peak_amplitude=1 min._peak_distance=0 min._value=[] max._value=[] exclude list");
	    peaks_idx = Table.getColumn("X1");
	    selectWindow("Plot Values");
	    run("Close");
   	    Array.sort(peaks_idx);
}
setBatchMode(false);
</code></pre>
<p>The code works without batch mode but it causes an exception. I suspects it is because the plot profile is not shown. Does anyone know a way around this?</p> ;;;; <p>Hey folks,</p>
<p>I want to set up an acquisition for taking videos (~30s with 20fps) at multiple locations (5-10 locations) every hour. As far as I know the multiD-acq is obviously able to do images easily but I don’t know how to get it to stay at a single position to acquire all the frames I want to be able to make the video before moving to the next location.</p>
<p>Any suggestions?</p> ;;;; <p>Hi <a class="mention" href="/u/christian_tischer">@Christian_Tischer</a>,</p>
<p>You need to go via the ImageProcessor. The following should work, at least it does for me on your example image:</p>
<pre><code class="lang-auto">...
ImageProcessor ip = imp.getProcessor();
ip.getMin();
ip.getMax();
...
</code></pre>
<p>The following minimal working example should show this:</p>
<pre><code class="lang-auto">import ij.ImagePlus;
import ij.WindowManager;
import ij.process.ImageProcessor;

public class Test {

	public static void main(String[] args) {

		ImagePlus imp = new ImagePlus("pat/to/the/image/"); //or WindowManager.getCurrentImage();
		ImageProcessor ip = imp.getProcessor();
		System.out.println(ip.getMin());
		System.out.println(ip.getMax());

	}

}
</code></pre> ;;;; <p>You’ll need to create new objects, and I’m not sure how easy it will be to merge the measurement lists so: <a href="https://forum.image.sc/t/transferring-segmentation-predictions-from-custom-masks-to-qupath/43408/12" class="inline-onebox">Transferring segmentation predictions from Custom Masks to QuPath - #12 by petebankhead</a></p>
<p>The function to take two ROIs to create a cell is <a href="https://qupath.github.io/javadoc/docs/qupath/lib/objects/PathObjects.html#createCellObject(qupath.lib.roi.interfaces.ROI,qupath.lib.roi.interfaces.ROI,qupath.lib.objects.classes.PathClass,qupath.lib.measurements.MeasurementList)" class="inline-onebox">PathObjects (QuPath 0.4.0)</a><br>
Which I found by searching for “createcell” in the javadocs.</p>
<aside class="quote" data-post="124" data-topic="58901">
  <div class="title">
    <div class="quote-controls"></div>
    <img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png" class="avatar">
    <a href="https://forum.image.sc/t/cellpose-in-qupath-qupath-extension-cellpose/58901/124">Cellpose in QuPath - QuPath Extension Cellpose</a> <a class="badge-wrapper  bullet" href="/c/announcements/10"><span class="badge-category-bg" style="background-color: #AB9364;"></span><span style="" data-drop-close="true" class="badge-category clear-badge" title="This category is for announcements, such as new software releases and upcoming community events.">Announcements</span></a>
  </div>
  <blockquote>
    I haven’t had the opportunity to try it yet, but I was thinking that I could use StarDist to get the nuclei and Cellpose to get the cytoplasm, get their ROIs, and then create cell objects with createCellObject(roiCell, roiNucleus, pathClass, measurements) (<a href="https://qupath.github.io/javadoc/docs/qupath/lib/objects/PathObjects.html#createCellObject(qupath.lib.roi.interfaces.ROI,qupath.lib.roi.interfaces.ROI,qupath.lib.objects.classes.PathClass,qupath.lib.measurements.MeasurementList)" rel="noopener nofollow ugc">javadocs</a>). This would probably take a loooooong time to run if you have a lot of cells in your image though.
  </blockquote>
</aside>
 ;;;; <p>Thank you for the info <a class="mention" href="/u/will-moore">@will-moore</a> ,</p>
<p>Though sub-optimal, we also implemented a work-around, depending on the initial value returned by <code>getResolutionLevel()</code> which seems to correspong to the largest pyramid size.</p> ;;;; <p>Hi,</p>
<p>I have a script that, first, uses cellpose to detect cells. These are converted to annotations. Then stardist is run on these annotations before converting them back to detections to create “cells”.</p>
<pre><code class="lang-auto">import qupath.ext.stardist.StarDist2D
import qupath.ext.biop.cellpose.Cellpose2D

var imageData = getCurrentImageData()
var pathObjects = getSelectedObjects()

pathModel = 'cyto2'
def cellpose = Cellpose2D.builder(pathModel)
        .channels('Channel 2')
        .normalizePercentiles(1,99)
        .pixelSize( 0.25 )
        .diameter(0.0)
        .measureShape()
        .measureIntensity()
        .build()

cellpose.detectObjects(imageData, pathObjects)

def detections = getDetectionObjects()
def newAnnotations = detections.collect {
    return PathObjects.createAnnotationObject(it.getROI(), it.getPathClass())
    }    
removeObjects(detections, true)
addObjects(newAnnotations)

pathModel = 'C:/Program Files/QuPath/dsb2018_heavy_augment.pb'
var stardist = StarDist2D.builder(pathModel)
        .threshold(0.5)             
        .channels('Channel 1')
        .normalizePercentiles(1, 99) 
        .pixelSize(0.5) 
	    .cellExpansion(0)
        .measureShape()
        .measureIntensity() 
        .includeProbability(true)
        .build()

selectAnnotations()
stardist.detectObjects(imageData, pathObjects)

def newDetections = newAnnotations.collect {
    return PathObjects.createDetectionObject(it.getROI(), it.getPathClass())
    }
    
removeObjects(newAnnotations, true)
addObjects(newDetections)

fireHierarchyUpdate()
println("Done!")
</code></pre>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f.jpeg" data-download-href="/uploads/short-url/oQ1q1gGCKbpM8seSiMER94f3bpt.jpeg?dl=1" title="Capture" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_322x250.jpeg" alt="Capture" data-base62-sha1="oQ1q1gGCKbpM8seSiMER94f3bpt" width="322" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_322x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_483x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_644x500.jpeg 2x" data-dominant-color="2E0E02"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Capture</span><span class="informations">1006×779 109 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Is there any way I can convert these into actual qupath cells? Or at least have the nuclei as “children” to the cellpose detections? Ideally I would want to remove the nuclear ROI from the cytoplasm so they are separate compartments. I can’t figure out how to do this… Any advice is welcome!</p> ;;;; <p>Yep, here’s an example of using a ImageJ based script that might require downsampling on large images to find objects of unique values <a href="https://forum.image.sc/t/import-external-masks-in-qupath/75516/5" class="inline-onebox">Import external masks in QuPath - #5 by sophia1</a></p>
<p>Usually intended for masks of cell objects.</p> ;;;; <p>of course this is a big image that is why I use Qupath…but I can try</p> ;;;; <p>After a vote of <a href="https://quarep.org/working-groups/wg-9-over-all-planning-funding/">WG9</a> including the other WG chairs, I’ve now made this topic public.</p>
<p>If there are no questions/comments from the image.sc <a class="mention-group notify" href="/groups/team">@team</a>, I’ll move forward with adding the appropriate logos from above.</p>
<p>~Josh</p> ;;;; <p>If the images are small enough, it might be easier to do in ImageJ, or through a script calling ImageJ.</p>
<p>There might be some other pixel level functions that might work better if you search the forum, but I don’t remember them off the top of my head.</p> ;;;; <p>So you mean doing that in two times yes correct it should work , it is a bit long with 4 channels possibilities (15 levels) …I will try thanks</p> ;;;; <p>Ok, thanks <a class="mention" href="/u/petebankhead">@petebankhead</a></p>
<blockquote>
<p>My <em>guess</em> is that at some point the import was shifted to other threads to prevent blocking, and consequently the list isn’t populated in time.</p>
</blockquote>
<p>it can explain, indeed</p>
<blockquote>
<p>I guess a workaround might be to cache a <code>Set</code> of all the entries before the import, and then remove these from the <code>Set</code> of all entries after the import, to see what is new</p>
</blockquote>
<p>Ok, I’ll probably use this option temporary</p> ;;;; <p>I think to create mid-level masks you run multiple thresholders. In your case, run 2 and above, and then run below 3 within that.</p> ;;;; <p><a class="mention" href="/u/wayne">@Wayne</a> <a class="mention" href="/u/dgault">@dgault</a> <a class="mention" href="/u/nicokiaru">@NicoKiaru</a></p>
<p>If have this image and the display settings for the first channel are such that <code>min = 10000</code> and <code>max = 17000</code>. You can see this is you open the image in Fiji.</p>
<p>My question is: how can I extract those values programatically from the ImagePlus?</p>
<p><a class="attachment" href="/uploads/short-url/sfu7Y9uYwCR30qNJSnyasrEl9rU.zip">2023_01_18–Extract_8hr_1-1.zip</a> (2.2 MB)</p>
<p>The code that is used here does not seem to return those values:</p><aside class="onebox githubblob" data-onebox-src="https://github.com/BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122">
  <header class="source">

      <a href="https://github.com/BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122" target="_blank" rel="noopener">BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122</a></h4>



    <pre class="onebox"><code class="lang-java">
      <ol class="start lines" start="112" style="counter-reset: li-counter 111 ;">
          <li>		final int numTimepoints = imp.getNFrames();</li>
          <li>		final int numSetups = imp.getNChannels();</li>
          <li>
          </li>
<li>		// create setups from channels</li>
          <li>		final HashMap&lt;Integer, BasicViewSetup&gt; setups = new HashMap&lt;&gt;(numSetups);</li>
          <li>		for (int s = 0; s &lt; numSetups; ++s) {</li>
          <li>			final BasicViewSetup setup = new BasicViewSetup(s, String.format(imp</li>
          <li>					.getTitle() + " channel %d", s + 1), size, voxelSize);</li>
          <li>			setup.setAttribute(new Channel(s + 1));</li>
          <li>			Displaysettings ds = new Displaysettings(s + 1);</li>
          <li class="selected">			imp.setPositionWithoutUpdate(s+1,1,1);</li>
          <li>			ds.min = imp.getDisplayRangeMin();</li>
          <li>			ds.max = imp.getDisplayRangeMax();</li>
          <li>			if (imp.getType() == ImagePlus.COLOR_RGB) {</li>
          <li>				ds.isSet = false;</li>
          <li>			}</li>
          <li>			else {</li>
          <li>				ds.isSet = true;</li>
          <li>				LUT[] luts = imp.getLuts();</li>
          <li>				LUT lut = luts.length&gt;s ? luts[s]:luts[0];</li>
          <li>				ds.color = new int[] { lut.getRed(255), lut.getGreen(255), lut.getBlue(</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>and it would be great to fix this.</p>
<p>Thank you for your insights!</p> ;;;; <p>Hello !! did you save your issue ?</p> ;;;; <p>Hi Dominik !</p>
<p>Don’t apologize, you are already so fast in helping out, it’s really great to have such support !<br>
So I removed the space after the equal sign, and I removed the --stack_along (I thought it was necessary for timelapse, I didn’t get that it was only if the images were in separated files).<br>
I tried direct invocation, then <code>ls .</code>, and it gave me the list of files in my main directory (which is the working directory here), where I could find the .out and .err files saved from my previous attempt for example, but no trace of the log file.<br>
Actually, the job end so quickly that it feels like it’s never really “invoking” (not sure what the right word is here…) Ilastik at all. Which would explain the total absence of log file.<br>
But then it’s strange that I do not get any error message saying for example that it cannot find Ilastik when I call it…<br>
I will try to re-download Ilastik and re-put it there, maybe some problem during copy to the server?<br>
If you have any other idea or suggestion, I’m all ears <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi Erik, if you are still having issues could you also provide some sample code showing how you are reading the data and creating the output image?</p> ;;;; <p>Hello,</p>
<p>I want to ask if there is a way to get the underlying array from a dataset object. I am reading dataset from</p>
<pre><code class="lang-java">@Parameter
private Dataset currentData;
</code></pre>
<p>using</p>
<pre><code class="lang-java">final Img&lt;T&gt; image = (Img&lt;T&gt;)currentData.getImgPlus();
</code></pre>
<p>The image has several channels, which I want to combine subsets of (typically sum), to get new images.</p> ;;;; <p>Hi <a class="mention" href="/u/jmuhlich">@jmuhlich</a>!<br>
I am trying to perform image registration of three separate multiplex OMETIFF-files which are all divided into three channels. The images have been scanned using GeoMx DSP, rather than a conventional microscope, have you hear of anyone else trying to accomplish that?</p>
<p>I have installed Ashlar in an anaconda environment, and run in through the terminal (using a mac), but I cannot get it to work. It can read the first input file, but then errors occur:<br>
WARNING: Stage coordinates undefined; falling back to (0, 0).<br>
Exception: Can’t handle non-square pixels (0.3990423, 0.398596942).</p>
<p>I know that Ashlar is only compatible with tiled, non-stitched images but I have not been able to confirm that the output data from the GeomX is in fact stitched/unstitched. Does the error messages above imply that this is the problem, or could it be something else that I could actually fix?</p>
<p>Thank you for your help!</p> ;;;; <p>Try changing to an orthographic camera. If the camera is perspective, it sort of mimics depth in the image. The  scalebar will have the length of the depth it is sitting in, which might not be the same depth as the thing you want to measure.</p> ;;;; <p>Hi David,<br>
here are 2 scanned files.<br>
<a href="https://1drv.ms/u/s!AmbIFTYxhOuig_1D0gxy30_UM-O0nA?e=UaphWP" rel="noopener nofollow ugc">Olympus VSI</a></p> ;;;; <p>Dear Qupath user,</p>
<p>I try to find a way to extract cells objects based on two thershold ?  We develop some mask colocalisation that gives the identification obtect based on the mask number (Ch1=1, Ch2=2 so Ch1+Ch2 = 3). This measure is created on each detections. I would like to extract the Ch2 object that gives 2.In fact that is a bit more complicated because I have 4 channels.  I can put a low or high threshold to extract the two others values  but not the value  in between …<br>
Thanks for your help.</p>
<p>Cheers, Mathieu</p> ;;;; <p>Continuing the discussion from <a href="https://forum.image.sc/t/stardist-error-message-version-issue/75803/4">Stardist error message - version issue?</a>:</p>
<p>Dear all,</p>
<p>I am quite new to this forum and Bioimage Analysis in general and I have trouble with the StarDist Plugin in Fiji.<br>
I have been trying to use the StarDist Plugin in Fiji for the segmentation of DAPI-stained nuclei in my IF images (1560.43x1710.88 microns, 16-bit, 176 MB, TIFF)</p>
<p>When I follow the usual path (Plugins&gt;StarDist&gt;StarDist 2D) the software either crashes completely or shows the following error message/exception:</p>
<pre><code class="lang-auto">java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)
	... 10 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	... 4 more
Caused by: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.nio.Buffer.&lt;init&gt;(Buffer.java:199)
	at java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:281)
	at java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:289)
	at java.nio.MappedByteBuffer.&lt;init&gt;(MappedByteBuffer.java:89)
	at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:162)
	at org.tensorflow.Tensor.buffer(Native Method)
	at org.tensorflow.Tensor.buffer(Tensor.java:570)
	at org.tensorflow.Tensor.writeTo(Tensor.java:473)
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:171)
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)
	at de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)
	at de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)
	... 10 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	... 4 more
Caused by: java.lang.IllegalArgumentException: Negative capacity: -991022080
	at java.nio.Buffer.&lt;init&gt;(Buffer.java:199)
	at java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:281)
	at java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:289)
	at java.nio.MappedByteBuffer.&lt;init&gt;(MappedByteBuffer.java:89)
	at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:162)
	at org.tensorflow.Tensor.buffer(Native Method)
	at org.tensorflow.Tensor.buffer(Tensor.java:570)
	at org.tensorflow.Tensor.writeTo(Tensor.java:473)
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:171)
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)
	at de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)
	at de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[INFO] CSBDeep plugin exit (took 17145 milliseconds)
[INFO] Using default TensorFlow version from JAR: TF 1.15.0 CPU
[INFO] Loading TensorFlow model GenericNetwork_aea3be563cb56b8824f53a8c2382aaa5 from source file file:/C:/Users/AG_SCH~1/AppData/Local/Temp/stardist_model_7870431095929648409.zip
[INFO] Shape of input tensor: [-1, -1, -1, 1]
[INFO] Shape of output tensor: [-1, -1, -1, 33]
[INFO] Normalize .. 
[INFO] Dataset type: 32-bit signed float, converting to FloatType.
[INFO] Dataset dimensions: [9179, 10064]
[INFO] INPUT NODE: 
[INFO] Mapping of tensor input: 
[INFO]    datasetAxes:[X, Y]
[INFO]    nodeAxes:[(Time, -1), (Y, -1), (X, -1), (Channel, 1)]
[INFO]    mapping:[2, 1, 0, 3]
[INFO] OUTPUT NODE: 
[INFO] Mapping of tensor output: 
[INFO]    datasetAxes:[X, Y]
[INFO]    nodeAxes:[(Time, -1), (Y, -1), (X, -1), (Channel, 33)]
[INFO]    mapping:[2, 1, 0, 3]
[INFO] Complete input axes: [X, Y, Time, Channel]
[INFO] Tiling actions: [TILE_WITH_PADDING, TILE_WITH_PADDING, TILE_WITHOUT_PADDING, NO_TILING]
[INFO] Dividing image into 1 tile(s)..
[INFO] Size of single image tile: [9184, 10064, 1, 1]
[INFO] Final image tiling: [1, 1, 1, 1]
[INFO] Network input size: [9184, 10064, 1, 1]
[INFO] Processing tile 1..
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)
	... 10 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	... 4 more
Caused by: java.lang.NegativeArraySizeException
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:170)
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)
	at de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)
	at de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)
	at de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)
	at de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)
	... 10 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	... 4 more
Caused by: java.lang.NegativeArraySizeException
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:170)
	at net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)
	at de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)
	at de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)
	at de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[INFO] CSBDeep plugin exit (took 41176 milliseconds)
</code></pre>
<p>I increased the number of tiles to 2 in “Advanced Options” as suggested with previous crashing problems with large images, see <a href="https://forum.image.sc/t/error-with-stardist-plugin-on-a-large-image/56465" class="inline-onebox">Error with stardist plugin on a large image</a>. The segmentation runs without crashing but is rather incomplete as shown below…</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c.jpeg" data-download-href="/uploads/short-url/4PqJsz82JmHSOkmyCSb4ywiDMcc.jpeg?dl=1" title="Screenshot StarDist tiles" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_199x374.jpeg" alt="Screenshot StarDist tiles" data-base62-sha1="4PqJsz82JmHSOkmyCSb4ywiDMcc" width="199" height="374" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_199x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_298x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_398x748.jpeg 2x" data-dominant-color="EBEBEB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot StarDist tiles</span><span class="informations">525×985 89.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e.jpeg" data-download-href="/uploads/short-url/sWYSxNUfMrcd4F7GRvxo6VWJWDY.jpeg?dl=1" title="Label Image_2tiles" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_342x374.jpeg" alt="Label Image_2tiles" data-base62-sha1="sWYSxNUfMrcd4F7GRvxo6VWJWDY" width="342" height="374" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_342x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_513x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_684x748.jpeg 2x" data-dominant-color="0F0F0F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Label Image_2tiles</span><span class="informations">1920×2105 233 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Does anyone have any ideas on how to solve this problem?</p>
<p>Thank you very much in advance</p>
<p>Theresa</p> ;;;; <p>This is a cross-post from the Scientific Python Discourse. The original post can be <a href="https://discuss.scientific-python.org/t/would-you-like-to-see-a-imageio-community-meetings-mailing-list-forum/667/1" rel="noopener nofollow ugc">found here</a>.</p>
<hr>
<p><strong>TL:DR</strong> <a href="https://github.com/imageio/imageio" rel="noopener nofollow ugc">ImageIO</a> is looking for a place to build a community for people that have an interest or stake in reading/writing image data in Python. If you are interested, head over to the <a href="https://discuss.scientific-python.org/t/would-you-like-to-see-a-imageio-community-meetings-mailing-list-forum/667/1" rel="noopener nofollow ugc">Scientific Python Discourse</a> and let us know your preferred format.</p>
<p>I am the maintainer of <a href="https://github.com/imageio/imageio" rel="noopener nofollow ugc">ImageIO</a>, a library that makes it easy to read/write image and video data and that powers many other libraries including scikit-image, napari, cellprofiler, moviepy, etc.</p>
<p>While there is a lot of interest in the library (currently ~ 13M downloads/month), it doesn’t yet have an obvious place for discussion, feedback, and community beyond GitHub Issues. I want to create this place and am currently figuring out which format to choose. If you are interested in joining (e.g., because you are actively using ImageIO, or because you are curious about the library) please do head over to the <a href="https://discuss.scientific-python.org/t/would-you-like-to-see-a-imageio-community-meetings-mailing-list-forum/667/1" rel="noopener nofollow ugc">discussion thread</a> and let me know which format would work well for you <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi Barbara,</p>
<p>thank you very much! It works perfect.<br>
Another question. For ring detection, I would like to define instead of a fest pixel number, 50% of the distance between Nuclei border to Cell border. Do you know method to do this?</p>
<p>Best<br>
Shu</p> ;;;; <p>I’m using Mac M1 Pro and Cellpose via both Jupyter and its GUI. I’d like to speed up my workflow (currently 1 image segmentation ~ 10 s) by running Cellpose on my GPU. I’m aware CUDA is designed for NVidia GPUs, though from <a href="https://forum.image.sc/t/cellpose-on-macos-m1-pro-apple-silicon-arm64/68018">this thread</a> it seems like there are ways to get Cellpose running on M1 Macs using Metal Performance Shaders (MPS). I’ve tried replicating the steps listed there, but I always end with Cellpose running on CPU.</p>
<pre><code class="lang-auto">from cellpose import models, core
core.use_gpu()
</code></pre>
<p>always gives me False.</p>
<pre><code class="lang-auto">from cellpose.io import logger_setup
logger_setup();
</code></pre>
<p>gives</p>
<pre><code class="lang-auto">2023-03-07 12:55:10,463 [INFO] WRITING LOG OUTPUT TO /Users/bocan/.cellpose/run.log
2023-03-07 12:55:10,474 [INFO] 
cellpose version: 	2.2 
platform:       	darwin 
python version: 	3.10.9 
torch version:  	1.13.1
</code></pre>
<p>Running the cellpose model itself also prints</p>
<pre><code class="lang-auto">[INFO] TORCH CUDA version not installed/working.
[INFO] &gt;&gt;&gt;&gt; using CPU
[INFO] WARNING: MKL version on torch not working/installed - CPU version will be slightly slower.
</code></pre>
<pre><code class="lang-auto">import torch
print(torch.backends.mps.is_available())
print(torch.backends.mps.is_built())
</code></pre>
<p>returns both True.</p>
<p>I also noticed that</p>
<pre><code class="lang-auto">print(*torch.__config__.show().split("\n"), sep="\n")
</code></pre>
<p>prints</p>
<pre><code class="lang-auto">... LAPACK_INFO=accelerate, TORCH_VERSION=1.13.1, USE_CUDA=0, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=OFF, USE_ROCM=OFF, 
</code></pre>
<p>so there might be problem with MKL(DNN)?<br>
This seems to be way over my beginner skills so I’d really appreciate any insight and recommendations.<br>
Thanks a ton!</p> ;;;; <p>Fixed a bug when Surfaces exceeded image borders. Updated extension can be found at:</p>
<p><a href="https://github.com/sommerc/PyImarisExportSWCWithSurfaceIntersection/blob/bb654a35dba95262695142248056c8326843c98d/xt_swc/export_surface_label_image.py" rel="noopener nofollow ugc">export_surface_label_image.py</a></p>
<p>C</p> ;;;; <p>Installed ABBA with the new windows installer, but ABBA does not start on clicking the icon. I do see a bat file run for a second. I deactivated my antivirus software and still the same.</p> ;;;; <p>Thank you for the interleaving suggestion. Looks like the issue is with non-interleaved images. So far I haven’t found a solution thought I will have to do some more searching. I am not writing the files myself, sharing any files is difficult due to their size.</p> ;;;; <p>Hi <a class="mention" href="/u/amaranth123">@amaranth123</a>,</p>
<p>The cellfinder command-line tool doesn’t support 3D tiffs. However, the napari plugin should work. How big is the image?</p>
<p>Could you try creating a new conda environment and installing everything from scratch, then try again?</p>
<p>Thanks,<br>
Adam</p> ;;;; <p>As just another mention regarding this. Here is an example without scripting and using Pivot Tables from Google Sheets. You can build the results you want simply by exporting the cell  measurements from QuPath and ordering the different columns however you want. You can choose any statistic very easily and dynamically, without the need for a script/</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c.jpeg" data-download-href="/uploads/short-url/h3oru19TxV06RzsU8FLQfSuQeUA.jpeg?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_690x311.jpeg" alt="image" data-base62-sha1="h3oru19TxV06RzsU8FLQfSuQeUA" width="690" height="311" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_690x311.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_1035x466.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_1380x622.jpeg 2x" data-dominant-color="BEBAC1"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1920×867 268 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>The example Sheet Pivot Table is here</p><aside class="onebox googledocs" data-onebox-src="https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing">
  <header class="source">

      <a href="https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing" target="_blank" rel="noopener">docs.google.com</a>
  </header>

  <article class="onebox-body">
    <a href="https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing" target="_blank" rel="noopener"><span class="googledocs-onebox-logo g-sheets-logo"></span></a>

<h3><a href="https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing" target="_blank" rel="noopener">QuPath Pivot Table Example</a></h3>

<p>measurements

Image,Object ID,Name,Class,Parent,ROI,Centroid X 碌m,Centroid Y 碌m,Nucleus: Area 碌m^2,Nucleus: Length 碌m,Nucleus: Circularity,Nucleus: Solidity,Nucleus: Max diameter 碌m,Nucleus: Min diameter 碌m,Cell: Area 碌m^2,Cell: Length 碌m,Cell:...</p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>Disclaimer: I generally use Excel which has the same functionality but is much much faster. Google Sheets was used as an example. It’s not really built to handle all the results QuPath can produce.</p> ;;;; <p>Hello <a class="mention" href="/u/gabriel_gibson">@Gabriel_Gibson</a>,</p>
<p>I have good news. Thanks to <a class="mention" href="/u/nicokiaru">@NicoKiaru</a>, we found out that <code>MosaicJ</code> plugin is still working with the old Fiji layout called <code>Metal</code> but breaks with the new <code>FlatLaf</code> layout.</p>
<p>As a temporary fix, you can open your Fiji and go on <code>Edit -&gt; Options -&gt; Look and feel...</code> and select <code>Metal</code> in the drop-down menu. Then, close Fiji and open it again. MosaicJ should work as expected.</p>
<p>I open an <a href="https://github.com/fiji-BIG/MosaicJ/pull/1" rel="noopener nofollow ugc">Pull Request</a> on Github to make the code working on FlatLaf layout as well. I hope the new working version will be released soon.</p>
<p>Rémy.</p> ;;;; <p>Deleting .jgo folder seems to have done it. Now Abba initializes with no problems, executes deepslice cell, but does not apply the AP coordinates and now is “locked” after the registration run. It has been running for a few hours.</p>
<p>Not sure what I broke this time… <img src="https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12" title=":sweat_smile:" class="emoji" alt=":sweat_smile:" loading="lazy" width="20" height="20"></p>
<p>See screenshot attached.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd.png" data-download-href="/uploads/short-url/2D8Icgv276qOkbbpSZSb8YxE24J.png?dl=1" title="abba" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_690x456.png" alt="abba" data-base62-sha1="2D8Icgv276qOkbbpSZSb8YxE24J" width="690" height="456" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_690x456.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_1035x684.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_1380x912.png 2x" data-dominant-color="080808"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">abba</span><span class="informations">1541×1020 65.9 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>THis is a pytorch update issue; you can install the latest from master, or you can wait until tomorrow and a new pypi version will be up!</p> ;;;; <p>Okay, sorry, I didn’t have time to properly read your last replies before.</p>
<p>In your direct invocation, you have <code>--log_file= "log.txt" \</code> with a space after the eqals sign, that will not be interpreted correctly, I think (so that should be removed).<br>
The <code>--stack-along</code> argument is not needed (doesn’t make sense without a “pattern” and I think your data is a single dataset in the hdf5 file).</p>
<p>could you maybe do an <code>ls ./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/</code> <em>at the end</em> of your script, to verify that no output has been written? (that is for the python version, for the direct invocation, could you <code>ls .</code> (file is written to the working directly I think).</p> ;;;; <p>Hi <a class="mention" href="/u/folterj">@folterj</a> - apologies for delayed response…</p>
<p>I’m afraid that appears to be the way that the API works, even though this may not be the expected behaviour - and I see that the documentation at <a href="https://docs.openmicroscopy.org/omero-blitz/5.5.12/slice2html/omero/api/PyramidService.html#setResolutionLevel" class="inline-onebox">OMERO API Index - omero::api::PyramidService</a> doesn’t give any clues about how this works.</p>
<p>Another example of this usage, where I’m handling that “inverse” relationship is at <a href="https://github.com/ome/omero-figure/blob/13ccea055a45452f1c02272682dd75b142b1d41e/omero_figure/views.py#L207" class="inline-onebox">omero-figure/views.py at 13ccea055a45452f1c02272682dd75b142b1d41e · ome/omero-figure · GitHub</a></p> ;;;; <p>The images are stored in a 3D stacked tiff. And when I run it on cellfinder, I just meet this error and don’t konw how to process(Fig1):</p>
<p>I also tried it on napari with cell detection plugin, but it was just stuck like this(Fig2):<br>
Thanks a lot for your help!<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218.png" data-download-href="/uploads/short-url/xlysw4FhbGzkfWhkqeFu3vza65a.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_690x250.png" alt="image" data-base62-sha1="xlysw4FhbGzkfWhkqeFu3vza65a" width="690" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_690x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_1035x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_1380x500.png 2x" data-dominant-color="151616"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3516×1276 371 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b.png" data-download-href="/uploads/short-url/lf0b5IIoQCCp2gVqfNaARwMFsJd.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_690x190.png" alt="image" data-base62-sha1="lf0b5IIoQCCp2gVqfNaARwMFsJd" width="690" height="190" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_690x190.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_1035x285.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_1380x380.png 2x" data-dominant-color="222222"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3815×1055 196 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/vipul">@Vipul</a>,</p>
<p>Please try the following procedure:</p>
<p>Use the threshold-adjuster (Image&gt;Adjust&gt;Threshold…), select Stack Histogram to set your min/max threshold values, press “apply” and deselect “Calculate threshold for each image”.</p>
<p>Do you get what you want that way?</p>
<p>Best,<br>
Volker</p> ;;;; <p>Good day to everyone.<br>
I am currently conducting a project to analyze the integral and differential uniformity values of gamma camera images acquired via a QC test. Is there a way to produce these results in ImageJ without using plugins? I am hoping to obtain the values for:</p>
<p>Integral uniformity<br>
X differential uniformity<br>
Y differential uniformity</p>
<p>This is my first time using ImageJ. Thank you in advance.</p> ;;;; <p>Closing is maximum filter fiollowed by minimum filter of the same size.</p> ;;;; <p>Have you noticed that <code>regionprops</code> has an <code>extra_properties=</code> keyword argument that lets you define arbitrary properties for each region? So your imagination is the limit! <img src="https://emoji.discourse-cdn.com/twitter/blush.png?v=12" title=":blush:" class="emoji" alt=":blush:" loading="lazy" width="20" height="20"> If you’re asking for what sorts of things you can pass to this argument, then you might get more answers if you can give us more details about what it is you are trying to accomplish with the region features and why those provided by regionprops are insufficient for that goal.</p> ;;;; <p>Thank you very much for your help. I really appreciate it.</p> ;;;; <p>Hi <a class="mention" href="/u/dam">@Dam</a> you could try setting the property directly through a script:</p>
<pre><code class="lang-groovy">qupath.lib.gui.prefs.PathPrefs.userPathProperty().set('/path/to/user/path/')
</code></pre>
<p>This should then be retained in the preferences. Note that you need to give the user path; ‘extensions’ is a subdirectory inside that.</p>
<p>I haven’t tried it in a scenario like the one you describe, but it’s my best guess at a workaround.</p> ;;;; <p>Did you try reuploading the video or clearing your browser cache? Also, try uploading it from mobile and it might solve your issue. Refer to this article if you need more info or help <a href="https://geekydane.com/google-drive-still-processing-video/" rel="noopener nofollow ugc">https://geekydane.com/google-drive-still-processing-video/</a></p> ;;;; <p>Could you explain how do you do that with ImageJ (morphology plugin ?)  on the ellipse ROI ? This image is just an exemple/simulation on what I want do on a real image containing a lot of folicles (ellipse) with marginal zone (ROI inside in white)  that I want to supplement, the distance between the marginal  zone and the folicles will be different for each object…so the radius you utilize have to be adapted automatically for each object… not a  simple problem isn’t it ?<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/9/e9dc84e9491696381ab30fefd4a202fae6b033bf.jpeg" alt="Clipboard" data-base62-sha1="xmPI4gRXoWfDeov11fFmFkLFMHJ" width="587" height="417"></p> ;;;; <p>Are you writing the file yourself ? How do you set and read the tile size ? Do you read the ‘isinterleaved’ property ?</p>
<p>Can you share a file ?</p> ;;;; <p>Hello,everyone:</p>
<p>I am trying to use cellfinder to analyze my wholebrain images, they are stored in a 3D stacked tiff. And when I run cellfinder, I just meet this error and don’t konw how to process:<br>
Thanks a lot for your help!</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/3/839d3dea3f19d2da1d07ba7e5110e897ebc089fd.png" data-download-href="/uploads/short-url/iMjr8IeToxtBwnlHExS8UCToQ57.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/839d3dea3f19d2da1d07ba7e5110e897ebc089fd_2_690x250.png" alt="image" data-base62-sha1="iMjr8IeToxtBwnlHExS8UCToQ57" width="690" height="250" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/839d3dea3f19d2da1d07ba7e5110e897ebc089fd_2_690x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/839d3dea3f19d2da1d07ba7e5110e897ebc089fd_2_1035x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/839d3dea3f19d2da1d07ba7e5110e897ebc089fd_2_1380x500.png 2x" data-dominant-color="151616"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">3516×1276 411 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
(cellfinder) C:\Users\ps&gt;cellfinder -s G:/cz/retryno70/No.70.tiff -b G:/cz/retryno70bg/no70bg.tiff -o G:/cz/output -v 5 5 5 --orientation sal --atlas allen_mouse_50um<br>
2023-03-07 15:06:45 PM INFO     2023-03-07 15:06:45 PM - INFO - MainProcess fancylog.py:321 - Starting   fancylog.py:321                                logging<br>
INFO     2023-03-07 15:06:45 PM - INFO - MainProcess fancylog.py:322 - Not        fancylog.py:322                                logging multiple processes<br>
2023-03-07 15:06:47 PM INFO     2023-03-07 15:06:47 PM - INFO - MainProcess main.py:54 - Registering to atlas main.py:542023-03-07 15:06:50 PM INFO     2023-03-07 15:06:50 PM - INFO - MainProcess main.py:52 - Loading raw image    main.py:52                                data<br>
100%|██████████████████████████████████████████████████████████████████████████████| 1233/1233 [02:33&lt;00:00,  8.06it/s]<br>
filtering: 100%|█████████████████████████████████████████████████████████████████| 228/228 [00:00&lt;00:00, 356.16plane/s]<br>
filtering: 100%|█████████████████████████████████████████████████████████████████| 225/225 [00:00&lt;00:00, 649.39plane/s]<br>
2023-03-07 15:09:30 PM INFO     2023-03-07 15:09:30 PM - INFO - MainProcess run.py:87 - Registering            run.py:87                       INFO     2023-03-07 15:09:30 PM - INFO - MainProcess run.py:105 - Starting affine      run.py:105                                registration<br>
2023-03-07 15:09:42 PM INFO     2023-03-07 15:09:42 PM - INFO - MainProcess run.py:108 - Starting freeform    run.py:108                                registration<br>
2023-03-07 15:09:43 PM INFO     2023-03-07 15:09:43 PM - INFO - MainProcess run.py:111 - Starting             run.py:111                                segmentation<br>
INFO     2023-03-07 15:09:43 PM - INFO - MainProcess run.py:114 - Segmenting           run.py:114                                hemispheres<br>
INFO     2023-03-07 15:09:43 PM - INFO - MainProcess run.py:117 - Generating inverse   run.py:117                                (sample to atlas) transforms<br>
INFO     2023-03-07 15:09:43 PM - INFO - MainProcess run.py:120 - Transforming image   run.py:120                                to standard space<br>
2023-03-07 15:09:44 PM INFO     2023-03-07 15:09:44 PM - INFO - MainProcess run.py:126 - Generating           run.py:126                                deformation field<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess registration.py:277 -    registration.py:277                                Generating deformation field<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess run.py:129 - Exporting images as  run.py:129                                tiff<br>
pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess batteryrunners.py:268  batteryrunners.py:268                                - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess batteryrunners.py:268  batteryrunners.py:268                                - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess batteryrunners.py:268  batteryrunners.py:268                                - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess batteryrunners.py:268  batteryrunners.py:268                                - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess run.py:179 - Saving additional    run.py:179                                downsampled images<br>
INFO     2023-03-07 15:09:44 PM - INFO - MainProcess run.py:181 - Processing:          run.py:181                                channel_0<br>
100%|██████████████████████████████████████████████████████████████████████████████| 1233/1233 [02:32&lt;00:00,  8.07it/s]<br>
2023-03-07 15:12:21 PM INFO     2023-03-07 15:12:21 PM - INFO - MainProcess run.py:226 - Transforming to      run.py:226                                standard space<br>
pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:12:21 PM - INFO - MainProcess batteryrunners.py:268  batteryrunners.py:268                                - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1<br>
INFO     2023-03-07 15:12:21 PM - INFO - MainProcess run.py:241 - Deleting             run.py:241                                intermediate niftyreg files<br>
INFO     2023-03-07 15:12:21 PM - INFO - MainProcess main.py:92 - Calculating volumes  main.py:92                                of each brain area<br>
WARNING  2023-03-07 15:12:21 PM - WARNING - MainProcess volume.py:91 - Atlas value:  volume.py:91                                526322264 not found in registered atlas. Setting registered volume to 0.<br>
INFO     2023-03-07 15:12:21 PM - INFO - MainProcess main.py:104 - Generating         main.py:104                                boundary image<br>
INFO     2023-03-07 15:12:21 PM - INFO - MainProcess main.py:107 - brainreg           main.py:107                                completed. Results can be found here: G:/cz/output\registration<br>
2023-03-07 15:12:23 PM WARNING  2023-03-07 15:12:23 PM - WARNING - MainProcess prep.py:259 - Registered      prep.py:259                                atlas exists, assuming already run. Skipping.<br>
INFO     2023-03-07 15:12:23 PM - INFO - MainProcess main.py:121 - Detecting cell     main.py:121                                candidates<br>
Traceback (most recent call last):<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\runpy.py”, line 197, in _run_module_as_main<br>
return _run_code(code, main_globals, None,<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\runpy.py”, line 87, in <em>run_code<br>
exec(code, run_globals)<br>
File "C:\Users\ps.conda\envs\cellfinder\Scripts\cellfinder.exe_<em>main</em></em>.py", line 7, in <br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder\main.py”, line 96, in main<br>
run_all(args, what_to_run, atlas)<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder\main.py”, line 123, in run_all<br>
signal_array = read_with_dask(<br>
File “C:\Users\ps.conda\envs\cellfinder\lib\site-packages\cellfinder_core\tools\IO.py”, line 41, in read_with_dask<br>
shape, dtype = get_tiff_meta(filenames[0])<br>
IndexError: list index out of range</p> ;;;; <p>Hello team.<br>
I installed qupath on a distant server (calculation cluster), and I am running it through command line, with the help of this documentation: <a href="https://qupath.readthedocs.io/en/stable/docs/advanced/command_line.html" class="inline-onebox" rel="noopener nofollow ugc">Command line — QuPath 0.4.3 documentation</a>.</p>
<p>It works perfectly, and confirm that your work on this softawre is amazing.</p>
<p>But I am kind of stuck, I can’t use extensions. I didn’t find a way to set the extension folder path, and I have the import error ‘import.ext.StarDist.StaDist2D’.<br>
I tried to open the distant program on my local machine to set this parameter (the absolute path on distant server), but it just set it on my local machine. I tried to put my extension folder (or directly files inside) on the QuPath app folder. Nothing worked.<br>
I don’t see a way to set this parameter as a command line argument.</p>
<p>Do you see a way to set it modifying a file or something?</p>
<p>Thank you very much for your solid work.<br>
Have a nice day.</p> ;;;; <p><strong>2nd</strong> <strong>BigDataViewer &amp; ImgLib2</strong> <strong>community meeting 20th March 17:00 CET</strong></p>
<p>March is busy times for all of us and it has been hard to find a slot. However, we finally converged on 20th of march. This time we have an impressive line-up of speakers: Mark Kittisopikul (ByteBuffer-backed images) and Curtis Rueden (<em>Appose</em>) will present their work and Tobias Pietzsch will give a brief update.</p>
<p>Looking forward to meet you all.</p>
<p><strong>DATE</strong>: Monday 20th March<br>
<strong>TIME</strong>: 17:00 CET<br>
<strong>LOCATION</strong>: Zoom<br>
<strong>AGENDA</strong>: Mark Kittisopikul and Curtis Rueden will present their work. Tobias Pietzsch will give a brief update + general discussion</p>
<p>If you registered for the first meeting you will get an invitation and link by email.</p>
<p>Others please register here: <a href="https://forms.gle/e9iPbafFTvcmc7Sf8" rel="noopener nofollow ugc">https://forms.gle/e9iPbafFTvcmc7Sf8</a><br>
(Please also register if you want info on future events but not able to join on this occasion)</p> ;;;; <p>I’m having trouble opening tiles of RGB24 format .ome.tif pyramidal files. This problem only occurs with RGB images of some formats like ome.tif while, 8 bit, &amp; 16 bit images open up fine. What is the format bytes are returned by the OpenBytes() function for RGB tiles? Attached an image showing the incorrect image when using the bytes directly to create a bitmap.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3.jpeg" data-download-href="/uploads/short-url/1inMOrtFYVMNMFXVkjcWVzA9A1d.jpeg?dl=1" title="Capture" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_690x335.jpeg" alt="Capture" data-base62-sha1="1inMOrtFYVMNMFXVkjcWVzA9A1d" width="690" height="335" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_690x335.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_1035x502.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_1380x670.jpeg 2x" data-dominant-color="656565"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Capture</span><span class="informations">1911×928 398 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I deleted the <code>selectImage(z+1);</code> line, which seemed to be causing the skipping of each file. But now when I process an Image, it doesn’t perform the macro on all of the slices, say if I have 31 Images, it will only save the last 15? It seems like another issue now instead of saving every 2nd Image results, it is only saving half of the images. It won’t even perform the analyze particles section on the first 14 images, it just says the macro is complete.</p> ;;;; <p>Hello</p>
<p>I want to reduce the size of the mesh file (.stl).</p>
<p>so I used imagej-mesh on github to make code to reduce file size.</p>
<p>The codes I used</p>
<ul>
<li>github</li>
</ul>
<p>[imagej-mesh]</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/imagej/imagej-mesh">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/imagej/imagej-mesh" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/7/d769809e7b0d2a6df8aa3217f759520e3ee07b6f_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/7/d769809e7b0d2a6df8aa3217f759520e3ee07b6f_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/7/d769809e7b0d2a6df8aa3217f759520e3ee07b6f_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d769809e7b0d2a6df8aa3217f759520e3ee07b6f.png 2x" data-dominant-color="F3F3F1"></div>

<h3><a href="https://github.com/imagej/imagej-mesh" target="_blank" rel="noopener nofollow ugc">GitHub - imagej/imagej-mesh: Data structures and I/O for meshes</a></h3>

  <p>Data structures and I/O for meshes. Contribute to imagej/imagej-mesh development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
<p>
[imagej-mesh-io]</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://github.com/imagej/imagej-mesh-io">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/imagej/imagej-mesh-io" target="_blank" rel="noopener nofollow ugc">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/345;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/76e569531e0697a6c49d6176be9b36d3719b4a11_2_690x345.png" class="thumbnail" width="690" height="345" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/76e569531e0697a6c49d6176be9b36d3719b4a11_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/76e569531e0697a6c49d6176be9b36d3719b4a11_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/76e569531e0697a6c49d6176be9b36d3719b4a11.png 2x" data-dominant-color="F2F2F0"></div>

<h3><a href="https://github.com/imagej/imagej-mesh-io" target="_blank" rel="noopener nofollow ugc">GitHub - imagej/imagej-mesh-io: I/O plugins for ImageJ meshes and related...</a></h3>

  <p>I/O plugins for ImageJ meshes and related data structures - GitHub - imagej/imagej-mesh-io: I/O plugins for ImageJ meshes and related data structures</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>The code I wrote</p>
<ul>
<li>Main.java</li>
</ul>
<pre><code class="lang-auto">package demo;

import java.io.IOException;

import net.imagej.mesh.Mesh;
import net.imagej.mesh.SimplifyMesh;
import net.imagej.mesh.io.stl.STLMeshIO;

public class Main {
	public static void main(String[] args) {
		try {

			STLMeshIO io = new STLMeshIO();
			Mesh ms = io.open("C:\\photo\\body.stl");
			SimplifyMesh sm = new SimplifyMesh(ms);
			Mesh ms_simplify = sm.simplify(0.50f, 10);
			io.save(ms_simplify, "C:\\photo\\body-simplify.stl");

		} catch (IOException e) {
			e.printStackTrace();
		}
	}
}
</code></pre>
<p>But the file size is the same as before.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/662021a1b13e5ed738baf176e66aa9d1d9a89e4c.png" data-download-href="/uploads/short-url/ezrxFOvTYQErOoz1T97Io6x0L9i.png?dl=1" title="sameFileSize" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/662021a1b13e5ed738baf176e66aa9d1d9a89e4c.png" alt="sameFileSize" data-base62-sha1="ezrxFOvTYQErOoz1T97Io6x0L9i" width="690" height="474" data-dominant-color="F3F2F2"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">sameFileSize</span><span class="informations">842×579 21.4 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Do you know why?</p> ;;;; <aside class="quote no-group quote-modified" data-username="Mm1" data-post="1" data-topic="78176">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/m/47e85d/40.png" class="avatar"> Maddie:</div>
<blockquote>
<pre><code class="lang-auto">for(z=0; z&lt;nImages; z++) // It goes through all the images
{
			selectImage(z+1);    // it saves slice 1, 3, 5, 7 etc.
</code></pre>
</blockquote>
</aside>
<p>Your problem is here.</p> ;;;; <p>Yes. To my limited knowledge, the focus mechanism is an encoder. After I replied here, I just got the Z- drive focusing working with Micromanager, but not for automated stacking of images. From my understanding, Micromanager doesn’t support DSLR camera control.</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://pbase.com/smokedaddy/leica_dm_irbe">
  <header class="source">

      <a href="https://pbase.com/smokedaddy/leica_dm_irbe" target="_blank" rel="noopener nofollow ugc">PBase</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:400/225;"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1263f6e3a57c4f79ab8da461464f9d826788ae3b.png" class="thumbnail" width="400" height="225"></div>

<h3><a href="https://pbase.com/smokedaddy/leica_dm_irbe" target="_blank" rel="noopener nofollow ugc">Leica DM IRBE by Squatting Dog</a></h3>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>-JW:</p> ;;;; <p>Thanks, this is helpful.</p> ;;;; <p>I have a folder of multiple Tif images, that I wish to analyze the particles, using analyze particles function. I need one channel, with all of its z-stack slices to be analysed and saved into a folder. My problem is that only every second summary and results table is being saved. So if my image has 10 z-stack slices, it saves slice 1, 3, 5, 7 etc. I must just be missing something small!</p>
<pre><code class="lang-auto">// Choose input and output folders
dir1 = getDirectory("Choose Source Directory ");
resultsDir = dir1+"results/";
File.makeDirectory(resultsDir);
dir2 = getDirectory("Choose Destination Directory ");
list = getFileList(dir1);

processFolder(dir1);
function processFolder(dir1){
	list = getFileList(dir1);
	list = Array.sort(list);
	for (i = 0; i &lt; list.length; i++) {
		if(File.isDirectory(dir1 + File.separator + list[i]))
			processFolder(dir1 + File.separator + list[i]);
		if(endsWith(list[i], ".tif"))
			processFile(dir1, dir2, list[i]);
	}
}	 

function processFile(dir1, dir2, file){
	open(dir1 + File.separator + file);
	
	
// Split channels and rename	 
title = getTitle();
run("Split Channels");
selectWindow("C2-" + title);
rename("live");
selectWindow("C3-" + title);
rename("dead");
selectWindow("C4-" + title);
rename("total");



//Apply pre-processing filters and threshold live cells
selectWindow("live");
run("Duplicate...", "duplicate"); // Duplicates live channel so accurate thresholding can be done in the following step
rename("duplicate");
selectWindow("live");
run("Gaussian Blur...", "sigma=2 stack");
run("Threshold...");
waitForUser("Adjust threshold, press ok on this pop-up when the threshold has been set. Do not press anything on the threshold screen when finished. Just press ok on action required screen");
run("Make Binary", "method=Default background=Dark calculate black");
//run("Auto Local Threshold", "method=Phansalkar radius=8 parameter_1=0 parameter_2=0 white stack");
run("Fill Holes", "stack");
//run("Watershed", "stack");
run("Stack to Images"); //Makes z-slices individual images



//For every image that is binary, rename the slice and analyze particles
for(z=0; z&lt;nImages; z++){
			selectImage(z+1);
		if(is("binary")){
			name = getTitle();
			rename(name + "_" + title);
			run("Analyze Particles...", "size=0.50-Infinity show=[Overlay Masks] display clear summarize overlay add");
			selectWindow("Summary");
			saveAs("Results", dir2 + "Live_Summary_" + z + "_" + title + ".csv");
			selectWindow("Results");
			saveAs("Results", dir2  + name +"_" + z + "_" + title + ".csv"); 
			close();
		}
	}		
		close("live*");
		close("duplicate");
		run("Close");
		close();
		// Leave the print statements until things work, then remove them.
	print("Processing: " + dir1 + File.separator + file);
	print("Saving to: " + dir2);
}
</code></pre> ;;;; <p>Why not make a Config Group in the User Interface?  Or better yet, create the group “System”, add those properties, and make a group “Startup” in which you set them to the intended Physical Shutters.  Then save the config file.  See: <a href="https://micro-manager.org/Micro-Manager_Configuration_Guide#startup-presets" class="inline-onebox">Micro-Manager Configuration Guide</a>.</p> ;;;; <p>Hi, I have tried that, and I did get the <code>Device,FooMyShutter,Utilities,Multi Shutter</code> line for the shutter from doing this. I have tried loading my config file, going to the Property browser and setting the <code>Physical shutter</code> fields - that does work -  but when I save this config in a new file I see no changes related to the physical shutters.<br>
I could potentially still set on my channels the right shutter to use, but I would still have to go the Property browser each time to set the <code>Physical Shutter 1</code> and 2 fields.<br>
Thanks for your help!</p> ;;;; <p>Welcome to this forum <a class="mention" href="/u/evanj">@EvanJ</a>!</p>
<p>Is the camera switched on and connected to the computer?  Does the Andor software work with the camera?  If all that checks out, you may want to contact Andor support.</p> ;;;; <p>Did you upload the AOTFController sketch (<a href="https://github.com/micro-manager/mmCoreAndDevices/tree/main/DeviceAdapters/Arduino/AOTFcontroller" class="inline-onebox">mmCoreAndDevices/DeviceAdapters/Arduino/AOTFcontroller at main · micro-manager/mmCoreAndDevices · GitHub</a>) to your Arduino before connecting with Micro-Manager?</p> ;;;; <p>Hi <a class="mention" href="/u/pcamello">@pcamello</a>,</p>
<p>No programming needed.  The State Device Shutter Device has one property: set it in the group System &gt; Startup to the State Device of your choice.  Hopefully your State Device has a property “ClosedPosition” or similar.</p> ;;;; <p>Why not add these in the System Startup group using the UI, and save the config file?  If you really want to edit it by hand, you can then copy the lines with the properties you are interested in.</p> ;;;; <p>Hi <a class="mention" href="/u/pcamello">@pcamello</a>!</p>
<p>Yes, you can do such a thing.  You will need the Utilities &gt; State Device Shutter, and set it up to use your Optoscan as the state device (if it is a state device).  There may be code missing in the Optoscan adapter (or it may not be written as a State Device), so please ask again if you get stuck.</p> ;;;; <p>You can write a script (either in Beanshell or in Python - using Pycromanager) that will do this.</p> ;;;; <p>HI <a class="mention" href="/u/jww">@JWW</a>, the pgfocus is a module for auofocussing, i.e. it does not include a focus drive.  Does you IRBE have a motorized Z drive?</p> ;;;; <p>Quite sure that you can write a script (either in Beanshell or in Python - using Pycromanager) that will do this.</p> ;;;; <p>Since this seems to be tied to the PVCAM camera that you are using, you may want to ask Photometrics for support.</p> ;;;; <p>Looks good.  You could ask them to add the code for their adapter to the Micro-Manager source code repository, so that newer version of Micro-Manager will still work with their adapter.  Otherwise, they will need to compile versions for each hardware version of MM (I believe that we are at v72).</p> ;;;; <p>Welcome to this forum <a class="mention" href="/u/uwcheme_jesse">@uwcheme_jesse</a> !</p>
<p>Probably an issue with your configuration.  Please start a new thread and post the config file you are using.</p> ;;;; <p>Ok, two other things to test:</p>
<p>Option 1: delete your <code>.jgo</code> folder, somewhere in your root user profile folder and re-start the notebook.<br>
Option 2: if you’re interested in DeepSlice, but less in the python scripting, you can try the new windows installer: <a href="https://docs.google.com/presentation/d/1S6U1-yMAoaHALiIuvs90rlktF-OCec8T1xMkt7ej378/edit#slide=id.g21680bf206c_0_0" class="inline-onebox">Aligning Sections and Atlases with ABBA - Installation - Google Slides</a>. DeepSlice is included into ABBA directly, no need to make the complicated forth and back DnD on the web interface.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486.png" data-download-href="/uploads/short-url/nufklJjge3SkHoMBHNLVcU7yH42.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486_2_690x324.png" alt="image" data-base62-sha1="nufklJjge3SkHoMBHNLVcU7yH42" width="690" height="324" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486_2_690x324.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486.png 2x" data-dominant-color="676565"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">807×380 62.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hello <a class="mention" href="/u/paschamber">@paschamber</a> and welcome to the forum!</p>
<blockquote>
<p>I was wondering if it was possible to import registrations made using QuickNii into ABBA.</p>
</blockquote>
<p>Not right now, but part of the code is already there: I’ve written this for DeepSlice compatibility:</p>
<aside class="onebox githubfolder" data-onebox-src="https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" class="site-icon" width="32" height="32">

      <a href="https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h3><a href="https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii" target="_blank" rel="noopener">ijp-imagetoatlas/src/main/java/ch/epfl/biop/quicknii at master ·...</a></h3>

  <p><a href="https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii" target="_blank" rel="noopener">master/src/main/java/ch/epfl/biop/quicknii</a></p>

  <p><span class="label1">2D sections to 3D atlas registration with Fiji and QuPath - ijp-imagetoatlas/src/main/java/ch/epfl/biop/quicknii at master · BIOP/ijp-imagetoatlas</span></p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>If you have a demo dataset to share, I could try to see how feasible it is. (Note that ABBA is working optimally with multiresolution file formats. I do not know if quicknii is supporting that.)</p>
<blockquote>
<p>and spline transforms, and ideally export back into VisuAlign for finetuning</p>
</blockquote>
<p>Exporting splines transform for visualign is going to be more complex I’m afraid.</p>
<blockquote>
<p>(I need very precise registrations)</p>
</blockquote>
<p>Did you have a look at BigWarp ? You can manually edit the spline registration already in ABBA, if that helps: next to last timestamp on youtube tutorial (<a href="https://www.youtube.com/watch?v=sERGONVw4zE&amp;t=2534s" class="inline-onebox">Aligning Serial Brain Sections to the Allen Brain Atlas with ABBA - YouTube</a>). I believe it’s more of less the same functionality than VisuAlign.</p>
<p>I opened an issue a while ago about compatibility with Quicknii, but closed it because nobody seemed to be interested so far. I’ve reopened it (<a href="https://github.com/BIOP/ijp-imagetoatlas/issues/75" class="inline-onebox">Make QuickNII exporter for sending DeepSlice corrected dataset · Issue #75 · BIOP/ijp-imagetoatlas · GitHub</a>).</p>
<p>It’s not on my priority list right now, but maybe I find some time in the coming months to look at it. If you can share a simple quicknii-like dataset, that would make things easier.</p>
<p>Cheers,</p>
<p>Nicolas</p> ;;;; <p>Hello, I was trying to reproduce the steps here and I managed to bult MPS on my M1 Mac, though cellpose is still running from CPU. Setting <code>gpu=True</code> in <code>models.CellposeModel()</code> just leads to</p>
<pre><code class="lang-auto">TORCH CUDA version not installed/working.
&gt;&gt;&gt;&gt; using CPU
</code></pre>
<p>so I must be missing something. Or do I get it all wrong and cellpose still does not run on M1 GPU even with MPS?<br>
The architecture I use is macOS-13.2.1-arm64-arm-64bit.<br>
Thanks and sorry, this has been a bit confusing to me. I really miss some simplistic youtube tutorials for us beginners.</p> ;;;; <p>Aside from the above information about the lookup table (The calculation would be slower), it’s the standard equation for OD from other fields, log(incominglight/outgoinglight)<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/326dcf441f9da4f09db15d7c1333e41cbbefa175.png" data-download-href="/uploads/short-url/7c78Ex0ZrjcMnWAGyj4Dpighh0V.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/326dcf441f9da4f09db15d7c1333e41cbbefa175_2_345x175.png" alt="image" data-base62-sha1="7c78Ex0ZrjcMnWAGyj4Dpighh0V" width="345" height="175" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/326dcf441f9da4f09db15d7c1333e41cbbefa175_2_345x175.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/326dcf441f9da4f09db15d7c1333e41cbbefa175_2_517x262.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/326dcf441f9da4f09db15d7c1333e41cbbefa175_2_690x350.png 2x" data-dominant-color="EFEFEF"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">889×451 36 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
As mentioned, it can be rewritten a variety of ways, but roughly speaking, it’s a logarithmic function where the value gets larger as the amount of light blocked increases. Thus, it is very dependent on the estimation of the amount of incoming light, or background, as set in QuPath.</p>
<p>If the background is assumed to be bright, 255,255,255 (which is the default), but isn’t, your optical densities will be incorrectly high.</p>
<p>Measurements are also impacted by the thickness of the tissue slice and any residual coloration of the tissue after fixation, much like fluorescence, so it can be a little tricky to read too much into it between samples.</p> ;;;; <p>Yea, this is probably easier than what I said…</p> ;;;; <p>What exactly is the optical density as it is used in QuPath? What is the definition of the optical density as it is used in QuPath? Thank you for your help.</p> ;;;; <p>Hello everyone,<br>
I am using FiJi’s Analyze-&gt;Local Thickness, then Histogram to measure the trabecular thickness of a 2D radiographic image. Is there a similar way that you are aware of to measure the trabecular separation/spacing as well?<br>
Also, there is a BoneJ command as Slice Geometry which gives the option to measure the mean 2D thickness of a binary image.<br>
However, the results of the two methods are different.<br>
Is there any chance that someone knows which should be used to measure trabecular thickness?<br>
Thank you for your help</p> ;;;; <p>CellPose or DeepCell might also be options, since they take a cytoplasmic channel into account.</p> ;;;; <p>Hello All,</p>
<p>I am new to imagej and I am trying to create a binary stack from the image that I threshold. If I do it only for one image it works well, but when I try converting the stack to binary the image is converted to white and only the background is left black. I don’t understand why this is happening.<br>
I would really appreciate it if you can help me with this.</p>
<p>I have also attached a link to the stack of 50 images and I am trying to segregate the black space/voids from the remaining portion.</p>
<p><a href="https://drive.google.com/drive/folders/1d9ydyrI_KDrp2AIucB-P2WQKf4ncCtfC?usp=sharing" class="onebox" target="_blank" rel="noopener nofollow ugc">https://drive.google.com/drive/folders/1d9ydyrI_KDrp2AIucB-P2WQKf4ncCtfC?usp=sharing</a><br>
I have also attached the images when I work with one slice and stack for referencing the issue.<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/235c5161b6a049bdd92affbc135d941eea2e92a2.png" data-download-href="/uploads/short-url/52Ov6kQA2hB3fB2Vh6X8huv70ie.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/235c5161b6a049bdd92affbc135d941eea2e92a2.png" alt="image" data-base62-sha1="52Ov6kQA2hB3fB2Vh6X8huv70ie" width="690" height="394" data-dominant-color="BBBBBB"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1161×664 49.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>I have some images generated with 2D numerical simulations that represent fractures. They often cross-cut each other like in this example:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/5/75dc65f1bcbff3346349845d61172f918849a1bd.png" alt="image" data-base62-sha1="gOE423r47f043yTZom2pPEdSwZn" width="144" height="186"></p>
<p>I want to perform a topological analysis: extract nodes and branches and get statistics about them. Basically, the type of data that <code>Analyze Skeleton (2D/3D)</code> gives. My workflow so far is as follows:</p>
<ol>
<li>Split channels,</li>
<li>Keep the red channel and</li>
<li>Convert to binary:</li>
</ol>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/4322841eea2b0779212e409185e3dfb41f177fa5.png" alt="image" data-base62-sha1="9zTWkSGKn7rPM7jXdbTUYt0YLL7" width="144" height="186"></p>
<ol start="4">
<li>Apply <code>Morphological filters &gt; Closing</code> (with “Disk” as element shape) to close some holes and make my shape more regular:</li>
</ol>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/5/0586c211cc9d9a48c2f468dc7dd0ce2ddaec82d2.png" alt="image" data-base62-sha1="MT6wrT0sOtqddM2S4wfydo2TWq" width="144" height="186"></p>
<ol start="5">
<li>At this point, I use <code>skeletonize</code>:</li>
</ol>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/7/e7fbd9abdcd73612204fce3093e345384966df3c.png" alt="image" data-base62-sha1="x6dSHYlES610QWllLel6WybRfMo" width="144" height="186"></p>
<ol start="6">
<li>and run <code>Analyze Skeleton (2D/3D)</code>.</li>
</ol>
<p>I’m quite happy with the type of data that I get this way (information about junctions, branches, etc). However, I don’t like the way <code>skeletonize</code> deals with some of the junctions. For example, a human eye would treat the top one as a single junction with four branches, caused by the intersection of two lines going <em>top left</em> → <em>bottom right</em>  and  <em>top right</em> → <em>bottom left</em> respectively. Ideally, I’d like to get this kind of skeleton:</p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2f88afd5313c6d5ed640bdb0ee8fe07e8026937.png" alt="image" data-base62-sha1="nfHQLalOmvEkRMEe3dNurbbM1Uj" width="144" height="186"></p>
<p><small>(branches and nodes sketched by hand).</small></p>
<p>Any advice on how to get something like the last image? Maybe I need to pre-process my image better? Is there a way to simplify paths?</p>
<p>Happy to give other tools a go. This <a href="https://github.com/danvk/extract-raster-network" rel="noopener nofollow ugc">python code</a> (<em>Extract Raster Network</em>) looks really interesting, for example. It looks like it can merge nodes that are close to each other.</p>
<p>Thanks!</p> ;;;; <p>I was wondering if it was possible to import registrations made using QuickNii into ABBA. The idea is to manually order the sections in QuickNii as I feel the UI is better for this, but take advantage of ABBA’s automated affine and spline transforms, and ideally export back into VisuAlign for finetuning (I need very precise registrations). I guess this would rely on being able to coerce the quicknii json output into a form ABBA can interpret. Not sure if this is possible but seems like it should work, any suggestions or advice?</p> ;;;; <p>Hello Forum,</p>
<p>I’m using RS-FISH to quantify some smFISH images. I’m wondering whether it is possible save the detections to ROI-manager using the batch processing macro script. It is possible to add detections to ROI-manager using the “Interactive” and “Advanced” interface, but I would like to save the ROIs during batch processing too. Thanks!</p> ;;;; <p>Hi all!</p>
<p>I’m designating 2 areas in an image as ROIs and applying them as masks. For some reason, one of these masks (_sVZ) alters the pixel intensity and the other does not (_CP). Does anyone know why this is happening or have any ideas on how to circumvent this?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f.png" data-download-href="/uploads/short-url/3S66KtX49XLHXf2Lpijjl4hNiBV.png?dl=1" title="Screen Shot 2023-03-06 at 3.24.59 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f_2_689x419.png" alt="Screen Shot 2023-03-06 at 3.24.59 PM" data-base62-sha1="3S66KtX49XLHXf2Lpijjl4hNiBV" width="689" height="419" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f_2_689x419.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f_2_1033x628.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f.png 2x" data-dominant-color="606060"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-06 at 3.24.59 PM</span><span class="informations">1283×780 285 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c.png" data-download-href="/uploads/short-url/oLue8PSGmfMLmBRpMKX9UgizBik.png?dl=1" title="Screen Shot 2023-03-06 at 3.25.24 PM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c_2_690x377.png" alt="Screen Shot 2023-03-06 at 3.25.24 PM" data-base62-sha1="oLue8PSGmfMLmBRpMKX9UgizBik" width="690" height="377" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c_2_690x377.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c_2_1035x565.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c.png 2x" data-dominant-color="575858"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screen Shot 2023-03-06 at 3.25.24 PM</span><span class="informations">1289×706 291 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Thanks in advance!<br>
pdg</p>
<p><a class="attachment" href="/uploads/short-url/lJEy3eii9iHCoz8JxjRKK0MJIpu.cpproj">PDG Zones TBR1 Ver. 23.03.02 copy.cpproj</a> (731.2 KB)</p> ;;;; <p>Hey <a class="mention" href="/u/kj2613">@kj2613</a>,<br>
I have two potential solutions for you:</p>
<p>One trick that works for me sometimes is to specifically annotate a “cell boundary*” class. Carefully draw annotations between the positive cells in an ignored class. Make sure you are using Laplacian of Gaussian, weighted derivation, and gradient features in your classifier to have the best chance of seeing the cell edges. Then, when you create objects, it should (more frequently) break up the big areas into small. [<em>Note</em>: there’s actually a built-in QuPath feature to do this, called <a href="https://petebankhead.github.io/qupath/2019/11/02/fifth-milestone.html#advanced-options">boundary strategy</a>. I’ve actually never used it, so I can’t explain exactly how it works, but it’s probably easier than what I just suggested.]</p>
<p>Alternatively, or in addition, you can use import each shape to Fiji and use watershed detection to separate the cells. Typical watershed will not get the boundaries correct, so I would create objects around the cell nuclei, turn them into points, and then use <a href="https://imagej.net/plugins/marker-controlled-watershed">marker-controlled watershed</a>. It still won’t be 100% accurate, but better than your current results. You’ll need a script to cycle this over all objects in your image.</p> ;;;; <p>Try a morphological closing of radius 27.  Unfortunately the oval is too close to the image border, so it generates that undesired extension. Make sure the image is larger and the ellipse is far from the border.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d016d4b390eb66f94cc8b9bc215ae6bd8071c9ab.png" data-download-href="/uploads/short-url/tGQebkXm7fHt5SelT9FFruATUEj.png?dl=1" title="closing" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d016d4b390eb66f94cc8b9bc215ae6bd8071c9ab.png" alt="closing" data-base62-sha1="tGQebkXm7fHt5SelT9FFruATUEj" width="500" height="500" data-dominant-color="0D0D0D"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">closing</span><span class="informations">512×512 2.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi,</p>
<p>I think I have a related issue, that’s the reason I am posting in this closed issue. Please, if I should start a new topic, let me know and I apologize for that.</p>
<p>I am using config.json, weights_best.h5, and threshold.json from the pre-trained ‘2D_versatile_he’. In my image, I have roundish cells and some elongated cells. When I segment the latter, the cells are split into (2 or more) chaining and overlapping cells.</p>
<p>I want to play with different n_rays values as a potential way to improve the segmentation for the elongated cells. Here is what I am trying:</p>
<p>In the the .py code, I re-assign the n_rays parameter (from 32 to 64).</p>
<pre><code class="lang-auto">model = StarDist2D(None, name=model_folder, basedir=base_dir)
model.config.n_rays = 64
</code></pre>
<p>Right after re-assigning, I have the following model.config:</p>
<pre><code class="lang-auto">Config2D(axes='YXC', backbone='unet', grid=(2, 2), n_channel_in=3, n_channel_out=33, n_classes=None, n_dim=2, **n_rays=64**, net_conv_after_unet=128, net_input_shape=[None, None, 3], net_mask_shape=[None, None, 1], train_background_reg=0.0001, train_batch_size=8, train_checkpoint='weights_best.h5', train_checkpoint_epoch='weights_now.h5', train_checkpoint_last='weights_last.h5', train_class_weights=(1, 1), train_completion_crop=32, train_dist_loss='mae', train_epochs=200, train_foreground_only=0.9, train_learning_rate=0.0003, train_loss_weights=[1, 0.1], train_n_val_patches=3, train_patch_size=[512, 512], train_reduce_lr={'factor': 0.5, 'patience': 50, 'min_delta': 0}, train_sample_cache=True, train_shape_completion=False, train_steps_per_epoch=200, train_tensorboard=True, unet_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_kernel_size=[3, 3], unet_last_activation='relu', unet_n_conv_per_depth=2, unet_n_depth=3, unet_n_filter_base=32, unet_pool=[2, 2], unet_prefix='', use_gpu=False)
</code></pre>
<p>As soon as I call model.predict_instances(), I have the following error:</p>
<pre><code class="lang-auto">Traceback (most recent call last):
  File "/opt/venv/lib/python3.8/site-packages/stardist/models/base.py", line 775, in predict_instances
    for r in self._predict_instances_generator(*args, **kwargs):
  File "/opt/venv/lib/python3.8/site-packages/stardist/models/base.py", line 727, in _predict_instances_generator
    for res in self._predict_sparse_generator(img, axes=axes, normalizer=normalizer, n_tiles=n_tiles,
  File "/opt/venv/lib/python3.8/site-packages/stardist/models/base.py", line 605, in _predict_sparse_generator
    dista = np.asarray(dista).reshape((-1,self.config.n_rays))
ValueError: cannot reshape array of size 6085856 into shape (64)
</code></pre>
<p>When I use the default value n_rays=32, everything works as expected.</p>
<p>My question is: is it possible to change the n_rays value for a pre-trained model? If not, how should I proceed in order to test different n_rays values?</p>
<p>Thank you!</p> ;;;; <p>I am working to train a pixel classifier to identify whole cells with their cytoplasm (eg. astrocytes with variable cell bodies. hence why cell detection doesn’t typically work). The issue I’m running into is that throughout my image the intensity of GFP varies, so in one spot a very dim area of green may be the whole of a cell’s cytoplasm, but elsewhere that same intensity is nothing more than a fragment or noise. When I create annotations from my pixel classifier, it will either not include entire dim cells, or it will include a large number of cells in one annotation, connected by dim areas of fragments, which I don’t want. I want to be able to separate my cells when I create annotations from my pixel classifier and not have multiple cells included in one annotation. I’m currently using “Local Mean Subtraction” in the Features menu in my pixel classifier, but haven’t been able to find much else that aids with this.</p>
<p>Here’s a dim cell:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41793a1596b5993c1afeadb48e5d320e0be0dff5.jpeg" alt="image" data-base62-sha1="9lcLiXC3prJVEKruU1A5BSC0v8F" width="467" height="446"></p>
<p>Here’s a clump of cells separated by area of dim intensity:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/93cbfebe512cbeb5802a1901ee32b61540229bd1.jpeg" data-download-href="/uploads/short-url/l5tfeb2YIX5DGQmqTA5ulmJpzjz.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/93cbfebe512cbeb5802a1901ee32b61540229bd1_2_524x500.jpeg" alt="image" data-base62-sha1="l5tfeb2YIX5DGQmqTA5ulmJpzjz" width="524" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/93cbfebe512cbeb5802a1901ee32b61540229bd1_2_524x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/93cbfebe512cbeb5802a1901ee32b61540229bd1_2_786x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/93cbfebe512cbeb5802a1901ee32b61540229bd1.jpeg 2x" data-dominant-color="0B3D38"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">973×928 93.3 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>I know I probably won’t be able to separate/resolve every single cell, but separating as many as possible while maintaining the information in dimmer cells is the goal. Maybe a script to set a limit on annotation size when creating annotations from the pixel classifier? Or using different features in pixel classification? Maybe I’m thinking about it in the wrong way and someone could point out a better way to resolve this issue that I haven’t considered?<br>
Thank you in advance.</p> ;;;; <p><a class="mention" href="/u/lmurphy">@lmurphy</a> Do you know of anyone that could help guide me on which modules to use to solve this problem? I feel as if this topic is likely buried under new topics, so without a direct  ‘@’ request I worry that this will be buried.</p>
<p>Thanks,</p>
<p>Nick</p> ;;;; <p>Thank you for solve my doubt! I’m looking for delete the partition.</p> ;;;; <p>This is a reminder for the registration for <strong>IN-PERSON</strong> NEUBIAS training school and the symposium, the registration of which is closing soon. Significantly, the training school registration deadline is on Wednesday, the day after tomorrow!</p>
<p>After two years of being exclusively online, we will see each other non-virtually.</p>
<p>More details:</p>
<ul>
<li>
<p><strong>TRACK 1: the “Defragmentation Training School, 2nd edition: bringing BioImage Analysts to the cloud!”,</strong><br>
Defragmentation Training School 2 + NEUBIAS Symposium 2023<br>
May 8-12, 2023, in Porto (Portugal).  <strong>APPLICATION DEADLINE: March 8th, 2023</strong> !!<br>
Maximum number of students: 40 selected applications.<br>
Training School Link:  <a href="https://tinyurl.com/rjcd95yt" class="inline-onebox">*NEW* Defragmentation Training School 2023 - NEUBIAS: Network of BioImage Analysts</a></p>
</li>
<li>
<p><strong>TRACK 2: NEUBIAS Symposium, May 11-12, in Porto (Portugal).</strong><br>
 → register to the Symposium: OPEN to everyone.<br>
<strong>ABSTRACT SUBMISSION DEADLINE: March 15th, 2023 !</strong><br>
Symposium link:  <a href="https://tinyurl.com/yck2mr74" class="inline-onebox">Welcome, Overview - NEUBIAS: Network of BioImage Analysts</a></p>
</li>
</ul>
<p><strong>Training School 2023</strong></p>
<ul>
<li>Introduction to bioimage analysis, tools, and workflows</li>
<li>FAIR principles</li>
<li>Cloud-hosted image data and cloud infrastructures</li>
<li>OMERO and NGFF</li>
<li>Machine and Deep Learning on the cloud</li>
<li>Bioimage analysis workflows with Jupyter, Colab, and CellProfiler</li>
<li>Parallelization: from CPU to GPU - how to speed up workflows</li>
<li>Work on your own data</li>
<li>Benchmarking theory and tools</li>
</ul>
<p><strong>Symposium Programme 2023:</strong></p>
<ul>
<li>BioImage Data Analysis: Tools &amp; Workflows in Life Sciences</li>
<li>Deep Learning for Image Data (AI4Life)</li>
<li>Open source Software Lounge</li>
<li>Image Data Management and Infrastructures</li>
<li>High-Throughput Imaging Data Analysis</li>
<li>Industry talks</li>
<li>Talks Selected from abstracts on topics:
<ul>
<li>Object segmentation, Tracking, Atlases, Registration, Correlation, Fusion, Automation, Machine/Deep learning</li>
<li>Parallelization CPU-&gt;GPU, Infrastructures, Bioimage Analysis Facilities, Open Science, FAIR, Data Management</li>
</ul>
</li>
</ul>
<p>Looking forward to welcoming many of you in Porto, and with apologies for the short notice,</p>
<p>Best greetings,<br>
NEUBIAS conference 2023 organizers</p> ;;;; <p>Hi Shu, similar to the Harmony you can use the cell objects to create a new object for the membrane, there is a module in CellProfiler under ObjectProcessing “ExpandOrShrinkObjects” there you can use the cell objects shrink them the “5 pixels”, and then use the IdentifyTertiaryObjects just like you did for the cytoplasm but using he Cell minus the ShrunkCells that will create the Membrane object.</p>
<p>Barbara</p> ;;;; <p>Yeah, think that was what you mentioned when I was whinging about it breaking a script a while back. That and “project” being restricted to def are the two things I’ve noticed most in terms of breaking back compatibility.</p> ;;;; <p>Interesting. Sounds like something changed in Groovy then – more recent QuPaths use more recent Groov…ies(?)</p> ;;;; <p>Thanks, and please do let me know if you see anything that needs to be updated. I still have not gotten back to a few things for 0.4.x, but I should be able to get to that soon. Quals exams are all done now.</p>
<p>My only comments on the above image is that sometimes the ANN is a little bit tricky to use, and I would recommend starting with the Random Forest with the variable importance checked. If the areas being used for training are not sufficiently different <em>given the Features selected in that third option</em>, the class will often be ignored. Random Forests has, anecdotally, been more robust to that sort of behavior.</p>
<p>So:</p>
<ol>
<li>Add more features</li>
<li>Check which features are important using decision trees, select the best subset of features, then go back to ANN if you want.</li>
</ol> ;;;; <p>Most functions include the option for a z-slice or time value. Through the interface, this is handled with the slider in the upper left, but scripting may be needed to target particular Z-slices for processing.<br>
If you just want to export, then target the z-slices during the export process: <a href="https://forum.image.sc/t/exporting-annotation-for-each-slice-in-a-z-stack/52822/3" class="inline-onebox">Exporting annotation for each slice in a z-stack - #3 by thanushipeiris</a></p> ;;;; <p>If the images are part of a project and have the same number of channels, checking the box in Brightness and Contrast to keep the same display settings should work. Otherwise, it will use new brightness and contrast settings per image as you have seen.</p> ;;;; <p>It also <em>used to</em> work in QuPath, thus that script was probably intended for pre-0.4.x</p> ;;;; <p>Of all the steps only labelling and training can be resumed if stopped at some point. The rest have to be run from the start.</p> ;;;; <p>Ah, didn’t realize there were shortcuts. The only one I tried was holding down spacebar, before I realized I could zoom in and find the one annotation.</p> ;;;; <p>I recommend trying the <a href="https://github.com/qupath/qupath-extension-align">built-in image alignment extension</a> first. It’s a lot easier to figure out what is going wrong from that than from a script.</p> ;;;; <p>Thanks for the quick response Nico!</p>
<p>I am on windows 10. It seems I broke something with JVM with that version name change.</p>
<p>This is what I get now on initializing Abba</p>
<pre><code class="lang-auto">Failed to bootstrap the artifact. Possible solutions: * Double check the endpoint for correctness (https://search.maven.org/). * Add needed repositories to [~/.jgorc](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/Fus/Desktop/2-ABBA/ABBA-Python/~/.jgorc) [repositories] block (see README). * Try with an explicit version number (release metadata might be wrong). Full Maven error output:

Output exceeds the [size limit](command:workbench.action.openSettings?%5B%22notebook.output.textLineLimit%22%5D). Open the full output data [in a text editor](command:workbench.action.openLargeOutput?cae93389-20b4-42d1-a591-1f20f631c5b2)

--------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[4], line 9 1 # -- FOR DEBUGGING 2 # import imagej.doctor 3 # imagej.doctor.checkup() 4 # imagej.doctor.debug_to_stderr() 7 from abba import Abba ----&gt; 9 abba = Abba('Adult Mouse Brain - Allen Brain Atlas V3') # Simply put the name of the BrainGlobe atlas 10 abba.show_bdv_ui() # creates and show a bdv view 12 # !! Warning : it takes time... first : downloading the atlas if not present, but also: one part of the conversion has an abysmal performance 13 # it can take up to a minute... File [c:\Users\Soheib\Desktop\2-ABBA\ABBA-Python\abba\Abba.py:71](file:///C:/Users/Soheib/Desktop/2-ABBA/ABBA-Python/abba/Abba.py:71), in Abba.__init__(self, atlas_name, ij, slicing_mode, headless, enable_jupyter_ui) 69 enable_jupyter_ui() 70 else: ---&gt; 71 ij = imagej.init(get_java_dependencies(), mode='interactive') 72 ij.ui().showUI() 73 self.ij = ij File [c:\ProgramData\Anaconda3\envs\abba\lib\site-packages\imagej\__init__.py:1498](file:///C:/ProgramData/Anaconda3/envs/abba/lib/site-packages/imagej/__init__.py:1498), in init(ij_dir_or_version_or_endpoint, mode, add_legacy, headless) 1496 success = _create_jvm(ij_dir_or_version_or_endpoint, mode, add_legacy) 1497 if not success: -&gt; 1498 raise RuntimeError("Failed to create a JVM with the requested environment.") 1500 if mode == Mode.GUI:

...

1501 # Show the GUI and block. 1502 if macos: 1503 # NB: This will block the calling (main) thread forever! RuntimeError: Failed to create a JVM with the requested environment.
</code></pre>
<p>ch.epfl.biop:ImageToAtlasRegister version was 0.3.7 and I changed it to either 0.4.3  or 0.5.1.</p> ;;;; <p>The pie chart on the pixel classifier window indicates the relative proportion of training data.</p>
<p>Mike’s (<a class="mention" href="/u/mike_nelson">@Mike_Nelson</a>) blog was super useful when I needed to dive more into the Qupath’s pixel classifier:</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://www.imagescientist.com/brightfield-4-pixel-classifier">
  <header class="source">
      <img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c6b0cf7d4866291864036815c968bbc0536bd810.png" class="site-icon" width="100" height="100">

      <a href="https://www.imagescientist.com/brightfield-4-pixel-classifier" target="_blank" rel="noopener nofollow ugc">Image Scientist</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://www.imagescientist.com/brightfield-4-pixel-classifier" target="_blank" rel="noopener nofollow ugc">Analyzing brightfield images - Pixel classifier — Image Scientist</a></h3>

  <p>Explore the use of one of QuPath's most powerful tools to isolate different regions of the tissue based on stains and texture.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>
 ;;;; <p>Hello to everyone!<br>
First, I’m glad about this comunity, it solves some doubts.</p>
<p>My question is about how to restart the evaluation from a training network? Cuase, I was saving all files in one partition of the external HDD memory, but it was filling up into 100%. So, I moved the half percent to the other partition and, when i want to restart, the evaluation initialize from scratch.<br>
So, is there any command like training the model (changing the parameter init_weights)</p>
<p>Thank you so much!!</p> ;;;; <p>(this is in fact, such a common/annoying issue, that I don’t even pin my minimum requirements on packages that use typing-extensions, exactly <em>because</em> they would fail to install with tensorflow … see <a href="https://github.com/pyapp-kit/superqt/issues/45" class="inline-onebox">Older version typing-extensions causing trouble with superqt · Issue #45 · pyapp-kit/superqt · GitHub</a> and <a href="https://github.com/pyapp-kit/superqt/issues/100" class="inline-onebox">Dependency version conflict with noise2void/keras~=2.3/tensorflow~=2.5 · Issue #100 · pyapp-kit/superqt · GitHub</a> for example.  Until older versions of tensorflow become unused… i think just force upgrading typing-extensions is probably the best you can do)</p> ;;;; <p>yes, that’s a common issue.  tensorflow has unfortunately committed the “cardinal sin” here of hard pinning a <em>very</em> commonly used community package (demonstrating what we discussed above that using hard pins as a library make it hard for the rest of the ecosystem <img src="https://emoji.discourse-cdn.com/twitter/joy.png?v=12" title=":joy:" class="emoji" alt=":joy:" loading="lazy" width="20" height="20">)</p>
<p>they fixed this in <a href="https://github.com/tensorflow/tensorflow/pull/53250" class="inline-onebox">🔧 Loosen version constraints on `typing-extensions` by connorbrinton · Pull Request #53250 · tensorflow/tensorflow · GitHub</a></p>
<p>and from the dates on that PR, it looks like tensorflow v2.7.1 would be the first version to “play well” with others.</p>
<p>If you must use an earlier version, one thing you could try here is to <code>pip install -U typing-extensions</code> <em>after</em> installing tensorflow.</p> ;;;; <p>I’m trying to add a scale bar to this 3D rendered image. In the 2D composite channel, the scale bar says the approximate length across is 153 microns:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74.png" data-download-href="/uploads/short-url/Abgb9oSPvzJmdAkFxAUjwjX7RRO.png?dl=1" title="Screenshot 2023-03-06 at 11.38.41 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74_2_579x500.png" alt="Screenshot 2023-03-06 at 11.38.41 AM" data-base62-sha1="Abgb9oSPvzJmdAkFxAUjwjX7RRO" width="579" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74_2_579x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74_2_868x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74.png 2x" data-dominant-color="0B051F"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-06 at 11.38.41 AM</span><span class="informations">878×758 88.6 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>But when I 3D render the bot, and zoom in and out, then take snapshots, the scale bar length is changing with the change of zoom.</p>
<p>When I zoom farther away, the scale bar length decreases:<br>
<img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c7565a9d1b95ecdd87b170b2371f2f771c0fd24.png" alt="Screenshot 2023-03-06 at 11.40.51 AM" data-base62-sha1="hL0JaotpNdxwy7kacVCDGVjqChK" width="422" height="318"></p>
<p><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a96d9a0f4b04cdb237342d2adf1b48e6d28dd90c.png" alt="Screenshot 2023-03-06 at 11.40.45 AM" data-base62-sha1="oaPvP0JIDhMym9lJlJqM1rIiDGQ" width="568" height="472"></p>
<p>When I zoom closer, the scale bar length increases:<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181.png" data-download-href="/uploads/short-url/p5vAO7b3LSSsymFHcOvbDBDrKU1.png?dl=1" title="Screenshot 2023-03-06 at 11.40.55 AM" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181_2_625x500.png" alt="Screenshot 2023-03-06 at 11.40.55 AM" data-base62-sha1="p5vAO7b3LSSsymFHcOvbDBDrKU1" width="625" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181_2_625x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181.png 2x" data-dominant-color="401B31"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot 2023-03-06 at 11.40.55 AM</span><span class="informations">866×692 217 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p>
<p>Does anyone know why this is happening? And how to fix it/add scale bars for 3D rendered images in an accurate way?</p> ;;;; <p>It was some error of my napari settings. Resetting those to the defaults worked <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> Thank you!</p> ;;;; <p>Dear all, I try to find a way to unlarge a ROI only in a certain direction; in the direction of  an other ROI (ellipse).  I try EDM of each ROI and mathematic operation between them , it should work but I don’t find the good operation. thanks if you any idea to solve the problem simply ?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9f06d1d10ee67b0983d22a23b7b53892d09f6db.png" data-download-href="/uploads/short-url/v5YIKYAmTH5wewYTCZ0vsPUaSvN.png?dl=1" title="Result of C1-Untitled" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9f06d1d10ee67b0983d22a23b7b53892d09f6db.png" alt="Result of C1-Untitled" data-base62-sha1="v5YIKYAmTH5wewYTCZ0vsPUaSvN" width="500" height="500" data-dominant-color="070707"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Result of C1-Untitled</span><span class="informations">512×512 1.61 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div><br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e087ea68a786700eaf9fcd4111c6d81a1697a4c.png" data-download-href="/uploads/short-url/fHoIJUF4Sf2qZltBWXVqkndHwSw.png?dl=1" title="Result of C1-Untitled" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e087ea68a786700eaf9fcd4111c6d81a1697a4c.png" alt="Result of C1-Untitled" data-base62-sha1="fHoIJUF4Sf2qZltBWXVqkndHwSw" width="500" height="500" data-dominant-color="080808"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Result of C1-Untitled</span><span class="informations">512×512 1.79 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Hi <a class="mention" href="/u/talley">@talley</a> ,<br>
thank you for the help.<br>
we tried the ’ mamba create ’ command in step 1. now everything is installed from conda-forge.</p>
<p>Unfortunately, I still have issues starting napari, the stack trace is here</p>
<pre><code class="lang-auto">Traceback (most recent call last):
  File "/opt/conda/envs/napari/bin/napari", line 10, in &lt;module&gt;
	sys.exit(main())
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/__main__.py", line 561, in main
	_run()
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/__main__.py", line 218, in _run
	from napari import Viewer, run
  File "&lt;frozen importlib._bootstrap&gt;", line 1039, in _handle_fromlist
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/_lazy.py", line 48, in __getattr__
	submod = import_module(
  File "/opt/conda/envs/napari/lib/python3.8/importlib/__init__.py", line 127, in import_module
	return _bootstrap._gcd_import(name[level:], package, level)
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/viewer.py", line 8, in &lt;module&gt;
	from .components.viewer_model import ViewerModel
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/components/__init__.py", line 24, in &lt;module&gt;
	from . import _viewer_key_bindings  # isort:skip
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/components/_viewer_key_bindings.py", line 9, in &lt;module&gt;
	from .viewer_model import ViewerModel
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/components/viewer_model.py", line 38, in &lt;module&gt;
	from ..plugins.utils import get_potential_readers, get_preferred_reader
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/plugins/__init__.py", line 8, in &lt;module&gt;
	from . import _npe2
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/napari/plugins/_npe2.py", line 17, in &lt;module&gt;
	from app_model.types import SubmenuItem
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/app_model/__init__.py", line 10, in &lt;module&gt;
	from ._app import Application
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/app_model/_app.py", line 9, in &lt;module&gt;
	from .registries import (
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/app_model/registries/__init__.py", line 3, in &lt;module&gt;
	from ._keybindings_reg import KeyBindingsRegistry
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/app_model/registries/_keybindings_reg.py", line 7, in &lt;module&gt;
	from ..types._keys import KeyBinding
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/app_model/types/__init__.py", line 2, in &lt;module&gt;
	from ._action import Action
  File "/opt/conda/envs/napari/lib/python3.8/site-packages/app_model/types/_action.py", line 27, in &lt;module&gt;
	class Action(CommandRule, Generic[P, R]):
  File "pydantic/main.py", line 188, in pydantic.main.ModelMetaclass.__new__
  File "pydantic/typing.py", line 419, in pydantic.typing.resolve_annotations
	For example:
  File "/opt/conda/envs/napari/lib/python3.8/typing.py", line 270, in _eval_type
	return t._evaluate(globalns, localns)
  File "/opt/conda/envs/napari/lib/python3.8/typing.py", line 518, in _evaluate
	eval(self.__forward_code__, globalns, localns),
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "/opt/conda/envs/napari/lib/python3.8/typing.py", line 813, in __getitem__
	raise TypeError(f"Callable[args, result]: args must be a list."
TypeError: Callable[args, result]: args must be a list. Got ~P

</code></pre>
<p>conda list shows:</p>
<pre><code class="lang-auto">_libgcc_mutex         	0.1             	conda_forge	conda-forge
_openmp_mutex         	4.5                   	2_gnu	conda-forge
abseil-cpp            	20210324.2       	h9c3ff4c_0	conda-forge
absl-py               	0.15.0         	pyhd8ed1ab_0	conda-forge
aiohttp               	3.8.4        	py38h1de0b5d_0	conda-forge
aiosignal             	1.3.1          	pyhd8ed1ab_0	conda-forge
alabaster             	0.7.13         	pyhd8ed1ab_0	conda-forge
alsa-lib              	1.2.3.2          	h166bdaf_0	conda-forge
app-model             	0.1.1          	pyhd8ed1ab_0	conda-forge
appdirs               	1.4.4          	pyh9f0ad1d_0	conda-forge
asciitree             	0.3.3                  	py_2	conda-forge
asttokens             	2.2.1          	pyhd8ed1ab_0	conda-forge
astunparse            	1.6.3          	pyhd8ed1ab_0	conda-forge
async-timeout         	4.0.2          	pyhd8ed1ab_0	conda-forge
attrs                 	22.2.0         	pyh71513ae_0	conda-forge
babel                 	2.12.1         	pyhd8ed1ab_1	conda-forge
backcall              	0.2.0          	pyh9f0ad1d_0	conda-forge
backports             	1.0            	pyhd8ed1ab_3	conda-forge
backports.functools_lru_cache 1.6.4          	pyhd8ed1ab_0	conda-forge
blinker               	1.5            	pyhd8ed1ab_0	conda-forge
bokeh                 	3.0.3          	pyhd8ed1ab_0	conda-forge
brotli                	1.0.9            	h166bdaf_8	conda-forge
brotli-bin            	1.0.9            	h166bdaf_8	conda-forge
brotlipy              	0.7.0       	py38h0a891b7_1005	conda-forge
bzip2                 	1.0.8            	h7f98852_4	conda-forge
c-ares                	1.18.1           	h7f98852_0	conda-forge
ca-certificates       	2022.12.7        	ha878542_0	conda-forge
cached-property       	1.5.2            	hd8ed1ab_1	conda-forge
cached_property       	1.5.2          	pyha770c72_1	conda-forge
cachetools            	4.2.4          	pyhd8ed1ab_0	conda-forge
cachey                	0.2.1          	pyh9f0ad1d_0	conda-forge
certifi               	2022.12.7      	pyhd8ed1ab_0	conda-forge
cffi                  	1.15.1       	py38h4a40e3a_3	conda-forge
charset-normalizer    	2.1.1          	pyhd8ed1ab_0	conda-forge
click                 	8.1.3       	unix_pyhd8ed1ab_2	conda-forge
cloudpickle           	2.2.1          	pyhd8ed1ab_0	conda-forge
colorama              	0.4.6          	pyhd8ed1ab_0	conda-forge
comm                  	0.1.2          	pyhd8ed1ab_0	conda-forge
commonmark            	0.9.1                  	py_0	conda-forge
contourpy             	1.0.7        	py38hfbd4bf9_0	conda-forge
cryptography          	39.0.0       	py38h1724139_0	conda-forge
csbdeep               	0.7.3        	py38h578d9bd_0	conda-forge
cudatoolkit           	11.2.2          	hbe64b41_11	conda-forge
cudnn                 	8.4.1.50         	hed8a83a_0	conda-forge
cycler                	0.11.0         	pyhd8ed1ab_0	conda-forge
cytoolz               	0.12.0       	py38h0a891b7_1	conda-forge
dask                  	2022.11.0      	pyhd8ed1ab_0	conda-forge
dask-core             	2022.11.0      	pyhd8ed1ab_0	conda-forge
dataclasses           	0.8            	pyhc8e2a94_3	conda-forge
dbus                  	1.13.6           	h5008d03_3	conda-forge
debugpy               	1.6.6        	py38h8dc9893_0	conda-forge
decorator             	5.1.1          	pyhd8ed1ab_0	conda-forge
distributed           	2022.11.0      	pyhd8ed1ab_0	conda-forge
docstring_parser      	0.15           	pyhd8ed1ab_0	conda-forge
docutils              	0.17.1       	py38h578d9bd_3	conda-forge
entrypoints           	0.4            	pyhd8ed1ab_0	conda-forge
executing             	1.2.0          	pyhd8ed1ab_0	conda-forge
expat                 	2.5.0            	h27087fc_0	conda-forge
fasteners             	0.17.3         	pyhd8ed1ab_0	conda-forge
font-ttf-dejavu-sans-mono 2.37             	hab24e00_0	conda-forge
font-ttf-inconsolata  	3.000            	h77eed37_0	conda-forge
font-ttf-source-code-pro  2.038            	h77eed37_0	conda-forge
font-ttf-ubuntu       	0.83             	hab24e00_0	conda-forge
fontconfig            	2.14.2           	h14ed4e7_0	conda-forge
fonts-conda-ecosystem 	1                         	0	conda-forge
fonts-conda-forge     	1                         	0	conda-forge
fonttools             	4.38.0       	py38h0a891b7_1	conda-forge
freetype              	2.12.1           	hca18f0e_1	conda-forge
freetype-py           	2.3.0          	pyhd8ed1ab_0	conda-forge
frozenlist            	1.3.3        	py38h0a891b7_0	conda-forge
fsspec                	2023.3.0       	pyhd8ed1ab_1	conda-forge
future                	0.18.3         	pyhd8ed1ab_0	conda-forge
gast                  	0.4.0          	pyh9f0ad1d_0	conda-forge
gettext               	0.21.1           	h27087fc_0	conda-forge
giflib                	5.2.1            	h36c2ea0_2	conda-forge
glib                  	2.74.1           	h6239696_1	conda-forge
glib-tools            	2.74.1           	h6239696_1	conda-forge
google-auth           	1.35.0         	pyh6c4a22f_0	conda-forge
google-auth-oauthlib  	0.4.6          	pyhd8ed1ab_0	conda-forge
google-pasta          	0.2.0          	pyh8c360ce_0	conda-forge
grpc-cpp              	1.41.1           	h75e9d12_2	conda-forge
grpcio                	1.41.1       	py38hdd6454d_1	conda-forge
gst-plugins-base      	1.20.2           	hcf0ee16_0	conda-forge
gstreamer             	1.20.3           	hd4edc92_2	conda-forge
h5py                  	3.1.0       	nompi_py38hafa665b_100	conda-forge
hdf5                  	1.10.6      	nompi_h6a2412b_1114	conda-forge
heapdict              	1.0.1                  	py_0	conda-forge
hsluv                 	5.0.2          	pyh44b312d_0	conda-forge
icu                   	69.1             	h9c3ff4c_0	conda-forge
idna                  	3.4            	pyhd8ed1ab_0	conda-forge
imagecodecs-lite      	2019.12.3    	py38h71d37f0_5	conda-forge
imageio               	2.26.0         	pyh24c5eb1_0	conda-forge
imagesize             	1.4.1          	pyhd8ed1ab_0	conda-forge
importlib-metadata    	6.0.0          	pyha770c72_0	conda-forge
importlib_resources   	5.12.0         	pyhd8ed1ab_0	conda-forge
in-n-out              	0.1.7          	pyhd8ed1ab_0	conda-forge
ipykernel             	6.21.2         	pyh210e3f2_0	conda-forge
ipython               	8.11.0         	pyh41d4057_0	conda-forge
ipython_genutils      	0.2.0                  	py_1	conda-forge
jedi                  	0.18.2         	pyhd8ed1ab_0	conda-forge
jinja2                	3.1.2          	pyhd8ed1ab_1	conda-forge
jpeg                  	9e               	h0b41bf4_3	conda-forge
jsonschema            	4.17.3         	pyhd8ed1ab_0	conda-forge
jupyter_client        	7.3.4          	pyhd8ed1ab_0	conda-forge
jupyter_core          	5.2.0        	py38h578d9bd_0	conda-forge
keras                 	2.6.0          	pyhd8ed1ab_1	conda-forge
keras-preprocessing   	1.1.2          	pyhd8ed1ab_0	conda-forge
keyutils              	1.6.1            	h166bdaf_0	conda-forge
kiwisolver            	1.4.4        	py38h43d8883_1	conda-forge
krb5                  	1.20.1           	hf9c8cef_0	conda-forge
lcms2                 	2.14             	hfd0df8a_1	conda-forge
ld_impl_linux-64      	2.40             	h41732ed_0	conda-forge
lerc                  	4.0.0            	h27087fc_0	conda-forge
libblas               	3.9.0       	16_linux64_openblas	conda-forge
libbrotlicommon       	1.0.9            	h166bdaf_8	conda-forge
libbrotlidec          	1.0.9            	h166bdaf_8	conda-forge
libbrotlienc          	1.0.9            	h166bdaf_8	conda-forge
libcblas              	3.9.0       	16_linux64_openblas	conda-forge
libclang              	13.0.1      	default_had23c3d_1	conda-forge
libcurl               	7.87.0           	h6312ad2_0	conda-forge
libdeflate            	1.17             	h0b41bf4_0	conda-forge
libedit               	3.1.20191231     	he28a2e2_2	conda-forge
libev                 	4.33             	h516909a_1	conda-forge
libevent              	2.1.10           	h9b69904_4	conda-forge
libffi                	3.4.2            	h7f98852_5	conda-forge
libgcc-ng             	12.2.0          	h65d4601_19	conda-forge
libgfortran-ng        	12.2.0          	h69a702a_19	conda-forge
libgfortran5          	12.2.0          	h337968e_19	conda-forge
libglib               	2.74.1           	h606061b_1	conda-forge
libgomp               	12.2.0          	h65d4601_19	conda-forge
libiconv              	1.17             	h166bdaf_0	conda-forge
liblapack             	3.9.0       	16_linux64_openblas	conda-forge
libllvm11             	11.1.0           	he0ac6c6_5	conda-forge
libllvm13             	13.0.1           	hf817b99_2	conda-forge
libnghttp2            	1.51.0           	hdcd2b5c_0	conda-forge
libnsl                	2.0.0            	h7f98852_0	conda-forge
libogg                	1.3.4            	h7f98852_1	conda-forge
libopenblas           	0.3.21      	pthreads_h78a6416_3	conda-forge
libopus               	1.3.1            	h7f98852_1	conda-forge
libpng                	1.6.39           	h753d276_0	conda-forge
libpq                 	14.5             	h2baec63_5	conda-forge
libprotobuf           	3.18.3           	h6239696_0	conda-forge
libsodium             	1.0.18           	h36c2ea0_1	conda-forge
libsqlite             	3.40.0           	h753d276_0	conda-forge
libssh2               	1.10.0           	haa6b8db_3	conda-forge
libstdcxx-ng          	12.2.0          	h46fd767_19	conda-forge
libtiff               	4.5.0            	h6adf6a1_2	conda-forge
libuuid               	2.32.1        	h7f98852_1000	conda-forge
libvorbis             	1.3.7            	h9c3ff4c_0	conda-forge
libwebp-base          	1.2.4            	h166bdaf_0	conda-forge
libxcb                	1.13          	h7f98852_1004	conda-forge
libxkbcommon          	1.0.3            	he3ba5ed_0	conda-forge
libxml2               	2.9.14           	haae042b_4	conda-forge
libzlib               	1.2.13           	h166bdaf_4	conda-forge
llvmlite              	0.38.1       	py38h38d86a4_0	conda-forge
locket                	1.0.0          	pyhd8ed1ab_0	conda-forge
lz4                   	4.3.2        	py38hd012fdc_0	conda-forge
lz4-c                 	1.9.4            	hcb278e6_0	conda-forge
magicgui              	0.7.2          	pyhd8ed1ab_0	conda-forge
markdown              	3.4.1          	pyhd8ed1ab_0	conda-forge
markupsafe            	2.1.2        	py38h1de0b5d_0	conda-forge
matplotlib-base       	3.5.3        	py38h38b5ce0_2	conda-forge
matplotlib-inline     	0.1.6          	pyhd8ed1ab_0	conda-forge
msgpack-python        	1.0.4        	py38h43d8883_1	conda-forge
multidict             	6.0.4        	py38h1de0b5d_0	conda-forge
munkres               	1.1.4          	pyh9f0ad1d_0	conda-forge
mypy_extensions       	1.0.0          	pyha770c72_0	conda-forge
mysql-common          	8.0.32           	h14678bc_0	conda-forge
mysql-libs            	8.0.32           	h54cf53e_0	conda-forge
napari                	0.4.17      	pyh275ddea_0_pyqt	conda-forge
napari-console        	0.0.7          	pyhd8ed1ab_0	conda-forge
napari-ome-zarr       	0.5.2          	pyhd8ed1ab_0	conda-forge
napari-plugin-engine  	0.2.0          	pyhd8ed1ab_2	conda-forge
napari-svg            	0.1.6          	pyhd8ed1ab_1	conda-forge
nccl                  	2.14.3.1         	h0800d71_0	conda-forge
ncurses               	6.3              	h27087fc_1	conda-forge
nest-asyncio          	1.5.6          	pyhd8ed1ab_0	conda-forge
networkx              	3.0            	pyhd8ed1ab_0	conda-forge
npe2                  	0.6.2          	pyhd8ed1ab_0	conda-forge
nspr                  	4.35             	h27087fc_0	conda-forge
nss                   	3.88             	he45b914_0	conda-forge
numba                 	0.55.2       	py38hdc3674a_0	conda-forge
numcodecs             	0.11.0       	py38h8dc9893_1	conda-forge
numpy                 	1.19.5       	py38h8246c76_3	conda-forge
numpydoc              	1.5.0          	pyhd8ed1ab_0	conda-forge
oauthlib              	3.2.2          	pyhd8ed1ab_0	conda-forge
ome-zarr              	0.6.1          	pyhd8ed1ab_0	conda-forge
openjpeg              	2.5.0            	hfec8fc6_2	conda-forge
openssl               	1.1.1t           	h0b41bf4_0	conda-forge
opt_einsum            	3.3.0          	pyhd8ed1ab_1	conda-forge
packaging             	23.0           	pyhd8ed1ab_0	conda-forge
pandas                	1.4.4        	py38h47df419_0	conda-forge
parso                 	0.8.3          	pyhd8ed1ab_0	conda-forge
partd                 	1.3.0          	pyhd8ed1ab_0	conda-forge
pcre2                 	10.40            	hc3806b6_0	conda-forge
pexpect               	4.8.0          	pyh1a96a4e_2	conda-forge
pickleshare           	0.7.5               	py_1003	conda-forge
pillow                	9.4.0        	py38hde6dc18_1	conda-forge
pint                  	0.20.1         	pyhd8ed1ab_0	conda-forge
pip                   	23.0.1         	pyhd8ed1ab_0	conda-forge
pkgutil-resolve-name  	1.3.10         	pyhd8ed1ab_0	conda-forge
platformdirs          	2.6.0          	pyhd8ed1ab_0	conda-forge
pooch                 	1.7.0          	pyhd8ed1ab_0	conda-forge
prompt-toolkit        	3.0.38         	pyha770c72_0	conda-forge
prompt_toolkit        	3.0.38           	hd8ed1ab_0	conda-forge
protobuf              	3.18.3       	py38hfa26641_0	conda-forge
psutil                	5.9.4        	py38h0a891b7_0	conda-forge
psygnal               	0.8.1          	pyhd8ed1ab_0	conda-forge
pthread-stubs         	0.4           	h36c2ea0_1001	conda-forge
ptyprocess            	0.7.0          	pyhd3deb0d_0	conda-forge
pure_eval             	0.2.2          	pyhd8ed1ab_0	conda-forge
pyasn1                	0.4.8                  	py_0	conda-forge
pyasn1-modules        	0.2.7                  	py_0	conda-forge
pycparser             	2.21           	pyhd8ed1ab_0	conda-forge
pydantic              	1.9.2        	py38h0a891b7_0	conda-forge
pygments              	2.14.0         	pyhd8ed1ab_0	conda-forge
pyjwt                 	2.6.0          	pyhd8ed1ab_0	conda-forge
pyopengl              	3.1.6          	pyhd8ed1ab_1	conda-forge
pyopenssl             	23.0.0         	pyhd8ed1ab_0	conda-forge
pyparsing             	3.0.9          	pyhd8ed1ab_0	conda-forge
pyproject_hooks       	1.0.0          	pyhd8ed1ab_0	conda-forge
pyqt                  	5.12.3       	py38h578d9bd_8	conda-forge
pyqt-impl             	5.12.3       	py38h0ffb2e6_8	conda-forge
pyqt5-sip             	4.19.18      	py38h709712a_8	conda-forge
pyqtchart             	5.12         	py38h7400c14_8	conda-forge
pyqtwebengine         	5.12.1       	py38h7400c14_8	conda-forge
pyrsistent            	0.19.3       	py38h1de0b5d_0	conda-forge
pysocks               	1.7.1          	pyha2e5f31_6	conda-forge
python                	3.8.15      	h257c98d_0_cpython	conda-forge
python-build          	0.10.0         	pyhd8ed1ab_0	conda-forge
python-dateutil       	2.8.2          	pyhd8ed1ab_0	conda-forge
python-flatbuffers    	1.12           	pyhd8ed1ab_1	conda-forge
python_abi            	3.8                  	3_cp38	conda-forge
pytomlpp              	1.0.11       	py38h43d8883_1	conda-forge
pytz                  	2022.7.1       	pyhd8ed1ab_0	conda-forge
pyu2f                 	0.1.5          	pyhd8ed1ab_0	conda-forge
pywavelets            	1.3.0        	py38h71d37f0_1	conda-forge
pyyaml                	6.0          	py38h0a891b7_5	conda-forge
pyzmq                 	25.0.0       	py38he24dcef_0	conda-forge
qt                    	5.12.9           	h1304e3e_6	conda-forge
qtconsole-base        	5.4.0          	pyha770c72_0	conda-forge
qtpy                  	2.3.0          	pyhd8ed1ab_0	conda-forge
re2                   	2021.11.01       	h9c3ff4c_0	conda-forge
readline              	8.1.2            	h0f457ee_0	conda-forge
requests              	2.28.2         	pyhd8ed1ab_0	conda-forge
requests-oauthlib     	1.3.1          	pyhd8ed1ab_0	conda-forge
rich                  	12.4.1         	pyhd8ed1ab_0	conda-forge
rsa                   	4.9            	pyhd8ed1ab_0	conda-forge
scikit-image          	0.19.3       	py38h47df419_1	conda-forge
scipy                 	1.9.1        	py38hea3f02b_0	conda-forge
setuptools            	67.5.1         	pyhd8ed1ab_0	conda-forge
shellingham           	1.5.1          	pyhd8ed1ab_0	conda-forge
six                   	1.15.0         	pyh9f0ad1d_0	conda-forge
snappy                	1.1.9            	hbd366e4_2	conda-forge
snowballstemmer       	2.2.0          	pyhd8ed1ab_0	conda-forge
sortedcontainers      	2.4.0          	pyhd8ed1ab_0	conda-forge
sphinx                	4.5.0          	pyh6c4a22f_0	conda-forge
sphinxcontrib-applehelp   1.0.4          	pyhd8ed1ab_0	conda-forge
sphinxcontrib-devhelp 	1.0.2                  	py_0	conda-forge
sphinxcontrib-htmlhelp	2.0.1          	pyhd8ed1ab_0	conda-forge
sphinxcontrib-jsmath  	1.0.1                  	py_0	conda-forge
sphinxcontrib-qthelp  	1.0.3                  	py_0	conda-forge
sphinxcontrib-serializinghtml 1.1.5          	pyhd8ed1ab_2	conda-forge
sqlite                	3.40.0           	h4ff8645_0	conda-forge
stack_data            	0.6.2          	pyhd8ed1ab_0	conda-forge
stardist              	0.8.3        	py38h47df419_0	conda-forge
stardist-napari       	2022.12.6      	pyhd8ed1ab_0	conda-forge
superqt               	0.4.1          	pyhd8ed1ab_0	conda-forge
tblib                 	1.7.0          	pyhd8ed1ab_0	conda-forge
tensorboard           	2.6.0          	pyhd8ed1ab_1	conda-forge
tensorboard-data-server   0.6.1        	py38h2b5fc30_4	conda-forge
tensorboard-plugin-wit	1.8.1          	pyhd8ed1ab_0	conda-forge
tensorflow            	2.6.2       	cuda112py38ha230376_1	conda-forge
tensorflow-base       	2.6.2       	cuda112py38h8955826_1	conda-forge
tensorflow-estimator  	2.6.2       	cuda112py38ha230376_1	conda-forge
termcolor             	1.1.0          	pyhd8ed1ab_3	conda-forge
tifffile              	2020.6.3               	py_0	conda-forge
tk                    	8.6.12           	h27826a3_0	conda-forge
tomli                 	2.0.1          	pyhd8ed1ab_0	conda-forge
toolz                 	0.12.0         	pyhd8ed1ab_0	conda-forge
tornado               	6.1          	py38h0a891b7_3	conda-forge
tqdm                  	4.65.0         	pyhd8ed1ab_1	conda-forge
traitlets             	5.9.0          	pyhd8ed1ab_0	conda-forge
typer                 	0.7.0          	pyhd8ed1ab_0	conda-forge
typing-extensions     	3.7.4.3                   	0	conda-forge
typing_extensions     	3.7.4.3                	py_0	conda-forge
unicodedata2          	15.0.0       	py38h0a891b7_0	conda-forge
urllib3               	1.26.14        	pyhd8ed1ab_0	conda-forge
vispy                 	0.11.0       	py38hb22ca3e_0	conda-forge
wcwidth               	0.2.6          	pyhd8ed1ab_0	conda-forge
werkzeug              	2.2.3          	pyhd8ed1ab_0	conda-forge
wheel                 	0.38.4         	pyhd8ed1ab_0	conda-forge
wrapt                 	1.12.1       	py38h497a2fe_3	conda-forge
xorg-libxau           	1.0.9            	h7f98852_0	conda-forge
xorg-libxdmcp         	1.1.3            	h7f98852_0	conda-forge
xyzservices           	2023.2.0       	pyhd8ed1ab_0	conda-forge
xz                    	5.2.6            	h166bdaf_0	conda-forge
yaml                  	0.2.5            	h7f98852_2	conda-forge
yarl                  	1.8.2        	py38h0a891b7_0	conda-forge
zarr                  	2.13.6         	pyhd8ed1ab_0	conda-forge
zeromq                	4.3.4            	h9c3ff4c_1	conda-forge
zict                  	2.2.0          	pyhd8ed1ab_0	conda-forge
zipp                  	3.15.0         	pyhd8ed1ab_0	conda-forge
zlib                  	1.2.13           	h166bdaf_4	conda-forge
zstd                  	1.5.2            	h3eb15da_6	conda-forge
</code></pre>
<p>It seems there’are version issues with tensorflow, typing-extensions, numpy etc…</p>
<p>I’m trying install from the environment.yml using mamba currently.</p>
<p>thanks,</p> ;;;; <p>Hi,<br>
I’ve been struggling with the same problem for hours. I try to train classifiers, and sometimes (sometimes not), it actually finds 4 classes in non-neglectable proportions, but one of them is not displayed. Here you can see it found a purple label, but I cannot see it. What do I do wrong ?<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be34ef19729c7243c1d313abf5e0bd94aa7cf0d5.png" data-download-href="/uploads/short-url/r8E83pi99B66reQtvxcQjj6mJBr.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be34ef19729c7243c1d313abf5e0bd94aa7cf0d5_2_582x500.png" alt="image" data-base62-sha1="r8E83pi99B66reQtvxcQjj6mJBr" width="582" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be34ef19729c7243c1d313abf5e0bd94aa7cf0d5_2_582x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be34ef19729c7243c1d313abf5e0bd94aa7cf0d5_2_873x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be34ef19729c7243c1d313abf5e0bd94aa7cf0d5.png 2x" data-dominant-color="4C534C"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1089×934 109 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>0</p>
<p>I am working with MRI images which in their raw form are 5-dimensional. I can load them in using <code>nibabel</code> package which gives me <code>nibabel.Nifti1Image</code> objects. Then I can slice that object into a bunch of 3D numpy arrays from which I create <code>SimpleITK.Image</code> objects, but I would like to preserve the metadata (affine, header) contained in <code>nibabel.Nifti1Image</code>. When working with just <code>SimpleITK.Image</code> objects, you can use the <code>.CopyInformation()</code> method, unfortunately that does not work with <code>nibabel.Nifti1Image</code>. How do I copy the information across?</p>
<p>I don’t know how to copy the information across manually as I do not understand the difference in how either package approaches defining spatial information.</p> ;;;; <p>Hello. Can you elaborate a bit on how to reproduce the issue?</p> ;;;; <p>Hi all,<br>
Thanks a lot! That was exactly what I was searching for!</p>
<p>For completeness, here is the now working code: <a href="https://gist.github.com/cecad-imaging/1fe4513b9cccb03a7c1266715928f6d1" class="inline-onebox" rel="noopener nofollow ugc">QuPath-script to convert Hamamatsu-NDPI to ome.tiff-pyramid. Separated tissue pieces are stored in individual files named according to metadata provided as Omero key-value-pairs read via QuPath OMERO BIOP extension. · GitHub</a></p>
<p>Cheers,<br>
Peter</p> ;;;; <p>Hi Albert, could you provide your pipeline and a sample image/image set?</p> ;;;; <p>hello<br>
I have done a machine learning with the trainable Weka segmentation and now I want to apply it to other images. This works fine when I run it myself. However, I want to run it through a macro and unfortunately I get some problems. To do this, I use this macro.</p>
<p>run(“Trainable Weka Segmentation”);<br>
selectWindow(“Trainable Weka Segmentation v3.3.2”);<br>
call(“trainableSegmentation.Weka_Segmentation.addTrace”, “0”, “1”);<br>
call(“trainableSegmentation.Weka_Segmentation.changeClassName”, “0”, “Hintergrund”);<br>
call(“trainableSegmentation.Weka_Segmentation.changeClassName”, “1”, “Zellkerne”);<br>
call(“trainableSegmentation.Weka_Segmentation.loadData”, “…\Training\data31.arff”);<br>
call(“trainableSegmentation.Weka_Segmentation.trainClassifier”);<br>
wait(1800000);<br>
call(“trainableSegmentation.Weka_Segmentation.getResult”);<br>
selectWindow(“Classified image”);<br>
run(“RGB Color”);<br>
run(“Split Channels”);<br>
selectWindow(“Classified image.jpg (blue)”);<br>
close();<br>
selectWindow(“Classified image.jpg (red)”);<br>
close();<br>
run(“Auto Threshold”, “method=Default ignore_white”);<br>
setOption(“BlackBackground”, true);<br>
run(“Convert to Mask”);<br>
run(“Analyze Particles…”, “size=20- show=Outlines display summarize add”);<br>
selectWindow(“Drawing of Classified image.jpg (green)”);</p>
<p>I recorded it with the macro recorder in Image J and when I run the commands myself it works. The 1st problem is the following error message. I get this for lines 4, 5, 6 and 8.</p>
<p>java.lang.reflect.InvocationTargetException<br>
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>
at java.lang.reflect.Method.invoke(Method.java:498)<br>
at ij.macro.Functions.call(Functions.java:4620)<br>
at ij.macro.Functions.getStringFunction(Functions.java:277)<br>
at ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)<br>
at ij.macro.Interpreter.getString(Interpreter.java:1498)<br>
at ij.macro.Interpreter.doStatement(Interpreter.java:336)<br>
at ij.macro.Interpreter.doStatements(Interpreter.java:267)<br>
at ij.macro.Interpreter.run(Interpreter.java:163)<br>
at ij.macro.Interpreter.run(Interpreter.java:93)<br>
at ij.macro.Interpreter.run(Interpreter.java:107)<br>
at ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)<br>
at ij.plugin.Macro_Runner.runMacroFile(Macro_Runner.java:146)<br>
at ij.plugin.Macro_Runner.run(Macro_Runner.java:39)<br>
at ij.IJ.runPlugIn(IJ.java:209)<br>
at ij.Executer.runCommand(Executer.java:152)<br>
at ij.Executer.run(Executer.java:70)<br>
at java.lang.Thread.run(Thread.java:750)<br>
Caused by: java.lang.NullPointerException<br>
at trainableSegmentation.Weka_Segmentation$CustomWindow.updateAddClassButtons(Weka_Segmentation.java:1323)<br>
at trainableSegmentation.Weka_Segmentation.changeClassName(Weka_Segmentation.java:3250)<br>
… 20 more</p>
<p>Despite this error message, Image J continues to work. The 2nd problem is the error message at the end that it cannot find the classified image. That’s why I entered a wait time of 30 minutes, because that’s how long it normally takes the program to apply the record. Still, I get the same error message. It says I need to train the classifier, which I do in the macro, but it still doesn’t work.</p>
<p>I would be very grateful for any suggestions on how I can fix the problem. Unfortunately, I don’t know much about programming.</p> ;;;; <aside class="quote no-group" data-username="Davide_Cangelosi" data-post="10" data-topic="77953">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/davide_cangelosi/40/68366_2.png" class="avatar"> Davide Cangelosi:</div>
<blockquote>
<p>Do you believe these can help?</p>
</blockquote>
</aside>
<p>It’s hard to say from the outside, but I do believe that currently you are not able to acquire locks on your NFS, yes.</p>
<p>~J.</p> ;;;; <p>Hi Steve, could yo provide a sample image? or tell us more about what file type was the image stored as? How are you loading it into CellProfiler?</p> ;;;; <p>I’m playing with GPU in pyclesperanto on my Mac with Apple M1 Pro chip. In macOS-13.2.1-arm64-arm-64bit. environment, when I try to select device by</p>
<pre><code class="lang-auto">cle.select_device(dev_type='gpu')
</code></pre>
<p>I get</p>
<pre><code class="lang-auto">---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[51], line 2
      1 # Select GPU for processing
----&gt; 2 cle.select_device(dev_type='gpu')

File ~/mambaforge/envs/cellpose-env/lib/python3.9/site-packages/pyclesperanto_prototype/_tier0/_device.py:75, in select_device(name, dev_type, score_key, device_index)
     71 except:
     72     pass
---&gt; 75 device = filter_devices(name, dev_type, score_key)[device_index]
     76 if name is not None and name not in device.name:
     77     warnings.warn(f"No OpenCL device found with {name} in their name. Using {device.name} instead.")

IndexError: list index out of range
</code></pre>
<p>The same command in macOS-10.16-x86_64-i386-64bit. environment gives</p>
<pre><code class="lang-auto">&lt;Apple M1 Pro on Platform: Apple (2 refs)&gt;
</code></pre>
<p>As expected.</p>
<p>Is this a feature, or a bug? I’m just starting with GPUs and I got confused by this behaviour.<br>
Thanks!</p>
<p>Edit: Now I realise this might actually be coming from the fact that the arm64 environment is set up by mamba, and the x86 environment by conda…</p> ;;;; <p>Oh yes, it’s just that it was a bit long to put in the middle of a sentence ! I tried writing just “log_results.txt” (so it would be saved in the current directory, like my .out and .err), or putting the same address as my results directory ! Sorry it wasn’t clear</p> ;;;; <p>Hi <a class="mention" href="/u/vedsharma">@vedsharma</a> thanks for testing Fast4DReg <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Some tips for working with large datasets: You can try the RAM saving mode, it keeps the original bit-depth of your image, as on default Fast4DReg returns a 32-bit image. This might help if you are working with 8-bit data, but if you are working with 200GB files, I doubt this will work.</p>
<p>Another solution would be to divide your video into shorter sequences, depending on what frame you use as a reference. If you use the previous frame as a reference, you can split your video into sequences so that the last frame of a sequence is if first of the following sequence (eg. sequence 1: frames 1-10, sequence 2: frames 10-20 etc…). If you use the first frame as a reference, you need to for the splitting so that you always add the first frame of the whole sequence as the first of the shorter sequences (eg. sequence 1: frames 1-10, sequence 2: frames 1,11-20). After correction, you need to concatenate the short sequences and remove the overlapping frames. This way you can correct your data in pieces and save RAM. Remember to untick the crop option If you use this approach.</p>
<p>Please let me know if this helps!  <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Best,<br>
Joanna</p> ;;;; <aside class="quote no-group" data-username="Judith_Pineau" data-post="3" data-topic="78122">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/judith_pineau/40/68638_2.png" class="avatar"> Judith Pineau:</div>
<blockquote>
<p>Weirdly, it doesn’t seem to save a log file, I don’t know why. I tried with log_file=… and log file … (space of equal sign), and I never see anything. In the python script, I create a folder in the directory and this works, so I don’t think it’s an authorization problem (?).</p>
</blockquote>
</aside>
<p>just really quick, by <code>...</code> I meant that you’d have to put a filename there that you could find again… Just to clarify that you’ve used <code>...</code> here also as a placeholder…</p> ;;;; <p>Hello Dominik !</p>
<p>Thank you for your super fast answer and warm welcome, and for editing my post - indeed it is a lot easier to read like this!</p>
<p>So, I tried to run the command directly in a .sh code (see below, please tell me if you see a mistake in the code <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"> ), and it’s not giving any output either. I had converted my images to hdf5 as advised on the Ilastik website, and I trained the model on timelapse file in hdf5 (so yes, it should run on the complete timelapse hopefully).</p>
<pre><code class="lang-python">#!/bin/bash

#SBATCH --job-name=imageanalysisgast
#SBATCH --time=5:00:00
#SBATCH --cpus-per-task=5
#SBATCH --mail-type=end
#SBATCH --array=1-1%5
#SBATCH --mail-user=judith.pineau@pasteur.fr
#SBATCH --error first_batch.err
#SBATCH --output output_file.out

srun ./Python_Ilastik/ilastik-1.4.0-Linux/run_ilastik.sh --headless \
													--readonly	\
													--project=./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp \
													--stack_along="t" \
													--export_source="Probabilities Stage 2" \
													--ouput_format="hdf5" \
													--output_filename_format="{nickname}_{result_type}.h5" \
													--log_file= "log.txt" \
													./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA1-1_1stitch.tif_z.tif.h5


echo "This is array task ${SLURM_ARRAY_TASK_ID}, the sample name is ${nickname}." &gt;&gt; output.txt
exit 0
</code></pre>
<p>I already had access to the.out files actually, and for the python run all they printed was what I asked them to:</p>
<pre><code class="lang-python">

./run_ilastik.sh --headless --project="./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp" --export_source="Probabilities Stage 2" --stack_along="t" --log_file "./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/log_results.txt" --output_filename_format="./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/{nickname}_results.h5" "./TL_BrightField_IsmaProject/Test_HemiBra_hdf5./221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA10-1_1stitch.tif_z.tif.h5"
0


./run_ilastik.sh --headless --project="./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp" --export_source="Probabilities Stage 2" --stack_along="t" --log_file "./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/log_results.txt" --output_filename_format="./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/{nickname}_results.h5" "./TL_BrightField_IsmaProject/Test_HemiBra_hdf5./221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA1-1_1stitch.tif_z.tif.h5"
0
</code></pre>
<p>(actually I don’t know what the 0 at the end is, but the rest is the output from the “print(g)”</p>
<p>Weirdly, it doesn’t seem to save a log file, I don’t know why. I tried with log_file=… and log file … (space of equal sign), and I never see anything. In the python script, I create a folder in the directory and this works, so I don’t think it’s an authorization problem (?).</p>
<p>Also, exit status is code 0, so it’s not indicating any issue.</p>
<p>I tried to check if the node on the cluster can see my ilastik and my model, so I checked by going into the directory and typing ls -l, and it can see it (I also made sure it had execution permission).</p>
<p>And I will look into the link you put, my python is definitely still a bit messy and a patchwork from bits of code I find here and there, I should definitely make it a bit nicer and human-friendly !</p>
<p>Cheers</p>
<p>Judith</p> ;;;; <p>Hi,</p>
<p>I try to set the measurements exported in the xls files : What ever I choose the xls files contains always the same :<br>
Detection # Surface|x|y|z|t min intensity max intensity average intensity</p>
<p>I would like to get the Area (in µm) of the ROI but it never saves it in batch mode.<br>
Doing the export individually works !!!<br>
Any advice ?</p> ;;;; <p>Dear all, Is there a way to open the different slices in Qupath directly from a z stcak images (czi) and to process these differents images as series of  images ? I see that is possible to export one slice (in Export images) but not all individually ? Waiting for a 3D version of Qupath that will be incredible but I presume that in is not is the box ? Thanks, Mathieu</p> ;;;; <p>Hi,<br>
I try to set the measurements exported in the xls files : What ever I choose the xls files contains always the same :<br>
Detection # Surface|x|y|z|t min intensity max intensity  average intensity</p>
<p>I would like to get the Area (in µm) of the ROI but it never saves it in batch mode.<br>
Doing the export individually works !!!<br>
Any advice ?</p> ;;;; <p>Hello <a class="mention" href="/u/judith_pineau">@Judith_Pineau</a>,</p>
<p>first of all, welcome to the image.sc community <img src="https://emoji.discourse-cdn.com/twitter/sun_with_face.png?v=12" title=":sun_with_face:" class="emoji" alt=":sun_with_face:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/tada.png?v=12" title=":tada:" class="emoji" alt=":tada:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/handshake.png?v=12" title=":handshake:" class="emoji" alt=":handshake:" loading="lazy" width="20" height="20"></p>
<p>Let’s try debugging it together. I think you’re probably almost there.</p>
<p>Absolute first thing I’d try would be to run the command I run on slurm locally (so running ilastik with the same project file and one of the images you want to process). What is your data format? And also, the job is supposed to run on the complete image (time series), right?</p>
<p>First thing I would try, would be also redirecting <code>stdout</code> via the the <code>#SBATCH --output ...</code> option, so you would at least get the print statements from the python script.</p>
<p>What exit status did your slurm job have?</p>
<p>You can also specify the logfile for ilastik, which could give you additional insights, via <code>run_ilastik.sh --headless --log_file /path/to/desired/location.txt...</code></p>
<aside class="quote no-group" data-username="Judith_Pineau" data-post="1" data-topic="78122">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/judith_pineau/40/68638_2.png" class="avatar"> Judith Pineau:</div>
<blockquote>
<p>Also, my model was trained on the Ilastik on my desktop, so on Windows 10 and the previous ilastik version, could that be an issue?</p>
</blockquote>
</aside>
<p>This should not be a problem if you made sure the classifier was saved in ilastik (make sure you have live-update on, see some predictions, (then you can turn live-update off again) then save). Then it doesn’t access the training data anymore. If the classifier isn’t saved, ilastik will try to train it in headless mode and fail, if it cannot reach the data (which can happen if data is not saved into the project file).</p>
<p>In the python script, are you sure that the node on the cluster can see ilastik in the directory you specified? (<code>"./Python_Ilastik/ilastik-1.4.0-Linux"</code>). Same goes for the project file.</p>
<p>You also might want to look into (if you want to broaden your python game a little):</p>
<ul>
<li>
<a href="https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals">Formatted String Literals (f-strings)</a> for easier (human) readability instead of the “old” string-formatting with the <code>%</code> operator.</li>
<li>
<a href="https://docs.python.org/3/library/pathlib.html"><code>pathlib</code></a> for all operations where you compose paths, make directories and so on. Just so you don’t have to worry about separators and such.</li>
</ul>
<p>Cheers<br>
Dominik</p>
<p><em>Edit</em>: I have edited your post a bit to make it a little more readable, by adding formatting to your code blocks. See this post for details: <a href="https://forum.image.sc/t/how-to-put-code-in-a-post/3698/3" class="inline-onebox">How to put code in a post? - #3 by imagejan</a></p> ;;;; <p>Hi again,<br>
I am pretty sure that the crashes are due to lack of memory.<br>
Just wanted to let you know that I found “a solution” (not  necessary the best): Instead of stitching all the tiles at ones, I split them into N sets such that each one can be stitch and analysed by the machine without issues. I used the tif metadata of the distances to establish the different sets. This solution worked well for me but now I have a two new question…</p>
<ol>
<li>Is there any metadata (and some way to access it) in the sititched ome.tif image? I tried to use the same tif tags and it did not work</li>
</ol>
<p>2)How is the min/max of the channels determined in a stitched image?</p>
<p>I have a set of many tiles and I split them into 2 subsets (see image. I apologise for the aspect ratio, but I was trying to do an illustrative example using only screenshots).  It’s all good but for some reason the two subsets have different colours. My guess is that the stitching script takes some sort of average from the tiles used in the stitching and that subset happen to have a different mean?<br>
Many thanks in advance<br>
<div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce32d609d0543fa78b0372291ceb623f2696ba0b.jpeg" data-download-href="/uploads/short-url/tq7h23aaVH2U2oadhqkGowjrxxN.jpeg?dl=1" title="Screenshot from 2023-03-06 13-05-27" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce32d609d0543fa78b0372291ceb623f2696ba0b_2_319x500.jpeg" alt="Screenshot from 2023-03-06 13-05-27" data-base62-sha1="tq7h23aaVH2U2oadhqkGowjrxxN" width="319" height="500" srcset="https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce32d609d0543fa78b0372291ceb623f2696ba0b_2_319x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce32d609d0543fa78b0372291ceb623f2696ba0b_2_478x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce32d609d0543fa78b0372291ceb623f2696ba0b.jpeg 2x" data-dominant-color="4C5B56"><div class="meta">
<svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">Screenshot from 2023-03-06 13-05-27</span><span class="informations">523×819 69.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg>
</div></a></div></p> ;;;; <p>Dear <a class="mention" href="/u/anna_teruel">@Anna_Teruel</a>,</p>
<p>The Object Predictions image is a integer value image (so it has values like <code>1, 2, 3, 4...</code> depending on how many classes you have. With three classes/labels, you’d get an image where all pixels of objects for the first label/class will have a value of 1, objects that get the second class/label will have a value of 2… and so on. <code>0</code> is  a special label that is assigned in Object classification to the background of the image (-&gt; not any object). When exporting to png there are some pitfals. One would be related to normalization - did you change any settings there? Could you check?</p>
<p>Another common one would be related to viewing the result - from <a href="https://www.ilastik.org/documentation/basics/common_problems#4-my-exported-results-are-all-black">No. 4 in our FAQ</a>: In any case you could try to change the lookup table when looking at the results, this makes small differences (as, e.g. between pixels that have 1s and 2s) apparent - we recommend glasbey (<em>Image</em> → <em>Lookup Tables</em> → <em>glasbey</em>), or a similar non-sequential lookup table.</p>
<p>Hope we can resolve this quickly</p>
<p>Cheers<br>
Dominik</p> ;;;; <p>Hi Arif,<br>
can you show me how you got the area for each cell</p> ;;;; <p>It finally works ! Thank you very much ! <img src="https://emoji.discourse-cdn.com/twitter/smile.png?v=12" title=":smile:" class="emoji" alt=":smile:" loading="lazy" width="20" height="20"></p> ;;;; <p>Thank you.</p>
<p>I’ll add more body parts in my next attempt.</p>
<p>I’ll let you know how I go during the week.</p>
<p>Have a great day</p> ;;;; <p>More is generally better. You have to make one for visualization, but on the training part it will make the fully redundant skeleton on it’s own</p> ;;;; <p>Thank you for your prompt reply <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>Sounds good - do I need to create a skeleton as well?</p>
<p>Is 2 the minimum number of body parts or would 4 (for example) be better?</p>
<p>Thanks</p>
<p>Grant</p> ;;;; <p>I think the issue is the Z spacing of your data. As I mentioned, <code>50um</code> isn’t sufficient to accurately detect cells. I think you could likely get the software running by setting <code>--ball-z-size</code> to something like 40, but this won’t result in good results.</p> ;;;; <p>You cannot use maDLC with one bodypart per animal. The tracklets are empty because there are no tracklets. There has to be at least a pair of bodyparts per animal.</p> ;;;; <p>It was quite a long time ago, but reading the stack as a numpy array and saving it with nibabel should do the job.</p>
<p>You can find an example code <a href="https://github.com/JacobBumgarner/VesselVio/issues/16#issuecomment-1083477924" rel="noopener nofollow ugc">here</a></p>
<p>Let us know if you have any issues.</p> ;;;; <p>Hi</p>
<p>I’ve been only using DeepLabCut for a short period and have successfully used the GUI to perform a single animal project.</p>
<p>However, I was hoping to create a multi-animal project to track 2 individuals with a single body part.</p>
<p>I’ve spent a couple of weeks trying to work it out, including following instructions and forum discussions, but I can’t seem to figure it out.</p>
<p>To begin, I’ve been using the GUI to create a multi-animal project, extract frames, label frames (20 frames), create training dataset, and train network (30K iterations - apparently fewer iterations are required for multi-animal projects…?). Then, I used coding in the terminal to evaluate network, and with my limited knowledge, the performance looks ok (pixel error below 2 [results in uploaded file] and RMSE below 5 [most RMSE scores were 1-3 with one body part scoring a 5 for a single frame]).</p>
<p>I also performed the create_video_with_all_detections command and from what I can tell the targets in the full.mp4 video look fine.</p>
<p>I tried the extract_outlier_frames command (thinking I could remove less accurate frames) but I couldn’t figure it out…? (just read you can’t extract_outlier_frames until after stitch_tracklets…?)</p>
<p>Anyway, everything looks ok, so I moved to analysing_video with auto-track = FALSE (because I couldn’t work out why an h5 file wasn’t been generated when auto-track = TRUE…?) and executed the convert_ detections2tracklets command with a prompt indicating that tracklets were created. However, when I performed the stitch_tracklets command I receive a message saying tracklets are empty…?</p>
<p>From what I’ve read, tracklets are empty indicates the training has not been accurate. But, from my understanding, the metrics look OK… but I guess I’m wrong…?</p>
<p>From what I’ve read, I might need to perform further training (does that mean more iterations in training?) or include more body parts, or label more frames… (or all the above?)</p>
<p>Please find my command information attached.</p>
<p>Any help will be greatly appreciated <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p><a class="attachment" href="/uploads/short-url/cwGzxnHD3P81ENMfkVMqRy6MALO.txt">060323 Script.txt</a> (8.8 KB)</p> ;;;; <p>I’m currently making a pretty big update. I’ll update the repository and will ping you back. Normally you should be able to keep the same env.</p>
<p>Which OS are you on ?</p>
<p>PS: The error is on the Java side, what you could quickly try is look for this line:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27">
  <header class="source">

      <a href="https://github.com/NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27" target="_blank" rel="noopener">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27" target="_blank" rel="noopener">NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27</a></h4>



    <pre class="onebox"><code class="lang-py">
      <ol class="start lines" start="17" style="counter-reset: li-counter 16 ;">
          <li>
          </li>
<li>def get_java_dependencies():</li>
          <li>    """</li>
          <li>    Returns the jar files that need to be included into the classpath</li>
          <li>    of an imagej object in order to have a functional ABBA app</li>
          <li>    these jars should be available in https://maven.scijava.org/</li>
          <li>    :return:</li>
          <li>    """</li>
          <li>    imagej_core_dep = 'net.imagej:imagej:2.9.0'</li>
          <li>    imagej_legacy_dep = 'net.imagej:imagej-legacy:0.39.3'</li>
          <li class="selected">    abba_dep = 'ch.epfl.biop:ImageToAtlasRegister:0.4.3'</li>
          <li>    return [imagej_core_dep, imagej_legacy_dep, abba_dep]</li>
          <li>
          </li>
<li>def add_brainglobe_atlases(ij):</li>
          <li>    # TODO : check connection available or not</li>
          <li>    try:</li>
          <li>        check_internet_connection()</li>
          <li>        available_atlases = get_all_atlases_lastversions()</li>
          <li>    except ConnectionError:</li>
          <li>        available_atlases_nodict = get_downloaded_atlases()</li>
          <li>        available_atlases = dict()</li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>And replace <code>ch.epfl.biop:ImageToAtlasRegister:0.4.3</code> with <code>ch.epfl.biop:ImageToAtlasRegister:0.5.1</code></p> ;;;; <p>Great, thank you!</p> ;;;; <p>Hi <a class="mention" href="/u/mihovil_p">@Mihovil_P</a>, would you be able to provide a link to a sample file that reproduces the issue. If you need a suitable upload location then we recommend using <a href="https://zenodo.org/">https://zenodo.org/</a></p> ;;;; <p>Hello !<br>
I have been trying to do segmentation of very bad/noisy bright field images, and Ilastik helped me tremendously. However, as it is using a lot of memory and I am analyzing a lot of timelapse, I wanted to move the analysis to a HPC in my institute, as my desktop couldn’t deal with it anymore.<br>
However, I tried to (1) run directly Ilastik using the run_ilastik.sh or (2) run Ilastik via python using subprocess, and in both cases I don’t have an error message, but I also don’t have any output.</p>
<p>Here is what I am doing:<br>
on the cluster:</p>
<pre><code class="lang-auto">#!/bin/bash

#SBATCH --job-name=imageanalysisgast
#SBATCH --time=5:00:00
#SBATCH --cpus-per-task=5
#SBATCH --mail-type=end
#SBATCH --array=1-2%5
#SBATCH --mail-user=judith.pineau@pasteur.fr
#SBATCH --error first_batch.err


IFS=''
readarray -t config &lt; './TL_BrightField_IsmaProject/Test_HemiBra_hdf5/FileList.txt'

sample=${config[(($SLURM_ARRAY_TASK_ID - 1))]}
module load Python/3.11.0 || exit 1

source Bash/202303_JP_Ilastik/bin/activate || exit 2


python3 "./Bash/Python_scripts/20230303_Ilastik_cluster.py" "./TL_BrightField_IsmaProject/Test_HemiBra_hdf5" $sample

echo "This is array task ${SLURM_ARRAY_TASK_ID}, the sample name is ${sample}." &gt;&gt; output.txt
exit 0
</code></pre>
<p>And my Python script:</p>
<pre><code class="lang-python">#!/usr/bin/python3

# -*- coding: utf-8 -*-
"""
Created on Thu Oct 13 11:50:41 2022

@author: Judith
"""

#!/usr/bin/python
#coding:utf-8
import os

import sys # to handle exceptions
#Source directory
import re, glob, os #navigation in diff repertory if script not in the same folder
from os import walk #Find folders and files
from os.path import isfile, join
import subprocess

#Test from forum @CellKai
ilastik_location = "./Python_Ilastik/ilastik-1.4.0-Linux"
ilastik_project = './TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp'

indir = sys.argv[1] #my dataset
infile = sys.argv[2]

os.makedirs(indir+"/results", exist_ok = True)

os.chdir(ilastik_location)

command = './run_ilastik.sh --headless --project="%s" --export_source="Probabilities Stage 2" --stack_along="t" --output_filename_format="%s/results/{nickname}_results.h5" "%s%s"' % (
        ilastik_project,
        indir,
        indir,
        infile)
print("\n\n%s" % command)
g=subprocess.call(command, shell=True)
print(g)
</code></pre>
<p>I don’t know why it doesn’t work, maybe my environment is not the proper one. I know there is a python interpreter included in Ilastik but I don’t know if I should use it, and if so, how to use it on the cluster ?</p>
<p>Also, my model was trained on the Ilastik on my desktop, so on Windows 10 and the previous ilastik version, could that be an issue?</p>
<p>I am sorry, I hope no information is missing in my question, I am relatively new to python, and even newer to using the cluster !</p>
<p>I hope someone can help me as I have been stuck for some time, and I really don’t know what to try (since I don’t see any error message)</p>
<p>Thanks !</p>
<p>Judith</p> ;;;; <p>Dear Josh,<br>
trying to figure it out I searched for potential solutions and I found something at this link</p><aside class="onebox stackexchange" data-onebox-src="https://serverfault.com/questions/969963/nfs-clients-fail-to-get-lock-on-files">
  <header class="source">

      <a href="https://serverfault.com/questions/969963/nfs-clients-fail-to-get-lock-on-files" target="_blank" rel="noopener nofollow ugc">serverfault.com</a>
  </header>

  <article class="onebox-body">
      <a href="https://serverfault.com/users/281874/rocky" target="_blank" rel="noopener nofollow ugc">
    <img alt="rocky" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/5/354a993161dddf7d8873e00cd4c1fffe33c5b8a4.png" class="thumbnail onebox-avatar" width="256" height="256">
  </a>

<h4>
  <a href="https://serverfault.com/questions/969963/nfs-clients-fail-to-get-lock-on-files" target="_blank" rel="noopener nofollow ugc">NFS clients fail to get lock on files</a>
</h4>

<div class="tags">
  <strong>centos7, nfs, freebsd, locking</strong>
</div>

<div class="date">
  asked by
  
  <a href="https://serverfault.com/users/281874/rocky" target="_blank" rel="noopener nofollow ugc">
    rocky
  </a>
  on <a href="https://serverfault.com/questions/969963/nfs-clients-fail-to-get-lock-on-files" target="_blank" rel="noopener nofollow ugc">04:28PM - 03 Jun 19 UTC</a>
</div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>or at this other link<br>
<a href="https://www.veritas.com/support/en_US/article.100011882" class="onebox" target="_blank" rel="noopener nofollow ugc">https://www.veritas.com/support/en_US/article.100011882</a></p>
<p>Do you believe these can help?</p> ;;;; <p>Hello <a class="mention" href="/u/netama">@netama</a>,</p>
<p>does this really happen when you <em>start</em> a new project? I would expect this code path to fire when using the <em>Suggest Features</em> functionality…</p>
<p>I tried reproducing it, and indeed I can reach it, if I</p>
<ol>
<li>create a new pixel classification project</li>
<li>load some data</li>
<li>select feauteres</li>
<li>click on <em>Suggest Features</em>, without adding <em>any</em> annotations.</li>
</ol>
<p>It would be important to know whether you arrived there with a different path. The one I outlined is a bug, and this we can fix.</p>
<p>Sorry that you ran into this!<br>
Cheers<br>
Dominik</p> ;;;; <p>Hello <a class="mention" href="/u/wang1">@wang1</a>,</p>
<p>first of all, welcome to the image.sc community! <img src="https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12" title=":partying_face:" class="emoji" alt=":partying_face:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/tada.png?v=12" title=":tada:" class="emoji" alt=":tada:" loading="lazy" width="20" height="20"> <img src="https://emoji.discourse-cdn.com/twitter/sun_with_face.png?v=12" title=":sun_with_face:" class="emoji" alt=":sun_with_face:" loading="lazy" width="20" height="20"></p>
<p>I’m afraid this could be related to the non-ascii characters in your file path :(. The library we use to load tiff stacks exhibits this unfortunate <a href="https://github.com/ilastik/ilastik/issues/2431">issue</a>. The current workaround would be to either</p>
<ol>
<li>move the file to a path with only ascii characters, or</li>
<li>
<a href="https://www.ilastik.org/documentation/fiji_export/plugin">convert the file to hdf5 using our import/export plugin in fiji</a> - the problem with the pathnames does not surface with hdf5.</li>
</ol>
<p>Hope this helps!</p>
<p>Cheers<br>
Dominik</p> ;;;; <aside class="quote no-group" data-username="kyang" data-post="4" data-topic="77899">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/k/71e660/40.png" class="avatar"> Katie:</div>
<blockquote>
<p>Do you have a suggestion on how to segment from the background?</p>
</blockquote>
</aside>
<p>Maybe by thresholding it, but it’s hard to tell without seeing an image.</p> ;;;; <p>Hi <a class="mention" href="/u/davide_cangelosi">@Davide_Cangelosi</a>,</p>
<p>The Tables info you’ve seen looks reasonable, but that won’t be your fundamental issue since it’s more of an additional service.</p>
<p>~Josh</p> ;;;; <aside class="quote no-group" data-username="Seoungwan" data-post="1" data-topic="78110">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/seoungwan/40/68633_2.png" class="avatar"> Seoung Wan Chae:</div>
<blockquote>
<p><code>files.size</code></p>
</blockquote>
</aside>
<p><code>files.size</code> should be <code>files.size()</code></p>
<p>Reason: <code>file.size</code> looks for a size property of the file – but there is no such property available. On the other hand, <code>file.size()</code> calls a method that returns the size.</p>
<p>Groovy is relaxed about many things, but seems to be strict about this distinction between properties and methods.</p>
<p>(Confusingly, Groovy isn’t always strict. If the method was called <code>file.getSize()</code> then <code>file.size</code> would work… because methods in the format <code>getXXX()</code> and <code>setXXX()</code> are treated differently…)</p> ;;;; <p>Awesome, thank you! <img src="https://emoji.discourse-cdn.com/twitter/smiley.png?v=12" title=":smiley:" class="emoji" alt=":smiley:" loading="lazy" width="20" height="20"></p> ;;;; <p>Hi <a class="mention" href="/u/moseyic">@moseyic</a> , what exactly are you trying to replicate and doesn’t look right in napari?</p> ;;;; <p>Hello everyone<br>
I have already segmented images of single cells and want to extract features of each individual cell. So far I used scikit.regionprops for the feature extraction, but I wondered if there is any other option with more possibilities.<br>
Thanks!</p> ;;;; <p>Hey everyone I am trying to run ABBA via the python notebooks for registering my slices to the Allen Mouse Brain Atlas imported via QuPath.</p>
<p>the execution fails at the Deepslice regsitration phase:</p>
<pre><code class="lang-auto"># a first deepslice registration round : possible because it's the Allen CCF atlas, cut in coronal mode

# what's assumed : the sections are already in the correct order

abba.register_slices_deepslice(channels=[0, 1])
</code></pre>
<p>It seems to export the slices to the temp folder but fails at export</p>
<p><code>[java.lang.Enum.toString] Export of slice Slide 1-Scene-15-TR12.czi_DAPI done (3/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-01-TR1.czi_DAPI done (5/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-05-TR4.czi_DAPI done (7/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-09-TR9.czi_DAPI done (9/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-13-TR6.czi_DAPI done (2/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-07-TR17.czi_DAPI done (8/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-03-TR2.czi_DAPI done (6/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-17-TR13.czi_DAPI done (4/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-11-TR8.czi_DAPI done (1/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export as QuickNii Dataset done - Folder : [c:\Users\Fus\Desktop\1-ABBA\temp\deepslice](file:///C:/Users/Fus/Desktop/1-ABBA/temp/deepslice)[java.lang.Enum.toString]</code></p>
<p>The full output of the cell is as follows:</p>
<pre><code class="lang-auto">{
	"name": "java.util.concurrent.ExecutionException",
	"message": "java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception",
	"stack": "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31morg.jpype.PyExceptionProxy\u001b[0m                Traceback (most recent call last)\nFile \u001b[1;32mThread.java:833\u001b[0m, in \u001b[0;36mjava.lang.Thread.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mThreadPoolExecutor.java:635\u001b[0m, in \u001b[0;36mjava.util.concurrent.ThreadPoolExecutor$Worker.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mThreadPoolExecutor.java:1136\u001b[0m, in \u001b[0;36mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mFutureTask.java:264\u001b[0m, in \u001b[0;36mjava.util.concurrent.FutureTask.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mDefaultThreadService.java:225\u001b[0m, in \u001b[0;36morg.scijava.thread.DefaultThreadService.lambda$wrap$2\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mModuleRunner.java:63\u001b[0m, in \u001b[0;36morg.scijava.module.ModuleRunner.call\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mModuleRunner.java:124\u001b[0m, in \u001b[0;36morg.scijava.module.ModuleRunner.call\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mModuleRunner.java:163\u001b[0m, in \u001b[0;36morg.scijava.module.ModuleRunner.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mCommandModule.java:196\u001b[0m, in \u001b[0;36morg.scijava.command.CommandModule.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mRegisterSlicesDeepSliceCommand.java:158\u001b[0m, in \u001b[0;36mch.epfl.biop.atlas.aligner.command.RegisterSlicesDeepSliceCommand.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mjdk.proxy2.$Proxy23.java:-1\u001b[0m, in \u001b[0;36mjdk.proxy2.$Proxy23.apply\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32morg.jpype.proxy.JPypeProxy.java:-1\u001b[0m, in \u001b[0;36morg.jpype.proxy.JPypeProxy.invoke\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32morg.jpype.proxy.JPypeProxy.java:-2\u001b[0m, in \u001b[0;36morg.jpype.proxy.JPypeProxy.hostInvoke\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32morg.jpype.JPypeContext.java:-1\u001b[0m, in \u001b[0;36morg.jpype.JPypeContext.createException\u001b[1;34m()\u001b[0m\n\n\u001b[1;31morg.jpype.PyExceptionProxy\u001b[0m: org.jpype.PyExceptionProxy\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[1;31mjava.lang.RuntimeException\u001b[0m                Traceback (most recent call last)\nFile \u001b[1;32mThread.java:833\u001b[0m, in \u001b[0;36mjava.lang.Thread.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mThreadPoolExecutor.java:635\u001b[0m, in \u001b[0;36mjava.util.concurrent.ThreadPoolExecutor$Worker.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mThreadPoolExecutor.java:1136\u001b[0m, in \u001b[0;36mjava.util.concurrent.ThreadPoolExecutor.runWorker\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mFutureTask.java:264\u001b[0m, in \u001b[0;36mjava.util.concurrent.FutureTask.run\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mDefaultThreadService.java:225\u001b[0m, in \u001b[0;36morg.scijava.thread.DefaultThreadService.lambda$wrap$2\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mModuleRunner.java:63\u001b[0m, in \u001b[0;36morg.scijava.module.ModuleRunner.call\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mModuleRunner.java:127\u001b[0m, in \u001b[0;36morg.scijava.module.ModuleRunner.call\u001b[1;34m()\u001b[0m\n\n\u001b[1;31mjava.lang.RuntimeException\u001b[0m: java.lang.RuntimeException: Module threw exception\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[1;32mFutureTask.java:191\u001b[0m, in \u001b[0;36mjava.util.concurrent.FutureTask.get\u001b[1;34m()\u001b[0m\n\nFile \u001b[1;32mFutureTask.java:122\u001b[0m, in \u001b[0;36mjava.util.concurrent.FutureTask.report\u001b[1;34m()\u001b[0m\n\n\u001b[1;31mException\u001b[0m: Java Exception\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[1;31mjava.util.concurrent.ExecutionException\u001b[0m   Traceback (most recent call last)\nCell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# a first deepslice registration round : possible because it's the Allen CCF atlas, cut in coronal mode\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# what's assumed : the sections are already in the correct order\u001b[39;00m\n\u001b[1;32m----&gt; 3\u001b[0m abba\u001b[39m.\u001b[39;49mregister_slices_deepslice(channels\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m])\n\u001b[0;32m      5\u001b[0m \u001b[39m# second deepslice registration: because the slices are resampled for the registration,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# we usually get a slightly better positioning along z and cutting angle\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# also: it's fast, and the combination of two affine transforms is\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# an affine transform, so it's not like we are adding extra degrees of freedom\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m#abba.register_slices_deepslice(channels=[0, 1])\u001b[39;00m\n\nFile \u001b[1;32mc:\\Users\\Fus\\Desktop\\1-ABBA\\abba\\Abba.py:237\u001b[0m, in \u001b[0;36mAbba.register_slices_deepslice\u001b[1;34m(self, channels, allow_slicing_angle_change, allow_change_slicing_position, maintain_slices_order, affine_transform)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFailed to delete \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Reason: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (file_path, e))\n\u001b[0;32m    236\u001b[0m \u001b[39m# Any missing input parameter will lead to a popup window asking the missing argument to the user\u001b[39;00m\n\u001b[1;32m--&gt; 237\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mij\u001b[39m.\u001b[39;49mcommand()\u001b[39m.\u001b[39;49mrun(RegisterSlicesDeepSliceCommand, \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    238\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mchannels\u001b[39;49m\u001b[39m\"\u001b[39;49m, JString(\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mmap\u001b[39;49m(\u001b[39mstr\u001b[39;49m, channels))),\n\u001b[0;32m    239\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mimage_name_prefix\u001b[39;49m\u001b[39m\"\u001b[39;49m, JString(\u001b[39m'\u001b[39;49m\u001b[39mSection\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    240\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mmp\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmp,\n\u001b[0;32m    241\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mallow_slicing_angle_change\u001b[39;49m\u001b[39m\"\u001b[39;49m, allow_slicing_angle_change,\n\u001b[0;32m    242\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mallow_change_slicing_position\u001b[39;49m\u001b[39m\"\u001b[39;49m, allow_change_slicing_position,\n\u001b[0;32m    243\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mmaintain_slices_order\u001b[39;49m\u001b[39m\"\u001b[39;49m, maintain_slices_order,\n\u001b[0;32m    244\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39maffine_transform\u001b[39;49m\u001b[39m\"\u001b[39;49m, affine_transform,\n\u001b[0;32m    245\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mdeepSliceProcessor\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_deep_slice,\n\u001b[0;32m    246\u001b[0m                              \u001b[39m\"\u001b[39;49m\u001b[39mdataset_folder\u001b[39;49m\u001b[39m\"\u001b[39;49m, JString(temp_folder)\n\u001b[0;32m    247\u001b[0m                              )\u001b[39m.\u001b[39;49mget()\n\n\u001b[1;31mjava.util.concurrent.ExecutionException\u001b[0m: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception"
}
</code></pre>
<p>I have tried creating a new conda environment from the yml file within ABBA-Python but that did not solve the issue.</p>
<p>Would greatly appreciate any help as I am sort of stuck.</p>
<p>Thank you.</p> ;;;; <p>I’m afraid I don’t know what’s going on and don’t have a solution – the <code>promptToImportImages</code> method looks horrendous (of course I wrote it…) and really looks like it should be refactored. Refactoring will be one of the main focuses of v0.5.0.</p>
<p>It looks like a bug to me, so I’ve created an issue on GitHub at</p><aside class="onebox githubissue" data-onebox-src="https://github.com/qupath/qupath/issues/1251">
  <header class="source">

      <a href="https://github.com/qupath/qupath/issues/1251" target="_blank" rel="noopener">github.com/qupath/qupath</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <div class="github-icon-container" title="Issue">
	  <svg width="60" height="60" class="github-icon" viewbox="0 0 14 16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
  </div>

  <div class="github-info-container">
    <h4>
      <a href="https://github.com/qupath/qupath/issues/1251" target="_blank" rel="noopener">ProjectCommands.promptToImportImages always returns an empty list</a>
    </h4>

    <div class="github-info">
      <div class="date">
        opened <span class="discourse-local-date" data-format="ll" data-date="2023-03-06" data-time="08:58:50" data-timezone="UTC">08:58AM - 06 Mar 23 UTC</span>
      </div>


      <div class="user">
        <a href="https://github.com/petebankhead" target="_blank" rel="noopener">
          <img alt="petebankhead" src="https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/bedfaac2750bf058873bc67485ebe9dac1f5ade4.png" class="onebox-avatar-inline" width="20" height="20">
          petebankhead
        </a>
      </div>
    </div>

    <div class="labels">
        <span style="display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;">
          bug
        </span>
    </div>
  </div>
</div>

  <div class="github-row">
    <p class="github-body-container">## Bug report

**Describe the bug**
`ProjectCommands.promptToImportImages` cl<span class="show-more-container"><a href="" rel="noopener" class="show-more">…</a></span><span class="excerpt hidden">aims to return a list of imported images, but it appears that it does not.

**To Reproduce**
Steps to reproduce the behavior:
1. Run in an IDE using debug mode, with a breakpoint around https://github.com/qupath/qupath/blob/f2a1f9c002726f1ee14c5685bf1ea67f4051f8d7/qupath-gui-fx/src/main/java/qupath/lib/gui/QuPathGUI.java#L3170
2. Open a project
3. Drag a new image onto the project and agree to import it
4. Check the list contains the image (spoiler: it doesn't)

**Expected behavior**
A list is returned, as the docs suggest it should.

Or, if not possible (because the import is delayed), the method signature should be updated or there should be an async version explicitly provided.

**Desktop (please complete the following information):**
 - OS: All, presumably
 - QuPath Version: v0.4.3 (and likely before)

**Additional context**
Reported by @Rdornier at at https://forum.image.sc/t/prompttoimportimages-returns-empty-list-of-images/78112</span></p>
  </div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>My <em>guess</em> is that at some point the import was shifted to other threads to prevent blocking, and consequently the list isn’t populated in time. But this could be wrong since I haven’t really checked the overly-long code in detail.</p>
<p>I guess a workaround might be to cache a <code>Set</code> of all the entries before the import, and then remove these from the <code>Set</code> of all entries after the import, to see what is new. Awkward, but the best idea I have for now.</p> ;;;; <aside class="quote no-group" data-username="Research_Associate" data-post="7" data-topic="77703">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png" class="avatar"> MicroscopyRA:</div>
<blockquote>
<p>Not sure if this is as intended <a class="mention" href="/u/petebankhead">@petebankhead</a></p>
</blockquote>
</aside>
<p><a class="mention" href="/u/finglis">@finglis</a> and I talked about this a few days ago <img src="https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>It’s debatable whether it’s a good idea, but it is indeed intended.</p>
<p>One justification: It’s common in QuPath for the ‘one main selected object’ to have significance, even when multiple objects are selected. If you have many annotations, it allows you to zoom in and focus on just aligning one – and with the ability to choose which. If the box was at the top, then that is potentially far away from any interesting structure in the image - and might require zooming out a lot.</p>
<p>You can choose the ‘main’ selected annotation by click on one with the ‘Alt’ key pressed before running ‘Transform annotations’.</p>
<p>Extra tips:</p>
<ul>
<li>Holding down Ctrl (or Cmd), you can use the arrow keys to translate the annotations. This is zoom-dependent (i.e. zoom out for larger translations).</li>
<li>Holding down Ctrl + Shift (or Cmd + Shift) the arrow keys perform rotation (left and right rotate faster than up and down).</li>
</ul>
<p>Using the shortcuts, finding the box doesn’t really matter so much.</p> ;;;; <p>Hello,</p>
<p>I would like to get the list of newly imported images. I saw that <a href="https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-gui-fx/src/main/java/qupath/lib/gui/commands/ProjectImportImagesCommand.java#L124" rel="noopener nofollow ugc">promptToImportImages</a> returns the list of imported images but when I try to get this list (from <a href="https://github.com/BIOP/qupath-extension-biop-omero/blob/fd5a717e1aa908df11c762c278a20af8b6a33f80/src/main/java/qupath/ext/biop/servers/omero/raw/OmeroRawImageServerBrowserCommand.java#L381" rel="noopener nofollow ugc">here</a> for example), I always get it empty.</p>
<p>Do you know how can I get this list ?<br>
Thanks,</p>
<p>Rémy.</p> ;;;; <p>In QuPath version 0.4.3, when importing a predictive model with importTiles.groovy, the following error occurs. Is this due to something changes in QuPath 0.4?</p>
<p><a href="https://github.com/andreped/NoCodeSeg/blob/61f6d76fe2d17874a807150c2aec13a247484cd0/source/importTiles.groovy" rel="noopener nofollow ugc">importTiles.groovy</a></p>
<p>error log</p>
<pre><code class="lang-auto">ERROR: It looks like you've tried to access a property 'size' that doesn't exist

ERROR: Exception evaluating property 'size' for java.util.ArrayList, Reason: groovy.lang.MissingPropertyException: No such property: size for class: java.io.File
Possible solutions: file, name in 03_Import_Tile.groovy at line number 84

ERROR: org.codehaus.groovy.runtime.DefaultGroovyMethods.getAtIterable(DefaultGroovyMethods.java:8542)
    org.codehaus.groovy.runtime.DefaultGroovyMethods.getAt(DefaultGroovyMethods.java:8530)
    groovy.lang.MetaClassImpl$7.getProperty(MetaClassImpl.java:2117)
    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
    03_Import_Tile.run(03_Import_Tile.groovy:84)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)
    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)
    qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)
    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)
    qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)
    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    java.base/java.lang.Thread.run(Unknown Source)

ERROR: 
For help interpreting this error, please search the forum at https://forum.image.sc/tag/qupath
You can also start a new discussion there, including both your script &amp; the messages in this log.

</code></pre>
<p>The line 84 is “int nbPatches = files.size;”</p>
<pre><code class="lang-auto">// loading bar
int spaces = 40;
float progress = 100.0;
int counter = 0;
int nbPatches = files.size;
</code></pre>
<p>Thank you so much,</p>
<p>Seoung Wan Chae.</p> ;;;; <aside class="quote no-group" data-username="Youngolaf" data-post="19" data-topic="70119">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png" class="avatar"> Youngolaf:</div>
<blockquote>
<p>I am still confused. I tried changing the variable in the code but it still isnt working. I also uncommented all the print statements and I’m getting the same output (weird).</p>
</blockquote>
</aside>
<p>Haha, I am confused too then. It sounds like you are not running the script. Even adding a <code>print "Hi"</code> at the beginning of the script should change the output to reflect the added print statement.</p>
<aside class="quote no-group" data-username="Youngolaf" data-post="19" data-topic="70119">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png" class="avatar"> Youngolaf:</div>
<blockquote>
<p>You said that the 900 outside is correct, and that the training area should be &gt; 0,</p>
</blockquote>
</aside>
<p>Yes, the script cycles through all images in the project. The first image, “Training set” has no objects in testing areas, it was where the classifier was trained. The second image, “Validation 1” has 102<br>
cells in testing areas. The training area count is 1, as listed in the output above.</p>
<p>Have you been able to get the scripts to work for the test project?</p> ;;;; <p>Not sure, haven’t used that particular script before. In general the alignment fails to converge when the images are too different from one another. A pixel classifier to pick out the parts of the image you want to align (size exclusion etc), might function better than using the base image, but while the tissue looks similar in both images, the images themselves are very different (see image height).</p>