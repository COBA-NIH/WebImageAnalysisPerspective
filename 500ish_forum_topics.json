{"47121": ["<p>Hello! I\u2019d like to ask for help. Our lab has updated the computer and we want to use the new MM 2.0. Before that, we used MM 1.4. And we want to connect the gamepad. However, the ASI (xbox) Gamepad plugin does not work in version 2.0. I am not a programmer, but I wanted to know if there is a way to run this plugin on the new version? Or do I need to write a new plugin?</p>", "<p>Does the plugin already exist in MM 2.0 or does it still need to be ported?</p>\n<p>If the plugin exists in MM 2.0 but doesn\u2019t work, can you first verify that the plugin works on the exact same computer in MM 1.4?  That will eliminate the possibility of it being a driver or similar issue.</p>", "<p>Indeed, the plugin was never ported to 2.0.  I started working on it, but it is a bit more work than I expected.  Hopefully will get to something that at least compiles in not too long, and I have an XBox controller in the garage, so may even be tempted to test it finally (one of those things that I always wanted to do but never got around to doing;).  You can follow progress in this issue: <a href=\"https://github.com/micro-manager/micro-manager/issues/1073\" class=\"inline-onebox\">Gamepad plugin is not included in 2.0 \u00b7 Issue #1073 \u00b7 micro-manager/micro-manager \u00b7 GitHub</a></p>", "<p>I ported the plugin.  I even made the plugin remember its settings in the user profile (without needing to load them from disk), and updated the UI here and there (there are still a few uncomfortable things, such as having to click twice in the Beanshell script column to get the file chooser dialog, but those are all relatively minor and need to be dealt with only when setting up.  I can not update the documentation, since it is at the ASI website.  I like my Xbox controller!  The plugin should be in the 20120105 and later builds of 2.0.</p>", "<p>Wow Nico, you are fast!</p>\n<p>If there is a better place to put the documentation I am happy to move it (maybe on <a href=\"http://micrco-manager.org\" rel=\"noopener nofollow ugc\">micrco-manager.org</a>?)  I suspect Vik just put it where he had easy edit access\u2026</p>\n<p>The plugin was the brainchild of my former colleague Vik.  We decided to release it to the public domain with ASI\u2019s name as a nod to ASI\u2019s funding its development instead of trying to monetize it.  But obviously without your help it wouldn\u2019t be continuing, so maybe you should rename it \u201cNico\u2019s Gamepad\u201d <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"></p>", "<p>I bought an X Box controller more than a year ago (maybe two), and brought it home to play with, so this was the final nudge I needed to get started;)</p>\n<p>Yes, it would be nice to move the documentation to the Micro-Manager website.  There even is a link (<a href=\"https://micro-manager.org/wiki/Plugins\" class=\"inline-onebox\">Plugins - Micro-Manager</a>) that now points to the ASI website, but that could be a document on the wiki.  Thanks!</p>", "<p>Documentation copied over to the Micro-manager website: <a href=\"https://micro-manager.org/wiki/ASI_Gamepad_Plugin\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ASI Gamepad Plugin - Micro-Manager</a>.</p>\n<p>I changed the internal link to point to the new page and marked the page on ASI\u2019s website as deprecated.</p>", "<p>Thank you so much, Nico! I\u2019ve already tested the gamepad and everything works! It was very fast!</p>", "<p>Hi\uff0c I need your help, because my English is not very good, I hope this paragraph can clearly express my problem. Now I want to connect my piezoconcept with MM 2.0, and according to Piezoconcept, I have succeeded, thank you very much. I now want to use my XBox controller to take control of the shift console, piezoconcept, but I, according to the Web site, have not succeeded.<a href=\"https://micro-manager.org/ASI_Gamepad_Plugin\" rel=\"noopener nofollow ugc\">https://micro-manager.org/ASI_Gamepad_Plugin</a><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb538f85e198b32a4a3b5484fb731350e57bfaa2.png\" data-download-href=\"/uploads/short-url/t0HXatOcWuYbFss3DFwzjXjmBI6.png?dl=1\" title=\"\u56fe\u7247\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb538f85e198b32a4a3b5484fb731350e57bfaa2.png\" alt=\"\u56fe\u7247\" data-base62-sha1=\"t0HXatOcWuYbFss3DFwzjXjmBI6\" width=\"624\" height=\"500\" data-dominant-color=\"686B6E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\u56fe\u7247</span><span class=\"informations\">808\u00d7647 30.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nI don\u2019t know what the problem is. Hope you see this post.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bdd874d2be8cf4109930fc90625bee90878f252.png\" data-download-href=\"/uploads/short-url/1GXSeTyhdwpIG9rL4f5N3StpRF8.png?dl=1\" title=\"\u56fe\u7247\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bdd874d2be8cf4109930fc90625bee90878f252.png\" alt=\"\u56fe\u7247\" data-base62-sha1=\"1GXSeTyhdwpIG9rL4f5N3StpRF8\" width=\"624\" height=\"500\" data-dominant-color=\"69696C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\u56fe\u7247</span><span class=\"informations\">808\u00d7647 34.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIs that the reason?<br>\nThank you so much!  <img src=\"https://emoji.discourse-cdn.com/twitter/love_letter.png?v=12\" title=\":love_letter:\" class=\"emoji\" alt=\":love_letter:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "61476": ["<p>Hi <a class=\"mention\" href=\"/u/ilastik_team\">@ilastik_team</a>,<br>\nIs there any recommendation on how to convert videos for animal tracking into h5 files? I have .moc and .mp4 raw data, but I cannot find a solution for their conversion to .h5.<br>\nI\u2019m a linux user.<br>\nThanks in advance!</p>", "<p>Hello <a class=\"mention\" href=\"/u/jakub_stenc\">@Jakub_Stenc</a>,</p>\n<p>converting to/from formats is still a pain in 2022 (happy new year :D, <img src=\"https://emoji.discourse-cdn.com/twitter/tada.png?v=10\" title=\":tada:\" class=\"emoji\" alt=\":tada:\"> ). If you have any option to already load your data into fiji, then I\u2019d suggest to do that and use the <a href=\"https://github.com/ilastik/ilastik4ij#export\">ilastik plugin to save an hdf5</a> file.</p>\n<p>A different approach would be using <a href=\"https://www.ffmpeg.org/download.html\">ffmpg</a> to convert the data to an image sequence first. (something like <code>ffmpeg -i input.mp4 output_%05d.png</code>). You can <a href=\"https://www.ilastik.org/documentation/basics/dataselection.html#image_stack\">load the image sequence in ilastik directly</a>, which will,  internally convert it to hdf5 and attach it to the project file (project file size will be large). If you prefer to have a separate hdf5 file, you could import the image sequence either into the ilastik data conversion workflow, or fiji and export as hdf5.</p>\n<p>Cheers!<br>\nDominik</p>", "<p>Hello <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>,</p>\n<p>thanks for your advice! It took me some time, but I may solve the issue. Converting is still a pain in 2023 (happy another new year <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"> ), the main problem I found is that even by using <a href=\"https://www.ffmpeg.org/download.html\" rel=\"noopener nofollow ugc\">ffmpg</a>, FIJI and Ilastik were refusing to accept the video. I found that the easiest way is to use ffmpg for converting to .avi format and later the Ilastic plugin. It\u2019s a bit time and CPU-consuming, but I think it may work even with my low-hundreds of videos I wanna track. What is worthy mentioning is that problem is often caused by compression which could be avoided by telling ffmpg to do the jedi trick and say <a href=\"https://superuser.com/questions/1536000/how-can-you-decompress-a-videofile-in-ffmpeg\" rel=\"noopener nofollow ugc\">\u201cthere is no compression you are looking for\u201d</a>.</p>\n<p>Cheers, Kuba</p>"], "77895": ["<p>I have lots (~2000?) of images showing a square with some algae growing in it. The squares are not all centered and often at various angles. To prepare the images for analysis I am trying to batch align the squares to be centered and level. Unfortunately (presumably due to the messiness of the squares?), I haven\u2019t had any luck with the imageJ automatic image registering tools/plugins (register virtual image slices, linear stack alignment, trackem2). I have had some limited success with stackreg and using an \u201cidealized\u201d starting reference image consisting of a white square on a black background, but because (if I understand correctly) stackreg uses the preceding image in a stack as the reference, I get a progressively worsening \u201cdrift\u201d away from the idealized reference image.<br>\nDoes anyone have any suggestions for aligning multiple off-center and rotated images without having to do them individually? It would be ideal if each image could be matched against a single idealized reference image.<br>\nIf I can get these images aligned, my next goal will be to try to determine percent cover of narrow and wide filaments (males and females), as well as count incidences of stress morphologies (large round clear cells). This however is a secondary issue for now.<br>\nThanks in advance!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0b1f2c22388e82fed555caf17f66d5c461a05d54.jpeg\" data-download-href=\"/uploads/short-url/1Ao2oRVRPsHDNwfxoXQLZqymdhi.jpeg?dl=1\" title=\"A7 (3)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b1f2c22388e82fed555caf17f66d5c461a05d54_2_666x500.jpeg\" alt=\"A7 (3)\" data-base62-sha1=\"1Ao2oRVRPsHDNwfxoXQLZqymdhi\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b1f2c22388e82fed555caf17f66d5c461a05d54_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b1f2c22388e82fed555caf17f66d5c461a05d54_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b1f2c22388e82fed555caf17f66d5c461a05d54_2_1332x1000.jpeg 2x\" data-dominant-color=\"959089\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">A7 (3)</span><span class=\"informations\">1600\u00d71200 405 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/59bd58ce230123b825040fad0dbbf6a5d195acbd.jpeg\" data-download-href=\"/uploads/short-url/cNSaszgXCLobzQ6u25AezCtSZvD.jpeg?dl=1\" title=\"A8 (1)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59bd58ce230123b825040fad0dbbf6a5d195acbd_2_666x500.jpeg\" alt=\"A8 (1)\" data-base62-sha1=\"cNSaszgXCLobzQ6u25AezCtSZvD\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59bd58ce230123b825040fad0dbbf6a5d195acbd_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59bd58ce230123b825040fad0dbbf6a5d195acbd_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59bd58ce230123b825040fad0dbbf6a5d195acbd_2_1332x1000.jpeg 2x\" data-dominant-color=\"7D7467\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">A8 (1)</span><span class=\"informations\">1600\u00d71200 279 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/a/8ada6728c7cdf0b499612500ad9c51e0c0a7a564.jpeg\" data-download-href=\"/uploads/short-url/jOlOF0eUz0nboT38qj5kJ7RRZmk.jpeg?dl=1\" title=\"D2 (5)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/a/8ada6728c7cdf0b499612500ad9c51e0c0a7a564_2_666x500.jpeg\" alt=\"D2 (5)\" data-base62-sha1=\"jOlOF0eUz0nboT38qj5kJ7RRZmk\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/a/8ada6728c7cdf0b499612500ad9c51e0c0a7a564_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/a/8ada6728c7cdf0b499612500ad9c51e0c0a7a564_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/a/8ada6728c7cdf0b499612500ad9c51e0c0a7a564_2_1332x1000.jpeg 2x\" data-dominant-color=\"736A5C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">D2 (5)</span><span class=\"informations\">1600\u00d71200 267 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/b/bbbc8b835bb67ac00e6c9bbd8dd1615e195b7a3c.jpeg\" data-download-href=\"/uploads/short-url/qMNf2ZZ0JSkRFtbJeIz4TV4xlBq.jpeg?dl=1\" title=\"K4 (2)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bbbc8b835bb67ac00e6c9bbd8dd1615e195b7a3c_2_666x500.jpeg\" alt=\"K4 (2)\" data-base62-sha1=\"qMNf2ZZ0JSkRFtbJeIz4TV4xlBq\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bbbc8b835bb67ac00e6c9bbd8dd1615e195b7a3c_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bbbc8b835bb67ac00e6c9bbd8dd1615e195b7a3c_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bbbc8b835bb67ac00e6c9bbd8dd1615e195b7a3c_2_1332x1000.jpeg 2x\" data-dominant-color=\"7C7264\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">K4 (2)</span><span class=\"informations\">1600\u00d71200 255 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>This issue is addressed here:<br>\n<a href=\"https://www.reddit.com/r/ImageJ/comments/11emdv9/struggling_to_automatically_align_multiple_messy/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://www.reddit.com/r/ImageJ/comments/11emdv9/struggling_to_automatically_align_multiple_messy/</a></p>\n<p>A possible solution:</p>\n<pre><code class=\"lang-auto\">macro \"reshape_all_images\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\n//--------------------------------\n// Start processing\n//looping through images\ndir_input = getDirectory(\"Choose Source Directory \");\nlist = getFileList(dir_input);\nprint(list.length);\nprint(\"The name of the folder input is:\"+dir_input);\n//--------------------------------\ndir_out = getDirectory(\"Choose output Directory \");\nprint(\"The name of the folder output is:\"+dir_out);\n//--------------------------------\n// Start batchmode\nsetBatchMode(true); \n\ni=0;\n \n\t while(i &lt;list.length){ \n\t\t//open\n\t \topen(dir_input + list[i]); \n\t \tselectImage(list[i]);\n\t \tname = getTitle();\n\t\tprint(name);\n//--------------------------------\nreshape();\n//--------------------------------\n\t \ti=i+1;\n\t\tprint(i);\nclose(i+\".jpeg\");\nclose(i+\"-1.jpeg\");\n\t }\n\n// End of processing\n//--------------------------------\n// End of batchmode\nsetBatchMode(false);\n//--------------------------------\nexit(\"All is done !\");\n//--------------------------------\nfunction reshape() {\nrun(\"Duplicate...\", \"title=temp\");\nrun(\"8-bit\");\nrun(\"Maximum...\", \"radius=10\");\nrun(\"Convert to Mask\");\nrun(\"Invert\");\nrun(\"Fill Holes\");\n//setTool(\"wand\");\ndoWand(846, 536);\nrun(\"Enlarge...\", \"enlarge=-100\");\n//setTool(\"rotrect\");\nrun(\"Fit Rectangle\");\n//run(\"Enlarge...\", \"enlarge=100\");\nclose(\"temp\");\nrun(\"Restore Selection\");\nrun(\"Enlarge...\", \"enlarge=50\");\n//setTool(\"rotrect\");\nrun(\"Fit Rectangle\");\nrun(\"Duplicate...\", \" \");\nrun(\"Scale...\", \"x=- y=- width=960 height=930 interpolation=Bilinear average create\");\nsaveAs(\"Jpeg\", dir_out+\"Result of reshape\"+(i+1)+\".jpg\");\n}}\n</code></pre>\n<p>Appreciate any feedback. <a class=\"mention\" href=\"/u/vsupratya\">@VSupratya</a> Thanks in advance.</p>", "<p>To attempt to complete the request:</p>\n<aside class=\"quote no-group\" data-username=\"VSupratya\" data-post=\"1\" data-topic=\"77895\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/v/aeb1de/40.png\" class=\"avatar\"> VSupratya:</div>\n<blockquote>\n<p>my next goal will be to try to determine percent cover</p>\n</blockquote>\n</aside>\n<p>With the images obtained with the previous macro:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fcac09c4c9ccfccd852840699dce4c3de7f3ab5.jpeg\" data-download-href=\"/uploads/short-url/2fHyxfLu6g7dErmJvmKsiWvCQol.jpeg?dl=1\" title=\"Result of reshape 1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fcac09c4c9ccfccd852840699dce4c3de7f3ab5_2_258x249.jpeg\" alt=\"Result of reshape 1\" data-base62-sha1=\"2fHyxfLu6g7dErmJvmKsiWvCQol\" width=\"258\" height=\"249\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fcac09c4c9ccfccd852840699dce4c3de7f3ab5_2_258x249.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fcac09c4c9ccfccd852840699dce4c3de7f3ab5_2_387x373.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fcac09c4c9ccfccd852840699dce4c3de7f3ab5_2_516x498.jpeg 2x\" data-dominant-color=\"E4DFD7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Result of reshape 1</span><span class=\"informations\">960\u00d7930 139 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/4/e45f8ac4302a20ab9e98143091121f56b5d9394f.jpeg\" data-download-href=\"/uploads/short-url/wAhyN9miRI9kpDEAgFik1NdfliD.jpeg?dl=1\" title=\"Result of reshape 2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e45f8ac4302a20ab9e98143091121f56b5d9394f_2_258x249.jpeg\" alt=\"Result of reshape 2\" data-base62-sha1=\"wAhyN9miRI9kpDEAgFik1NdfliD\" width=\"258\" height=\"249\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e45f8ac4302a20ab9e98143091121f56b5d9394f_2_258x249.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e45f8ac4302a20ab9e98143091121f56b5d9394f_2_387x373.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e45f8ac4302a20ab9e98143091121f56b5d9394f_2_516x498.jpeg 2x\" data-dominant-color=\"CABDAD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Result of reshape 2</span><span class=\"informations\">960\u00d7930 206 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/e/fed2303b89a7f4fac3e25f7943cf083be38792c1.jpeg\" data-download-href=\"/uploads/short-url/AmfBs5JtHEsThAugQrt8sr35qb7.jpeg?dl=1\" title=\"Result of reshape 3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fed2303b89a7f4fac3e25f7943cf083be38792c1_2_258x249.jpeg\" alt=\"Result of reshape 3\" data-base62-sha1=\"AmfBs5JtHEsThAugQrt8sr35qb7\" width=\"258\" height=\"249\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fed2303b89a7f4fac3e25f7943cf083be38792c1_2_258x249.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fed2303b89a7f4fac3e25f7943cf083be38792c1_2_387x373.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fed2303b89a7f4fac3e25f7943cf083be38792c1_2_516x498.jpeg 2x\" data-dominant-color=\"B7AB95\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Result of reshape 3</span><span class=\"informations\">960\u00d7930 210 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/6/d6f20889d85f5cd8467d53755394bb3445e2dac6.jpeg\" data-download-href=\"/uploads/short-url/uFuJqFr6TZzEmFWinY1BDFCAcWG.jpeg?dl=1\" title=\"Result of reshape 4\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d6f20889d85f5cd8467d53755394bb3445e2dac6_2_258x249.jpeg\" alt=\"Result of reshape 4\" data-base62-sha1=\"uFuJqFr6TZzEmFWinY1BDFCAcWG\" width=\"258\" height=\"249\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d6f20889d85f5cd8467d53755394bb3445e2dac6_2_258x249.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d6f20889d85f5cd8467d53755394bb3445e2dac6_2_387x373.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d6f20889d85f5cd8467d53755394bb3445e2dac6_2_516x498.jpeg 2x\" data-dominant-color=\"C3B5A1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Result of reshape 4</span><span class=\"informations\">960\u00d7930 188 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-auto\">macro \"ToScan_coverage\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\n//--------------------------------\n// Start processing\n//looping through images\ndir_input = getDirectory(\"Choose Source Directory \");\nlist = getFileList(dir_input);\nprint(list.length);\nprint(\"The name of the folder input is:\"+dir_input);\n//--------------------------------\ndir_out = getDirectory(\"Choose output Directory \");\nprint(\"The name of the folder output is:\"+dir_out);\n//--------------------------------\n// Start batchmode\nsetBatchMode(true); \nk=0; \n\t while(k &lt;list.length){ \n\t\t//open\n\t \topen(dir_input + list[k]); \n\t \tselectImage(list[k]);\n\t \tname = getTitle();\n\t\tprint(name);\n\t\trun(\"Duplicate...\", \"title=temp\");\n\t\tclose(name+ k+\".jpeg\");\n\t\trun(\"8-bit\");\n\t\trun(\"Enhance Contrast...\", \"saturated=3 equalize\");\n//--------------------------------\ntoScan();\n//--------------------------------\n\t\tk=k+1;\n\t close(\"temp\");\n\t }\n\n// End of processing\n//--------------------------------\n// End of batchmode\nsetBatchMode(false);\n//--------------------------------\nexit(\"All is done !\");\n//--------------------------------\nfunction toScan() {\n//----------------------------------\nw=getWidth();\nh=getHeight();\n//----------------------------------\n// Initialize counters\nnt=0; // number of non white pixels\n//----------------------------------\n// horizontal swipe\nfor(j=0;j&lt;=w; j++) {\n  \t//x=j;\n//print(\"x=\",x);\n//----------------------------------\n// vertical scan\nn=0 ; // number of non white pixels\n\tfor(i=0;i&lt;=h; i++) {\t\t\n   \t\tif ( getPixel(j,i)&lt;=200 )\n\t\t\tn+=1;\n                                         }\n//wait(10);   \n//print(\"For the column \"+ j + \" the number of non white pixels is : \" + n) ;\n//----------------------------------\nnt+=n ;\n//makeLine(j,0,j,h);// Not necessary. To have fun ...\n\n}\n//print(j);\n//print(\"For the image the number of non white pixels is :\" +nt ) ;\n//----------------------------------\n// Calculation of the coverage rate\nrun(\"Set Measurements...\", \"mean display redirect=None decimal=2\");\narea_image=w*h;\ncov_rate=(nt/area_image)*100;\nprint(\"The coverage rate is:\", cov_rate +\" %\");\n//----------------------------------\nsetResult(\"cov_rate\"+\" %\", k, cov_rate);\nupdateResults();\n}}\n</code></pre>\n<p>Anyway, this is just an exercise\u2026not sure how precise the process (and result) is.</p>", "<p>Thank you so much!</p>"], "77905": ["<p>Hello and greetings!<br>\nI have been trying to use the dockerfile provided on the GitHub page of stardist to build a docker image. But I stumbled upon the following errors.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c5971cdac9e3044df9b193741e8a7d63168f726e.png\" data-download-href=\"/uploads/short-url/sbXOMU4qcMtjXnG3p0uEmuAvu7Y.png?dl=1\" title=\"Screenshot 2023-03-01 at 7.55.10 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c5971cdac9e3044df9b193741e8a7d63168f726e_2_690x198.png\" alt=\"Screenshot 2023-03-01 at 7.55.10 AM\" data-base62-sha1=\"sbXOMU4qcMtjXnG3p0uEmuAvu7Y\" width=\"690\" height=\"198\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c5971cdac9e3044df9b193741e8a7d63168f726e_2_690x198.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c5971cdac9e3044df9b193741e8a7d63168f726e_2_1035x297.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c5971cdac9e3044df9b193741e8a7d63168f726e_2_1380x396.png 2x\" data-dominant-color=\"E7E7E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-01 at 7.55.10 AM</span><span class=\"informations\">2204\u00d7634 110 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIs there any existing solution to fix this problem?<br>\nThanks<br>\nAnnesha</p>", "<p>Hi <a class=\"mention\" href=\"/u/on-nesha\">@on-nesha</a>, the Dockerfile was out of date. I\u2019ve just updated it here:</p>\n<aside class=\"onebox githubfolder\" data-onebox-src=\"https://github.com/stardist/stardist/tree/dev/docker\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/stardist/stardist/tree/dev/docker\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h3><a href=\"https://github.com/stardist/stardist/tree/dev/docker\" target=\"_blank\" rel=\"noopener\">stardist/docker at dev \u00b7 stardist/stardist</a></h3>\n\n  <p><a href=\"https://github.com/stardist/stardist/tree/dev/docker\" target=\"_blank\" rel=\"noopener\">dev/docker</a></p>\n\n  <p><span class=\"label1\">StarDist - Object Detection with Star-convex Shapes</span></p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Dear <a class=\"mention\" href=\"/u/uschmidt83\">@uschmidt83</a> ,</p>\n<p>Thank you so much for updating the Dockerfile.</p>\n<p>Annesha</p>"], "73814": ["<p>Hi,<br>\nI have a question / feature request regarding the position list.<br>\nI use extensively the \u201cCreate Grid\u201d dialog for various applications. Lately, I ran more and more often into situations where I would like to acquire positions defined along a line rather than a grid. I attach an image  which illustrates the current behaviour (upper cartoon) and the desired one (lower cartoon) with 2 stage positions of interest given as top-left and bottom-right in the dialog.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/f/5f3fc19b7ff72b5aa49c36debd28a5b3e0c115de.png\" data-download-href=\"/uploads/short-url/dABXkhMiytqMeemRBm8XlL8KkHQ.png?dl=1\" title=\"MM_grid_cartoon\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/f/5f3fc19b7ff72b5aa49c36debd28a5b3e0c115de.png\" alt=\"MM_grid_cartoon\" data-base62-sha1=\"dABXkhMiytqMeemRBm8XlL8KkHQ\" width=\"588\" height=\"499\" data-dominant-color=\"FDF5F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">MM_grid_cartoon</span><span class=\"informations\">1000\u00d7849 6.31 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is there already a built-in feature for producing such position list?<br>\nOtherwise is it something that could be easily implemented? I have thought of a few ways to incorporate it in the current GUI but I guess the main questions is how straightforward it is to add it programming-wise\u2026</p>\n<p>Thank you very much for your help. Best,<br>\nThomas</p>", "<p>This should be straightforward to code.  I believe the hardest thing is the GUI.  What are your ideas for that?</p>", "<p>Hi Nico,</p>\n<p>Thanks for your reply! Very glad to read that there is no a priori hurdle.</p>\n<p>I can think of the possible way to extend the GUI:</p>\n<p>(a) duplicate the \u201cCreate Grid\u201d window and the corresponding button in the position list window, to have a \u201cCreate Grid\u201d and a \u201cCreate Line\u201d button.<br>\n<em>This is probably the least confusing to the user but I guess it requires to duplicate code that one would rather not duplicate as a developer.</em></p>\n<p>(b) use the current Create Grid window, either without a change (b1) or by replacing the \u201cOK\u201d button by two buttons \u201cCreate Grid\u201d and \u201cCreate Line\u201d (b2).<br>\n<em>I guess in both cases, one\u2019d need to check that the user provided only 2 positions and raise an error otherwise. With b1, hitting the OK button would create a line if 2 positions only are \u201cset\u201d as corners \u2013 this is probably the leanest but it bears obvious backward compatibility issues since some users must be used to create a grid by feeding 2 corners only. b2 addresses this issue but requires a slight modification of the Create Grid window (namely replacing one button by two).</em></p>\n<p>Overall, b2 seems the most appropriate to me, but I might well overlook several important aspects.</p>\n<p>Writing this reply, I realise that I dont know if any of the create grid functions are scriptable, and hence whether any of the scripting aspect must be extended too. I hope (and suspect) that not.</p>\n<p>Best,<br>\nThomas</p>", "<p>Thanks <a class=\"mention\" href=\"/u/julou\">@julou</a>!  Those are great suggestions.</p>\n<p>B1 is indeed a problem, because setting just two corners can be nice and easy, so it would be a shame to take that away.  B2 is a bit confusing, because there is nothing guiding the user that for one OK button you can not set more than 2 corners (and there is no way to \u201cunset\u201d a corner).</p>\n<p>What about two radiobuttons, just above the \u201cOK\u201d button, one with \u201cGrid\u201d (default), the other with \u201cLine\u201d?  When switching to Line, I could make the upper and lower \u201cSet\u201d and \u201cGoto\u201d buttons de-activated.</p>", "<p>Your solution sounds excellent from where I stand.<br>\nI would of course be more than grateful if you were to give it a shot!</p>\n<p>I probably goes without saying but keeping the overlap parameter (and the fact that it can be both positive or negative) is super useful.</p>", "<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/9435b8b776c2b16920f5c9ea79a6b85ff4c8a9db.png\" alt=\"image\" data-base62-sha1=\"l97LkX4EO3zxlYT2ojAeCfL8aqn\" width=\"342\" height=\"279\"></p>\n<p>Now implementing the UI;)</p>", "<p>This feature was just merged and is in the next nightly build.  Let us know how it works and what needs adjustment.</p>", "<p>Awesome! We\u2019ll try it soon and let you know how it goes, of course.<br>\nThank you so much for making it happen so quick!</p>", "<p>hi there, we didnt manage to test it very soon. But we have now installed it and used in on all our microscopes and it\u2019s super handy for our application!<br>\nSo far, we havent found any glitches <img src=\"https://emoji.discourse-cdn.com/twitter/+1.png?v=12\" title=\":+1:\" class=\"emoji\" alt=\":+1:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThank you so much <a class=\"mention\" href=\"/u/nicost\">@nicost</a> for implementing this!!</p>"], "55384": ["<p>Hi and congrats on the official release of micromanager 2.0,<br>\nThat finally pushed me into trying to upgrade from the 1.4 that\u2019s driving our light sheet. So far I really like it and it seems faster than 1.4. One issue we had was that saving z-axis data was taking 200ms in 1.4, it doesn\u2019t seem to be the case anymore.</p>\n<p>Anyway, on to my issue, we are acquiring volumetric time series (3D+time) at 10ms per frame. I am currently using a RAMdatastore and the display works well for the 3D + time axis, however, when I use MULTIPAGE_TIFF it saves 1 tif file where the 4D image is flattened to 3D (we get z_planes*time 2D images).<br>\nIf I save it as single page tifs, they have the right coordinates in their filename, so it seems the builder coords are working (the display working was a good hint).<br>\nIf I convert the datastore to imageJ, it opens as a hyperstack, and I can then save it as a 4D tif from imageJ.</p>\n<p>Any idea on what I could change to be able to use MULTIPAGE_TIFF to save it as a 4D tif? I included the relevant bit of the script</p>\n<pre><code class=\"lang-auto\">Datastore store = mm.data().createRAMDatastore();\nDisplayWindow display = mm.displays().createDisplay(store);\nmm.displays().manage(store);\nmmc.startSequenceAcquisition(nrStacks, 0, true);\nint nrFrames = range/increments;\nbuilder = mm.data().coordsBuilder().time(Timepoints).z(nrFrames).channel(0).stagePosition(0);\nint curFrame = 0;\n\nwhile (mmc.getRemainingImageCount() &gt; 0 || mmc.isSequenceRunning(mmc.getCameraDevice())) {\n\tif (mmc.getRemainingImageCount() &gt; 0) {\n\t\ttagged = mmc.popNextTaggedImage();\n      // Convert to an Image at the desired timepoint. And setup the z and time positions\n      z_plane=curFrame % nrFrames; \n      int time_point = curFrame/nrFrames;\n      image = mm.data().convertTaggedImage(tagged,\n         builder.time(time_point).z(z_plane).build(), null);\n      image=image.copyAtCoords(image.getCoords().copy().time(time_point).z(z_plane).build());\n      store.putImage(image);      \n\t\tcurFrame++;\t\n\t}\t\n\telse {\n      // Wait for another image to arrive.\n      mmc.sleep(Math.min(.5 * exposure, 20));\n   }\n}\nstore.freeze();\nstore.save(Datastore.SaveMode.MULTIPAGE_TIFF, saveLocation);\nstore.close();\n</code></pre>", "<p>I am a bit confused about the meaning of \u201c4D Tiff\u201d.  Is that one 3D Tiff per time point?  Or is it a multipage Tiff file that opens as you expect it to in ImageJ?  In the latter case, there is likely an ImageJ tag that is not added to the file.</p>\n<p>Does this work as you expect when you run the acquisition from the MDA window (i.e., acquire Z stacks at a couple of time points)?</p>", "<p>It displays as 3D tiff per time point when we do the acquisition ( z and t sliders), but when we save it from within the script with the <code>store.save(Datastore.SaveMode.MULTIPAGE_TIFF, saveLocation);</code> command, it opens as just 3D (one slider, with a number of slices equals to true-z * time, no data is lost).</p>\n<p>I will try the MDA acquisition to test, but we need the script to run stimuli at the same time as we do the acquisition.</p>", "<p>Does</p>\n<aside class=\"quote no-group\" data-username=\"Vanwalleghem\" data-post=\"3\" data-topic=\"55384\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/vanwalleghem/40/37672_2.png\" class=\"avatar\"> Vanwalleghem:</div>\n<blockquote>\n<p>it opens</p>\n</blockquote>\n</aside>\n<p>mean opens in ImageJ or opens in Micro-Manager, or somewhere else completely?</p>\n<p>You can run the MDA test with a demo configuration.  If it does not have the problem, then there may be something missing in the scripts.  If it does have the same problem, then there is something missing in the saving code.</p>\n<p>It would be very helpful if you could create a simple procedure (i.e., MDA settings, and/or simple beanshell script using the demo conifguration) with steps how to reproduce the problem.</p>", "<p>Has this problem been solved or disappeared? I wrote a plugin with very similar code and now I have exactly the same problem.</p>\n<p>My plugin acquires z-size * t-count images and assigns coordinates. No matter if I save via Java code or \u201cSave\u201d button: When I reopen the created file I get the correct total number of images, but flat.</p>\n<p>I can use ImageJ\u2019s \u201cImage \u2192 Hyperstacks \u2192 Stack to Hyperstack\u2026\u201d to reintroduce proper coordinates. Oddly, when doing so I have to provide the order of coordinates before saving, and I cannot even see an equivalent method in Datastore. How is that even supposed to work? Or isn\u2019t it?</p>"], "77913": ["<p>Hi all,</p>\n<p>I\u00b4m trying to set up micromanager to make fura2 ratiometric acquisition using an ancient Optoscan monochromator as illuminator. My routine acquisition is at 1 -0.5 Hz, and exposure times are in the range 20-60 mseconds. The problem is that my Optoscan has no shutter. This means that the last position of the cycle (380 nm) will be excitating the cells untill the next cycle.<br>\nIn Metafluor the solution is to move the illuminator to a wavelength without power (700nm) and wait ther until the next 340/380 excitation cycle.<br>\nIs it possible to do this in Micromanager?<br>\nThanks for the help, and apologies if the question is naive\u2026 is my first contact with Micromanager!</p>\n<p>Pedro Camello</p>", "<p>Hi <a class=\"mention\" href=\"/u/pcamello\">@pcamello</a>!</p>\n<p>Yes, you can do such a thing.  You will need the Utilities &gt; State Device Shutter, and set it up to use your Optoscan as the state device (if it is a state device).  There may be code missing in the Optoscan adapter (or it may not be written as a State Device), so please ask again if you get stuck.</p>", "<p>Hi Nico,</p>\n<p>I tried to set up State Device Shutter within my configuration but honestly, I\u00b4m stuck.  When I include State Device Shutter in a new Group or within the group created to change the wavelenght of my monochromator, all I get within Presets is a \u201cNone\u201d option\u2026 May be the Optoscan is not a State Device?<br>\nI think I don\u00b4t understand the StateDeviceShutter function, I tried to find some documentation but I failed.</p>\n<p>Thanks for the help</p>", "<p>Alternatively, would not be possible to change the channel to my \u201cshutter\u201d wavelenght but avoiding frame capture in that channel?</p>", "<p>What is the name of the device adapter you use to control the Optoscan?  Looking at the source code of that device adapter would allow me to give you some more solid advice.</p>", "<p>I think is Cairn_NI</p>", "<p>That is a closed source device adapter.  Not much I can do for you.  Please ask Cairn for help.  Please also ask them to make the source code open source and contribute to the Micro-Manager source code repository on github.</p>"], "77918": ["<p>Dear all,</p>\n<p>I would like to use CellProfiler segment membrane as what Harmony can do: shrink cells based on cytoplasm stain several pixels and select the shrinked pixel as membrane.</p>\n<p>With CellProfiler I can only use nuclei expand-N expand ring with for example 5 pixels, then identiy tertiary object from Cytoplasm minus ring to try to get membrane, however the results are not optimal.</p>\n<p>Do you have other recommendations?<br>\nThank you<br>\nShu</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a.png\" data-download-href=\"/uploads/short-url/qZbKpNBZYq0poAJQZ4jnuwxICRY.png?dl=1\" title=\"Bild1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a_2_578x500.png\" alt=\"Bild1\" data-base62-sha1=\"qZbKpNBZYq0poAJQZ4jnuwxICRY\" width=\"578\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a_2_578x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a_2_867x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a.png 2x\" data-dominant-color=\"5B5E5C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Bild1</span><span class=\"informations\">1061\u00d7917 750 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Shu, similar to the Harmony you can use the cell objects to create a new object for the membrane, there is a module in CellProfiler under ObjectProcessing \u201cExpandOrShrinkObjects\u201d there you can use the cell objects shrink them the \u201c5 pixels\u201d, and then use the IdentifyTertiaryObjects just like you did for the cytoplasm but using he Cell minus the ShrunkCells that will create the Membrane object.</p>\n<p>Barbara</p>", "<p>Hi Barbara,</p>\n<p>thank you very much! It works perfect.<br>\nAnother question. For ring detection, I would like to define instead of a fest pixel number, 50% of the distance between Nuclei border to Cell border. Do you know method to do this?</p>\n<p>Best<br>\nShu</p>"], "77928": ["<p>Hi all,</p>\n<p>I just uploaded the new version of the <strong>BioVoxxel Figure Tools</strong>. Those are automatically installed when activating the <em>BioVoxxel Toolbox update site</em>.</p>\n<p>Besides the known functionalities of exporting images and their overlays into SVG files the new version now also holds an LUT Channels Tool panel. Thanks to the nice development of <a class=\"mention\" href=\"/u/wayne\">@Wayne</a>, <a class=\"mention\" href=\"/u/k_taz\">@K_Taz</a> and <a class=\"mention\" href=\"/u/nicodf\">@NicoDF</a> and the related <a href=\"https://forum.image.sc/t/multi-channel-composite-view-with-inverted-luts-in-imagej-fiji/61163\">discussion</a> started by <a class=\"mention\" href=\"/u/christlet\">@christlet</a>, this idea came up and was made possible.</p>\n<p>It re-implements some of the standard functionalities of the normal ImageJ Channels tool and should just serve as an alternative complement for that.</p>\n<p><strong>So, what is NEW:</strong></p>\n<p>Every user can customize the contained buttons very easily with own favorite LUTs.<br>\nJust copy or save your LUT files in the <code>/luts/LUTButtonPanel/</code> subfolder and restart the <em>LUT Channels Tool</em>.<br>\nThe LUTs should be added to the panel in order of appearance in the folder (normally alphabetically sorted).</p>\n<p>You can influence the order by adding numbers before the file names to enable sorting (consider leading zeros). Just check out the <code>LUTButtonPanel</code> folder to get an idea of how that works.</p>\n<p>The buttons are displaying the related LUTs for quicker choice. The button tooltips (by mouse-hovering over the button) mentions the LUT name.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d949369c2e088835627d40fdfd7c1eba2c3a1a0e.jpeg\" data-download-href=\"/uploads/short-url/v0ct8j94j7ZPUghpgPL2hWbdNoO.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d949369c2e088835627d40fdfd7c1eba2c3a1a0e_2_690x377.jpeg\" alt=\"image\" data-base62-sha1=\"v0ct8j94j7ZPUghpgPL2hWbdNoO\" width=\"690\" height=\"377\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d949369c2e088835627d40fdfd7c1eba2c3a1a0e_2_690x377.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d949369c2e088835627d40fdfd7c1eba2c3a1a0e_2_1035x565.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d949369c2e088835627d40fdfd7c1eba2c3a1a0e_2_1380x754.jpeg 2x\" data-dominant-color=\"929393\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1776\u00d7972 132 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The button <code>CDV Test</code> will create an RGB stack from the current image showing the 3 most common color deficient vision simulations from the <code>Image \u203a Color \u203a Simulate Color Blindness</code> function to quickly test fo optimal output.</p>\n<p>The <code>LUT Channels Tool</code> will normally remember its position and size after the first use.</p>\n<p>After installation you can easily start it from the <code> Plugins \u203a BioVoxxel Figure Tools \u203a LUT Channels Tool</code> folder or using the convenience icon menu tool installed and available from the <code>More &gt;&gt;</code> tools menu.</p>\n<p>Feedback after testing is very welcome.</p>", "<p><s>In the new version (1.8.1b), you can now also specify an alternative LUT folder and basically keep different LUT sets in different folders between which you can switch as needed.<br>\nTo specify an alternative LUT folder right-click in an empty area of the panel which will show a pop-up message to do so.</s></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50a10cce7183797a52edf9fdca68414e646783fb.png\" data-download-href=\"/uploads/short-url/bvhfeyK87bhb5kRaZ85DyNvmEht.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50a10cce7183797a52edf9fdca68414e646783fb.png\" alt=\"image\" data-base62-sha1=\"bvhfeyK87bhb5kRaZ85DyNvmEht\" width=\"503\" height=\"500\" data-dominant-color=\"4D514C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">656\u00d7652 13.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>EDIT: This post part is obsolete since the choice of LUT folders is now handled different as seen <a href=\"https://forum.image.sc/t/biovoxxel-figure-tools-1-8-0b-released-new-lut-channels-tool/77928/6\">described below</a>.</p>", "<p>I <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a> ,</p>\n<p>Thanks a lot for this tool I love it !</p>\n<p>After testing I have some suggestions :</p>\n<p>about the layout ,<br>\nThe composite mode menu as only 4 entries so it could be easier to change it to 4 buttons on the top and to move the cool colorblind test button to the bottom</p>\n<p>about the composite invert,<br>\nIn my opinion, the fact that the composte invert mode automatically inverts the current image LUTs makes it difficult to handle. It may be better to let the user choose the LUTs and adapt the composite mode accordingly? If you like this idea then adding an \u201cInvert LUTs\u201d button would be usefull too</p>\n<p>Best,</p>\n<p>Kevin</p>", "<p>Hi Kevin (<a class=\"mention\" href=\"/u/k_taz\">@K_Taz</a>,),</p>\n<p>thanks for the feedback.</p>\n<aside class=\"quote no-group\" data-username=\"K_Taz\" data-post=\"3\" data-topic=\"77928\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k_taz/40/40333_2.png\" class=\"avatar\"> K\u00e9vin Terretaz:</div>\n<blockquote>\n<p>The composite mode menu as only 4 entries so it could be easier to change it to 4 buttons on the top and to move the cool colorblind test button to the bottom</p>\n</blockquote>\n</aside>\n<p>I think this is a good alternative since most likely quicker to press (than the current list selection).</p>\n<aside class=\"quote no-group\" data-username=\"K_Taz\" data-post=\"3\" data-topic=\"77928\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k_taz/40/40333_2.png\" class=\"avatar\"> K\u00e9vin Terretaz:</div>\n<blockquote>\n<p>about the composite invert,<br>\nIn my opinion, the fact that the composte invert mode automatically inverts the current image LUTs makes it difficult to handle.</p>\n</blockquote>\n</aside>\n<p>I am not sure if I completely follow (?). At the moment the invert mode is reproducing 100% what the normal IJ Channels tool is doing (and a button would not change the way it would switch the mode).</p>\n<aside class=\"quote no-group\" data-username=\"K_Taz\" data-post=\"3\" data-topic=\"77928\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k_taz/40/40333_2.png\" class=\"avatar\"> K\u00e9vin Terretaz:</div>\n<blockquote>\n<p>It may be better to let the user choose the LUTs and adapt the composite mode accordingly? If you like this idea then adding an \u201cInvert LUTs\u201d button would be usefull too</p>\n</blockquote>\n</aside>\n<p>Currently it actually does not matter if you first switch to <code>Composite Invert</code> IJ inverts the colors which works for the normal red, green, blue, cyan, magenta, yellow etc.<br>\nIf the user directly uses an \u201cInverted LUT\u201d (defines by starting with white = 0) the composite mode needs to be set to \u201cInvert\u201d to correctly display the composite. Do you mean that this inversion should happen automatically dependent on the chosen LUT?</p>", "<p>I tested the 4 button option and one issue with it is that when resizing the window to a reduced width, they do not display well. So, for the moment I think I will keep the current setup.</p>\n<p>But regarding the invert function, we can still see how to improve usability.</p>", "<p>Different LUT folders are now automatically detected, if you place them in the normal LUT folder of Fiji/ImageJ and can be selected via a right-click on the empty areas of the button panel.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026.png\" data-download-href=\"/uploads/short-url/5B7lHspitqLHwDT8nIrQHFsoBRs.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_690x303.png\" alt=\"image\" data-base62-sha1=\"5B7lHspitqLHwDT8nIrQHFsoBRs\" width=\"690\" height=\"303\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_690x303.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_1035x454.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/273d0464c20f3637208b86f51b3402888bc56026_2_1380x606.png 2x\" data-dominant-color=\"A2A2A1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1480\u00d7652 60.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "55418": ["<p>Hello,</p>\n<p>For those who use NDPView to draw their annotations but would <em>much</em> rather use QuPath, there\u2019s hope!</p>\n<p>We already had some code to <a href=\"https://forum.image.sc/t/importing-ndpa-annotation-files-into-qupath-code-attached/33721\">import NDPA annotations into QuPath</a> and here\u2019s the reverse. Code below will export the annotations as (filename.ndpi + \u2018.ndpa\u2019).</p>\n<p>The logic is super basic for now, and will only export polygons and rectangles\u2026 but can be expanded if needed <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"></p>\n<p>Here\u2019s a basic test (annotations attached, image is a <a href=\"http://openslide.cs.cmu.edu/download/openslide-testdata/Hamamatsu/OS-2.ndpi\" rel=\"noopener nofollow ugc\">test image</a> from OpenSlide):</p>\n<p>Annotations in QuPath 2.0.3:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/796904d9ae7dd7dd2b09265bba5eb132be817a47.jpeg\" data-download-href=\"/uploads/short-url/hk2M6RFQq0CDCZWHnheex23imCr.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/796904d9ae7dd7dd2b09265bba5eb132be817a47_2_690x459.jpeg\" alt=\"image\" data-base62-sha1=\"hk2M6RFQq0CDCZWHnheex23imCr\" width=\"690\" height=\"459\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/796904d9ae7dd7dd2b09265bba5eb132be817a47_2_690x459.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/796904d9ae7dd7dd2b09265bba5eb132be817a47.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/796904d9ae7dd7dd2b09265bba5eb132be817a47.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/796904d9ae7dd7dd2b09265bba5eb132be817a47_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1006\u00d7670 264 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Exported and displayed in NDP.view2:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/3/f317c3866a6bd72f9cc0cd5f09d55431bf6d571a.jpeg\" data-download-href=\"/uploads/short-url/yGuWdmHRlCY0om1EOYjpCMUW31w.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f317c3866a6bd72f9cc0cd5f09d55431bf6d571a_2_690x385.jpeg\" alt=\"image\" data-base62-sha1=\"yGuWdmHRlCY0om1EOYjpCMUW31w\" width=\"690\" height=\"385\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f317c3866a6bd72f9cc0cd5f09d55431bf6d571a_2_690x385.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/3/f317c3866a6bd72f9cc0cd5f09d55431bf6d571a.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/3/f317c3866a6bd72f9cc0cd5f09d55431bf6d571a.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f317c3866a6bd72f9cc0cd5f09d55431bf6d571a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">859\u00d7480 84.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Imported into Visiopharm:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50ce793114aa5d5a8e4f3a9680edbf2fc948f208.jpeg\" data-download-href=\"/uploads/short-url/bwQz1lsnjJwpi5nIJlHgNPae2Ja.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/0/50ce793114aa5d5a8e4f3a9680edbf2fc948f208_2_672x500.jpeg\" alt=\"image\" data-base62-sha1=\"bwQz1lsnjJwpi5nIJlHgNPae2Ja\" width=\"672\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/0/50ce793114aa5d5a8e4f3a9680edbf2fc948f208_2_672x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50ce793114aa5d5a8e4f3a9680edbf2fc948f208.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50ce793114aa5d5a8e4f3a9680edbf2fc948f208.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/0/50ce793114aa5d5a8e4f3a9680edbf2fc948f208_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">837\u00d7622 84.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Origin and sizes seem to match <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"></p>\n<p>\u2026 and the code:</p>\n<pre><code class=\"lang-auto\">import groovy.xml.MarkupBuilder\nimport qupath.lib.scripting.QP\nimport qupath.lib.gui.tools.ColorToolsFX\nimport qupath.lib.geom.Point2\nimport qupath.lib.images.servers.ImageServer\nimport qupath.lib.common.GeneralTools\nimport qupath.lib.objects.PathAnnotationObject\nimport qupath.lib.objects.classes.PathClassFactory\nimport qupath.lib.roi.*\n\ndef write_ndpa() {\n    def server = QP.getCurrentImageData().getServer()\n\n    // We need the pixel size\n    def cal = server.getPixelCalibration()\n    if (!cal.hasPixelSizeMicrons()) {\n        Dialogs.showMessageDialog(\"Metadata check\", \"No pixel information for this image!\");\n        return\n    }\n\n    // Here we get the pixel size\n    def md = server.getMetadata()\n    def pixelsPerMicron_X = 1 / cal.getPixelWidthMicrons() //md[\"pixelWidthMicrons\"]\n    def pixelsPerMicron_Y = 1 / cal.getPixelHeightMicrons() //md[\"pixelHeightMicrons\"]\n\n    //Aperio Image Scope displays images in a different orientation\n    //TODO Is this in the metadatata? Is this likely to be a problem?\n    //print(server.dumpMetadata())\n    def rotated = false\n\n    def h = server.getHeight()\n    def w = server.getWidth()\n    def ImageCenter_X = (w/2)*1000/pixelsPerMicron_X\n    def ImageCenter_Y = (h/2)*1000/pixelsPerMicron_Y \n\n    // need to add annotations to hierarchy so qupath sees them\n    def hierarchy = QP.getCurrentHierarchy()\n        \n    //Get X Reference from OPENSLIDE data\n    //The Open slide numbers are actually offset from IMAGE center (not physical slide center). \n    //This is annoying, but you can calculate the value you need -- Offset from top left in Nanometers. \n\n    def map = getCurrentImageData().getServer().osr.getProperties()\n    map.each { k, v -&gt;\n        if(k.equals(\"hamamatsu.XOffsetFromSlideCentre\")){\n            OffSet_From_Image_Center_X = v\n            //print OffSet_From_Image_Center_X\n            //print ImageCenter_X\n            OffSet_From_Top_Left_X = ImageCenter_X.toDouble() - OffSet_From_Image_Center_X.toDouble()\n            X_Reference =  OffSet_From_Top_Left_X\n            //print X_Reference\n            }\n        if(k.equals(\"hamamatsu.YOffsetFromSlideCentre\")){\n            OffSet_From_Image_Center_Y = v\n            //print    OffSet_From_Image_Center_Y\n            //print ImageCenter_Y\n\n            OffSet_From_Top_Left_Y = ImageCenter_Y.toDouble() - OffSet_From_Image_Center_Y.toDouble() \n            Y_Reference =  OffSet_From_Top_Left_Y\n            //print Y_Reference\n            }\n    }\n\n    def writer = new StringWriter()\n    def xml = new MarkupBuilder(writer)\n    def pathObjects = hierarchy.getAnnotationObjects()\n\n    xml.mkp.xmlDeclaration(version: \"1.0\", encoding: \"utf-8\", standalone: \"yes\")\n    xml.annotations {\n        pathObjects.eachWithIndex { pathObject, index -&gt;\n            ndpviewstate('id' : index+1) {\n                title(pathObject.getName())\n                details(pathObject.getPathClass())\n                coordformat('nanometers')\n                lens(0.5)\n                x(ImageCenter_X.toInteger())\n                y(ImageCenter_Y.toInteger())\n                z(0) //FIXME should be link to a z parameter\n                showtitle(0)\n                showhistogram(0)\n                showlineprofile(0)\n\n                //Annotation object\n                annotation(type:'freehand',  //FIXME here we should use switch/case to associate the right variables here\n                        displayname:'AnnotatedFreehand', \n                        color:'#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)) {\n                    measuretype(0)\n                    closed(1) //FIXME polygon vs polyline\n                    pointlist {\n                        pathObject.getROI().getAllPoints().each { pt -&gt;\n                            point {\n                                x(((pt.getX() - X_Reference / 1000 * pixelsPerMicron_X) * 1000 / pixelsPerMicron_X).toInteger())\n                                y(((pt.getY() - Y_Reference / 1000 * pixelsPerMicron_Y) * 1000 / pixelsPerMicron_Y).toInteger())\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    //an NDPA file simply adds '.ndpa' ti the NDPI filename\n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()+\".ndpa\";\n    def NDPAfile = new File(path)\n    NDPAfile.write(writer.toString())\n}\n\nwrite_ndpa()\n</code></pre>\n<p>How do I use this?</p>\n<p>Basically, I use <a href=\"https://forum.image.sc/t/creating-project-from-command-line/45608/3\">this script</a> to add a QuPath project to a directory of NDPI images and let my more learned colleagues do the annotations in QuPath in the comfort of their own home. When we meet next, we\u2019ll try to import all the images / annotations into Visiopharm (for now) and take it from there.</p>\n<p>I\u2019ll come back to this and update the code if we have any issues to iron out.</p>\n<p>Cheers,<br>\nEgor</p>\n<p><strong>Edit:</strong> I removed the \\ from my code (brainfart when I added the FIXME comment) and we may have a metasata issue if the ndpi images were opened with Bioformats rather than openslide:</p>\n<blockquote>\n<p>ERROR: I cannot find \u2018osr\u2019!<br>\nERROR: MissingPropertyException at line 78: No such property: osr for class: qupath.lib.images.servers.bioformats.BioFormatsImageServer</p>\n</blockquote>\n<p>I\u2019m looking into this, to at least fail gracefully. This issue relates to this discussion: <a href=\"https://forum.image.sc/t/access-to-image-metadata/43842\" class=\"inline-onebox\">Access to image metadata</a></p>\n<hr>\n<p><a class=\"attachment\" href=\"/uploads/short-url/k7ZYoAuFSQIQzSUcPTTXd4vG1oR.zip\">OS-2.ndpi.ndpa.zip</a> (642 Bytes)</p>", "<p>I did not realize Vis could use that file type!</p>", "<p>Yes, it was introduced in the December 2019 beta if I remember.</p>\n<p>Open \u2192 Layer Data<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c36251ad87d8119d7c37a1885d5bb72058237b63.jpeg\" data-download-href=\"/uploads/short-url/rSrKLGe8JarkboVL2Qyq9D6whqj.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c36251ad87d8119d7c37a1885d5bb72058237b63_2_690x292.jpeg\" alt=\"image\" data-base62-sha1=\"rSrKLGe8JarkboVL2Qyq9D6whqj\" width=\"690\" height=\"292\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c36251ad87d8119d7c37a1885d5bb72058237b63_2_690x292.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c36251ad87d8119d7c37a1885d5bb72058237b63_2_1035x438.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c36251ad87d8119d7c37a1885d5bb72058237b63_2_1380x584.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c36251ad87d8119d7c37a1885d5bb72058237b63_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1406\u00d7597 77.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello,</p>\n<p>my above approach of using <code>getAllPoints()</code> was a bit na\u00efve if I\u2019m being honest, as it comes with a fair warning in <a href=\"https://github.com/qupath/qupath/blob/442c930f83d460b3b49aefbb47df1ffe0a87ea47/qupath-core/src/main/java/qupath/lib/roi/interfaces/ROI.java#L125-L132\" rel=\"noopener nofollow ugc\">ROI.java</a>:</p>\n<pre><code class=\"lang-auto\">/**\n\t * Get a list of points representing the vertices of the ROI.\n\t * &lt;p&gt;\n\t * This is only really well-defined for ROIs where a single set of vertices represents the shape completely; \n\t * the expected output for a ROI that contains holes or disconnected regions is (currently) undefined.\n\t * @return\n\t */\n</code></pre>\n<p>To understand how to do this properly, you need to dig inside <a href=\"https://github.com/qupath/qupath/blob/75ec9cebe5e3bc5843fc60b07b455ce1215e1fb9/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java\" rel=\"noopener nofollow ugc\">GeometryTools.java</a> and understand how to find and extract \u201cring coordinates\u201d, starting from geometries: <code>geometry = annotation.getROI().getGeometry()</code> then moving on to polygons with <code>polygons = PolygonExtracter.getPolygons(geometry)</code> then looping through the polygons \u2192 polygon, to the \u201cexterior ring\u201d <code>extRing = polygon.getExteriorRing()</code> then to its coordinates with <code>coords = extRing.getCoordinates()</code> and coordinates of the holes with <code>polygon.getNumInteriorRing()</code> and looping through <code>ring = polygon.getInteriorRingN(i)</code> and <code>ring.getCoordinates()</code>, one for each hole in the polygon.</p>\n<p>What\u2019s really neat here is that an annotation ROI can comprise multiple polygons (think merged annotations), each with or without holes.</p>\n<p>A bit more complicated than I had anticipated, and I\u2019m <em>only</em> dealing with multi-polygons here, but the result speaks for itself (before code and after code below, these are NDPView screenshots):<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/e/3ec8a3a9bc5c859396dd7f0cffc29befdf51d205.jpeg\" data-download-href=\"/uploads/short-url/8Xptc9F77BUCrHyYjB4AeGBvpFr.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec8a3a9bc5c859396dd7f0cffc29befdf51d205_2_416x374.jpeg\" alt=\"image\" data-base62-sha1=\"8Xptc9F77BUCrHyYjB4AeGBvpFr\" width=\"416\" height=\"374\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec8a3a9bc5c859396dd7f0cffc29befdf51d205_2_416x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec8a3a9bc5c859396dd7f0cffc29befdf51d205_2_624x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/e/3ec8a3a9bc5c859396dd7f0cffc29befdf51d205.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec8a3a9bc5c859396dd7f0cffc29befdf51d205_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">740\u00d7666 71.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/80b7b399ecf876db228eb0e4a0b76ee5987db89e.jpeg\" data-download-href=\"/uploads/short-url/imGH5CunH4aN5tl41HZLBPQOgX4.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80b7b399ecf876db228eb0e4a0b76ee5987db89e_2_416x374.jpeg\" alt=\"image\" data-base62-sha1=\"imGH5CunH4aN5tl41HZLBPQOgX4\" width=\"416\" height=\"374\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80b7b399ecf876db228eb0e4a0b76ee5987db89e_2_416x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80b7b399ecf876db228eb0e4a0b76ee5987db89e_2_624x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/80b7b399ecf876db228eb0e4a0b76ee5987db89e.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80b7b399ecf876db228eb0e4a0b76ee5987db89e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">740\u00d7666 59.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When working with polygons, there are also lots of interesting functions in <a href=\"https://github.com/qupath/qupath/blob/75ec9cebe5e3bc5843fc60b07b455ce1215e1fb9/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java\" rel=\"noopener nofollow ugc\">GeometryTools.java</a> for removing small holes or small fragments, as well as for simplifying the contours. I use these in the code and the results are absolutely stunning.</p>\n<p>Of course, the elephant in the room is that NDPView doesn\u2019t <em>have</em> a concept of holes, so hopefully, by labelling all the holes \u201cclear\u201d, I should be able to fudge something in Visiopharm.</p>\n<p>The code below also includes importing NDPA annotations, I just thought both functions might benefit from some code update / clean-up. No much in the way of testing yet though.</p>\n<p>Caveat Emptor <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<pre><code class=\"lang-auto\">import groovy.xml.MarkupBuilder\nimport org.openslide.OpenSlide\nimport qupath.lib.scripting.QP\nimport qupath.lib.gui.tools.ColorToolsFX\nimport qupath.lib.geom.Point2\nimport qupath.lib.images.servers.ImageServer\nimport qupath.lib.common.GeneralTools\nimport qupath.lib.objects.PathAnnotationObject\nimport qupath.lib.objects.classes.PathClassFactory\nimport qupath.lib.roi.*\nimport org.locationtech.jts.geom.util.PolygonExtracter\nimport org.locationtech.jts.simplify.TopologyPreservingSimplifier;\n\ndef get_osr() {\n    def server = QP.getCurrentImageData().getServer()\n\n    // If OpenSlide metadata isn't available, load it up!\n    if (server.hasProperty('osr') &amp;&amp; server.osr){\n        // Image was opened with OpenSlide\n        osr = server.osr\n    } else {\n        // Code borrowed from qupath/qupath-extension-openslide/src/main/java/qupath/lib/images/servers/openslide/OpenslideImageServer.java \n        // Ensure the garbage collector has run - otherwise any previous attempts to load the required native library\n        // from different classloader are likely to cause an error (although upon first further investigation it seems this doesn't really solve the problem...)\n        System.gc();\n        def uri = GeneralTools.toPath(server.getURIs()[0]).toString();\n        file = new File(uri);\n        osr = new OpenSlide(file);\n    }\n    return osr\n}\n\n// Convert a point from NDPA to QuPath coordinates\ndef convertPoint(point, pixelWidthNm=1, pixelHeightNm=1, OffSet_From_Top_Left_X=0, OffSet_From_Top_Left_Y=0) {\n    point.x = (point.x.toDouble() / pixelWidthNm) + OffSet_From_Top_Left_X \n    point.y = (point.y.toDouble() / pixelHeightNm) + OffSet_From_Top_Left_Y \n}\n\n\n// Read an NDPA file TODO check that the code still works\ndef read_ndpa()\n{\n    def server = QP.getCurrentImageData().getServer()\n\n    // We need the pixel size\n    def cal = server.getPixelCalibration()\n    if (!cal.hasPixelSizeMicrons()) {\n        Dialogs.showMessageDialog(\"Metadata check\", \"No pixel information for this image!\");\n        return\n    }\n\n    def pixelWidthNm = cal.getPixelWidthMicrons() * 1000\n    def pixelHeightNm = cal.getPixelHeightMicrons() * 1000\n    \n    //Aperio Image Scope displays images in a different orientation\n    //TODO Is this in the metadatata? Is this likely to be a problem?\n    //print(server.dumpMetadata())\n    def rotated = false\n\n    def ImageCenter_X = server.getWidth()/2\n    def ImageCenter_Y = server.getHeight()/2\n\n    def osr = get_osr()\n\n    // need to add annotations to hierarchy so qupath sees them\n    def hierarchy = QP.getCurrentHierarchy()\n        \n    //*********Get NDPA automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()+\".ndpa\";\n\n    def NDPAfile = new File(path)\n    if (!NDPAfile.exists()) {\n        println \"No NDPA file for this image...\"\n        return\n    }\n\n    //Get X Reference from OPENSLIDE data\n    //The Open slide numbers are actually offset from IMAGE center (not physical slide center). \n    //This is annoying, but you can calculate the value you need -- Offset from top left in Nanometers. \n    OffSet_From_Top_Left_X = ImageCenter_X\n    OffSet_From_Top_Left_Y = ImageCenter_Y\n\n    osr.getProperties().each { k, v -&gt;\n        if(k.equals(\"hamamatsu.XOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_X -= v.toDouble()/pixelWidthNm\n        }\n        if(k.equals(\"hamamatsu.YOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_Y -= v.toDouble()/pixelHeightNm\n        }\n    }\n\n    //Read files\n    def text = NDPAfile.getText()\n    def list = new XmlSlurper().parseText(text)\n\n    list.ndpviewstate.each { ndpviewstate -&gt;\n        def annotationName = ndpviewstate.title.toString().trim()\n        def annotationClassName = annotationName\n        def annotationType = ndpviewstate.annotation.@type.toString().toUpperCase()\n        def annotationColor = ndpviewstate.annotation.@color.toString().toUpperCase()\n\n        def details = ndpviewstate.details.toString()\n        //println annotationName+\" (\"+annotationType+\") (\"+annotationClassName+\") \"+details\n    \n        roi = null\n        \n        if (annotationType == \"CIRCLE\") {\n            //special case\n            def point = new Point2(ndpviewstate.annotation.x, ndpviewstate.annotation.y)\n            convertPoint(point, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n\n            def rx = ndpviewstate.annotation.radius.toDouble() / pixelWidthNm\n            def ry = ndpviewstate.annotation.radius.toDouble() / pixelHeightNm\n            roi = new EllipseROI(point.x-rx,point.y-ry,rx*2,ry*2,null);\n        }\n        \n        if (annotationType == \"LINEARMEASURE\") {\n            //special case\n            def pt1 = new Point2(ndpviewstate.annotation.x1, ndpviewstate.annotation.y1)\n            convertPoint(pt1, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            def pt2 = new Point2(ndpviewstate.annotation.x2, ndpviewstate.annotation.y2)\n            convertPoint(pt2, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            roi = new LineROI(pt1.x,pt1.y,pt2.x,pt2.y);\n        }\n\n        if (annotationType == \"PIN\") {\n            def point = new Point2(ndpviewstate.annotation.x, ndpviewstate.annotation.y)\n            convertPoint(point, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            roi = new PointsROI(point.x,point.y);\n        }\n            \n        // All that's left if FREEHAND which handles polygons, polylines, rectangles\n        ndpviewstate.annotation.pointlist.each { pointlist -&gt;\n            def tmp_points_list = []\n            pointlist.point.each{ point -&gt;\n                if (rotated) {\n                    X = point.x.toDouble()\n                    Y = h - point.y.toDouble()\n                }\n                else {\n                    X = point.x.toDouble()\n                    Y =  point.y.toDouble()\n                }\n\n                tmp_points_list.add(new Point2(X, Y))\n            } \n    \n            //Adjust each point relative to SLIDECENTER coordinates and adjust for pixelsPerMicron\n            for ( point in tmp_points_list){\n                convertPoint(point, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            }\n            \n            if (annotationType == \"FREEHAND\") {\n                isClosed = 1 //ndpviewstate.annotation.closed.toBoolean() //XXX PLEASE PLEASE PLEASE USE THE POLYGON TOOL!!!!!\n                isRectangle = (ndpviewstate.annotation.specialtype.toString() == \"rectangle\")\n                if (isRectangle) {\n                    x1 = tmp_points_list[0].x\n                    y1 = tmp_points_list[0].y\n                    x3 = tmp_points_list[2].x\n                    y3 = tmp_points_list[2].y\n                    roi = new RectangleROI(x1,y1,x3-x1,y3-y1);\n                }\n                else if (isClosed)\n                    roi = new PolygonROI(tmp_points_list);\n                else\n                    roi = new PolylineROI(tmp_points_list, null);\n            }\n            \n        }\n        \n        if (roi != null)\n        {\n            def annotation = new PathAnnotationObject(roi)\n            annotation.setName(annotationName)        \n            if (annotationClassName)\n            {\n                //TODO (validate) and add the new class if it doesn't already exist:\n                //if (!PathClassFactory.classExists(annotationClassName))\n                annotation.setPathClass(PathClassFactory.getPathClass(annotationClassName))\n            }\n            \n            if (details) {          \n                annotation.setDescription(details)\n            }\n\n            annotation.setLocked(true)\n            hierarchy.addPathObject(annotation) //, false)\n        }\n\n    }\n\n}\n\n//Here we extract the polygon coords\ndef getPointList(ring) {\n    def pointlist = []\n    coords = ring.getCoordinates()\n    coords[0..&lt;coords.length-1].each { coord -&gt;\n        pointlist.add([coord.x, coord.y])\n    }\n    //println \"---\"\n    return pointlist\n}\n\ndef write_ndpa() {\n    def server = QP.getCurrentImageData().getServer()\n\n    // We need the pixel size\n    def cal = server.getPixelCalibration()\n    if (!cal.hasPixelSizeMicrons()) {\n        Dialogs.showMessageDialog(\"Metadata check\", \"No pixel information for this image!\");\n        return\n    }\n\n    def pixelWidthNm = cal.getPixelWidthMicrons() * 1000\n    def pixelHeightNm = cal.getPixelHeightMicrons() * 1000\n    \n    //Aperio Image Scope displays images in a different orientation\n    //TODO Is this in the metadatata? Is this likely to be a problem?\n    //print(server.dumpMetadata())\n    def rotated = false\n\n    def ImageCenter_X = server.getWidth()/2\n    def ImageCenter_Y = server.getHeight()/2\n\n    def osr = get_osr()\n\n    //*********Get NDPA automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()+\".ndpa\";\n\n    def NDPAfile = new File(path)\n\n    //Get X Reference from OPENSLIDE data\n    //The Open slide numbers are actually offset from IMAGE center (not physical slide center). \n    //This is annoying, but you can calculate the value you need -- Offset from top left in Nanometers. \n    OffSet_From_Top_Left_X = ImageCenter_X\n    OffSet_From_Top_Left_Y = ImageCenter_Y\n\n    osr.getProperties().each { k, v -&gt;\n        if(k.equals(\"hamamatsu.XOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_X -= v.toDouble()/pixelWidthNm\n        }\n        if(k.equals(\"hamamatsu.YOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_Y -= v.toDouble()/pixelHeightNm\n        }\n    }\n\n    // need to add annotations to hierarchy so qupath sees them\n    def hierarchy = QP.getCurrentHierarchy()\n    def pathObjects = hierarchy.getAnnotationObjects()\n\n    //create a list of annotations\n    def list_annot = []\n    def ndpIndex = 0\n\n    pathObjects.each { pathObject -&gt;\n        //We make a list of polygons, each has an exterior and interior rings\n        geometry = pathObject.getROI().getGeometry()\n\n        //Here we do some processing to simplify the outlines and remove small holes\n        geometry = TopologyPreservingSimplifier.simplify(geometry, 5.0);\n        geometry = GeometryTools.refineAreas(geometry, 200, 200)\n        var polygons = PolygonExtracter.getPolygons(geometry);\n\n        polygons.each { polygon -&gt;\n\n            //here we create a list of rings, we'll need to treat the first one differently\n            def rings = [ polygon.getExteriorRing() ]\n            def nRings = polygon.getNumInteriorRing();\n            for (int i = 0; i &lt; nRings; i++) {\n                var ring = polygon.getInteriorRingN(i);\n                rings.add(ring)\n            }\n\n            rings.eachWithIndex { ring, index -&gt;\n                def annot = [:]\n                if (index == 0) {\n                    annot['title'] = pathObject.getName()\n                    annot['details'] = pathObject.getPathClass()\n                    annot['color'] = '#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)\n                    isFirst = false\n                } else {\n                    annot['title'] = \"clear\"\n                    annot['details'] = \"clear\"\n                    annot['color'] = '#000000'\n                }\n\n                annot['id'] = ++ndpIndex\n                annot['coordformat'] = 'nanometers'\n                annot['lens'] = 0.445623\n                annot['x'] = ImageCenter_X.toInteger()\n                annot['y'] = ImageCenter_Y.toInteger()\n                annot['z'] = 0\n                annot['showtitle'] = 0\n                annot['showhistogram'] = 0\n                annot['showlineprofile'] = 0\n                annot['type'] = 'freehand'\n                annot['displayname'] = 'AnnotateFreehand'\n                annot['measuretype'] = 0\n                annot['closed'] = 1\n\n                //add the point list\n                annot['pointlist'] = getPointList(ring)\n                list_annot.add(annot)\n            }\n        }\n    }\n\n    //make an XML string\n    def writer = new StringWriter()\n    def xml = new MarkupBuilder(writer)\n    xml.mkp.xmlDeclaration(version: \"1.0\", encoding: \"utf-8\", standalone: \"yes\")\n    xml.annotations {\n        list_annot.each { annot -&gt;\n            ndpviewstate('id':annot['id']) {\n                title(annot['title'])\n                details(annot['details'])\n                coordformat(annot['coordformat'])\n                lens(annot['lens'])\n                x(annot['x'])\n                y(annot['y'])\n                z(annot['z'])\n                showtitle(annot['showtitle'])\n                showhistogram(annot['showhistogram'])\n                showlineprofile(annot['showlineprofile'])\n\n                //Annotation object\n                annotation(type:annot['type'], displayname:annot['displayname'], color:annot['color']) {\n                    measuretype(annot['measuretype'])\n                    closed(annot['closed'])\n                    pointlist {\n                        annot['pointlist'].each { pt -&gt;\n                            point {\n                                x( ((pt[0] - OffSet_From_Top_Left_X ) * pixelWidthNm).toInteger() )\n                                y( ((pt[1] - OffSet_From_Top_Left_Y ) * pixelHeightNm).toInteger() )\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    NDPAfile.write(writer.toString())\n}\n\n//read_ndpa()\nwrite_ndpa()\n</code></pre>", "<p>Thank you for working on this! I was attempting to use the export script but seem to run into the following error:</p>\n<p>ERROR: MultipleCompilationErrorsException at line 151: startup failed:<br>\nScript6.groovy: 152: Unexpected input: \u2018(writer.toString())\\n}\u2019 @ line 152, column 1.<br>\n}<br>\n^</p>\n<p>1 error</p>\n<p>ERROR: org.codehaus.groovy.control.ErrorCollector.failIfErrors(ErrorCollector.java:295)<br>\norg.codehaus.groovy.control.ErrorCollector.addFatalError(ErrorCollector.java:151)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.collectSyntaxError(AstBuilder.java:4388)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.access$000(AstBuilder.java:176)<br>\norg.apache.groovy.parser.antlr4.AstBuilder$1.syntaxError(AstBuilder.java:4399)<br>\ngroovyjarjarantlr4.v4.runtime.ProxyErrorListener.syntaxError(ProxyErrorListener.java:44)<br>\ngroovyjarjarantlr4.v4.runtime.Parser.notifyErrorListeners(Parser.java:543)<br>\ngroovyjarjarantlr4.v4.runtime.DefaultErrorStrategy.notifyErrorListeners(DefaultErrorStrategy.java:154)<br>\norg.apache.groovy.parser.antlr4.internal.DescriptiveErrorStrategy.reportNoViableAlternative(DescriptiveErrorStrategy.java:92)<br>\ngroovyjarjarantlr4.v4.runtime.DefaultErrorStrategy.reportError(DefaultErrorStrategy.java:139)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.pathExpression(GroovyParser.java:9500)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.postfixExpression(GroovyParser.java:8108)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.expression(GroovyParser.java:8650)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.commandExpression(GroovyParser.java:9299)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.statementExpression(GroovyParser.java:8067)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.statement(GroovyParser.java:6915)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.scriptStatement(GroovyParser.java:514)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.scriptStatements(GroovyParser.java:421)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.compilationUnit(GroovyParser.java:357)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:250)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:228)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.buildAST(AstBuilder.java:269)<br>\norg.apache.groovy.parser.antlr4.Antlr4ParserPlugin.buildAST(Antlr4ParserPlugin.java:58)<br>\norg.codehaus.groovy.control.SourceUnit.buildAST(SourceUnit.java:257)<br>\njava.base/java.util.Iterator.forEachRemaining(Unknown Source)<br>\njava.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Unknown Source)<br>\njava.base/java.util.stream.ReferencePipeline$Head.forEach(Unknown Source)<br>\norg.codehaus.groovy.control.CompilationUnit.buildASTs(CompilationUnit.java:666)<br>\norg.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:632)<br>\ngroovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:389)<br>\ngroovy.lang.GroovyClassLoader.lambda$parseClass$3(GroovyClassLoader.java:332)<br>\norg.codehaus.groovy.runtime.memoize.StampedCommonCache.compute(StampedCommonCache.java:163)<br>\norg.codehaus.groovy.runtime.memoize.StampedCommonCache.getAndPut(StampedCommonCache.java:154)<br>\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:330)<br>\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:314)<br>\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:257)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getScriptClass(GroovyScriptEngineImpl.java:336)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:153)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:982)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:914)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:829)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1345)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>Do you happen to have any insight into how to fix this? I\u2019m attempting to export the annotations on the following image:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99156978ce07e13807a030a66c8340887669b469.jpeg\" data-download-href=\"/uploads/short-url/lQeVx8ILr8TSFHTsU0ITB1RLUtb.jpeg?dl=1\" title=\"Screen Shot 2022-02-18 at 11.52.33 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99156978ce07e13807a030a66c8340887669b469_2_526x500.jpeg\" alt=\"Screen Shot 2022-02-18 at 11.52.33 AM\" data-base62-sha1=\"lQeVx8ILr8TSFHTsU0ITB1RLUtb\" width=\"526\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99156978ce07e13807a030a66c8340887669b469_2_526x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99156978ce07e13807a030a66c8340887669b469_2_789x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99156978ce07e13807a030a66c8340887669b469_2_1052x1000.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99156978ce07e13807a030a66c8340887669b469_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-02-18 at 11.52.33 AM</span><span class=\"informations\">1568\u00d71490 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello <a class=\"mention\" href=\"/u/deluca_lab\">@DeLuca_Lab</a></p>\n<p>this bit here is very strange</p>\n<pre><code class=\"lang-auto\">ERROR: MultipleCompilationErrorsException at line 151: startup failed:\nScript6.groovy: 152: Unexpected input: \u2018(writer.toString())\\n}\u2019 @ line 152, column 1.\n}\n</code></pre>\n<p>Do you have all the lines when you copy the script over? The last write_ndpa() line should be around line 348, not line 151 ~ 152.</p>\n<p>I copied the exact content of the script just above into the QuPath v0.3.2 script editor and launched it and got the expected output for a few outlines I just doodled:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a48b8ba4efc35b5c3e5afbf643f3b4a36cc259c4.jpeg\" data-download-href=\"/uploads/short-url/ntDhmesDa4aYVoEeZX4RAdXSn6k.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a48b8ba4efc35b5c3e5afbf643f3b4a36cc259c4_2_690x397.jpeg\" alt=\"image\" data-base62-sha1=\"ntDhmesDa4aYVoEeZX4RAdXSn6k\" width=\"690\" height=\"397\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a48b8ba4efc35b5c3e5afbf643f3b4a36cc259c4_2_690x397.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a48b8ba4efc35b5c3e5afbf643f3b4a36cc259c4_2_1035x595.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a48b8ba4efc35b5c3e5afbf643f3b4a36cc259c4_2_1380x794.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a48b8ba4efc35b5c3e5afbf643f3b4a36cc259c4_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1381\u00d7796 145 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Would you mind telling me which OS, which QuPath version you used and how you launched the script? And also if you have all the lines in the script you copied (sorry if this is an obvious question).</p>\n<p>Cheers,<br>\nEgor</p>", "<p>Thanks! I didn\u2019t copy the whole script (just from //We need the pixel size) since I thought the first part was importing? I did try copying and pasting the whole script and it seemed to run without error but I didn\u2019t get an output? Should the annotations file be saved in my QuPath project directory?</p>\n<p>I am using QuPath 0.3.2 on a Macbook Pro running MacOS Mojave (Version 10.14.6).</p>", "<p>Wait, I found the npda output! I had imported the images from a previous project and they were therefore found in that project directory rather than the new one. Thank you for your help!</p>", "<p>For a new project, I\u2019ve tried doing this export using the following code:</p>\n<pre><code class=\"lang-auto\">import groovy.xml.MarkupBuilder\nimport qupath.lib.scripting.QP\nimport qupath.lib.gui.tools.ColorToolsFX\nimport qupath.lib.geom.Point2\nimport qupath.lib.images.servers.ImageServer\nimport qupath.lib.common.GeneralTools\nimport qupath.lib.objects.PathAnnotationObject\nimport qupath.lib.objects.classes.PathClassFactory\nimport qupath.lib.roi.*\n\ndef write_ndpa() {\n    def server = QP.getCurrentImageData().getServer()\n\n   // We need the pixel size\n    def cal = server.getPixelCalibration()\n    if (!cal.hasPixelSizeMicrons()) {\n        Dialogs.showMessageDialog(\"Metadata check\", \"No pixel information for this image!\");\n        return\n    }\n\n    def pixelWidthNm = cal.getPixelWidthMicrons() * 1000\n    def pixelHeightNm = cal.getPixelHeightMicrons() * 1000\n    \n    //Aperio Image Scope displays images in a different orientation\n    //TODO Is this in the metadatata? Is this likely to be a problem?\n    //print(server.dumpMetadata())\n    def rotated = false\n\n    def ImageCenter_X = server.getWidth()/2\n    def ImageCenter_Y = server.getHeight()/2\n\n    def osr = get_osr()\n\n    //*********Get NDPA automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()+\".ndpa\";\n\n    def NDPAfile = new File(path)\n\n    //Get X Reference from OPENSLIDE data\n    //The Open slide numbers are actually offset from IMAGE center (not physical slide center). \n    //This is annoying, but you can calculate the value you need -- Offset from top left in Nanometers. \n    OffSet_From_Top_Left_X = ImageCenter_X\n    OffSet_From_Top_Left_Y = ImageCenter_Y\n\n    osr.getProperties().each { k, v -&gt;\n        if(k.equals(\"hamamatsu.XOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_X -= v.toDouble()/pixelWidthNm\n        }\n        if(k.equals(\"hamamatsu.YOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_Y -= v.toDouble()/pixelHeightNm\n        }\n    }\n\n    // need to add annotations to hierarchy so qupath sees them\n    def hierarchy = QP.getCurrentHierarchy()\n    def pathObjects = hierarchy.getAnnotationObjects()\n\n    //create a list of annotations\n    def list_annot = []\n    def ndpIndex = 0\n\n    pathObjects.each { pathObject -&gt;\n        //We make a list of polygons, each has an exterior and interior rings\n        geometry = pathObject.getROI().getGeometry()\n\n        //Here we do some processing to simplify the outlines and remove small holes\n        geometry = TopologyPreservingSimplifier.simplify(geometry, 5.0);\n        geometry = GeometryTools.refineAreas(geometry, 200, 200)\n        var polygons = PolygonExtracter.getPolygons(geometry);\n\n        polygons.each { polygon -&gt;\n\n            //here we create a list of rings, we'll need to treat the first one differently\n            def rings = [ polygon.getExteriorRing() ]\n            def nRings = polygon.getNumInteriorRing();\n            for (int i = 0; i &lt; nRings; i++) {\n                var ring = polygon.getInteriorRingN(i);\n                rings.add(ring)\n            }\n\n            rings.eachWithIndex { ring, index -&gt;\n                def annot = [:]\n                if (index == 0) {\n                    annot['title'] = pathObject.getName()\n                    annot['details'] = pathObject.getPathClass()\n                    annot['color'] = '#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)\n                    isFirst = false\n                } else {\n                    annot['title'] = \"clear\"\n                    annot['details'] = \"clear\"\n                    annot['color'] = '#000000'\n                }\n\n                annot['id'] = ++ndpIndex\n                annot['coordformat'] = 'nanometers'\n                annot['lens'] = 0.445623\n                annot['x'] = ImageCenter_X.toInteger()\n                annot['y'] = ImageCenter_Y.toInteger()\n                annot['z'] = 0\n                annot['showtitle'] = 0\n                annot['showhistogram'] = 0\n                annot['showlineprofile'] = 0\n                annot['type'] = 'freehand'\n                annot['displayname'] = 'AnnotateFreehand'\n                annot['measuretype'] = 0\n                annot['closed'] = 1\n\n                //add the point list\n                annot['pointlist'] = getPointList(ring)\n                list_annot.add(annot)\n            }\n        }\n    }\n\n    //make an XML string\n    def writer = new StringWriter()\n    def xml = new MarkupBuilder(writer)\n    xml.mkp.xmlDeclaration(version: \"1.0\", encoding: \"utf-8\", standalone: \"yes\")\n    xml.annotations {\n        list_annot.each { annot -&gt;\n            ndpviewstate('id':annot['id']) {\n                title(annot['title'])\n                details(annot['details'])\n                coordformat(annot['coordformat'])\n                lens(annot['lens'])\n                x(annot['x'])\n                y(annot['y'])\n                z(annot['z'])\n                showtitle(annot['showtitle'])\n                showhistogram(annot['showhistogram'])\n                showlineprofile(annot['showlineprofile'])\n\n                //Annotation object\n                annotation(type:annot['type'], displayname:annot['displayname'], color:annot['color']) {\n                    measuretype(annot['measuretype'])\n                    closed(annot['closed'])\n                    pointlist {\n                        annot['pointlist'].each { pt -&gt;\n                            point {\n                                x( ((pt[0] - OffSet_From_Top_Left_X ) * pixelWidthNm).toInteger() )\n                                y( ((pt[1] - OffSet_From_Top_Left_Y ) * pixelHeightNm).toInteger() )\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    NDPAfile.write(writer.toString())\n}\n\n//read_ndpa()\nwrite_ndpa()\n</code></pre>\n<p>I then received an error I had never seen before:</p>\n<p>ERROR: MissingMethodException at line 154: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.get_osr() is applicable for argument types: () values: <span class=\"chcklst-box fa fa-square-o fa-fw\"></span><br>\nPossible solutions: getClass(), get(java.lang.String), getAt(java.lang.String)</p>\n<p>ERROR: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.callGlobal(GroovyScriptEngineImpl.java:404)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.access$100(GroovyScriptEngineImpl.java:90)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl$3.invokeMethod(GroovyScriptEngineImpl.java:303)<br>\norg.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:73)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:176)<br>\nScript3.write_ndpa(Script3.groovy:33)<br>\nScript3.run(Script3.groovy:155)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:982)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:914)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:829)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1345)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>Any thoughts?</p>", "<aside class=\"quote no-group\" data-username=\"DeLuca_Lab\" data-post=\"9\" data-topic=\"55418\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/deluca_lab/40/53091_2.png\" class=\"avatar\"> DeLuca Lab:</div>\n<blockquote>\n<p><code>def osr = get_osr()</code></p>\n</blockquote>\n</aside>\n<p>You tried to call a function that doesn\u2019t exist. In t<a href=\"https://forum.image.sc/t/exporting-ndpi-ndpa-annotation-files-from-qupath-code-attached/55418/4\">he script above</a> that function is defined, but in what you posted, it is not included.</p>", "<p>Thanks! I think I\u2019m getting closer. I have fixed that bit by just copying the script above and then receive the following error:</p>\n<p>ERROR: NullPointerException at line 348: Cannot invoke \u201corg.locationtech.jts.geom.Geometry.getNumGeometries()\u201d because \u201cgeometry\u201d is null</p>\n<p>ERROR: qupath.lib.roi.GeometryTools.flatten(GeometryTools.java:454)<br>\nqupath.lib.roi.GeometryTools.removeInteriorRings(GeometryTools.java:488)<br>\nqupath.lib.roi.GeometryTools.refineAreas(GeometryTools.java:737)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)<br>\norg.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrap.invoke(StaticMetaMethodSite.java:131)<br>\norg.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.call(StaticMetaMethodSite.java:89)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)<br>\norg.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.call(StaticMetaMethodSite.java:94)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:157)<br>\nScript23$_write_ndpa_closure5.doCall(Script23.groovy:263)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)<br>\norg.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)<br>\ngroovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)<br>\ngroovy.lang.Closure.call(Closure.java:412)<br>\ngroovy.lang.Closure.call(Closure.java:428)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2358)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2343)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2384)<br>\norg.codehaus.groovy.runtime.dgm$202.invoke(Unknown Source)<br>\norg.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:247)<br>\norg.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:56)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)<br>\nScript23.write_ndpa(Script23.groovy:257)<br>\nScript23.run(Script23.groovy:349)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:982)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:914)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:829)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1345)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>I\u2019ve attached a screenshot of what my annotations look like for reference in case that helps. Thanks!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3750f5f45c20b0d74184068657c3a9b11ae68.jpeg\" data-download-href=\"/uploads/short-url/4pNVAGQmXhDpI09PaImsMl4WJ3G.jpeg?dl=1\" title=\"Screen Shot 2022-09-23 at 3.06.21 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3750f5f45c20b0d74184068657c3a9b11ae68_2_690x407.jpeg\" alt=\"Screen Shot 2022-09-23 at 3.06.21 AM\" data-base62-sha1=\"4pNVAGQmXhDpI09PaImsMl4WJ3G\" width=\"690\" height=\"407\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3750f5f45c20b0d74184068657c3a9b11ae68_2_690x407.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3750f5f45c20b0d74184068657c3a9b11ae68_2_1035x610.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3750f5f45c20b0d74184068657c3a9b11ae68_2_1380x814.jpeg 2x\" data-dominant-color=\"B4B3B4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-09-23 at 3.06.21 AM</span><span class=\"informations\">1920\u00d71135 155 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"DeLuca_Lab\" data-post=\"11\" data-topic=\"55418\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/deluca_lab/40/53091_2.png\" class=\"avatar\"> DeLuca Lab:</div>\n<blockquote>\n<p>getNumGeom</p>\n</blockquote>\n</aside>\n<p>Can\u2019t really say since none of the scripts have that line in them. But the variable <code>geometry</code> is empty, so you aren\u2019t putting anything into it.</p>", "<p>I was also confused since the script I copied over did not have getNumGeom in it and line 348 is the following command:</p>\n<p>write_ndpa()</p>\n<p>I looked for the geometry variable and see that it is set with the following command in the script:</p>\n<p>geometry = pathObject.getROI().getGeometry()</p>\n<p>Do I need to list the annotation names when setting the variable geometry or something to ensure it doesn\u2019t come back null?</p>", "<p>The error line will be when the outer function is called, yes. Probably want to print things until you figure out where you start getting null.</p>", "<aside class=\"quote no-group\" data-username=\"DeLuca_Lab\" data-post=\"9\" data-topic=\"55418\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/deluca_lab/40/53091_2.png\" class=\"avatar\"> DeLuca Lab:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">        //Here we do some processing to simplify the outlines and remove small holes\n        geometry = TopologyPreservingSimplifier.simplify(geometry, 5.0);\n        geometry = GeometryTools.refineAreas(geometry, 200, 200)\n</code></pre>\n</blockquote>\n</aside>\n<p>I think the problem is happening there\u2026 specifically the <code>refineAreas</code> bit. I can replicate the error message if I use a line or polyline annotation (which can\u2019t be refined, because it doesn\u2019t define an area).</p>", "<p>Hello <a class=\"mention\" href=\"/u/deluca_lab\">@DeLuca_Lab</a></p>\n<p>This is just an idea but maybe for some pathological polygons (all the points are the same? 1 point? no points?) the geometry returned by <code>pathObject.getROI().getGeometry()</code> becomes null for some reason.</p>\n<p>Would you mind sending me your .ndpa file so I can investigate further? I\u2019d love to have a definite answer on this one.</p>\n<p>In the meantime, maybe something along the lines of adding <code>if (geometry == null) return</code> in the <code>pathObjects.each {}</code> loop will help (the equivalent of the <code>continue</code> statement in a for-loop if I\u2019m not mistaken):</p>\n<pre><code class=\"lang-java\">    pathObjects.each { pathObject -&gt;\n        //We make a list of polygons, each has an exterior and interior rings\n        geometry = pathObject.getROI().getGeometry()\n        if (geometry == null)\n            return\n</code></pre>\n<p><a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a></p>\n<blockquote>\n<p>Can\u2019t really say since none of the scripts have that line in them.</p>\n</blockquote>\n<p>That was code directly copy-pasted from <a href=\"https://github.com/qupath/qupath/blob/9eaf034e2cd0325d38967bde5ac43900ddc15e3f/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java#L197-L198\" rel=\"noopener nofollow ugc\">a comment left behind in GeometryTools</a>, maybe there was a reason why those lines were commented out <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><strong>Edit</strong> after reading <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>\u2019s comment, <em>after</em> the <code>simplify()</code> call and/or the <code>refineAreas()</code> call:</p>\n<pre><code class=\"lang-java\">    pathObjects.each { pathObject -&gt;\n        //We make a list of polygons, each has an exterior and interior rings\n        geometry = pathObject.getROI().getGeometry()\n\n        //Here we do some processing to simplify the outlines and remove small holes\n        geometry = TopologyPreservingSimplifier.simplify(geometry, 5.0);\n\n        if (geometry == null)\n            return\n\n        geometry = GeometryTools.refineAreas(geometry, 200, 200)\n\n        if (geometry == null)\n            return\n\n</code></pre>\n<p>Let me know what works.</p>\n<p>Cheers,<br>\nEgor</p>", "<p>If it fails with pathological polygons, please do let me know \u2013 but it already definitely fails with lines and points.</p>\n<p>I\u2019ve created an issue to explain; it <em>should</em> fail in those cases, but it should also give a better message when it does.</p>\n<aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/qupath/qupath/issues/1060\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/qupath/qupath/issues/1060\" target=\"_blank\" rel=\"noopener\">github.com/qupath/qupath</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/qupath/qupath/issues/1060\" target=\"_blank\" rel=\"noopener\">NPE if GeometryTools.refineAreas() is called with a non-area geometry</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-09-23\" data-time=\"05:48:23\" data-timezone=\"UTC\">05:48AM - 23 Sep 22 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/petebankhead\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"petebankhead\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32e3986c37c16f82cacf3728239c4ecbd37c1b44.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          petebankhead\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          bug\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">## Bug report\n\n**Describe the bug**\nThe exception thrown when passing a linea<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">r or point geometry to [`GeometryTools.refineAreas(Geometry geometry, double minSizePixels, double minHoleSizePixels)`](https://github.com/qupath/qupath/blob/v0.3.2/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java#L723) isn't exactly informative, e.g.\n\n```\nNullPointerException at line 6: Cannot invoke \"org.locationtech.jts.geom.Geometry.getNumGeometries()\" because \"geometry\" is null\nqupath.lib.roi.GeometryTools.flatten(GeometryTools.java:453)\n    qupath.lib.roi.GeometryTools.removeInteriorRings(GeometryTools.java:487)\n    qupath.lib.roi.GeometryTools.refineAreas(GeometryTools.java:736)\n    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)\n    Script21.run(Script21.groovy:7)\n```\n\n**To Reproduce**\nCode to reproduce the behavior:\n\n```groovy\n// Create a Line ROI\nvar roi = ROIs.createLineROI(0, 0, 100, 100, ImagePlane.getDefaultPlane())\n// Alternative to create a Point ROI (also fails in a similar way)\n// var roi = ROIs.createPointsROI(0, 10, ImagePlane.getDefaultPlane())\nvar geom = roi.getGeometry()\nGeometryTools.refineAreas(geom, 1000, 1000)\n```\n\n**Expected behavior**\nThe faulty input should be caught earlier and an `IllegalArgumentException` thrown with a more informative message.\n\nThis is what happens with the closest method from [`RoiTools`](https://github.com/qupath/qupath/blob/v0.3.2/qupath-core/src/main/java/qupath/lib/roi/RoiTools.java#L268), i.e. calling\n```groovy\nvar roi = ROIs.createLineROI(0, 0, 100, 100, ImagePlane.getDefaultPlane())\nRoiTools.removeSmallPieces(roi, 1000, 1000)\n```\nresults in\n```\nIllegalArgumentException at line 3: Only area ROIs supported!\nqupath.lib.roi.RoiTools.removeSmallPieces(RoiTools.java:382)\n    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)\n    Script23.run(Script23.groovy:4)\n```\n\nHowever there are at least two problems with the javadoc for the `RoiTools` method:\n* the `IllegalArgumentException` isn't mentioned\n* it isn't obvious that the method returns null if the entire ROI is removed (rather than an empty ROI) - so this should be documented\n\n**Desktop (please complete the following information):**\n - OS: All\n - QuPath Version: v0.3.2 (and earlier)\n\n**Additional context**\nI first became aware of the issue at https://forum.image.sc/t/exporting-ndpi-ndpa-annotation-files-from-qupath-code-attached/55418/11\n\nPotentially, the method names for `RoiTools` and `GeometryTools` should be standardised since they do much the same thing, and tests should be added.</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p><a class=\"mention\" href=\"/u/ep.zindy\">@EP.Zindy</a> I\u2019m attaching a google drive link with the QuPath project and image that I\u2019m hoping to extract the annotations from: <a href=\"https://drive.google.com/drive/folders/1OpBfXoBkCZaa2jLYWL3AgtDQzNAhGtrT?usp=sharing\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">NDPA Export - Google Drive</a></p>\n<p>I tried changing the code with the \u201cif statements\u201d you added and it looks like I got the same error as before:</p>\n<p>ERROR: NullPointerException at line 355: Cannot invoke \u201corg.locationtech.jts.geom.Geometry.getNumGeometries()\u201d because \u201cgeometry\u201d is null</p>\n<p>ERROR: qupath.lib.roi.GeometryTools.flatten(GeometryTools.java:454)<br>\nqupath.lib.roi.GeometryTools.removeInteriorRings(GeometryTools.java:488)<br>\nqupath.lib.roi.GeometryTools.refineAreas(GeometryTools.java:737)<br>\njdk.internal.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)<br>\norg.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrap.invoke(StaticMetaMethodSite.java:131)<br>\norg.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.call(StaticMetaMethodSite.java:89)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)<br>\norg.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.call(StaticMetaMethodSite.java:94)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:157)<br>\nScript3$_write_ndpa_closure5.doCall(Script3.groovy:267)<br>\njdk.internal.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)<br>\norg.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)<br>\ngroovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)<br>\ngroovy.lang.Closure.call(Closure.java:412)<br>\ngroovy.lang.Closure.call(Closure.java:428)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2358)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2343)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2384)<br>\norg.codehaus.groovy.runtime.dgm$202.invoke(Unknown Source)<br>\norg.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:247)<br>\norg.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:56)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)<br>\nScript3.write_ndpa(Script3.groovy:257)<br>\nScript3.run(Script3.groovy:356)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:982)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:914)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:829)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1345)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>", "<p>In this case it might be easier to choose at the annotation or ROI step whether the objects should be processed.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/PolygonROI.html#isArea()\">\n  <header class=\"source\">\n\n      <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/PolygonROI.html#isArea()\" target=\"_blank\" rel=\"noopener\">qupath.github.io</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/PolygonROI.html#isArea()\" target=\"_blank\" rel=\"noopener\">PolygonROI (QuPath 0.3.0)</a></h3>\n\n  <p>declaration: package: qupath.lib.roi, class: PolygonROI</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<pre><code class=\"lang-auto\">geometry = pathObject.getROI().getGeometry()\n</code></pre>\n<p>becomes</p>\n<pre><code class=\"lang-auto\">if (pathobject.getROI().isArea()){\ngeometry = pathObject.getROI().getGeometry()\n} else{\ncontinue\n}\n</code></pre>\n<p>or something similar. Writing that on the fly, so haven\u2019t tested. Maybe that should be return rather than continue?</p>\n<p>Alternatively before all the looping, something like<br>\n<code>pathObjects = getAnnotationObjects().findAll{it.getROI().isArea()}</code></p>", "<p>Thanks <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, it\u2019s a clever use of <code>getROI().isArea()</code>, I like it!</p>\n<p>Also <a class=\"mention\" href=\"/u/deluca_lab\">@DeLuca_Lab</a>, thanks for sending a QuPath project and an image, it was much easier to try and fix my code with this data.</p>\n<p>First, here\u2019s what the NDPA export looks like after the fix:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b003cf4f7fa54bc95a1d85b14c9592612798a387.jpeg\" data-download-href=\"/uploads/short-url/p76cejycbfSUpvU674si3vgWb5l.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b003cf4f7fa54bc95a1d85b14c9592612798a387_2_690x351.jpeg\" alt=\"image\" data-base62-sha1=\"p76cejycbfSUpvU674si3vgWb5l\" width=\"690\" height=\"351\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b003cf4f7fa54bc95a1d85b14c9592612798a387_2_690x351.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b003cf4f7fa54bc95a1d85b14c9592612798a387_2_1035x526.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b003cf4f7fa54bc95a1d85b14c9592612798a387_2_1380x702.jpeg 2x\" data-dominant-color=\"DBDBE0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1613\u00d7822 151 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I don\u2019t have NDPView plus on my machine to read SVS files, so I had to improvise as you can see <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>The warning messages generated  by <code>write_ndpa()</code> indicate that it can only export polygons <em>for now</em>, not lines, points or circles. This should be easy enough to add, so I\u2019ll use your annotations to try and implement the missing features.</p>\n<p>Talking of error messages, I\u2019ll just mention in this thread that you may get the following error if you don\u2019t <code>import groovy.xml.XmlSlurper</code> as I\u2019m doing in the new code below. This was mentioned in this bug report some time ago: <a href=\"https://github.com/qupath/qupath/issues/455\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">groovy.xml.XmlSlurper not included in build \u00b7 Issue #455 \u00b7 qupath/qupath \u00b7 GitHub</a></p>\n<blockquote>\n<p>ERROR: It looks like you have tried to import a class \u2018XmlSlurper\u2019 that doesn\u2019t exist!</p>\n<p>ERROR: MultipleCompilationErrorsException at line 95: startup failed:<br>\nScript10.groovy: 96: unable to resolve class XmlSlurper<br>\n@ line 96, column 16.<br>\ndef list = new XmlSlurper().parseText(text)<br>\n^</p>\n<p>1 error</p>\n</blockquote>\n<p>Anyway, new code below:</p>\n<pre><code class=\"lang-java\">import groovy.xml.XmlSlurper\nimport groovy.xml.MarkupBuilder\nimport org.openslide.OpenSlide\nimport qupath.lib.scripting.QP\nimport qupath.lib.gui.tools.ColorToolsFX\nimport qupath.lib.geom.Point2\nimport qupath.lib.images.servers.ImageServer\nimport qupath.lib.common.GeneralTools\nimport qupath.lib.objects.PathAnnotationObject\nimport qupath.lib.objects.classes.PathClassFactory\nimport qupath.lib.roi.*\nimport org.locationtech.jts.geom.util.PolygonExtracter\nimport org.locationtech.jts.simplify.TopologyPreservingSimplifier;\n\ndef get_osr() {\n    def server = QP.getCurrentImageData().getServer()\n\n    // If OpenSlide metadata isn't available, load it up!\n    if (server.hasProperty('osr') &amp;&amp; server.osr){\n        // Image was opened with OpenSlide\n        osr = server.osr\n    } else {\n        // Code borrowed from qupath/qupath-extension-openslide/src/main/java/qupath/lib/images/servers/openslide/OpenslideImageServer.java \n        // Ensure the garbage collector has run - otherwise any previous attempts to load the required native library\n        // from different classloader are likely to cause an error (although upon first further investigation it seems this doesn't really solve the problem...)\n        System.gc();\n        def uri = GeneralTools.toPath(server.getURIs()[0]).toString();\n        file = new File(uri);\n        osr = new OpenSlide(file);\n    }\n    return osr\n}\n\n// Convert a point from NDPA to QuPath coordinates\ndef convertPoint(point, pixelWidthNm=1, pixelHeightNm=1, OffSet_From_Top_Left_X=0, OffSet_From_Top_Left_Y=0) {\n    point.x = (point.x.toDouble() / pixelWidthNm) + OffSet_From_Top_Left_X \n    point.y = (point.y.toDouble() / pixelHeightNm) + OffSet_From_Top_Left_Y \n}\n\n\n// Read an NDPA file TODO check that the code still works\ndef read_ndpa()\n{\n    def server = QP.getCurrentImageData().getServer()\n\n    // We need the pixel size\n    def cal = server.getPixelCalibration()\n    if (!cal.hasPixelSizeMicrons()) {\n        Dialogs.showMessageDialog(\"Metadata check\", \"No pixel information for this image!\");\n        return\n    }\n\n    def pixelWidthNm = cal.getPixelWidthMicrons() * 1000\n    def pixelHeightNm = cal.getPixelHeightMicrons() * 1000\n    \n    //Aperio Image Scope displays images in a different orientation\n    //TODO Is this in the metadatata? Is this likely to be a problem?\n    //print(server.dumpMetadata())\n    def rotated = false\n\n    def ImageCenter_X = server.getWidth()/2\n    def ImageCenter_Y = server.getHeight()/2\n\n    def osr = get_osr()\n\n    // need to add annotations to hierarchy so qupath sees them\n    def hierarchy = QP.getCurrentHierarchy()\n        \n    //*********Get NDPA automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()+\".ndpa\";\n\n    def NDPAfile = new File(path)\n    if (!NDPAfile.exists()) {\n        println \"No NDPA file for this image...\"\n        return\n    }\n\n    //Get X Reference from OPENSLIDE data\n    //The Open slide numbers are actually offset from IMAGE center (not physical slide center). \n    //This is annoying, but you can calculate the value you need -- Offset from top left in Nanometers. \n    OffSet_From_Top_Left_X = ImageCenter_X\n    OffSet_From_Top_Left_Y = ImageCenter_Y\n\n    osr.getProperties().each { k, v -&gt;\n        if(k.equals(\"hamamatsu.XOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_X -= v.toDouble()/pixelWidthNm\n        }\n        if(k.equals(\"hamamatsu.YOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_Y -= v.toDouble()/pixelHeightNm\n        }\n    }\n\n    //Read files\n    def text = NDPAfile.getText()\n    def list = new XmlSlurper().parseText(text)\n\n    list.ndpviewstate.each { ndpviewstate -&gt;\n        def annotationName = ndpviewstate.title.toString().trim()\n        def annotationClassName = annotationName\n        def annotationType = ndpviewstate.annotation.@type.toString().toUpperCase()\n        def annotationColor = ndpviewstate.annotation.@color.toString().toUpperCase()\n\n        def details = ndpviewstate.details.toString()\n        //println annotationName+\" (\"+annotationType+\") (\"+annotationClassName+\") \"+details\n    \n        roi = null\n        \n        if (annotationType == \"CIRCLE\") {\n            //special case\n            def point = new Point2(ndpviewstate.annotation.x, ndpviewstate.annotation.y)\n            convertPoint(point, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n\n            def rx = ndpviewstate.annotation.radius.toDouble() / pixelWidthNm\n            def ry = ndpviewstate.annotation.radius.toDouble() / pixelHeightNm\n            roi = new EllipseROI(point.x-rx,point.y-ry,rx*2,ry*2,null);\n        }\n        \n        if (annotationType == \"LINEARMEASURE\") {\n            //special case\n            def pt1 = new Point2(ndpviewstate.annotation.x1, ndpviewstate.annotation.y1)\n            convertPoint(pt1, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            def pt2 = new Point2(ndpviewstate.annotation.x2, ndpviewstate.annotation.y2)\n            convertPoint(pt2, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            roi = new LineROI(pt1.x,pt1.y,pt2.x,pt2.y);\n        }\n\n        if (annotationType == \"PIN\") {\n            def point = new Point2(ndpviewstate.annotation.x, ndpviewstate.annotation.y)\n            convertPoint(point, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            roi = new PointsROI(point.x,point.y);\n        }\n            \n        // All that's left if FREEHAND which handles polygons, polylines, rectangles\n        ndpviewstate.annotation.pointlist.each { pointlist -&gt;\n            def tmp_points_list = []\n            pointlist.point.each{ point -&gt;\n                if (rotated) {\n                    X = point.x.toDouble()\n                    Y = h - point.y.toDouble()\n                }\n                else {\n                    X = point.x.toDouble()\n                    Y =  point.y.toDouble()\n                }\n\n                tmp_points_list.add(new Point2(X, Y))\n            } \n    \n            //Adjust each point relative to SLIDECENTER coordinates and adjust for pixelsPerMicron\n            for ( point in tmp_points_list){\n                convertPoint(point, pixelWidthNm, pixelHeightNm, OffSet_From_Top_Left_X, OffSet_From_Top_Left_Y)\n            }\n            \n            if (annotationType == \"FREEHAND\") {\n                isClosed = 1 //ndpviewstate.annotation.closed.toBoolean() //XXX PLEASE PLEASE PLEASE USE THE POLYGON TOOL!!!!!\n                isRectangle = (ndpviewstate.annotation.specialtype.toString() == \"rectangle\")\n                if (isRectangle) {\n                    x1 = tmp_points_list[0].x\n                    y1 = tmp_points_list[0].y\n                    x3 = tmp_points_list[2].x\n                    y3 = tmp_points_list[2].y\n                    roi = new RectangleROI(x1,y1,x3-x1,y3-y1);\n                }\n                else if (isClosed)\n                    roi = new PolygonROI(tmp_points_list);\n                else\n                    roi = new PolylineROI(tmp_points_list, null);\n            }\n            \n        }\n        \n        if (roi != null)\n        {\n            def annotation = new PathAnnotationObject(roi)\n            annotation.setName(annotationName)        \n            if (annotationClassName)\n            {\n                //TODO (validate) and add the new class if it doesn't already exist:\n                //if (!PathClassFactory.classExists(annotationClassName))\n                annotation.setPathClass(PathClassFactory.getPathClass(annotationClassName))\n            }\n            \n            if (details) {          \n                annotation.setDescription(details)\n            }\n\n            annotation.setLocked(true)\n            hierarchy.addPathObject(annotation) //, false)\n        }\n\n    }\n\n}\n\n//Here we extract the polygon coords\ndef getPointList(ring) {\n    def pointlist = []\n    coords = ring.getCoordinates()\n    coords[0..&lt;coords.length-1].each { coord -&gt;\n        pointlist.add([coord.x, coord.y])\n    }\n    //println \"---\"\n    return pointlist\n}\n\ndef write_ndpa() {\n    def server = QP.getCurrentImageData().getServer()\n\n    // We need the pixel size\n    def cal = server.getPixelCalibration()\n    if (!cal.hasPixelSizeMicrons()) {\n        Dialogs.showMessageDialog(\"Metadata check\", \"No pixel information for this image!\");\n        return\n    }\n\n    def pixelWidthNm = cal.getPixelWidthMicrons() * 1000\n    def pixelHeightNm = cal.getPixelHeightMicrons() * 1000\n    \n    //Aperio Image Scope displays images in a different orientation\n    //TODO Is this in the metadatata? Is this likely to be a problem?\n    //print(server.dumpMetadata())\n    def rotated = false\n\n    def ImageCenter_X = server.getWidth()/2\n    def ImageCenter_Y = server.getHeight()/2\n\n    def osr = get_osr()\n\n    //*********Get NDPA automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()+\".ndpa\";\n\n    def NDPAfile = new File(path)\n\n    //Get X Reference from OPENSLIDE data\n    //The Open slide numbers are actually offset from IMAGE center (not physical slide center). \n    //This is annoying, but you can calculate the value you need -- Offset from top left in Nanometers. \n    OffSet_From_Top_Left_X = ImageCenter_X\n    OffSet_From_Top_Left_Y = ImageCenter_Y\n\n    osr.getProperties().each { k, v -&gt;\n        if(k.equals(\"hamamatsu.XOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_X -= v.toDouble()/pixelWidthNm\n        }\n        if(k.equals(\"hamamatsu.YOffsetFromSlideCentre\")){\n            OffSet_From_Top_Left_Y -= v.toDouble()/pixelHeightNm\n        }\n    }\n\n    // need to add annotations to hierarchy so qupath sees them\n    def hierarchy = QP.getCurrentHierarchy()\n    def pathObjects = hierarchy.getAnnotationObjects()\n\n    //create a list of annotations\n    def list_annot = []\n    def ndpIndex = 0\n\n    pathObjects.each { pathObject -&gt;\n        if (!pathObject.getROI().isArea()) {\n            println \"Not a polygon: \"+pathObject\n            return\n        }\n        //We make a list of polygons, each has an exterior and interior rings\n        geometry = pathObject.getROI().getGeometry()\n        //Here we do some processing to simplify the outlines and remove small holes\n        geometry = TopologyPreservingSimplifier.simplify(geometry, 5.0);\n        geometry = GeometryTools.refineAreas(geometry, 200, 200)\n        var polygons = PolygonExtracter.getPolygons(geometry);\n\n        polygons.each { polygon -&gt;\n\n            //here we create a list of rings, we'll need to treat the first one differently\n            def rings = [ polygon.getExteriorRing() ]\n            def nRings = polygon.getNumInteriorRing();\n            for (int i = 0; i &lt; nRings; i++) {\n                var ring = polygon.getInteriorRingN(i);\n                rings.add(ring)\n            }\n\n            rings.eachWithIndex { ring, index -&gt;\n                def annot = [:]\n                if (index == 0) {\n                    annot['title'] = pathObject.getName()\n                    annot['details'] = pathObject.getPathClass()\n                    annot['color'] = '#' + Integer.toHexString(ColorToolsFX.getDisplayedColorARGB(pathObject)).substring(2)\n                    isFirst = false\n                } else {\n                    annot['title'] = \"clear\"\n                    annot['details'] = \"clear\"\n                    annot['color'] = '#000000'\n                }\n\n                annot['id'] = ++ndpIndex\n                annot['coordformat'] = 'nanometers'\n                annot['lens'] = 0.445623\n                annot['x'] = ImageCenter_X.toInteger()\n                annot['y'] = ImageCenter_Y.toInteger()\n                annot['z'] = 0\n                annot['showtitle'] = 0\n                annot['showhistogram'] = 0\n                annot['showlineprofile'] = 0\n                annot['type'] = 'freehand'\n                annot['displayname'] = 'AnnotateFreehand'\n                annot['measuretype'] = 0\n                annot['closed'] = 1\n\n                //add the point list\n                annot['pointlist'] = getPointList(ring)\n                list_annot.add(annot)\n            }\n        }\n    }\n\n    //make an XML string\n    def writer = new StringWriter()\n    def xml = new MarkupBuilder(writer)\n    xml.mkp.xmlDeclaration(version: \"1.0\", encoding: \"utf-8\", standalone: \"yes\")\n    xml.annotations {\n        list_annot.each { annot -&gt;\n            ndpviewstate('id':annot['id']) {\n                title(annot['title'])\n                details(annot['details'])\n                coordformat(annot['coordformat'])\n                lens(annot['lens'])\n                x(annot['x'])\n                y(annot['y'])\n                z(annot['z'])\n                showtitle(annot['showtitle'])\n                showhistogram(annot['showhistogram'])\n                showlineprofile(annot['showlineprofile'])\n\n                //Annotation object\n                annotation(type:annot['type'], displayname:annot['displayname'], color:annot['color']) {\n                    measuretype(annot['measuretype'])\n                    closed(annot['closed'])\n                    pointlist {\n                        annot['pointlist'].each { pt -&gt;\n                            point {\n                                x( ((pt[0] - OffSet_From_Top_Left_X ) * pixelWidthNm).toInteger() )\n                                y( ((pt[1] - OffSet_From_Top_Left_Y ) * pixelHeightNm).toInteger() )\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    NDPAfile.write(writer.toString())\n}\n\n//read_ndpa()\nwrite_ndpa()\n</code></pre>\n<p>I will try and add \u201clines\u201d to my NDPA export, watch this space!</p>\n<p>Cheers,<br>\nEgor</p>"], "59538": ["<p>Hello - not sure if this has been broached in another topic somewhere (searching didn\u2019t turn up anything), but is there any BTDT experience with shifting an existing set of data fronted by a Columbus server stack to one using OMERO or OMERO-Plus software? We\u2019re looking to move away from Columbus due to the aged OS requirements for the software and any/all options are being explored at the moment\u2026</p>\n<p>If this has some sticky forum topic(s) that I missed, fair play to just point me in that direction first and let me catch up on RTFM.</p>\n<p>Best,<br>\nAllan Harris - Research Computing, Harvard Medical School</p>", "<p>Hi Allan. Welcome to image.sc!</p>\n<p>I\u2019ve not got any BTDT experience (for those reading along, I had to look it up: \u201cbeen there - done that\u201d) but below is what I tell people.</p>\n<p>A few caveats first:</p>\n<ul>\n<li>neither Glencoe nor the open source project has access to any internal details of Columbus</li>\n<li>i.e. there could be changes to the database that we are unaware of</li>\n<li>your best port of call will be to contact your vendor directly</li>\n</ul>\n<p>Barring the caveats, I assume that the OMERO sitting within Columbus is an OMERO 4.4 variant. (The <code>dbpatch</code> table <em>should</em> be able to tell you more). There should be no harm in exporting the database (with <code>pg_dump</code>) and then running it through all the available upgrade scripts for OMEROs up to the present one. If that succeeds, then I would assume that you could copy your data directory and safely start a new, updated OMERO instance.</p>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/crossed_fingers.png?v=10\" title=\":crossed_fingers:\" class=\"emoji only-emoji\" alt=\":crossed_fingers:\"><br>\n~Josh</p>", "<p>Hi Josh - thanks very much for the reply! Sorry I\u2019m getting back to you so late, I didn\u2019t get a notification that the thread was bumped and only managed to find this while scouring the forums for an unrelated issue with Leica color correction metadata\u2026</p>\n<p>I ended up having a call with Glencoe to talk this through, and your assumptions are spot-on. If the customer does decide to put some $$ into the mix to go with OMERO-Plus, we have that option, but we may also explore replacing Columbus with the open source product using the techniques you describe. As you suggested, Glencoe confirmed that the lynchpin is the database version and running through all of the available schema updates to get it off of 4.4 (may even be 4.1, will need to check) and somewhere in the current version space. The rest of it seemed easy; basically move the data mount for the images from one system to the other and you\u2019d be in good shape.</p>\n<p>FWIW (for what it\u2019s worth - sorry for the acronym soup, habit!), I also spoke with Perkin-Elmer. They confirmed that V2.91 is the terminal release for Columbus, they will not support it on RHEL 7, and they\u2019re trying to move customers to a new product called SImA, which is container-based. I don\u2019t know yet if it\u2019s going to be OMERO compliant. From their support: <em>\u2026 we have developed a next generation image analysis system - called \u2018Signals Image Artist\u2019 (SImA) - which is intended to be the eventual successor to Columbus. SImA was released earlier this month; it uses Docker technology to containerise the software and as such supports a wide array of modern operating systems.</em></p>\n<p>Anyway, we\u2019ll continue to walk this down to some solution, and when we do I\u2019ll share our experience. Thanks again for the feedback, and if you\u2019re US based, have a great Thanksgiving!</p>\n<p>Best,<br>\nAllan</p>", "<p>Hi Allan, sorry to bump this topic but did you make any progress with this since then? We also have a similar problem and have been exploring medium-term and long-term options for replacing Columbus.</p>\n<p>The main problem which we have is with users being dependent on propriety PerkinElmer analysis pipelines which are not going to be supported in other software. We are investigating use of WIPP long-term for such analysis although think the software needs some user interface enhancements before it will be widely usable by users who are not from a computational background.</p>\n<p>In the medium-term we are investigating ways to run Columbus in the best possible way to manage its legacy software requirements such as through containerisation, as we would rather avoid the high cost of SImA if possible.</p>\n<p>Regards,</p>\n<p>Sam Braithwaite</p>"], "69788": ["<p>Hi,</p>\n<p>So, I tried using deeplabcut as dlc for 3D Video Analysis (two cameras) but receive a value error.</p>\n<p><strong>This is the input:</strong><br>\ndlc.create_labeled_video_3d(config_path3d,<br>\n[video_path],<br>\ndraw_skeleton=True,<br>\nvideotype = \u201cmp4\u201d)</p>\n<p><strong>And this is the output:</strong><br>\nAnalyzing all the videos in the directory<br>\n[[\u2018/home/dlc/evaluation/Experimental_Data/subject_1/practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_Dan.h5\u2019, \u2018/home/dlc/evaluation/Experimental_Data/subject_1/practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_top.mp4\u2019, \u2018/home/dlc/evaluation/Experimental_Data/subject_1/practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_side.mp4\u2019]]<br>\nCreating 3D video from practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_top.mp4 and practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_side.mp4 using practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_Dan.h5<br>\nLooking for filtered predictions\u2026<br>\nFound the following filtered data:  /home/dlc/evaluation/Experimental_Data/subject_1/<em>practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_topDLC_resnet50_Test_2D3D_one_modelJul20shuffle1_20000</em>filtered.h5 /home/dlc/evaluation/Experimental_Data/subject_1/<em>practice_yes_subject_1_block_0_trial_2_Staple_Gelbe_5_sideDLC_resnet50_Test_2D3D_one_modelJul20shuffle1_20000</em>filtered.h5</p>\n<hr>\n<p>ValueError                                Traceback (most recent call last)<br>\nInput In [39], in &lt;cell line: 1&gt;()<br>\n----&gt; 1 dlc.create_labeled_video_3d(config_path3d,<br>\n2                             [video_path],<br>\n3                             draw_skeleton=True,<br>\n4                             videotype = \u201cmp4\u201d)</p>\n<p>File ~/anaconda3/envs/DLC-GPU/lib/python3.8/site-packages/deeplabcut/pose_estimation_3d/plotting3D.py:272, in create_labeled_video_3d(config, path, videofolder, start, end, trailpoints, videotype, view, xlim, ylim, zlim, draw_skeleton, color_by, figsize, fps, dpi)<br>\n270 visible1 = xy1[\u2026, 2] &gt;= pcutoff<br>\n271 xy1[~visible1] = np.nan<br>\n \u2192 272 xy2 = df_cam2.loc[:, mask2d].to_numpy().reshape((len(df_cam1), -1, 3))<br>\n273 visible2 = xy2[\u2026, 2] &gt;= pcutoff<br>\n274 xy2[~visible2] = np.nan</p>\n<p>ValueError: cannot reshape array of size 85176 into shape (4083,newaxis,3)</p>\n<p><strong>I considered it being related to the following warning I received during triangulation but I am not sure whether it is relevant:</strong><br>\n/home/dlc/anaconda3/envs/DLC-GPU/lib/python3.8/site-packages/deeplabcut/pose_estimation_3d/triangulation.py:333: UserWarning: The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry! Excluding the extra frames from the longer video.<br>\nwarnings.warn(</p>\n<p><strong>For the sake of completeness, here the config file:</strong></p>\n<h1>\n<a name=\"project-definitions-do-not-edit-1\" class=\"anchor\" href=\"#project-definitions-do-not-edit-1\"></a>Project definitions (do not edit)</h1>\n<p>Task: Test_2D3D_one_model2<br>\nscorer: Dan<br>\ndate: Jul20</p>\n<h1>\n<a name=\"project-path-change-when-moving-around-2\" class=\"anchor\" href=\"#project-path-change-when-moving-around-2\"></a>Project path (change when moving around)</h1>\n<p>project_path: /home/dlc/Test_2D3D_one_model2-Dan-2022-07-20-3d</p>\n<h1>\n<a name=\"plotting-configuration-3\" class=\"anchor\" href=\"#plotting-configuration-3\"></a>Plotting configuration</h1>\n<p>skeleton:     # Note that the pairs must be defined, as you want them linked!</p>\n<ul>\n<li>\n<ul>\n<li>thumb1</li>\n<li>thumb2</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>thumb2</li>\n<li>thumb3</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>thumb3</li>\n<li>index4</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>index4</li>\n<li>index3</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>index3</li>\n<li>index2</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>index2</li>\n<li>index1<br>\nskeleton_color: black<br>\npcutoff: 0.4<br>\ncolormap: jet<br>\ndotsize: 15<br>\nalphaValue: 0.8<br>\nmarkerType: \u2018*\u2019<br>\nmarkerColor: r</li>\n</ul>\n</li>\n</ul>\n<h1>\n<a name=\"number-of-cameras-camera-names-path-of-the-config-files-shuffle-index-and-trainingsetindex-used-to-analyze-videos-4\" class=\"anchor\" href=\"#number-of-cameras-camera-names-path-of-the-config-files-shuffle-index-and-trainingsetindex-used-to-analyze-videos-4\"></a>Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:</h1>\n<p>num_cameras: 2<br>\ncamera_names:</p>\n<ul>\n<li>top</li>\n<li>side<br>\nscorername_3d: Dan # Enter the scorer name for the 3D output<br>\ntrainingsetindex_top: 0<br>\ntrainingsetindex_side: 0<br>\nconfig_file_top: /home/dlc/Test_2D3D_one_model-Dan-2022-07-20/config.yaml<br>\nconfig_file_side: /home/dlc/Test_2D3D_one_model-Dan-2022-07-20/config.yaml<br>\nshuffle_top: 1<br>\nshuffle_side: 1</li>\n</ul>\n<p>I had also tried training two networks separately on the different cameras but ended up with the same error.<br>\nAny ideas about what I am doing wrong?<br>\nThank you in advance!</p>", "<p>Hello !! did you save your issue ?</p>"], "78001": ["<p>hi</p>\n<p>i would like to ask help about the gpu seetings to run the last version of deeplabcut.</p>\n<p>i have a gpu nvidia rtx4000 that it was working very good with deeplacut, cuda 10 and drivers 460 until i decided to updated (due to some bugs in the tracklets).</p>\n<p>now i have the version 2.3.0 of the deeplabcut but i tried several combination of nvidia drivers and cuda but always the same problem: when i launch dlc:</p>\n<p>This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA<br>\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.<br>\n2023-03-02 21:05:13.960647: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered<br>\n2023-03-02 21:05:14.698631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \u2018libnvinfer.so.7\u2019; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory<br>\n2023-03-02 21:05:14.698715: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \u2018libnvinfer_plugin.so.7\u2019; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory<br>\n2023-03-02 21:05:14.698727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.<br>\nLoading DLC 2.3.0\u2026<br>\nStarting GUI\u2026</p>\n<p>can you help me to fix it?</p>\n<p>thanks in advance for your time!<br>\nbest<br>\njulia</p>", "<p>What did you update exactly?</p>", "<p>THis is a pytorch update issue; you can install the latest from master, or you can wait until tomorrow and a new pypi version will be up!</p>", "<p>Thanks <a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a> and <a class=\"mention\" href=\"/u/mwmathis\">@MWMathis</a> for the reply.</p>\n<p>I download the new Deeplabcut-master and I install it.</p>\n<p>The error is fixed, but now I have a new one!</p>\n<p>The error now is DLL load, very common in the forum.</p>\n<p>I tried to update the Path, install a different python version, update opencv-python, install h5py==2.7.0, different version of cuda,different version of tensorflow basically all the tips that I read in the forum. But nothing works!</p>\n<p>I notice in the forum that <a class=\"mention\" href=\"/u/mwmathis\">@MWMathis</a> faced the same problem in feb 2019, maybe you have another suggestion to me!</p>\n<p>thanks a lot!<br>\nJulia</p>", "<p>What are you cudatoolkit, cudnn and tensorflow versions?</p>", "<p><a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a></p>\n<p>cuda 10.0<br>\ncuDnn 7.4.2<br>\ntensorflow 2.3.0</p>\n<p>thanks for your time!</p>", "<p>Those cuda and cudnn are compatible with tensorflow 2.0.0. But I would recommend installing newer CUDA (11.8 for instance) with cudnn 8.4.1.50 and tensorflow 2.10.0</p>", "<p><a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a> i will try with 2.0.0</p>\n<p>if did not work, i will install everything again with the combination that you suggested. i have a Nvidia rtx a 4000 which drivers i should use with this combination?</p>\n<p>i was avoiding to increase a lot the cuda because i saw a comment of <a class=\"mention\" href=\"/u/mwmathis\">@MWMathis</a> saying to did did not go further 10.2. however, in the website now said \" <strong>Note, DeepLabCut is up to date with the latest CUDA and tensorflow versions</strong>\" Probably, this comment was outdated!</p>\n<p>Thanks for the fast replies!<br>\njulia</p>", "<p>did not allow me to install tensorflow below 2.2.0<br>\nwhen i wrote pip install tensorflow==2.0.0 or pip install tensorflow-gpu==2.0.0<br>\nerror: could not found a version that satisfies the required tensorflow==2.0.0</p>\n<p>Best<br>\nJulia</p>", "<p>Hi <a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a>,</p>\n<p>I uninstall everything and i did what you recommended:<br>\ncuda 11.8.89<br>\nNvidia drivers 530<br>\ntensorflow-gpu==2.10<br>\ncuDNN 8.4.1.50</p>\n<p>the same error persist!<br>\nThanks for your help!<br>\nJulia</p>", "<p><code>pip uninstall tensorflow-gpu</code></p>\n<p><code>pip install tensorflow==2.10</code></p>\n<p>Should fix this</p>", "<p>Hi <a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a></p>\n<p>I did what you suggested: <code>pip uninstall tensorflow-gpu</code> and <code>pip install tensorflow==2.10</code>! but now when i launch Deeplabcut:</p>\n<p>Error no module named \u00b4tensorflow\u00b4.</p>\n<p>then i uninstall tensorflow (pip uninstall tensorflow)and i install again:  <code>pip install tensorflow==2.10</code><br>\nthe same error than before DLL load failed</p>\n<p>Thanks for your help</p>", "<p>Could you create the env from scratch and then within the new env run <code>conda install -c conda-forge cudnn</code> then <code>conda list tensorflow</code> and if it\u2019s not <code>2.10.0</code> run <code>pip install --upgrade tensorflow==2.10.0</code>?</p>", "<p>Hi <a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a></p>\n<p>I created a new environment and inside the environment i call: <code>conda install -c conda-forge cudnn</code> and then <code>conda list tensorflow</code><br>\noutput tensorflow 2.10.0<br>\ntensorflow-estimator 2.10.0<br>\ntensorflor-io-gcs-filesystem 0.31.0</p>\n<p>when i call deeplabcut (python -m deeplabcut) the same error : dll load failed</p>\n<p>I try to run pip install --upgrade tensorflow==2.10.0 but of course the same happen!</p>\n<p>Julia</p>", "<p>That\u2019s very strange. Do you have a system wide CUDA installed? What if you try <code>pip install --upgrade tensorflow-io-gcs-filesystem==0.27.0</code></p>", "<p>After pip install --upgrade tensorflow-io-gcs-filesystem==0.27.0 the same error Dll load failed!</p>\n<p>I don\u2019t know what to do!</p>\n<p>I really appreciate help!</p>", "<p>Can you post a screenshot of the full traceback?</p>", "<p>(base) PS C:\\WINDOWS\\system32&gt; conda activate DEEPLABCUT<br>\n(DEEPLABCUT) PS C:\\WINDOWS\\system32&gt; python -m deeplabcut<br>\nLoading DLC 2.3.2\u2026<br>\nDLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\u201d, line 185, in _run_module_as_main<br>\nmod_name, mod_spec, code = <em>get_module_details(mod_name, <em>Error)<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\u201d, line 144, in <em>get_module_details<br>\nreturn <em>get_module_details(pkg_main_name, error)<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\u201d, line 111, in <em>get_module_details<br>\n<strong>import</strong>(pkg_name)<br>\nFile \"C:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut_<em>init</em></em>.py\", line 37, in <br>\nfrom deeplabcut.create_project import (<br>\nFile \"C:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\create_project_<em>init</em></em>.py\", line 12, in <br>\nfrom deeplabcut.create_project.demo_data import load_demo_data<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\create_project\\demo_data.py\u201d, line 16, in <br>\nfrom deeplabcut.utils import auxiliaryfunctions<br>\nFile \"C:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils_<em>init</em></em>.py\", line 11, in <br>\nfrom deeplabcut.utils.auxfun_multianimal import *<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\auxfun_multianimal.py\u201d, line 34, in <br>\nfrom deeplabcut.utils import auxiliaryfunctions, conversioncode<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\auxiliaryfunctions.py\u201d, line 31, in <br>\nfrom deeplabcut.pose_estimation_tensorflow.lib.trackingutils import TRACK_METHODS<br>\nFile \"C:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow_<em>init</em></em>.py\", line 17, in <br>\nfrom deeplabcut.pose_estimation_tensorflow.datasets import *<br>\nFile \"C:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\datasets_<em>init</em></em>.py\", line 13, in <br>\nfrom .pose_deterministic import DeterministicPoseDataset<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\datasets\\pose_deterministic.py\u201d, line 17, in <br>\nfrom deeplabcut.utils.auxfun_videos import imread, imresize<br>\nFile \u201cC:\\Users\\sie\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\auxfun_videos.py\u201d, line 25, in <br>\nimport cv2<br>\nImportError: DLL load failed while importing cv2: No se puede encontrar el m\u00f3dulo especificado.<br>\n(DEEPLABCUT) PS C:\\WINDOWS\\system32&gt;</p>", "<p>That\u2019s an <code>opencv</code> error. Can you check <code>opencv</code> versions in the env?</p>", "<p>opencv-python 4.7.0.72</p>"], "78007": ["<p>Hi,</p>\n<p>I wanted to find a way to get the triangle or 3D mesh point contained in a specific 2D pixel when viewing a mesh on vedo. In other words, when viewing a Mesh using Vedo\u2019s Plotter, I want to find a way to get the cells (or even the 3D point) shown at each pixel. There\u2019s currently a way to do this with callbacks, where you can click on any part of the viewer and get the triangle you clicked on. The code for the callback method is below (I got this from github btw):</p>\n<pre><code class=\"lang-auto\">from vedo import *\n\ndef callb(evt):\n    msh = evt.actor\n    if not msh:\n        return\n    pt = evt.picked3d\n    idcell = msh.closest_point(pt, return_cell_id=True)\n    msh.cellcolors[idcell] = [255,0,0,255] # red, opaque\n    \nm = Mesh(dataurl + \"290.vtk\")\nm.decimate().smooth().compute_normals()\nm.compute_quality().cmap(\"Blues\", on=\"cells\")\n\nplt = Plotter()\nplt.add_callback(\"mouse click\", callb)\nplt.show(m, m.labels(\"cellid\"))\nplt.close()\n</code></pre>\n<p>However, there doesn\u2019t seem to be any way to select a point in the viewer without actually clicking the image. I just want a simple unproject transformation, where given a certain camera angle, I can map the triangles in the mesh to the pixels in the image. The VTK event contains the 3D point that was clicked on (so if I click on the mesh, it gives the 3D point on the mesh that I clicked), and from that it can find the triangle I clicked on. Is there any easy way to do this without callbacks? I did some research and there doesn\u2019t seem to be much information on this. I tried messing around with the source code but that didn\u2019t work either. What\u2019s the easiest way to go from triangles back to pixels (basically unproject)?</p>\n<p>Does anyone have any suggestions?</p>", "<p>Hi, after</p>\n<pre><code class=\"lang-auto\">pip install -U git+https://github.com/marcomusy/vedo.git\n</code></pre>\n<p>try:</p>\n<pre><code class=\"lang-python\">from vedo import *\n\nelli = Ellipsoid().rotate_y(30)\nplt = Plotter(interactive=False)\nplt.show(elli)\n\n# find pixel location\nxyscreen = plt.compute_screen_coordinates(elli)\nprint('xyscreen coords:', xyscreen)\n\n# simulate an event happening at one point\n#event = plt.fill_event(pos=xyscreen[42])\n#print(event)\n</code></pre>\n<p>output:</p>\n<pre><code class=\"lang-auto\">xyscreen coords: \n[[668 480]\n [342 480]\n [679 480]\n ...\n [385 470]\n [369 474]\n [355 477]]\n</code></pre>"], "78025": ["<p>Hi everyone,</p>\n<p>MATLAB is now available on EMBL\u2019S BAND at <a href=\"https://band.embl.de\" rel=\"noopener nofollow ugc\">https://band.embl.de</a>. You can simply sign in with your MathWorks account and password and all your licensed products will be available on BAND\u2019s cloud. If you need support do reach out to <a href=\"mailto:shuboc@mathworks.com\">shuboc@mathworks.com</a> or <a href=\"mailto:rholt@mathworks.com\">rholt@mathworks.com</a></p>\n<p>Best,<br>\nShubo</p>", "<p>Hi Shubo,<br>\nwhen I enter my account info it gives this error:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73bc01de35133e967779e08e3332f674ef34d526.png\" alt=\"image\" data-base62-sha1=\"gvPIciEUHfzZGJTpgA9Vikj4DwW\" width=\"501\" height=\"465\"><br>\nShell I go via the activation route?</p>\n<p>Best regards,<br>\nIlya</p>", "<p>Hi <a class=\"mention\" href=\"/u/ilya_belevich\">@Ilya_Belevich</a></p>\n<p>You shouldnt have to activate. Let me check this out and I will get back to you. I might contact you via your email to have you test it again.</p>\n<p>Best,<br>\nShubo</p>", "<p><a class=\"mention\" href=\"/u/ilya_belevich\">@Ilya_Belevich</a> could you share the email address associated with your MathWorks account that you are trying to login with?</p>", "<p><a class=\"mention\" href=\"/u/shubo_chakrabarti\">@Shubo_Chakrabarti</a><br>\nilya.belevich @ <a href=\"http://helsinki.fi\">helsinki.fi</a></p>", "<p>Thanks <a class=\"mention\" href=\"/u/shubo_chakrabarti\">@Shubo_Chakrabarti</a>,<br>\nre-linking to the individual license solved the issue!</p>\n<p>What defines the list of toolboxes available on BAND?<br>\nFor example, I am missing \u201cComputer Vision Toolbox\u201d\u2026</p>\n<p>Best regards,<br>\nIlya</p>", "<p><a class=\"mention\" href=\"/u/ilya_belevich\">@Ilya_Belevich</a> We started with a selection of toolboxes we thought would cover most uses. However if you want the computer vision toolbox, we can install that in BAND. I can let you know once that is done</p>", "<p><a class=\"mention\" href=\"/u/ilya_belevich\">@Ilya_Belevich</a> : Computer Vision Toolbox has been installed.</p>"], "78037": ["<p>we just received new olympus vs200. it creates vsi format. while waiting to receive it, i set-up my omero server and i tested with publicly available vsi files. everything worked well, files were uploaded. now it is just failing.<br>\nhere is omero.insight log file</p>\n<pre><code class=\"lang-auto\">2023-03-03 08:36:54,417 WARN  [     o.o.s.a.m.editor.AnnotationTaskPane] (nitializer) UI for displaying ROIS annotations not implemented yet! \n2023-03-03 08:36:54,699 DEBUG [       omero.gateway.facility.Facility$1] (  Thread-7) Created new BrowseFacility \n2023-03-03 08:36:57,462 DEBUG [ org.scijava.nativelib.NativeLibraryUtil] (entQueue-0) processor is INTEL_64 os.arch is amd64 \n2023-03-03 08:36:57,462 DEBUG [ org.scijava.nativelib.NativeLibraryUtil] (entQueue-0) architecture is WINDOWS_64 os.name is windows 10 \n2023-03-03 08:36:57,462 DEBUG [ org.scijava.nativelib.NativeLibraryUtil] (entQueue-0) architecture is WINDOWS_64 os.name is windows 10 \n2023-03-03 08:36:57,462 DEBUG [ org.scijava.nativelib.NativeLibraryUtil] (entQueue-0) platform specific path is META-INF/lib/windows_64/ \n2023-03-03 08:36:57,462 DEBUG [  org.scijava.nativelib.BaseJniExtractor] (entQueue-0) mappedLib is turbojpeg.dll \n2023-03-03 08:36:57,462 DEBUG [  org.scijava.nativelib.BaseJniExtractor] (entQueue-0) URL is jar:file:/C:/Users/User1/AppData/Local/OMERO.insight/app/lib/turbojpeg-6.10.0.jar!/META-INF/lib/windows_64/turbojpeg.dll \n2023-03-03 08:36:57,462 DEBUG [  org.scijava.nativelib.BaseJniExtractor] (entQueue-0) URL path is file:/C:/Users/User1/AppData/Local/OMERO.insight/app/lib/turbojpeg-6.10.0.jar!/META-INF/lib/windows_64/turbojpeg.dll \n2023-03-03 08:36:57,462 DEBUG [  org.scijava.nativelib.BaseJniExtractor] (entQueue-0) Extracting 'jar:file:/C:/Users/User1/AppData/Local/OMERO.insight/app/lib/turbojpeg-6.10.0.jar!/META-INF/lib/windows_64/turbojpeg.dll' to 'C:\\Users\\User1\\AppData\\Local\\Temp\\turbojpeg1228020848689184749.dll' \n2023-03-03 08:36:59,255 INFO  [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) MetadataDialog: Use mdeConfiguration file at C:\\Users\\User1\\omero\\mdeConfiguration.xml \n2023-03-03 08:36:59,255 WARN  [          o.o.s.a.f.m.c.MDEConfiguration] (entQueue-0) [MDE] can't parse object definitions from conf file! \n2023-03-03 08:36:59,255 DEBUG [ o.o.s.a.f.m.components.ModuleController] (entQueue-0) [MDE] init DEFAULT content [ModuleController] \n2023-03-03 08:36:59,286 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] load setup: Universal \n2023-03-03 08:37:23,553 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view \n2023-03-03 08:37:24,907 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] PRESS import: save changes \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: null \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: \\\\server\\OM\\MIHOVIL_FILL\\OMERO\\Virtual Slide Images\\Image.vsi \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) \t mapAnnotation is null \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: \\\\server\\OM\\MIHOVIL_FILL\\OMERO\\Virtual Slide Images\\Image_01.vsi \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) \t mapAnnotation is null \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: \\\\server\\OM\\MIHOVIL_FILL\\OMERO\\Virtual Slide Images\\Image_02.vsi \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) \t mapAnnotation is null \n2023-03-03 08:37:24,921 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] clean up \n2023-03-03 08:37:24,922 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] load setup: Universal \n2023-03-03 08:37:24,984 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view \n2023-03-03 08:37:25,000 INFO  [       ome.formats.importer.ImportConfig] ( Thread-10) OMERO.blitz Version: 5.5.12 \n2023-03-03 08:37:25,000 INFO  [       ome.formats.importer.ImportConfig] ( Thread-10) Bioformats version: 6.10.0 revision: f8b46c2458c43cffdf5bc67cc4bf9dfc6e93167b date: 31 May 2022 \n2023-03-03 08:37:25,031 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-10) Depth: 4 Metadata Level: MINIMUM \n2023-03-03 08:37:25,127 INFO  [                loci.formats.ImageReader] ( Thread-10) CellSensReader initializing \\\\server\\OM\\MIHOVIL_FILL\\OMERO\\Virtual Slide Images\\Image.vsi \n2023-03-03 08:37:25,627 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-10) 1 file(s) parsed into 0 group(s) with 1 call(s) to setId in 594ms. (596ms total) [0 unknowns] \n2023-03-03 08:37:25,627 INFO  [       ome.formats.importer.ImportConfig] ( Thread-10) OMERO.blitz Version: 5.5.12 \n2023-03-03 08:37:25,627 INFO  [       ome.formats.importer.ImportConfig] ( Thread-10) Bioformats version: 6.10.0 revision: f8b46c2458c43cffdf5bc67cc4bf9dfc6e93167b date: 31 May 2022 \n2023-03-03 08:37:25,627 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-10) Depth: 4 Metadata Level: MINIMUM \n2023-03-03 08:37:25,705 INFO  [                loci.formats.ImageReader] ( Thread-10) CellSensReader initializing \\\\server\\OM\\MIHOVIL_FILL\\OMERO\\Virtual Slide Images\\Image_01.vsi \n2023-03-03 08:37:25,908 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-10) 1 file(s) parsed into 0 group(s) with 1 call(s) to setId in 281ms. (281ms total) [0 unknowns] \n2023-03-03 08:37:25,908 INFO  [       ome.formats.importer.ImportConfig] ( Thread-10) OMERO.blitz Version: 5.5.12 \n2023-03-03 08:37:25,908 INFO  [       ome.formats.importer.ImportConfig] ( Thread-10) Bioformats version: 6.10.0 revision: f8b46c2458c43cffdf5bc67cc4bf9dfc6e93167b date: 31 May 2022 \n2023-03-03 08:37:25,908 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-10) Depth: 4 Metadata Level: MINIMUM \n2023-03-03 08:37:25,939 INFO  [                loci.formats.ImageReader] ( Thread-10) CellSensReader initializing \\\\server\\OM\\MIHOVIL_FILL\\OMERO\\Virtual Slide Images\\Image_02.vsi \n2023-03-03 08:37:26,129 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-10) 1 file(s) parsed into 0 group(s) with 1 call(s) to setId in 221ms. (221ms total) [0 unknowns] \n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/mihovil_p\">@Mihovil_P</a>, would you be able to provide a link to a sample file that reproduces the issue. If you need a suitable upload location then we recommend using <a href=\"https://zenodo.org/\">https://zenodo.org/</a></p>", "<p>Hi David,<br>\nhere are 2 scanned files.<br>\n<a href=\"https://1drv.ms/u/s!AmbIFTYxhOuig_1D0gxy30_UM-O0nA?e=UaphWP\" rel=\"noopener nofollow ugc\">Olympus VSI</a></p>", "<p>additional info.<br>\nthis is log when old version of format is used</p>\n<pre><code class=\"lang-auto\">2023-03-07 13:56:01,460 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view \n2023-03-07 13:56:01,460 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] TreeselectEvent: valueChanged \n2023-03-07 13:56:01,461 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] Deselect node: null \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] PRESS import: save changes \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: null \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: C:\\Users\\User1\\Downloads\\OS-1\\OS-1.vsi \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) \t mapAnnotation is null \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] clean up \n2023-03-07 13:56:02,629 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] load setup: Universal \n2023-03-07 13:56:02,641 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) OMERO.blitz Version: 5.6.0 \n2023-03-07 13:56:02,641 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 \n2023-03-07 13:56:02,641 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view \n2023-03-07 13:56:02,643 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) Depth: 4 Metadata Level: MINIMUM \n2023-03-07 13:56:02,682 INFO  [                loci.formats.ImageReader] ( Thread-18) CellSensReader initializing C:\\Users\\User1\\Downloads\\OS-1\\OS-1.vsi \n2023-03-07 13:56:02,887 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) 1 file(s) parsed into 1 group(s) with 1 call(s) to setId in 240ms. (244ms total) [0 unknowns] \n2023-03-07 13:56:03,634 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) OMERO.blitz Version: 5.6.0 \n2023-03-07 13:56:03,634 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 \n2023-03-07 13:56:03,636 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) Depth: 4 Metadata Level: MINIMUM \n2023-03-07 13:56:03,648 INFO  [                loci.formats.ImageReader] ( Thread-18) CellSensReader initializing C:\\Users\\User1\\Downloads\\OS-1\\OS-1.vsi \n2023-03-07 13:56:03,675 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-18) 1 file(s) parsed into 1 group(s) with 1 call(s) to setId in 38ms. (40ms total) [0 unknowns] \n2023-03-07 13:56:03,676 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) OMERO.blitz Version: 5.6.0 \n2023-03-07 13:56:03,676 INFO  [       ome.formats.importer.ImportConfig] ( Thread-18) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 \n2023-03-07 13:56:04,055 INFO  [    ome.formats.OMEROMetadataStoreClient] ( Thread-18) Pinging session every 300s. \n2023-03-07 13:56:04,173 DEBUG [ ome.services.blitz.util.CurrentPlatform] ( Thread-18) recognized current operating system as being Microsoft Windows \n2023-03-07 13:56:04,932 INFO  [      o.f.i.transfers.UploadFileTransfer] ( Thread-18) Transferring C:\\Users\\User1\\Downloads\\OS-1\\OS-1.vsi... \n2023-03-07 13:56:05,153 INFO  [      o.f.i.transfers.UploadFileTransfer] ( Thread-18) Transferring C:\\Users\\User1\\Downloads\\OS-1\\_OS-1_\\stack1\\frame_t.ets... \n2023-03-07 13:56:05,703 INFO  [      o.f.i.transfers.UploadFileTransfer] ( Thread-18) Transferring C:\\Users\\User1\\Downloads\\OS-1\\_OS-1_\\stack10001\\frame_t.ets... \n</code></pre>\n<p>this is new file format</p>\n<pre><code class=\"lang-auto\">2023-03-07 13:56:34,189 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view \n2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] PRESS import: save changes \n2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: null \n2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- save map annotation: \\\\server\\OM\\MIHOVIL_FILL\\test\\20230307_Image_smaller\\20230307_Image_smaller.vsi \n2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) -- LEAF NODE MAP \n2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) \t mapAnnotation is null \n2023-03-07 13:56:35,235 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] clean up \n2023-03-07 13:56:35,236 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] load setup: Universal \n2023-03-07 13:56:35,259 INFO  [       ome.formats.importer.ImportConfig] ( Thread-23) OMERO.blitz Version: 5.6.0 \n2023-03-07 13:56:35,259 INFO  [       ome.formats.importer.ImportConfig] ( Thread-23) Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022 \n2023-03-07 13:56:35,259 DEBUG [   o.o.s.a.fsimporter.mde.MetaDataDialog] (entQueue-0) [MDE] refresh file view \n2023-03-07 13:56:35,260 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-23) Depth: 4 Metadata Level: MINIMUM \n2023-03-07 13:56:35,356 INFO  [                loci.formats.ImageReader] ( Thread-23) CellSensReader initializing \\\\server\\OM\\MIHOVIL_FILL\\test\\20230307_Image_smaller\\20230307_Image_smaller.vsi \n2023-03-07 13:56:35,534 INFO  [   ome.formats.importer.ImportCandidates] ( Thread-23) 1 file(s) parsed into 0 group(s) with 1 call(s) to setId in 273ms. (274ms total) [0 unknowns] \n</code></pre>", "<p>Thank you for providing the sample files. I was able to reproduce the issue with Bio-Formats 6.11.1 and I can confirm that a fix was put in place for Bio-Formats 6.12.0. With the most recent Bio-Formats the files can be opened and displayed as expected. The latest OMERO release is still using Bio-Formats 6.11.1 but this fix should be included in the next upcoming OMERO release.</p>", "<p>thank you! that is what i thought was happening. what version of Omero and Omero.insight will have Bio-Formats 6.12.0?</p>", "<p>I don\u2019t believe there is a release dat as of yet but it will be included in the next release of each component (likely to be OMERO 5.6.7 and Insight 5.7.3).</p>", "<p>what would be your recommendation to do until then? convert to ome.tiff or trying something with zarr? tiffs and bigtiffs are slower? with zarr, i am not sure how would i even proceed.</p>", "<p>Yeah, converting to OME-TIFF would likely be the easiest solution to get the files imported immediately. You would need to use the bfconvert tool from the latest 6.12.0 command line tools from <a href=\"https://www.openmicroscopy.org/bio-formats/downloads/\" class=\"inline-onebox\">Bio-Formats Downloads | Open Microscopy Environment (OME)</a></p>\n<p>The list of options available for bfconvert can be found at <a href=\"https://bio-formats.readthedocs.io/en/v6.12.0/users/comlinetools/conversion.html\" class=\"inline-onebox\">Converting a file to different format \u2014 Bio-Formats 6.12.0 documentation</a></p>"], "78040": ["<p>Dear <a class=\"hashtag\" href=\"/tag/mobie\">#<span>mobie</span></a> python community,</p>\n<p>Following up on my minimal examples of project generation I found a problem when using <code>mobie.add_image</code> with <code>file_format=\"ome.zarr\"</code>. The error seems to be related to the <code>downscale</code> function in <code>import_image_data</code>.</p>\n<blockquote>\n<p>RuntimeError: Downscaling failed</p>\n</blockquote>\n<p>You can find a minimal example, including the data used, in this repo: <a href=\"https://github.com/CamachoDejay/mobie-python-examples/blob/0bf649de7ca5794250897796bc99cb1b99000b83/mobie-project-generation_zarr.ipynb\" rel=\"noopener nofollow ugc\">mobie-project-generation_zarr.ipynb</a></p>\n<p>In the same repo you find an almost identical notebook using the option  <code>file_format=\"bdv.n5\"</code>, which runs without problems: <a href=\"https://github.com/CamachoDejay/mobie-python-examples/blob/0bf649de7ca5794250897796bc99cb1b99000b83/mobie-project-generation.ipynb\" rel=\"noopener nofollow ugc\">mobie-project-generation.ipynb with default bdv.n5</a></p>\n<p><a class=\"mention\" href=\"/u/constantinpape\">@constantinpape</a> or other mobie fans do you have any idea of what I am doing wrong?</p>\n<p>Thanks a lot in advance and have a great weekend,<br>\nRafa</p>", "<p>Hi Rafa,<br>\nthere were a few issues (both in your example code and my upstream dependencies).</p>\n<ul>\n<li>if you want true 2d data the <code>chunks</code>, <code>scale_factors</code> and <code>resolution</code> all need to be 2d instead of 3d. See the changes I have made here: <a href=\"https://github.com/constantinpape/mobie-python-examples/blob/9a8647de8dc6f13e579dc165faf1b4ed93cfdabf/mobie-project-generation_zarr.ipynb\" class=\"inline-onebox\">mobie-python-examples/mobie-project-generation_zarr.ipynb at 9a8647de8dc6f13e579dc165faf1b4ed93cfdabf \u00b7 constantinpape/mobie-python-examples \u00b7 GitHub</a>\n</li>\n<li>However, there was an error that prevented the 2d case to work. I have fixed it, but need to make a new release of that dependency. I will ping you as soon as the release is on conda-forge, and how you can upgrade to it.</li>\n<li>When you rerun it, make sure that you remove both the tmp folder and the mobie project folder, otherwise that can lead to errors due to incompatible data.</li>\n</ul>", "<p>Thanks a lot.</p>\n<p>I have taken note of the suggestions. Now it raises a:</p>\n<blockquote>\n<p><strong>ValueError</strong> : Expect same length of resolution as ndim, got: resolution=(454, 454), ndim=3</p>\n</blockquote>\n<p>Let me know when I can test the new release.</p>\n<p>Best regards,<br>\nRafa</p>", "<p>The releases are available now, you need to update</p>\n<ul>\n<li>elf to version 0.4.6</li>\n<li>mobie-python 0.4.3</li>\n</ul>\n<p>e.g. by running <code>conda install -c conda-forge \"python-elf&gt;=0.4.6 mobie_utils&gt;=0.4.3\"</code> in your conda env.</p>\n<p>This should fix all issues; let me know if not.</p>"], "78044": ["<p>Hey there,</p>\n<p>how can I read (in pycromanager) the directory from the loaded configuration file in micromanager? That seems pretty simple, but I cannot figure out how to do it.</p>\n<p>Best wishes<br>\nFred</p>", "<p>Im not sure that there is an API call for this, but you can load a config file yourself using</p>\n<p><code>core.load_system_configuration(\"/path/to/file\")</code>. Then you would know the directory</p>", "<p>Thanks, that will do the job. But then the user has to load a config twice; first when starting uManager (empty config) and then via pycromanager (config containing devices) or is there a way to only load a config file once?</p>", "<p>Maybe if you check the tools-options \u201cask for config at startup\u201d you can avoid the first one, and if the last config loaded was empty than it will default to doing nothing. Not sure if it will then remember the config loaded through PM later and default to that, but worth a try</p>", "<p>Thanks Henry,<br>\nit did not work, but it is not a big issue to load the config twice. We will continue with loading it twice</p>"], "78060": ["<p>I am building a piece of software and trying to combine my analysis scripts between Spyder 5.4.1 (Python 3.8) and and ImageJ macros. My datasets are a series of .tif stack images, so I have decided to use PyImageJ to access the ImageJ2 gateway interface to load and run my ImageJ macros through Spyder to have the analysis performed in one automated script.</p>\n<p>I utilised the installation method via conda/mamba described here <a href=\"https://pyimagej.readthedocs.io/en/latest/Install.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Installation \u2014 PyImageJ documentation</a> with the Powershell prompt from Anaconda Navigator 2.3.2</p>\n<p>When calling conda list for the pyimagej environment created, it shows the necessary packages were installed correctly. I change the environment to the one generated from the conda/mamba installation, called \u2018pyimagej\u2019 and open Spyder from the Anaconda Navigator.</p>\n<p>I can import and connect to the ImageJ2 gateway without issue.</p>\n<pre><code class=\"lang-auto\">  \"Connect to the ImageJ2 API\"\n    import imagej\n    ij = imagej.init()\n    dataset = ij.io().open('/Desktop/Test/Test-1.tif')\n</code></pre>\n<p>However, when trying to import an image I get the following error.</p>\n<pre><code class=\"lang-auto\">Exception in comms call get_namespace_view:\n      File \"\\Anaconda3\\envs\\pyimagej\\lib\\site-packages\\spyder_kernels\\comms\\commbase.py\", line 317, in _comm_message\n        buffer = cloudpickle.loads(msg['buffers'][0],\n    jpype._core.JVMNotRunning: Java Virtual Machine is not running\n    \n    Exception in comms call get_var_properties:\n      File \"\\Anaconda3\\envs\\pyimagej\\lib\\site-packages\\spyder_kernels\\comms\\commbase.py\", line 317, in _comm_message\n        buffer = cloudpickle.loads(msg['buffers'][0],\n    jpype._core.JVMNotRunning: Java Virtual Machine is not running\n</code></pre>\n<p>Trying to start a new JVM after initialising an ImageJ2 gateway <code>ij = imagej.init()</code> with the following code tells me a JVM is already running.</p>\n<pre><code class=\"lang-auto\">\"Start JVM\"\nimport jpype\nimport jpype.imports\njpype.startJVM()\nTraceback (most recent call last):\n\n  Cell In[7], line 4\n    jpype.startJVM()\n\n  File ~\\Anaconda3\\envs\\pyimagej\\lib\\site-packages\\jpype\\_core.py:166 in startJVM\n    raise OSError('JVM is already started')\n\nOSError: JVM is already started\n</code></pre>\n<p>Why am I getting an error that states the JVM is not running when it has been started with <code>imagej.init()</code>? I cannot do anything with PyImageJ because of this error.</p>", "<p>This is now a reported and known bug in Spyder &gt;5.4.1 and has to do with the updating of the variable explorer (see <a href=\"https://github.com/spyder-ide/spyder/issues/20635\" rel=\"noopener nofollow ugc\">Issue 20635</a>). A fix has been proposed with a goal for the release of Spyder 6.</p>\n<p>The current workaround is to define the</p>\n<blockquote>\n<p>data</p>\n</blockquote>\n<p>variable as a private variable by changing the code to it</p>\n<blockquote>\n<p>_data = <a href=\"http://ij.io\" rel=\"noopener nofollow ugc\">ij.io</a>().open(/\u2018Test.tif\u2019)</p>\n</blockquote>\n<p>After testing this workaround, it allows PyImageJ to work with images just fine in Spyder.</p>"], "71923": ["<p>Hi everyone,</p>\n<p>I am working on identifying cells with a secondary stain using my DAPI stain as the nuclear marker. The secondary stain incorporates cell processes, so it should be a diffuse stain within the cytoplasm. When I use IdentifySecondaryObject (with DAPI as my primary object), some of the secondary objects overlap entirely and exactly with the DAPI primary objects. I\u2019ve determined this is auto-fluorescence and not true staining, since the cytoplasm is not included. Is there a way to eliminate any secondary object that overlaps exactly with the primary object? I\u2019d rather not mess with the brightness/contrast and thresholding to try and eliminate the remaining auto-fluorescence at this point. Perhaps a filter option that I cannot find?</p>\n<p>Here is an example: the green is the DAPI nuclei and the magenta is the secondary staining in the cytoplasm.</p>\n<p>Thanks everyone!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95.png\" data-download-href=\"/uploads/short-url/5fIIELrj26PrFlANWWq7dB9uAGp.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95_2_690x417.png\" alt=\"image\" data-base62-sha1=\"5fIIELrj26PrFlANWWq7dB9uAGp\" width=\"690\" height=\"417\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95_2_690x417.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">835\u00d7505 86.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/svkremer\">@svkremer</a></p>\n<p>Welcome to the forum!</p>\n<p>Great question, and yes, after you Identify the primary and secondary object you can measure the overlap between them using the <strong>MeasureObjectOverlap</strong> module and than you can filter out the exact overlap objects (they should have value of 1) using the <strong>FilterObjects</strong> module.</p>\n<p>Best,<br>\nMario</p>", "<p>Hi <a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a> ,</p>\n<p>Thank you for your help, I think I am on the right track now!</p>\n<p>When I use the MeasureObjectOverlap module, the false positives result window is the staining that is true. I would like to just be able to take the false positive output from the MeasureObjectOverlap module and use that for my measurements. Is this possible? I do not see a way to name the false positive window and use it in future steps of the pipeline.</p>\n<p>I believe that is what the FilterObjects module is supposed to do, but I am having trouble using that module. There does not seem to be a direct link between the two modules, i.e. a way to take the false positive window and use it in the FilterObjects module. I believe I am missing a crucial piece of the FilterObjects module, so any advice would be appreciated!</p>", "<p>Hi <a class=\"mention\" href=\"/u/svkremer\">@svkremer</a>,</p>\n<p>I\u2019m looking here and I\u2019m not able to use the <strong>MeasureObjectOverlap</strong> modules output as a filter, sorry for that.</p>\n<p>We can use another strategy to remove your objects (without <strong>MeasureObjectOverlap</strong>). After the Identify Primary and Secondary objects we can add <strong>MeasureObjectSizeAndShape</strong> module, within this module we can measure the area of your nuclei and cell. After that we can add the <strong>CalculateMath</strong> module, here you can divide the Area of your cell per nuclei area (RatioCellPerNuclei).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18d62dc686b45407285436e316972922ce7f45dc.png\" data-download-href=\"/uploads/short-url/3xIkMNEQFYUnMcKK0p2CokcEz1i.png?dl=1\" title=\"Screen Shot 2022-09-26 at 8.59.20 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18d62dc686b45407285436e316972922ce7f45dc.png\" alt=\"Screen Shot 2022-09-26 at 8.59.20 AM\" data-base62-sha1=\"3xIkMNEQFYUnMcKK0p2CokcEz1i\" width=\"628\" height=\"500\" data-dominant-color=\"E6E7E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-09-26 at 8.59.20 AM</span><span class=\"informations\">659\u00d7524 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Finally you can use the <strong>FilterObject</strong> module to filter out the cells without cytoplasm.<br>\nFor that select the filtering method as Limits<br>\nCategory: Math<br>\nMeasurement: RatioCellPerNuclei<br>\nFilter using a minimum measurement value: Here you can choose the value you want to filter the objects. (values above 1 means the cytoplasm area are bigger than the nuclei area, but you can be more strict (eg.1.1) to filter objects with really small cytoplasm areas).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6.png\" data-download-href=\"/uploads/short-url/yE4yo4cNMaobxcKerL68gCGmLsy.png?dl=1\" title=\"Screen Shot 2022-09-26 at 9.02.16 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6_2_589x500.png\" alt=\"Screen Shot 2022-09-26 at 9.02.16 AM\" data-base62-sha1=\"yE4yo4cNMaobxcKerL68gCGmLsy\" width=\"589\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6_2_589x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6.png 2x\" data-dominant-color=\"E6E7E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-09-26 at 9.02.16 AM</span><span class=\"informations\">675\u00d7573 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario,</p>\n<p>Thank you so much! I had to step away from this project for a while, but your solution works perfectly! Really appreciate your help and expertise (:</p>\n<p>Best,<br>\nSarah</p>"], "78080": ["<p>I want to classify and determine the number of cells positive for individual signals and those positive for two or three signals. I am using ClassifyObjects module in the Cellprofiler software but can not seem to get the right data. ### Sample image and/or code</p>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-2\" class=\"anchor\" href=\"#analysis-goals-2\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding?</li>\n<li>What have you tried already?</li>\n<li>Have you found any related forum topics? If so, cross-link them.</li>\n<li>What software packages and/or plugins have you tried?</li>\n</ul>", "<p>Hi Albert, could you provide your pipeline and a sample image/image set?</p>", "<p>Sure. Sorry for the late reply. First of all, the aim of the work is to classify cells into cfos positive cells, Crh positive cells and ESR1 positive cells found in the Pontine micturition center of the brain. I then want to how many cfos positive cells also expresses Crh, Esr1 or both.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/dYZ1yuwdwTcEmzZuOYG7dutdKid.cpproj\">Nocturia Brain.cpproj</a> (1.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/dpXDG6YciQE11NhI4slxGIM435i.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c1.tif</a> (6.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/eSJSu0l3BtNrXWGod6kQser3moh.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c2.tif</a> (6.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/sfxj4nDJE5lpnTnIjnY8UxZSLF5.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c3.tif</a> (6.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/1wwMMh3EWOBSJTzH5hXeZtreNQb.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c4.tif</a> (6.5 MB)</p>\n<p>Also on a related but different inquiries, any suggestion on how to compress tiff files.</p>"], "78094": ["<p>Hello everyone!<br>\nI am new to Ilastik and I have some doubts about object classification. If someone can help me I would appreciate it.<br>\nI am trying to export the object predictions as png, but it seems that it exports both the signal and the background as the same label. I upload some screenshots to explain this better.</p>\n<p>In the object classification I see that it distinguishes correctly between the background (white) and my signal (red).  However, when I export the object prediction and open it with Fiji, it seems that what in ilastik is recognized as background, appears in my object prediction. I must be missing some step, but I have no idea.</p>\n<ol>\n<li>\n<p>Background in white<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/db15ac8013fe29c40318ef5b4c1591eb2331696c.jpeg\" data-download-href=\"/uploads/short-url/vg707F6wx2Pg2E6fKoZLrALVbys.jpeg?dl=1\" title=\"Captura de pantalla 2023-03-05 a les 16.25.58\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db15ac8013fe29c40318ef5b4c1591eb2331696c_2_690x431.jpeg\" alt=\"Captura de pantalla 2023-03-05 a les 16.25.58\" data-base62-sha1=\"vg707F6wx2Pg2E6fKoZLrALVbys\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db15ac8013fe29c40318ef5b4c1591eb2331696c_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db15ac8013fe29c40318ef5b4c1591eb2331696c_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db15ac8013fe29c40318ef5b4c1591eb2331696c_2_1380x862.jpeg 2x\" data-dominant-color=\"262630\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Captura de pantalla 2023-03-05 a les 16.25.58</span><span class=\"informations\">1920\u00d71200 148 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>Red label: correct<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/020ece0cdc0239ef659e73aa16a6f0655674cd68.jpeg\" data-download-href=\"/uploads/short-url/icFNQyr1ZqCPPDIRSRCn1pJYnu.jpeg?dl=1\" title=\"Captura de pantalla 2023-03-05 a les 16.26.09\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/020ece0cdc0239ef659e73aa16a6f0655674cd68_2_690x431.jpeg\" alt=\"Captura de pantalla 2023-03-05 a les 16.26.09\" data-base62-sha1=\"icFNQyr1ZqCPPDIRSRCn1pJYnu\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/020ece0cdc0239ef659e73aa16a6f0655674cd68_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/020ece0cdc0239ef659e73aa16a6f0655674cd68_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/020ece0cdc0239ef659e73aa16a6f0655674cd68_2_1380x862.jpeg 2x\" data-dominant-color=\"312739\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Captura de pantalla 2023-03-05 a les 16.26.09</span><span class=\"informations\">1920\u00d71200 167 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>What I see in Fiji:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8de255e80c5186df7f92264f5c48849b38023802.jpeg\" data-download-href=\"/uploads/short-url/kfafc0E266QrSOZ69ZlHJVFBDi2.jpeg?dl=1\" title=\"Captura de pantalla 2023-03-05 a les 16.40.32\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8de255e80c5186df7f92264f5c48849b38023802_2_690x431.jpeg\" alt=\"Captura de pantalla 2023-03-05 a les 16.40.32\" data-base62-sha1=\"kfafc0E266QrSOZ69ZlHJVFBDi2\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8de255e80c5186df7f92264f5c48849b38023802_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8de255e80c5186df7f92264f5c48849b38023802_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8de255e80c5186df7f92264f5c48849b38023802_2_1380x862.jpeg 2x\" data-dominant-color=\"F7F4F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Captura de pantalla 2023-03-05 a les 16.40.32</span><span class=\"informations\">1920\u00d71200 50 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n</ol>\n<p>If someone can help me I would appreciate it, because I\u2019ve been trying to solve it for a while but I don\u2019t understand it well.</p>", "<p>Dear <a class=\"mention\" href=\"/u/anna_teruel\">@Anna_Teruel</a>,</p>\n<p>The Object Predictions image is a integer value image (so it has values like <code>1, 2, 3, 4...</code> depending on how many classes you have. With three classes/labels, you\u2019d get an image where all pixels of objects for the first label/class will have a value of 1, objects that get the second class/label will have a value of 2\u2026 and so on. <code>0</code> is  a special label that is assigned in Object classification to the background of the image (-&gt; not any object). When exporting to png there are some pitfals. One would be related to normalization - did you change any settings there? Could you check?</p>\n<p>Another common one would be related to viewing the result - from <a href=\"https://www.ilastik.org/documentation/basics/common_problems#4-my-exported-results-are-all-black\">No. 4 in our FAQ</a>: In any case you could try to change the lookup table when looking at the results, this makes small differences (as, e.g. between pixels that have 1s and 2s) apparent - we recommend glasbey (<em>Image</em> \u2192 <em>Lookup Tables</em> \u2192 <em>glasbey</em>), or a similar non-sequential lookup table.</p>\n<p>Hope we can resolve this quickly</p>\n<p>Cheers<br>\nDominik</p>", "<p>Hi <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>, sorry for my late response.<br>\nAs you say, it was a matter of integers and channels I was considering. So all information clear and now it is working.</p>\n<p>Sorry for my stupid doubt, and thanks for your help!<br>\nAnna</p>"], "78095": ["<p>Hi, Everyone.<br>\nI have images that have 4 channels(DPAI+3 markers). Cells were classified to  3 classes by two markers. I want to get the MFI values of the last marker of those 3 classes. Is there a way to get the MFI values by an script?</p>\n<p>Thank you.</p>", "<p>That should already be in the measurement list for each cell, if the cells were classified by those values.<br>\nIt\u2019s not clear to me what you mean by</p>\n<aside class=\"quote no-group\" data-username=\"StevenZHUANG\" data-post=\"1\" data-topic=\"78095\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/96bed5/40.png\" class=\"avatar\"> StevenZHUANG:</div>\n<blockquote>\n<p>the last marker of those 3 classes</p>\n</blockquote>\n</aside>", "<p>Thank you for your reply. Sorry, I didn\u2019t make it clear. I draw two annotations manually. Cells were classified by channel 2 and channel 3. I want to get  the average mean intensity of channel 4 of cells belongs to different classes in different annotations, not the mean intensity of each cell. In my case, I want those values in the following table<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/e/4ed25236bdd64cb30bc2be56cc1c34934fedc07c.png\" data-download-href=\"/uploads/short-url/bfhQVwSYbghfxQhbReD3PdBaNgU.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/e/4ed25236bdd64cb30bc2be56cc1c34934fedc07c.png\" alt=\"image\" data-base62-sha1=\"bfhQVwSYbghfxQhbReD3PdBaNgU\" width=\"690\" height=\"148\" data-dominant-color=\"EEEFF0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">995\u00d7214 4.82 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nCan I get them by an script?</p>\n<p>thank you.</p>", "<p>Ah, you can absolutely calculate that by script, but it is generally the sort of thing that is recommended that you do in other software using the exported spreadsheets.</p><aside class=\"quote quote-modified\" data-post=\"4\" data-topic=\"43682\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/oburri/40/3464_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/calculate-the-average-value-of-each-cell-characteristic-of-the-positive-cell/43682/4\">Calculate the average value of each cell characteristic of the positive cell</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hi all, \nJust a short point here. We often recommend the users of our bioimaging and optics core facility to split the data gathering part from the data analysis part. \nQuPath is meant to be a visualisation and quantification tool (and much more), not a statistics or data analysis tool. My suggestion is to export all results that QuPath produces and use something like Pivot tables in Excel or your favorite software to group your data and get the summary stats you want\u2026 \nUsually makes things easi\u2026\n  </blockquote>\n</aside>\n<p>\nSome of your answer is here, though. You would need to change the measurement you were interested in summarizing, and also loop over annotations.<br>\nExample of looping through annotations <a href=\"https://forum.image.sc/t/batch-resize-annotations-qupath/77460/23\" class=\"inline-onebox\">Batch Resize Annotations QuPath - #23 by petebankhead</a></p>\n<p>If you do decide to do it in QuPath, the loop would look something like</p>\n<pre><code class=\"lang-auto\">for anno in annotations\n   for class in aListofClasses\n        get all the cells of that class, calculate the mean\n        probably save that mean measurement to the annotation using putMeasurement, which you can search for examples of\n</code></pre>", "<p>Thank you very much for your help. I really appreciate it.</p>", "<p>As just another mention regarding this. Here is an example without scripting and using Pivot Tables from Google Sheets. You can build the results you want simply by exporting the cell  measurements from QuPath and ordering the different columns however you want. You can choose any statistic very easily and dynamically, without the need for a script/</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c.jpeg\" data-download-href=\"/uploads/short-url/h3oru19TxV06RzsU8FLQfSuQeUA.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_690x311.jpeg\" alt=\"image\" data-base62-sha1=\"h3oru19TxV06RzsU8FLQfSuQeUA\" width=\"690\" height=\"311\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_690x311.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_1035x466.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/77872e60cf8fc7aa54efaf3772cfa42a1c26342c_2_1380x622.jpeg 2x\" data-dominant-color=\"BEBAC1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7867 268 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The example Sheet Pivot Table is here</p><aside class=\"onebox googledocs\" data-onebox-src=\"https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing\">\n  <header class=\"source\">\n\n      <a href=\"https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing\" target=\"_blank\" rel=\"noopener\">docs.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <a href=\"https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing\" target=\"_blank\" rel=\"noopener\"><span class=\"googledocs-onebox-logo g-sheets-logo\"></span></a>\n\n<h3><a href=\"https://docs.google.com/spreadsheets/d/1C1Fn_Jfk9Ju6RGv6iNY8b9Z_KzCn1aFgdhKiAfGUTvE/edit?usp=sharing\" target=\"_blank\" rel=\"noopener\">QuPath Pivot Table Example</a></h3>\n\n<p>measurements\n\nImage,Object ID,Name,Class,Parent,ROI,Centroid X \u788cm,Centroid Y \u788cm,Nucleus: Area \u788cm^2,Nucleus: Length \u788cm,Nucleus: Circularity,Nucleus: Solidity,Nucleus: Max diameter \u788cm,Nucleus: Min diameter \u788cm,Cell: Area \u788cm^2,Cell: Length \u788cm,Cell:...</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Disclaimer: I generally use Excel which has the same functionality but is much much faster. Google Sheets was used as an example. It\u2019s not really built to handle all the results QuPath can produce.</p>"], "61716": ["<p>Dear all,</p>\n<p>As a Food Technology student this is a very unfamiliar area for me.<br>\nI am currently writing my master thesis: Probing cheese microstructures under deformation. The aim is to identify a parameter that could be related to the fracture behaviour of cheese through microstructural observation during stepwise compressive deformation to gain insight on the deformation behaviour on cheese.</p>\n<p>I have no experience on coding nor on image analysis. However, I came across QuPath and tried it out. Somehow, with the cell detection, the program thinks the cheese fat droplets are \u2018nuclei\u2019 and determine the area and perimeter of these droplets. This helps me enormously.</p>\n<p>However, I also need to know the area of all the cavities present in the image. These cavities are black and unfortunately, I cannot use the cell detection for these black cavities as the options available for the channel are Red, Green and Blue.</p>\n<p>Is there a possibility for QuPath to determine the size of black cavities in some way?</p>\n<p>I would love to hear any suggestions.</p>\n<p>Kind regards,<br>\nJolien</p>", "<p>Welcome, and congrats on such an attention-grabbing title for your first post <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=10\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"></p>\n<p>QuPath\u2019s provides various ways to detect things that aren\u2019t nuclei. For example, the pixel classifier could be useful: <a href=\"https://qupath.readthedocs.io/en/stable/docs/tutorials/pixel_classification.html\" class=\"inline-onebox\">Pixel classification \u2014 QuPath 0.3.0 documentation</a></p>\n<p>If you can post some example images the others might have more detailed suggestions.</p>", "<p>Many thanks!</p>\n<p>I can show the following:</p>\n<p>This is the original image:</p>\n<p>I select the whole image, and with cell detection (detection channel on green), I get this<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c8ced39b00baeb9d33b63db7937dde04f48e0a43.jpeg\" data-download-href=\"/uploads/short-url/sEqCjBdC06zHEJJRsGfJ3yIXG2T.jpeg?dl=1\" title=\"Screenshot 2022-01-08 at 12.13.56\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8ced39b00baeb9d33b63db7937dde04f48e0a43_2_512x500.jpeg\" alt=\"Screenshot 2022-01-08 at 12.13.56\" data-base62-sha1=\"sEqCjBdC06zHEJJRsGfJ3yIXG2T\" width=\"512\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8ced39b00baeb9d33b63db7937dde04f48e0a43_2_512x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8ced39b00baeb9d33b63db7937dde04f48e0a43_2_768x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8ced39b00baeb9d33b63db7937dde04f48e0a43_2_1024x1000.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8ced39b00baeb9d33b63db7937dde04f48e0a43_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-01-08 at 12.13.56</span><span class=\"informations\">1250\u00d71220 218 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Which allows me to see the area of all the fat droplets.</p>\n<p>Some cavities are already visible while others are not as the fat droplets are situated in the cavities. I therefore select only the red channel to obtain:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/a/5aa86109f2b34dacb74ba2fb9483e097e6e780f0.jpeg\" data-download-href=\"/uploads/short-url/cVZIOtvsdXJIyC2t6BwpHcocsj6.jpeg?dl=1\" title=\"Screenshot 2022-01-08 at 12.17.56\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/a/5aa86109f2b34dacb74ba2fb9483e097e6e780f0_2_474x500.jpeg\" alt=\"Screenshot 2022-01-08 at 12.17.56\" data-base62-sha1=\"cVZIOtvsdXJIyC2t6BwpHcocsj6\" width=\"474\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/a/5aa86109f2b34dacb74ba2fb9483e097e6e780f0_2_474x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/a/5aa86109f2b34dacb74ba2fb9483e097e6e780f0_2_711x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/a/5aa86109f2b34dacb74ba2fb9483e097e6e780f0_2_948x1000.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/a/5aa86109f2b34dacb74ba2fb9483e097e6e780f0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-01-08 at 12.17.56</span><span class=\"informations\">1200\u00d71264 175 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>This is where I am stuck. I don\u2019t know how to obtain the area of all black cavities in this image. I thought about interchanging the colours, and use the cell detection function again. However, I don\u2019t know how to do this unfortunately. I have around 1200 images, so I am looking for a quick way.</p>", "<p>As <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> suggested, I am not sure cell detection is the right option for you, the pixel classifier might be better. Hard to provide exact steps though without an original image. Larger images may need to be hosted elsewhere and a link provided.</p>", "<p>I brie-lieve I have a solution to your problem: <a href=\"https://github.com/MarkZaidi/Universal-StarDist-for-QuPath\" rel=\"noopener nofollow ugc\">Universal StarDist for QuPath</a></p>\n<p>StarDist is a machine learning algorithm gaining increasing popularity in the digital pathology field for it\u2019s ability to serve as a robust means of segmenting nuclei. There are a few pretrained models available, those that are trained to segment hematoxylin-positive nuclei from H&amp;E-stained sections, and ones that are trained to segment DAPI-positive nuclei (from fluorescence microscopy images with a DAPI channel). I wanted to find a way to adapt it for use in other modalities, such as imaging mass cytometry. Within the builder, there are several preprocessing parameters that can be used to achieve this, such as the ability to perform normalization and pixelwise arithmetic operations. By inverting the image (divide by -1) and performing linear normalization, the fat droplets now mimic in appearance the stained nuclei that StarDist was trained on:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/b/9bf98a3d0ea3b1b21cf098568269f9c7f4a9dcc6.jpeg\" data-download-href=\"/uploads/short-url/mfODYXFyc68vXwSVsHDSCMDJfFk.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9bf98a3d0ea3b1b21cf098568269f9c7f4a9dcc6_2_690x434.jpeg\" alt=\"image\" data-base62-sha1=\"mfODYXFyc68vXwSVsHDSCMDJfFk\" width=\"690\" height=\"434\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9bf98a3d0ea3b1b21cf098568269f9c7f4a9dcc6_2_690x434.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9bf98a3d0ea3b1b21cf098568269f9c7f4a9dcc6_2_1035x651.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9bf98a3d0ea3b1b21cf098568269f9c7f4a9dcc6_2_1380x868.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9bf98a3d0ea3b1b21cf098568269f9c7f4a9dcc6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1514\u00d7954 140 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nRunning Universal StarDist for QuPath out-of-the-box, we get:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/c/9cbd69c926392877e5a59160d214453d6628fb5e.jpeg\" data-download-href=\"/uploads/short-url/mmAiGp8BsMBaXZAZlbwQadcE8IS.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9cbd69c926392877e5a59160d214453d6628fb5e_2_690x434.jpeg\" alt=\"image\" data-base62-sha1=\"mmAiGp8BsMBaXZAZlbwQadcE8IS\" width=\"690\" height=\"434\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9cbd69c926392877e5a59160d214453d6628fb5e_2_690x434.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9cbd69c926392877e5a59160d214453d6628fb5e_2_1035x651.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9cbd69c926392877e5a59160d214453d6628fb5e_2_1380x868.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9cbd69c926392877e5a59160d214453d6628fb5e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1514\u00d7954 337 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nAnd with the segmented droplets overlayed onto the original image:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/00ca789d5e58edd7b72c4f6267a8c8b45924c788.jpeg\" data-download-href=\"/uploads/short-url/6ZN88RsuiURWZbTjy1M3f4TGD6.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00ca789d5e58edd7b72c4f6267a8c8b45924c788_2_690x434.jpeg\" alt=\"image\" data-base62-sha1=\"6ZN88RsuiURWZbTjy1M3f4TGD6\" width=\"690\" height=\"434\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00ca789d5e58edd7b72c4f6267a8c8b45924c788_2_690x434.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00ca789d5e58edd7b72c4f6267a8c8b45924c788_2_1035x651.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00ca789d5e58edd7b72c4f6267a8c8b45924c788_2_1380x868.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/00ca789d5e58edd7b72c4f6267a8c8b45924c788_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1514\u00d7954 349 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThere are several shape related features we can extract from this. Area and perimeter, of course, being some of the more basic, but other morphology-related features such as circularity too!:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bfb8e81ad571c5cd570095c5bc79c8b838f9375.jpeg\" data-download-href=\"/uploads/short-url/1I0d0EkLqcwLwiQAnDHWtJrpmeN.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bfb8e81ad571c5cd570095c5bc79c8b838f9375_2_690x443.jpeg\" alt=\"image\" data-base62-sha1=\"1I0d0EkLqcwLwiQAnDHWtJrpmeN\" width=\"690\" height=\"443\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bfb8e81ad571c5cd570095c5bc79c8b838f9375_2_690x443.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bfb8e81ad571c5cd570095c5bc79c8b838f9375_2_1035x664.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bfb8e81ad571c5cd570095c5bc79c8b838f9375_2_1380x886.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bfb8e81ad571c5cd570095c5bc79c8b838f9375_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1601\u00d71029 252 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIf you have multiple images at varying degrees of compressive deformation of the cheese, you can use statistical methods to evaluate whether any of these features start to change. Or, if you\u2019re looking for some more advanced things, potentially train a machine learning classifier using these features to see if you can predict the extent of compressive deformation (might want to toss in some Haralick texture features too).</p>\n<p>There\u2019s a lot of exciting and novel things that can be done here, especially since I\u2019ve never heard of anyone using a digital pathology platform to characterize cheese microstructures. But hey, an image is an image, and there\u2019s nothing stopping you from applying tools to quantitatively characterize a completely different modality of images than that of which the software was originally developed for. Definiens (an older and relatively obsolete digital pathology platform) started off as a tool to analyze geospatial imaging data, before gaining widespread popularity in the digital pathology field.</p>\n<p>Anyways, as a first pass, I\u2019d recommend playing around with the various parameters in the below script (slightly modified from the Universal StarDist for QuPath script previously linked), which I used to segment the images above, and hopefully get a better segmentation than what I did. You\u2019ll need to get the StarDist extension for QuPath, and the DSB2018_heavy_augment pretrained model as instructed in: <a href=\"https://qupath.readthedocs.io/en/stable/docs/advanced/stardist.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">StarDist \u2014 QuPath 0.3.0 documentation</a></p>\n<p>Code:</p>\n<pre><code class=\"lang-auto\">//setImageType('OTHER');\n//clearAllObjects();\n//createSelectAllObject(true);\nselectAnnotations()\n/** Scripts enabling use of pretrained stardist models for nucleus segmentation on Brightfield or IF images\n * 3 pretrained models are available at https://github.com/stardist/stardist-imagej/tree/master/src/main/resources/models/2D\n * and must be downloaded prior to running this script. Furthermore, you need to build QuPath with tensorflow (verified with CPU)\n * using the instructions here: https://qupath.readthedocs.io/en/latest/docs/advanced/stardist.html\n * he_heavy_augment is a H&amp;E-trained model, and requires a 3-channel input (like H&amp;E or HDAB). dsb2018_paper and dsb2018_heavy_augment\n * are IF trained models, and requires a 1-channel input (either an IF nuclear marker like DAPI, or a deconvolved nuclear marker\n * from brightfield images like hematoxylin). This allows pretrained IF models to be used for both IF and brightfield segmentation\n */\n//Variables to set *************************************************************\n\nparam_channel=1 //channel to use for nucleus detection. First channel in image is channel 1. If working with H&amp;E or HDAB, channel 1 is hematoxylin.\nparam_median=0 //median preprocessing: Requires an int value corresponding to the radius of the median filter kernel. For radii larger than 2, the image must be in uint8 bit depth. Default 0\nparam_divide=1 //division preprocessing: int or floating point, divides selected channel intensity by value before segmenting. Useful when normalization is disabled. Default 1\nparam_add=0 //addition preprocessing: int or floating point, add value to selected channel intensity before segmenting. Useful when normalization is disabled. Default 0\nparam_threshold = 0.5//threshold for detection. All cells segmented by StarDist will have a detection probability associated with it, where higher values indicate more certain detections. Floating point, range is 0 to 1. Default 0.5\nparam_pixelsize=0 //Pixel scale to perform segmentation at. Set to 0 for image resolution (default). Int values accepted, greater values  will be faster but may yield poorer segmentations.\nparam_tilesize=1024 //size of tile in pixels for processing. Must be a multiple of 16. Lower values may solve any memory-related errors, but can take longer to process. Default is 1024.\nparam_expansion=5 //size of cell expansion in pixels. Default is 10.\ndef min_nuc_area=15 //remove any nuclei with an area less than or equal to this value\nnuc_area_measurement='Nucleus: Area \u00b5m^2'\ndef min_nuc_intensity=0.0 //remove any detections with an intensity less than or equal to this value\nnuc_intensity_measurement='Hematoxylin: Nucleus: Mean'\nnormalize_low_pct=1 //lower limit for normalization. Set to 0 to disable\nnormalize_high_pct=99 // upper limit for normalization. Set to 100 to disable.\ndef model_trained_on_single_channel=1 //Set to 1 if the pretrained model you're using was trained on IF sections, set to 0 if trained on brightfield\n\n// Specify the model directory (you will need to change this!). Uncomment the model you wish to use\n//Brightfield models\n    //def pathModel = 'C:/Users/Mark Zaidi/Documents/QuPath/Stardist Trained Models/he_heavy_augment'\n// IF models\n    //def pathModel = 'C:/Users/Mark Zaidi/Documents/QuPath/Stardist Trained Models/dsb2018_paper'\n//NOTE: .PB MODEL MUST BE USED FOR OPENCV-CUDA\n    def pathModel = 'C:/Users/Mark Zaidi/Documents/QuPath/Stardist Trained Models/dsb2018_heavy_augment.pb'\n\n//End of variables to set ******************************************************\nparam_channel=param_channel-1 // corrects for off-by-one error\nnormalize_high_pct=normalize_high_pct-0.000000000001 //corrects for some bizarre normalization issue when attempting to set 100 as the upper limit\n\n// Import plugins \n//import qupath.tensorflow.stardist.StarDist2D\nimport qupath.ext.stardist.StarDist2D //using 0.3.0's version of StarDist. Comment this line and uncomment out the one above if on 0.2.3\nimport static qupath.lib.gui.scripting.QPEx.*\nimport groovy.time.*\n//\n//REQUIRED CONVERSION FOR GPU PROCESSING\ndef dnn = DnnTools.builder(pathModel).build();\n\n// Specify whether the above model was trained using a single-channel image (e.g. IF DAPI)\n// Get current image - assumed to have color deconvolution stains set\ndef imageData = getCurrentImageData()\ndef isBrightfield=imageData.isBrightfield()\ndef stains = imageData.getColorDeconvolutionStains() //will be null if IF\n\nif (model_trained_on_single_channel!=1 &amp;&amp; isBrightfield==false) {\n    // If brightfield model but fluorescent image\n    throw new Exception(\"Cannot use brightfield trained model to segment nuclei on IF image\")\n} else if (model_trained_on_single_channel == 1 &amp;&amp; isBrightfield==true){\n    //If fluorescent model but brightfield image (use deconvolution)\n    println 'Performing detection on Brightfield image using single-channel trained model'\n     stardist = StarDist2D.builder(dnn)\n            .preprocess(\n                    ImageOps.Channels.deconvolve(stains),\n                    ImageOps.Channels.extract(param_channel),\n                    ImageOps.Filters.median(param_median),\n                    ImageOps.Core.divide(param_divide),\n                    ImageOps.Core.add(param_add)\n            ) // Optional preprocessing (can chain multiple ops)\n\n            .threshold(param_threshold)              // Prediction threshold\n            .normalizePercentiles(normalize_low_pct,normalize_high_pct) // Percentile normalization\n            .pixelSize(param_pixelsize)              // Resolution for detection\n            //.doLog()\n            .includeProbability(true)\n            .measureIntensity()\n            .tileSize(param_tilesize)\n            .measureShape()\n            .cellExpansion(param_expansion) //Cell expansion in microns\n            .constrainToParent(false)\n\n\n             .build()\n} else if (model_trained_on_single_channel == 1 &amp;&amp; isBrightfield==false){\n    //If IF model and IF image (no deconvolution preprocessing)\n    println 'Performing detection on single channel image using single-channel trained model'\n\n    stardist = StarDist2D.builder(dnn)\n            .preprocess(\n                    ImageOps.Channels.extract(param_channel),\n                    ImageOps.Filters.median(param_median),\n                    ImageOps.Core.divide(0-param_divide),//This inverts the channel\n                    ImageOps.Core.add(param_add)\n            ) // Optional preprocessing (can chain multiple ops)\n\n            .threshold(param_threshold)              // Prediction threshold\n            .normalizePercentiles(normalize_low_pct,normalize_high_pct) // Percentile normalization. REQUIRED FOR IMC DATA\n            .pixelSize(param_pixelsize)              // Resolution for detection\n            //.doLog()\n            .includeProbability(true)\n            //.measureIntensity()\n            .tileSize(param_tilesize)\n            .measureShape()\n            //.cellExpansion(param_expansion)\n            .constrainToParent(false)\n            .build()\n} else if (model_trained_on_single_channel == 0 &amp;&amp; isBrightfield==true) {\n    // If brightfield model and brightfield image\n    println 'Performing detection on brightfield image using brightfield trained model'\n\n    stardist = StarDist2D.builder(dnn)\n            .preprocess(\n                    ImageOps.Filters.median(param_median),\n                    ImageOps.Core.divide(param_divide),\n                    ImageOps.Core.add(param_add)\n            ) // Optional preprocessing (can chain multiple ops)\n\n            .threshold(param_threshold)              // Prediction threshold\n            .normalizePercentiles(normalize_low_pct,normalize_high_pct) // Percentile normalization. REQUIRED FOR IMC DATA\n            .pixelSize(param_pixelsize)              // Resolution for detection\n            //.doLog()\n            .includeProbability(true)\n            .measureIntensity()\n            .tileSize(param_tilesize)\n            .measureShape()\n            .cellExpansion(param_expansion)\n            .constrainToParent(false)\n            .build()\n\n}\n//Run stardist in selected annotation\ndef pathObjects = getSelectedObjects()\nprint(pathObjects)\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nclearDetections()\ndef timeStart_CellDetection = new Date()\nstardist.detectObjects(imageData, pathObjects)\nTimeDuration CellDetection_duration = TimeCategory.minus(new Date(), timeStart_CellDetection)\n\n//filter out small and low intensity nuclei\n\n\ndef toDelete = getDetectionObjects().findAll {measurement(it, nuc_area_measurement) &lt;= min_nuc_area}\nremoveObjects(toDelete, true)\ndef toDelete2 = getDetectionObjects().findAll {measurement(it, nuc_intensity_measurement) &lt;= min_nuc_intensity}\nremoveObjects(toDelete2, true)\n//CLOSE DNN TO FREE UP VRAM\ndnn.getPredictionFunction().net.close()\n//Free up normal RAM. Commented this out, as v0.3 should have better memory management anways\n//Alledgely, putting this at the end of a batch script will clear memory between each processed image\n//Thread.sleep(100)\n// Try to reclaim whatever memory we can, including emptying the tile cache\n//javafx.application.Platform.runLater {\n//    getCurrentViewer().getImageRegionStore().cache.clear()\n//    System.gc()\n//}\n//Thread.sleep(100)\n\nprintln ('Done in ' + CellDetection_duration)\n\n</code></pre>\n<p>I swiss you the best!</p>", "<p>Amazing!!<br>\nI tried it out and it works exactly how I need it to.<br>\nUnfortunately, not many cheese puns come to my mind to illustrate my happiness.<br>\nMany thanks!</p>", "<p>Just remembering this, and not particularly relevant, but\u2026 cheesoSPIM <a href=\"https://github.com/PRNicovich/cheesoSPIM\" class=\"inline-onebox\">GitHub - PRNicovich/cheesoSPIM: A cheap mesoscopic light sheet and optical projection tomography microscope</a></p>", "<p>Nice, love seeing initiatives that make microscopy accessible to more people. <a class=\"mention\" href=\"/u/p_tadrous\">@P_Tadrous</a> has a <a href=\"https://www.youtube.com/channel/UCOvBahuVgEmLB5ycQEsgEnQ\">Youtube series</a> on how to make his 3D printed PUMA microscope, which supports IF, phase contrast, and I think even augmented reality to overlay grids for manual cell counting through the eyepiece. Definitely worth checking out!</p>\n<p>And on the note of the optical clearing of wood, an old acquaintance of mine made this video, using polymerized methyl methacrylate as the medium instead: <a href=\"https://www.youtube.com/watch?v=uUU3jW7Y9Ak\" class=\"inline-onebox\">Making transparent wood - YouTube</a></p>"], "78102": ["<p>Hello everyone,<br>\nPlease help me with this project. I am pretty noobie with ImageJ, so I am in need of your help. I am trying to measure different retinal layers\u2019 thickness and measure their volume too. I have stacked various images of one sample of the retina, but I don\u2019t know how I can segment each layer (+ 9 layers), and measure their thickness and volume.</p>\n<p>Thanks a million.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8fbbf4718483ec5914256f4ede0c78903060c56a.gif\" alt=\"3D\" data-base62-sha1=\"kvwY7wi55QOYGN0hRdDxsNMV0MO\" width=\"512\" height=\"493\" class=\"animated\"></p>", "<p>Greetings everyone,<br>\nI am trying to measure the volume of a retinal lesions in an image obtained using OCT. Each section parameters (X and Z parameters) and the distance between each scans are available. I studied a few paper and it seems that Cavalieri principle is the main method that have been used to measure the volume using the area (by ImageJ). I just wanted to be sure that the proposed method is the accurate method so I can measure the volume of lesions.<br>\nMy scans of retinal lesion are as follows:<br>\n3D scan:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/5uN5exmHInCI63aiEKQbiVA91ta.tif\">3D 3.tif</a> (1.7 MB)<br>\n2D scan:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/uXdfX7ayKhmzm6emGJUmBhG5mXS.tif\">2D scan.tif</a> (451.1 KB)<br>\nProposed method to measure the retinal lesions:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d.jpeg\" data-download-href=\"/uploads/short-url/zgp1CslsE5VEt18cScN5OSpnpk9.jpeg?dl=1\" title=\"Picture1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d_2_690x419.jpeg\" alt=\"Picture1\" data-base62-sha1=\"zgp1CslsE5VEt18cScN5OSpnpk9\" width=\"690\" height=\"419\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d_2_690x419.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d_2_1035x628.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f726c84c182d12452bd7bd80ea262a40a3cc165d.jpeg 2x\" data-dominant-color=\"8F9192\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Picture1</span><span class=\"informations\">1224\u00d7744 161 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899.jpeg\" data-download-href=\"/uploads/short-url/1QMiTo2LrRg6MRrw0S2fKgpI0id.jpeg?dl=1\" title=\"Picture2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899_2_690x268.jpeg\" alt=\"Picture2\" data-base62-sha1=\"1QMiTo2LrRg6MRrw0S2fKgpI0id\" width=\"690\" height=\"268\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899_2_690x268.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899_2_1035x402.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/c/0cf982d80df70ff575672b0ea204f99229e01899.jpeg 2x\" data-dominant-color=\"CACBCC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Picture2</span><span class=\"informations\">1224\u00d7476 163 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIt would be great if you could give me your opinion or any suggestions.<br>\nThank you</p>"], "41248": ["<p>Hi,</p>\n<p>I am interested in automating a SLIC based Object Classification Pipeline but am stuck on a few points.</p>\n<p>I currently have annotated a training and test data set but would like to automate the tiling process to optimize different feature parameters. The pipeline that i have worked out manually that works the best is:</p>\n<p>1)Place large annotation around all of the current annotations on the slide or on whole slide<br>\n2)Run SLIC<br>\n3) Run feature calculations (Intensity, Smoothed, Shape, Ect)<br>\n4)Delete the Large annotation created in step 1<br>\n5)Resolve Hierarchy to associate detections with the pre made annotations</p>\n<p>I have this Groovy script made from the workflow/create script window which lacks step 1, 4, and 5 because it seems the workflow script creator does not capture exactly all the things i am doing manually.</p>\n<pre><code class=\"lang-auto\">runPlugin('qupath.imagej.superpixels.SLICSuperpixelsPlugin', '{...SLIC Params...}';\nselectDetections();\nrunPlugin('qupath.lib.algorithms.IntensityFeaturesPlugin', '{....Intensity feature params...}');\naddShapeMeasurements(\"....shape params\")\nselectAnnotations();\nrunPlugin('qupath.lib.plugins.objects.SmoothFeaturesPlugin', '{\"...smooth params...');\n</code></pre>\n<p>Any help would be great!</p>\n<p>Thanks<br>\nRichard</p>", "<p>Hi <a class=\"mention\" href=\"/u/rdbell3\">@rdbell3</a>,</p>\n<p>Would something like this work in your workflow?</p>\n<pre><code class=\"lang-auto\">// Create a full image annotation\ncreateSelectAllObject(true);\n\n// Run plugin\nrunPlugin('qupath.imagej.superpixels.SLICSuperpixelsPlugin', '{..}');\n\n// Resolve Hierarchy\nresolveHierarchy()\n\n// Find parent annotation (level == 0), aka annotation created in step 1\ndef firstAnnotation = getAnnotationObjects().findAll{it.getLevel() == 1}\n\n// Remove it\nremoveObjects(firstAnnotation, true)\n\n</code></pre>\n<p>Or alternatively, instead of creating a full image annotation, you can use the Thresholder to segment all your tissue in your slide (equivalent to the now deprecated <code>Simple tissue detection</code>), select it and run your plugin after.</p>", "<p>Thanks this works. What would be the command if i wanted to delete an annotation by class or by name?</p>\n<p>Also, can i now select and delete all the tiles not in annotations? Ie the ones formally only associated with the now deleted \u201cfirstAnnotation\u201d</p>", "<aside class=\"quote no-group\" data-username=\"rdbell3\" data-post=\"3\" data-topic=\"41248\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rdbell3/40/39394_2.png\" class=\"avatar\"> rdbell3:</div>\n<blockquote>\n<p>What would be the command if i wanted to delete an annotation by class or by nam</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/melvingelbard\">@melvingelbard</a> showed that here:</p>\n<pre><code class=\"lang-auto\">def firstAnnotation = getAnnotationObjects().findAll{it.getLevel() == 1}\n</code></pre>\n<p>Instead of getLevel you would use</p>\n<pre><code class=\"lang-auto\">def firstAnnotation = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Tumor\")}\n</code></pre>\n<p>Or whatever else you might want. Once you have the objects defined, you remove them the same way as above.<br>\nSimilarly, tiles could be removed with getDetectionObjects() instead of annotation objects using the same level logic (==1 for top, ==2 for one step down, etc).</p>\n<p>Note that \u201coutside\u201d and the level will depend on whether you remove them before or after removing the outer annotation.</p>", "<p>Thanks this makes sense. Is there a easy way i can look up these commands and the arguments accepted by them?  I have been looking through the github\\lib but there is a lot to get through.</p>\n<p>thanks again for all the help</p>", "<p>I mostly use other scripts, there are a lot of them floating around. Alternatively, use the code itself on GitHub.</p><aside class=\"quote quote-modified\" data-post=\"35\" data-topic=\"27906\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/35\">QuPath Intro: Choose your own analysis(adventure)</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Advanced coding\nNewest examples of code here on the new readthedocs site: \n<a href=\"https://qupath.readthedocs.io/en/latest/docs/scripting/index.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://qupath.readthedocs.io/en/latest/docs/scripting/index.html</a> \nFor more complex analyses, outside the purview of an introduction, you will probably want to do some scripting, above and beyond simple scripting. I have no plans for an advanced scripting guide until versions stabilize, but there are some <a href=\"https://gist.github.com/petebankhead\">code examples from Pete</a>, in <a href=\"https://petebankhead.github.io/\">Pete\u2019s blog</a>, and a <a href=\"https://gist.github.com/Svidro\">collection from various places on the forums</a>, somewhat organized by typ\u2026\n  </blockquote>\n</aside>\n\n<p>Also you can use CTRL+SPACEBAR to autocomplete within the QuPath scripting, or if you really want to get into it, set up IntelliJ to handle the scripting. Or use Eclipse and a setup like <a class=\"mention\" href=\"/u/arjun.vikram\">@Arjun.Vikram</a></p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"40857\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/a/57b2e6/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/develop-and-run-qupath-scripts-directly-from-ide-even-remotely/40857\">Develop and run QuPath scripts directly from IDE (even remotely)!</a> <a class=\"badge-wrapper  bullet\" href=\"/c/development/5\"><span class=\"badge-category-bg\" style=\"background-color: #F7941D;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for development (i.e., programming) questions about scientific image software.\">Development</span></a>\n  </div>\n  <blockquote>\n    cc: <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> \nsee: <a href=\"https://forum.image.sc/t/qupath-run-scripts-in-existing-instance/40633\" class=\"inline-onebox\">QuPath run scripts in existing instance</a> \nI\u2019ve written a script to allow running QuPath scripts directly from IDEs such as Eclipse, without having to switch back and forth between QuPath and your IDE. This allows you to press the run button from inside your IDE and see your code\u2019s output directly in the IDE\u2019s output console, without needing to interact with QuPath\u2019s script editor at all! This can even work when QuPath and the IDE are running on different computers, through\u2026\n  </blockquote>\n</aside>\n", "<p>A post was split to a new topic: <a href=\"/t/create-annotations-over-half-the-screen-or-image/78431\">Create annotations over half the screen (or image?)</a></p>"], "78112": ["<p>Hello,</p>\n<p>I would like to get the list of newly imported images. I saw that <a href=\"https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-gui-fx/src/main/java/qupath/lib/gui/commands/ProjectImportImagesCommand.java#L124\" rel=\"noopener nofollow ugc\">promptToImportImages</a> returns the list of imported images but when I try to get this list (from <a href=\"https://github.com/BIOP/qupath-extension-biop-omero/blob/fd5a717e1aa908df11c762c278a20af8b6a33f80/src/main/java/qupath/ext/biop/servers/omero/raw/OmeroRawImageServerBrowserCommand.java#L381\" rel=\"noopener nofollow ugc\">here</a> for example), I always get it empty.</p>\n<p>Do you know how can I get this list ?<br>\nThanks,</p>\n<p>R\u00e9my.</p>", "<p>I\u2019m afraid I don\u2019t know what\u2019s going on and don\u2019t have a solution \u2013 the <code>promptToImportImages</code> method looks horrendous (of course I wrote it\u2026) and really looks like it should be refactored. Refactoring will be one of the main focuses of v0.5.0.</p>\n<p>It looks like a bug to me, so I\u2019ve created an issue on GitHub at</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/qupath/qupath/issues/1251\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/qupath/qupath/issues/1251\" target=\"_blank\" rel=\"noopener\">github.com/qupath/qupath</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/qupath/qupath/issues/1251\" target=\"_blank\" rel=\"noopener\">ProjectCommands.promptToImportImages always returns an empty list</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2023-03-06\" data-time=\"08:58:50\" data-timezone=\"UTC\">08:58AM - 06 Mar 23 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/petebankhead\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"petebankhead\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/bedfaac2750bf058873bc67485ebe9dac1f5ade4.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          petebankhead\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          bug\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">## Bug report\n\n**Describe the bug**\n`ProjectCommands.promptToImportImages` cl<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">aims to return a list of imported images, but it appears that it does not.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run in an IDE using debug mode, with a breakpoint around https://github.com/qupath/qupath/blob/f2a1f9c002726f1ee14c5685bf1ea67f4051f8d7/qupath-gui-fx/src/main/java/qupath/lib/gui/QuPathGUI.java#L3170\n2. Open a project\n3. Drag a new image onto the project and agree to import it\n4. Check the list contains the image (spoiler: it doesn't)\n\n**Expected behavior**\nA list is returned, as the docs suggest it should.\n\nOr, if not possible (because the import is delayed), the method signature should be updated or there should be an async version explicitly provided.\n\n**Desktop (please complete the following information):**\n - OS: All, presumably\n - QuPath Version: v0.4.3 (and likely before)\n\n**Additional context**\nReported by @Rdornier at at https://forum.image.sc/t/prompttoimportimages-returns-empty-list-of-images/78112</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>My <em>guess</em> is that at some point the import was shifted to other threads to prevent blocking, and consequently the list isn\u2019t populated in time. But this could be wrong since I haven\u2019t really checked the overly-long code in detail.</p>\n<p>I guess a workaround might be to cache a <code>Set</code> of all the entries before the import, and then remove these from the <code>Set</code> of all entries after the import, to see what is new. Awkward, but the best idea I have for now.</p>", "<p>Ok, thanks <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a></p>\n<blockquote>\n<p>My <em>guess</em> is that at some point the import was shifted to other threads to prevent blocking, and consequently the list isn\u2019t populated in time.</p>\n</blockquote>\n<p>it can explain, indeed</p>\n<blockquote>\n<p>I guess a workaround might be to cache a <code>Set</code> of all the entries before the import, and then remove these from the <code>Set</code> of all entries after the import, to see what is new</p>\n</blockquote>\n<p>Ok, I\u2019ll probably use this option temporary</p>"], "78114": ["<p>Hey everyone I am trying to run ABBA via the python notebooks for registering my slices to the Allen Mouse Brain Atlas imported via QuPath.</p>\n<p>the execution fails at the Deepslice regsitration phase:</p>\n<pre><code class=\"lang-auto\"># a first deepslice registration round : possible because it's the Allen CCF atlas, cut in coronal mode\n\n# what's assumed : the sections are already in the correct order\n\nabba.register_slices_deepslice(channels=[0, 1])\n</code></pre>\n<p>It seems to export the slices to the temp folder but fails at export</p>\n<p><code>[java.lang.Enum.toString] Export of slice Slide 1-Scene-15-TR12.czi_DAPI done (3/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-01-TR1.czi_DAPI done (5/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-05-TR4.czi_DAPI done (7/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-09-TR9.czi_DAPI done (9/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-13-TR6.czi_DAPI done (2/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-07-TR17.czi_DAPI done (8/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-03-TR2.czi_DAPI done (6/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-17-TR13.czi_DAPI done (4/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export of slice Slide 1-Scene-11-TR8.czi_DAPI done (1/9)[java.lang.Enum.toString] [java.lang.Enum.toString] Export as QuickNii Dataset done - Folder : [c:\\Users\\Fus\\Desktop\\1-ABBA\\temp\\deepslice](file:///C:/Users/Fus/Desktop/1-ABBA/temp/deepslice)[java.lang.Enum.toString]</code></p>\n<p>The full output of the cell is as follows:</p>\n<pre><code class=\"lang-auto\">{\n\t\"name\": \"java.util.concurrent.ExecutionException\",\n\t\"message\": \"java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception\",\n\t\"stack\": \"\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\\n\\u001b[1;31morg.jpype.PyExceptionProxy\\u001b[0m                Traceback (most recent call last)\\nFile \\u001b[1;32mThread.java:833\\u001b[0m, in \\u001b[0;36mjava.lang.Thread.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mThreadPoolExecutor.java:635\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.ThreadPoolExecutor$Worker.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mThreadPoolExecutor.java:1136\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.ThreadPoolExecutor.runWorker\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mFutureTask.java:264\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.FutureTask.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mDefaultThreadService.java:225\\u001b[0m, in \\u001b[0;36morg.scijava.thread.DefaultThreadService.lambda$wrap$2\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mModuleRunner.java:63\\u001b[0m, in \\u001b[0;36morg.scijava.module.ModuleRunner.call\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mModuleRunner.java:124\\u001b[0m, in \\u001b[0;36morg.scijava.module.ModuleRunner.call\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mModuleRunner.java:163\\u001b[0m, in \\u001b[0;36morg.scijava.module.ModuleRunner.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mCommandModule.java:196\\u001b[0m, in \\u001b[0;36morg.scijava.command.CommandModule.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mRegisterSlicesDeepSliceCommand.java:158\\u001b[0m, in \\u001b[0;36mch.epfl.biop.atlas.aligner.command.RegisterSlicesDeepSliceCommand.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mjdk.proxy2.$Proxy23.java:-1\\u001b[0m, in \\u001b[0;36mjdk.proxy2.$Proxy23.apply\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32morg.jpype.proxy.JPypeProxy.java:-1\\u001b[0m, in \\u001b[0;36morg.jpype.proxy.JPypeProxy.invoke\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32morg.jpype.proxy.JPypeProxy.java:-2\\u001b[0m, in \\u001b[0;36morg.jpype.proxy.JPypeProxy.hostInvoke\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32morg.jpype.JPypeContext.java:-1\\u001b[0m, in \\u001b[0;36morg.jpype.JPypeContext.createException\\u001b[1;34m()\\u001b[0m\\n\\n\\u001b[1;31morg.jpype.PyExceptionProxy\\u001b[0m: org.jpype.PyExceptionProxy\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\u001b[1;31mjava.lang.RuntimeException\\u001b[0m                Traceback (most recent call last)\\nFile \\u001b[1;32mThread.java:833\\u001b[0m, in \\u001b[0;36mjava.lang.Thread.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mThreadPoolExecutor.java:635\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.ThreadPoolExecutor$Worker.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mThreadPoolExecutor.java:1136\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.ThreadPoolExecutor.runWorker\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mFutureTask.java:264\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.FutureTask.run\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mDefaultThreadService.java:225\\u001b[0m, in \\u001b[0;36morg.scijava.thread.DefaultThreadService.lambda$wrap$2\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mModuleRunner.java:63\\u001b[0m, in \\u001b[0;36morg.scijava.module.ModuleRunner.call\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mModuleRunner.java:127\\u001b[0m, in \\u001b[0;36morg.scijava.module.ModuleRunner.call\\u001b[1;34m()\\u001b[0m\\n\\n\\u001b[1;31mjava.lang.RuntimeException\\u001b[0m: java.lang.RuntimeException: Module threw exception\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\u001b[1;31mException\\u001b[0m                                 Traceback (most recent call last)\\nFile \\u001b[1;32mFutureTask.java:191\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.FutureTask.get\\u001b[1;34m()\\u001b[0m\\n\\nFile \\u001b[1;32mFutureTask.java:122\\u001b[0m, in \\u001b[0;36mjava.util.concurrent.FutureTask.report\\u001b[1;34m()\\u001b[0m\\n\\n\\u001b[1;31mException\\u001b[0m: Java Exception\\n\\nThe above exception was the direct cause of the following exception:\\n\\n\\u001b[1;31mjava.util.concurrent.ExecutionException\\u001b[0m   Traceback (most recent call last)\\nCell \\u001b[1;32mIn[9], line 3\\u001b[0m\\n\\u001b[0;32m      1\\u001b[0m \\u001b[39m# a first deepslice registration round : possible because it's the Allen CCF atlas, cut in coronal mode\\u001b[39;00m\\n\\u001b[0;32m      2\\u001b[0m \\u001b[39m# what's assumed : the sections are already in the correct order\\u001b[39;00m\\n\\u001b[1;32m----&gt; 3\\u001b[0m abba\\u001b[39m.\\u001b[39;49mregister_slices_deepslice(channels\\u001b[39m=\\u001b[39;49m[\\u001b[39m0\\u001b[39;49m, \\u001b[39m1\\u001b[39;49m])\\n\\u001b[0;32m      5\\u001b[0m \\u001b[39m# second deepslice registration: because the slices are resampled for the registration,\\u001b[39;00m\\n\\u001b[0;32m      6\\u001b[0m \\u001b[39m# we usually get a slightly better positioning along z and cutting angle\\u001b[39;00m\\n\\u001b[0;32m      7\\u001b[0m \\u001b[39m# also: it's fast, and the combination of two affine transforms is\\u001b[39;00m\\n\\u001b[0;32m      8\\u001b[0m \\u001b[39m# an affine transform, so it's not like we are adding extra degrees of freedom\\u001b[39;00m\\n\\u001b[0;32m      9\\u001b[0m \\u001b[39m#abba.register_slices_deepslice(channels=[0, 1])\\u001b[39;00m\\n\\nFile \\u001b[1;32mc:\\\\Users\\\\Fus\\\\Desktop\\\\1-ABBA\\\\abba\\\\Abba.py:237\\u001b[0m, in \\u001b[0;36mAbba.register_slices_deepslice\\u001b[1;34m(self, channels, allow_slicing_angle_change, allow_change_slicing_position, maintain_slices_order, affine_transform)\\u001b[0m\\n\\u001b[0;32m    234\\u001b[0m         \\u001b[39mprint\\u001b[39m(\\u001b[39m'\\u001b[39m\\u001b[39mFailed to delete \\u001b[39m\\u001b[39m%s\\u001b[39;00m\\u001b[39m. Reason: \\u001b[39m\\u001b[39m%s\\u001b[39;00m\\u001b[39m'\\u001b[39m \\u001b[39m%\\u001b[39m (file_path, e))\\n\\u001b[0;32m    236\\u001b[0m \\u001b[39m# Any missing input parameter will lead to a popup window asking the missing argument to the user\\u001b[39;00m\\n\\u001b[1;32m--&gt; 237\\u001b[0m \\u001b[39mreturn\\u001b[39;00m \\u001b[39mself\\u001b[39;49m\\u001b[39m.\\u001b[39;49mij\\u001b[39m.\\u001b[39;49mcommand()\\u001b[39m.\\u001b[39;49mrun(RegisterSlicesDeepSliceCommand, \\u001b[39mTrue\\u001b[39;49;00m,\\n\\u001b[0;32m    238\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mchannels\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, JString(\\u001b[39m'\\u001b[39;49m\\u001b[39m,\\u001b[39;49m\\u001b[39m'\\u001b[39;49m\\u001b[39m.\\u001b[39;49mjoin(\\u001b[39mmap\\u001b[39;49m(\\u001b[39mstr\\u001b[39;49m, channels))),\\n\\u001b[0;32m    239\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mimage_name_prefix\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, JString(\\u001b[39m'\\u001b[39;49m\\u001b[39mSection\\u001b[39;49m\\u001b[39m'\\u001b[39;49m),\\n\\u001b[0;32m    240\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mmp\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, \\u001b[39mself\\u001b[39;49m\\u001b[39m.\\u001b[39;49mmp,\\n\\u001b[0;32m    241\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mallow_slicing_angle_change\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, allow_slicing_angle_change,\\n\\u001b[0;32m    242\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mallow_change_slicing_position\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, allow_change_slicing_position,\\n\\u001b[0;32m    243\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mmaintain_slices_order\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, maintain_slices_order,\\n\\u001b[0;32m    244\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39maffine_transform\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, affine_transform,\\n\\u001b[0;32m    245\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mdeepSliceProcessor\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, \\u001b[39mself\\u001b[39;49m\\u001b[39m.\\u001b[39;49m_run_deep_slice,\\n\\u001b[0;32m    246\\u001b[0m                              \\u001b[39m\\\"\\u001b[39;49m\\u001b[39mdataset_folder\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m, JString(temp_folder)\\n\\u001b[0;32m    247\\u001b[0m                              )\\u001b[39m.\\u001b[39;49mget()\\n\\n\\u001b[1;31mjava.util.concurrent.ExecutionException\\u001b[0m: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception\"\n}\n</code></pre>\n<p>I have tried creating a new conda environment from the yml file within ABBA-Python but that did not solve the issue.</p>\n<p>Would greatly appreciate any help as I am sort of stuck.</p>\n<p>Thank you.</p>", "<p>I\u2019m currently making a pretty big update. I\u2019ll update the repository and will ping you back. Normally you should be able to keep the same env.</p>\n<p>Which OS are you on ?</p>\n<p>PS: The error is on the Java side, what you could quickly try is look for this line:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27\" target=\"_blank\" rel=\"noopener\">NicoKiaru/ABBA-Python/blob/6ac8a46f9f5826c4866c0d25c7de50bb1ab2c605/src/abba_python/Abba.py#L27</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"17\" style=\"counter-reset: li-counter 16 ;\">\n          <li>\n          </li>\n<li>def get_java_dependencies():</li>\n          <li>    \"\"\"</li>\n          <li>    Returns the jar files that need to be included into the classpath</li>\n          <li>    of an imagej object in order to have a functional ABBA app</li>\n          <li>    these jars should be available in https://maven.scijava.org/</li>\n          <li>    :return:</li>\n          <li>    \"\"\"</li>\n          <li>    imagej_core_dep = 'net.imagej:imagej:2.9.0'</li>\n          <li>    imagej_legacy_dep = 'net.imagej:imagej-legacy:0.39.3'</li>\n          <li class=\"selected\">    abba_dep = 'ch.epfl.biop:ImageToAtlasRegister:0.4.3'</li>\n          <li>    return [imagej_core_dep, imagej_legacy_dep, abba_dep]</li>\n          <li>\n          </li>\n<li>def add_brainglobe_atlases(ij):</li>\n          <li>    # TODO : check connection available or not</li>\n          <li>    try:</li>\n          <li>        check_internet_connection()</li>\n          <li>        available_atlases = get_all_atlases_lastversions()</li>\n          <li>    except ConnectionError:</li>\n          <li>        available_atlases_nodict = get_downloaded_atlases()</li>\n          <li>        available_atlases = dict()</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>And replace <code>ch.epfl.biop:ImageToAtlasRegister:0.4.3</code> with <code>ch.epfl.biop:ImageToAtlasRegister:0.5.1</code></p>", "<p>Thanks for the quick response Nico!</p>\n<p>I am on windows 10. It seems I broke something with JVM with that version name change.</p>\n<p>This is what I get now on initializing Abba</p>\n<pre><code class=\"lang-auto\">Failed to bootstrap the artifact. Possible solutions: * Double check the endpoint for correctness (https://search.maven.org/). * Add needed repositories to [~/.jgorc](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/Fus/Desktop/2-ABBA/ABBA-Python/~/.jgorc) [repositories] block (see README). * Try with an explicit version number (release metadata might be wrong). Full Maven error output:\n\nOutput exceeds the [size limit](command:workbench.action.openSettings?%5B%22notebook.output.textLineLimit%22%5D). Open the full output data [in a text editor](command:workbench.action.openLargeOutput?cae93389-20b4-42d1-a591-1f20f631c5b2)\n\n--------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[4], line 9 1 # -- FOR DEBUGGING 2 # import imagej.doctor 3 # imagej.doctor.checkup() 4 # imagej.doctor.debug_to_stderr() 7 from abba import Abba ----&gt; 9 abba = Abba('Adult Mouse Brain - Allen Brain Atlas V3') # Simply put the name of the BrainGlobe atlas 10 abba.show_bdv_ui() # creates and show a bdv view 12 # !! Warning : it takes time... first : downloading the atlas if not present, but also: one part of the conversion has an abysmal performance 13 # it can take up to a minute... File [c:\\Users\\Soheib\\Desktop\\2-ABBA\\ABBA-Python\\abba\\Abba.py:71](file:///C:/Users/Soheib/Desktop/2-ABBA/ABBA-Python/abba/Abba.py:71), in Abba.__init__(self, atlas_name, ij, slicing_mode, headless, enable_jupyter_ui) 69 enable_jupyter_ui() 70 else: ---&gt; 71 ij = imagej.init(get_java_dependencies(), mode='interactive') 72 ij.ui().showUI() 73 self.ij = ij File [c:\\ProgramData\\Anaconda3\\envs\\abba\\lib\\site-packages\\imagej\\__init__.py:1498](file:///C:/ProgramData/Anaconda3/envs/abba/lib/site-packages/imagej/__init__.py:1498), in init(ij_dir_or_version_or_endpoint, mode, add_legacy, headless) 1496 success = _create_jvm(ij_dir_or_version_or_endpoint, mode, add_legacy) 1497 if not success: -&gt; 1498 raise RuntimeError(\"Failed to create a JVM with the requested environment.\") 1500 if mode == Mode.GUI:\n\n...\n\n1501 # Show the GUI and block. 1502 if macos: 1503 # NB: This will block the calling (main) thread forever! RuntimeError: Failed to create a JVM with the requested environment.\n</code></pre>\n<p>ch.epfl.biop:ImageToAtlasRegister version was 0.3.7 and I changed it to either 0.4.3  or 0.5.1.</p>", "<p>Ok, two other things to test:</p>\n<p>Option 1: delete your <code>.jgo</code> folder, somewhere in your root user profile folder and re-start the notebook.<br>\nOption 2: if you\u2019re interested in DeepSlice, but less in the python scripting, you can try the new windows installer: <a href=\"https://docs.google.com/presentation/d/1S6U1-yMAoaHALiIuvs90rlktF-OCec8T1xMkt7ej378/edit#slide=id.g21680bf206c_0_0\" class=\"inline-onebox\">Aligning Sections and Atlases with ABBA - Installation - Google Slides</a>. DeepSlice is included into ABBA directly, no need to make the complicated forth and back DnD on the web interface.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486.png\" data-download-href=\"/uploads/short-url/nufklJjge3SkHoMBHNLVcU7yH42.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486_2_690x324.png\" alt=\"image\" data-base62-sha1=\"nufklJjge3SkHoMBHNLVcU7yH42\" width=\"690\" height=\"324\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486_2_690x324.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a49d4debdbd850f1c9be2b3eecfb98fdbf8cb486.png 2x\" data-dominant-color=\"676565\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">807\u00d7380 62.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Deleting .jgo folder seems to have done it. Now Abba initializes with no problems, executes deepslice cell, but does not apply the AP coordinates and now is \u201clocked\u201d after the registration run. It has been running for a few hours.</p>\n<p>Not sure what I broke this time\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>See screenshot attached.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd.png\" data-download-href=\"/uploads/short-url/2D8Icgv276qOkbbpSZSb8YxE24J.png?dl=1\" title=\"abba\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_690x456.png\" alt=\"abba\" data-base62-sha1=\"2D8Icgv276qOkbbpSZSb8YxE24J\" width=\"690\" height=\"456\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_690x456.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_1035x684.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12710208863886d07ad8cb40e7e25e16978960fd_2_1380x912.png 2x\" data-dominant-color=\"080808\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">abba</span><span class=\"informations\">1541\u00d71020 65.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Installed ABBA with the new windows installer, but ABBA does not start on clicking the icon. I do see a bat file run for a second. I deactivated my antivirus software and still the same.</p>", "<p>It works now. Activating the conda environment with filepath to the specific python version seems to work fine. The notebook then executes without any issues.</p>\n<p>Thanks <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a> for all the help.</p>"], "78116": ["<p>Hello everyone<br>\nI have already segmented images of single cells and want to extract features of each individual cell. So far I used scikit.regionprops for the feature extraction, but I wondered if there is any other option with more possibilities.<br>\nThanks!</p>", "<p>Have you noticed that <code>regionprops</code> has an <code>extra_properties=</code> keyword argument that lets you define arbitrary properties for each region? So your imagination is the limit! <img src=\"https://emoji.discourse-cdn.com/twitter/blush.png?v=12\" title=\":blush:\" class=\"emoji\" alt=\":blush:\" loading=\"lazy\" width=\"20\" height=\"20\"> If you\u2019re asking for what sorts of things you can pass to this argument, then you might get more answers if you can give us more details about what it is you are trying to accomplish with the region features and why those provided by regionprops are insufficient for that goal.</p>"], "18729": ["<h2>\n<a name=\"who-sponsors-this-forum-1\" class=\"anchor\" href=\"#who-sponsors-this-forum-1\"></a>Who sponsors this forum?</h2>\n<p>The Scientific Community Image Forum is proudly sponsored by the <a href=\"https://www.openbioimageanalysis.org/\"><strong>Center for Open Bioimage Analysis (COBA)</strong></a>. <strong>COBA</strong> was established in 2020 with a P41 grant from the <a href=\"https://www.nigms.nih.gov/\">NIH National Institute of General Medical Sciences</a>. As a <a href=\"http://www.btrportal.org/\">National Biomedical Technology Research Resource</a>, its purpose is to develop and disseminate novel technologies throughout the biomedical research community.</p>\n<h2>\n<a name=\"what-is-the-forums-mission-2\" class=\"anchor\" href=\"#what-is-the-forums-mission-2\"></a>What is the forum\u2019s mission?</h2>\n<p>The goal is to embrace the diversity of the scientific imaging community, while fostering independent learning:</p>\n<ul>\n<li>\n<p>Enable people to ask \u201chow do I do X?\u201d without prior knowledge of these various software programs.</p>\n</li>\n<li>\n<p>Improve the cross-visibility of software packages.</p>\n</li>\n<li>\n<p>Make searching for previous discussions simpler.</p>\n</li>\n<li>\n<p>Give users access to a wide breadth of experts on various softwares.</p>\n</li>\n<li>\n<p>Give experts a place to have detailed discussions about elements of the software.</p>\n</li>\n<li>\n<p>Educate software developers on the capabilities of the various projects being discussed, so that they can improve the links between tools, and develop features that are more likely to be novel.</p>\n</li>\n<li>\n<p>Encourage open science and reproducible research by advocating for open tools and their interoperability.</p>\n</li>\n<li>\n<p>Foster not only scientific independent thinking, but just as importantly, <a href=\"http://conference.imagej.net/2015/pariksheet-nanda/transcript.pdf\">independent learning</a>. We want to not only <a href=\"https://en.wiktionary.org/wiki/give_a_man_a_fish_and_you_feed_him_for_a_day;_teach_a_man_to_fish_and_you_feed_him_for_a_lifetime\">teach people how to fish</a>, but teach them how to learn.</p>\n</li>\n</ul>\n<h2>\n<a name=\"how-is-the-content-organized-3\" class=\"anchor\" href=\"#how-is-the-content-organized-3\"></a>How is the content organized?</h2>\n<p>Topics are organized non-hierarchically using tags. E.g., if you ask a question about calling ImageJ from CellProfiler, you can add the <code>imagej</code> and <code>cellprofiler</code> tags. This will help people to narrow the scope of their attention as needed based on their time and interest. The front page includes a top-level link to the tag feed of each partnered software project.</p>\n<h2>\n<a name=\"which-icons-appear-in-the-top-navigation-bar-4\" class=\"anchor\" href=\"#which-icons-appear-in-the-top-navigation-bar-4\"></a>Which icons appear in the top navigation bar?</h2>\n<p>A <strong>Community Partner</strong> is an open-source software project or community organization that uses this forum as a primary recommended discussion channel. This means that: A) project source code is provided under an <a href=\"https://opensource.org/licenses\">OSI-approved software license</a>; B) the organization links to the forum in its documentation regarding how users should seek support and/or discussion; and C) it does not promote additional, separate, project-specific discussion channels more prominently than this forum. The rationale is to facilitate unification and visibility of discussion across the communities; see \u201cWhat is the forum\u2019s mission?\u201d above.</p>\n<p>Each Community Partner appears in the top navigation with logo and link to its associated tag feed. Discussion of any and all scientific image software tools is warmly welcomed, but only those using the forum as a primary recommended discussion channel will be included in the navigation bar.</p>\n<h2>\n<a name=\"what-is-the-process-to-become-a-community-partner-5\" class=\"anchor\" href=\"#what-is-the-process-to-become-a-community-partner-5\"></a>What is the process to become a Community Partner?</h2>\n<ol>\n<li>\n<p>Meet the current guidelines for becoming a Community Partner:</p>\n<ul>\n<li>The organization\u2019s project source code is provided under an open-source license according to the <a href=\"https://opensource.org/osd\">Open Source Definition</a>; see OSI\u2019s <a href=\"https://opensource.org/licenses\">Licenses</a> page for details.</li>\n<li>The organization links to the forum in its documentation regarding how users should seek support and/or discussion.</li>\n<li>The organization does not promote additional, separate, project-specific discussion channels more prominently than this forum.</li>\n<li>The organization will elect an individual as an active representative on the Community Forum Team.</li>\n</ul>\n</li>\n<li>\n<p>Make a public post in the <a href=\"https://forum.image.sc/c/web-site\">Websites category</a> to request to join the forum as a Community Partner:</p>\n<ul>\n<li>Include an <code>@team</code> mention and add the <code>community-partner</code> tag, to make sure someone on the core team sees your post in a timely manner.</li>\n<li>Attach your project\u2019s icon to the post (icons should have a 1:1 aspect ratio). Depending on your logo, you may need two versions: one for <a href=\"https://forum.image.sc/?preview_theme_id=0\">light background</a> and one for <a href=\"https://forum.image.sc/?preview_theme_id=1\">dark background</a>.</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a name=\"what-is-the-role-of-the-community-forum-team-6\" class=\"anchor\" href=\"#what-is-the-role-of-the-community-forum-team-6\"></a>What is the role of the Community Forum Team?</h2>\n<p>The <strong>Community Forum Team</strong> is made up of active members of the forum, including appointed representatives from Community Partners.  Members are involved in general monitoring and responding on the forum, addressing and handling of flagging of inappropriate or unnecessary forum posts (defined by our <a href=\"https://forum.image.sc/guidelines\">Guidelines page</a>), and collectively determining general forum protocols and procedures.  The Community Forum Team category is for internal discussion of forum issues. Topics are only visible to members of the Community Forum Team ( <code>@team</code> ) group.</p>\n<h2>\n<a name=\"what-are-the-roles-of-administrators-and-moderators-7\" class=\"anchor\" href=\"#what-are-the-roles-of-administrators-and-moderators-7\"></a>What are the roles of administrators and moderators?</h2>\n<p>Administrators and Moderators are drawn from the Community Forum Team members, across a variety of software projects, who are most active in responding to user queries.  In addition to the work of the Community Forum Team, these users respond to flagged posts, remove user accounts when requested, and deal with overall site maintenance.  For more information on what each role is able to do, see <a href=\"https://forum.image.sc/t/new-mod-positions-available-how-do-we-want-to-select-new-ones-and-keep-them-up-to-date/33858\">this thread</a>.</p>\n<h2>\n<a name=\"what-is-a-dormant-community-partner-8\" class=\"anchor\" href=\"#what-is-a-dormant-community-partner-8\"></a>What is a Dormant Community Partner?</h2>\n<p>A Dormant Community Partner is a Community Partner project which has no posts with that project\u2019s tag, or no responses from the project\u2019s Community Forum Team representatives, within the past 12 months. Such projects are moved to the <a href=\"https://forum.image.sc/t/dormant-community-partners/68215\">Dormant Community Partners list</a> until such time as they become active again with a post and/or reply, and a Community Forum Team representative for that project requests the icon be reinstated in the Community Partners top navigation header.</p>\n<h2>\n<a name=\"what-about-homework-questions-9\" class=\"anchor\" href=\"#what-about-homework-questions-9\"></a>What about homework questions?</h2>\n<p>Questions requesting <em>homework help</em> are fine to include here on the forum.  However, they must include a summary of the work already attempted to solve the problem along with a description of the specific issues in solving part or all of the question.  For those answering such questions, just keep in mind to answer them in the best interest of the student, providing an answer that helps him/her learn from the exchange.  For more specific guidance on asking and answering such questions, please read <a href=\"https://meta.stackoverflow.com/questions/334822/how-do-i-ask-and-answer-homework-questions/334823#334823\">this post</a> from Stack Overflow; our forum follows the same policy presented there.</p>\n<h2>\n<a name=\"what-about-hardware-questions-10\" class=\"anchor\" href=\"#what-about-hardware-questions-10\"></a>What about hardware questions?</h2>\n<p>The forum\u2019s primary focus is on software, but sometimes discussion needs to touch on both, and the line between them is not always well defined. Any question relating somehow to scientific imaging is welcome and appropriate. For microscopy-related hardware questions, our sister forum <a href=\"https://forum.microlist.org/\">Microforum</a> is a great place to discuss.</p>\n<h2>\n<a name=\"what-about-commercial-andor-closed-source-software-11\" class=\"anchor\" href=\"#what-about-commercial-andor-closed-source-software-11\"></a>What about commercial and/or closed-source software?</h2>\n<p>Discussion of how to solve scientific problems with commercial software is very welcome, as long as it encourages <a href=\"/guidelines#independent-learning\">independent learning</a>, rather than being <a href=\"https://en.wikipedia.org/wiki/Advertorial\">advertorial</a>.</p>\n<p>Including closed-source and/or commercial software is a necessary part of embracing the community\u2019s diversity. In practice, many kinds of software are used for scientific image analysis, acquisition, etc. If members of the community want to discuss, e.g., how to improve a MATLAB-based workflow that leverages the image processing toolbox, then they are welcome to do so. This discussion may have benefits not only for MATLAB users, but for anyone with a similar workflow, and relating to any other topics touched on during the discussion. Those wishing to advocate for open science are welcome to chime in regarding how to meet the analysis requirements with <a href=\"https://en.wikipedia.org/wiki/Open-source_software\">OSS</a>-based tools. Those who wish to ignore such discussions can mute topics with unwanted tags; configure it at <code>https://forum.image.sc/u/&lt;your-username&gt;/preferences/tags</code>.</p>\n<h2>\n<a name=\"how-do-i-give-feedback-12\" class=\"anchor\" href=\"#how-do-i-give-feedback-12\"></a>How do I give feedback?</h2>\n<p>This site is operated by the Community Forum Team and driven by you, the community. If you have any further questions about how things should work here, start a new topic in the Websites category and mention the <code>@team</code> group. For concerns that cannot be discussed publicly, contact us as described on the <a href=\"/about\">About</a> page.</p>\n<h2>\n<a name=\"what-are-the-terms-of-service-13\" class=\"anchor\" href=\"#what-are-the-terms-of-service-13\"></a>What are the terms of service?</h2>\n<p>See the <a href=\"/tos\">Terms of Service</a> page.</p>\n<h2>\n<a name=\"how-can-i-cite-the-forum-14\" class=\"anchor\" href=\"#how-can-i-cite-the-forum-14\"></a>How can I cite the forum?</h2>\n<p>If the forum has been useful to your work, mentioning the Image.sc Forum in your Acknowledgements section is much appreciated!</p>\n<p>In cases where you feel a citation is appropriate, you can cite the following paper:</p>\n<p>Rueden, C.T., Ackerman, J., Arena, E.T., Eglinger, J., Cimini, B.A., Goodman, A., Carpenter, A.E. and Eliceiri, K.W. \u201cScientific Community Image Forum: A discussion forum for scientific image software.\u201d PLoS biology 17, no. 6 (2019): e3000340. <a href=\"https://doi.org/10.1371/journal.pbio.3000340\">doi:10.1371/journal.pbio.3000340</a></p>\n<p>There is no requirement or expectation to cite, but citations do help keep the forum funded. You can also cite specific topics as URLs if you wish.</p>", "", "<p>A post was split to a new topic: <a href=\"/t/how-to-identify-pore-geophysical-parameters-of-2d-thinsection-sandstone-image-by-image-processing/21837\">How to Identify Pore-Geophysical parameters of 2D_Thinsection_Sandstone Image by Image Processing</a></p>", "<p>A post was split to a new topic: <a href=\"/t/export-3d-model-from-microct-images-in-avizo/62888\">Export 3d model from microCT images in Avizo</a></p>", "<aside class=\"quote no-group\" data-username=\"ctrueden\" data-post=\"1\" data-topic=\"18729\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png\" class=\"avatar\"> Curtis Rueden:</div>\n<blockquote>\n<p>remove</p>\n</blockquote>\n</aside>\n<p>Just a question: How can I delete my account here? Or could you just do that for me? THanks a lot</p>", "<p>Also want to delete my account - how can we do this??</p>"], "78122": ["<p>Hello !<br>\nI have been trying to do segmentation of very bad/noisy bright field images, and Ilastik helped me tremendously. However, as it is using a lot of memory and I am analyzing a lot of timelapse, I wanted to move the analysis to a HPC in my institute, as my desktop couldn\u2019t deal with it anymore.<br>\nHowever, I tried to (1) run directly Ilastik using the run_ilastik.sh or (2) run Ilastik via python using subprocess, and in both cases I don\u2019t have an error message, but I also don\u2019t have any output.</p>\n<p>Here is what I am doing:<br>\non the cluster:</p>\n<pre><code class=\"lang-auto\">#!/bin/bash\n\n#SBATCH --job-name=imageanalysisgast\n#SBATCH --time=5:00:00\n#SBATCH --cpus-per-task=5\n#SBATCH --mail-type=end\n#SBATCH --array=1-2%5\n#SBATCH --mail-user=judith.pineau@pasteur.fr\n#SBATCH --error first_batch.err\n\n\nIFS=''\nreadarray -t config &lt; './TL_BrightField_IsmaProject/Test_HemiBra_hdf5/FileList.txt'\n\nsample=${config[(($SLURM_ARRAY_TASK_ID - 1))]}\nmodule load Python/3.11.0 || exit 1\n\nsource Bash/202303_JP_Ilastik/bin/activate || exit 2\n\n\npython3 \"./Bash/Python_scripts/20230303_Ilastik_cluster.py\" \"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5\" $sample\n\necho \"This is array task ${SLURM_ARRAY_TASK_ID}, the sample name is ${sample}.\" &gt;&gt; output.txt\nexit 0\n</code></pre>\n<p>And my Python script:</p>\n<pre><code class=\"lang-python\">#!/usr/bin/python3\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Oct 13 11:50:41 2022\n\n@author: Judith\n\"\"\"\n\n#!/usr/bin/python\n#coding:utf-8\nimport os\n\nimport sys # to handle exceptions\n#Source directory\nimport re, glob, os #navigation in diff repertory if script not in the same folder\nfrom os import walk #Find folders and files\nfrom os.path import isfile, join\nimport subprocess\n\n#Test from forum @CellKai\nilastik_location = \"./Python_Ilastik/ilastik-1.4.0-Linux\"\nilastik_project = './TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp'\n\nindir = sys.argv[1] #my dataset\ninfile = sys.argv[2]\n\nos.makedirs(indir+\"/results\", exist_ok = True)\n\nos.chdir(ilastik_location)\n\ncommand = './run_ilastik.sh --headless --project=\"%s\" --export_source=\"Probabilities Stage 2\" --stack_along=\"t\" --output_filename_format=\"%s/results/{nickname}_results.h5\" \"%s%s\"' % (\n        ilastik_project,\n        indir,\n        indir,\n        infile)\nprint(\"\\n\\n%s\" % command)\ng=subprocess.call(command, shell=True)\nprint(g)\n</code></pre>\n<p>I don\u2019t know why it doesn\u2019t work, maybe my environment is not the proper one. I know there is a python interpreter included in Ilastik but I don\u2019t know if I should use it, and if so, how to use it on the cluster ?</p>\n<p>Also, my model was trained on the Ilastik on my desktop, so on Windows 10 and the previous ilastik version, could that be an issue?</p>\n<p>I am sorry, I hope no information is missing in my question, I am relatively new to python, and even newer to using the cluster !</p>\n<p>I hope someone can help me as I have been stuck for some time, and I really don\u2019t know what to try (since I don\u2019t see any error message)</p>\n<p>Thanks !</p>\n<p>Judith</p>", "<p>Hello <a class=\"mention\" href=\"/u/judith_pineau\">@Judith_Pineau</a>,</p>\n<p>first of all, welcome to the image.sc community <img src=\"https://emoji.discourse-cdn.com/twitter/sun_with_face.png?v=12\" title=\":sun_with_face:\" class=\"emoji\" alt=\":sun_with_face:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/tada.png?v=12\" title=\":tada:\" class=\"emoji\" alt=\":tada:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/handshake.png?v=12\" title=\":handshake:\" class=\"emoji\" alt=\":handshake:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Let\u2019s try debugging it together. I think you\u2019re probably almost there.</p>\n<p>Absolute first thing I\u2019d try would be to run the command I run on slurm locally (so running ilastik with the same project file and one of the images you want to process). What is your data format? And also, the job is supposed to run on the complete image (time series), right?</p>\n<p>First thing I would try, would be also redirecting <code>stdout</code> via the the <code>#SBATCH --output ...</code> option, so you would at least get the print statements from the python script.</p>\n<p>What exit status did your slurm job have?</p>\n<p>You can also specify the logfile for ilastik, which could give you additional insights, via <code>run_ilastik.sh --headless --log_file /path/to/desired/location.txt...</code></p>\n<aside class=\"quote no-group\" data-username=\"Judith_Pineau\" data-post=\"1\" data-topic=\"78122\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/judith_pineau/40/68638_2.png\" class=\"avatar\"> Judith Pineau:</div>\n<blockquote>\n<p>Also, my model was trained on the Ilastik on my desktop, so on Windows 10 and the previous ilastik version, could that be an issue?</p>\n</blockquote>\n</aside>\n<p>This should not be a problem if you made sure the classifier was saved in ilastik (make sure you have live-update on, see some predictions, (then you can turn live-update off again) then save). Then it doesn\u2019t access the training data anymore. If the classifier isn\u2019t saved, ilastik will try to train it in headless mode and fail, if it cannot reach the data (which can happen if data is not saved into the project file).</p>\n<p>In the python script, are you sure that the node on the cluster can see ilastik in the directory you specified? (<code>\"./Python_Ilastik/ilastik-1.4.0-Linux\"</code>). Same goes for the project file.</p>\n<p>You also might want to look into (if you want to broaden your python game a little):</p>\n<ul>\n<li>\n<a href=\"https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals\">Formatted String Literals (f-strings)</a> for easier (human) readability instead of the \u201cold\u201d string-formatting with the <code>%</code> operator.</li>\n<li>\n<a href=\"https://docs.python.org/3/library/pathlib.html\"><code>pathlib</code></a> for all operations where you compose paths, make directories and so on. Just so you don\u2019t have to worry about separators and such.</li>\n</ul>\n<p>Cheers<br>\nDominik</p>\n<p><em>Edit</em>: I have edited your post a bit to make it a little more readable, by adding formatting to your code blocks. See this post for details: <a href=\"https://forum.image.sc/t/how-to-put-code-in-a-post/3698/3\" class=\"inline-onebox\">How to put code in a post? - #3 by imagejan</a></p>", "<p>Hello Dominik !</p>\n<p>Thank you for your super fast answer and warm welcome, and for editing my post - indeed it is a lot easier to read like this!</p>\n<p>So, I tried to run the command directly in a .sh code (see below, please tell me if you see a mistake in the code <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> ), and it\u2019s not giving any output either. I had converted my images to hdf5 as advised on the Ilastik website, and I trained the model on timelapse file in hdf5 (so yes, it should run on the complete timelapse hopefully).</p>\n<pre><code class=\"lang-python\">#!/bin/bash\n\n#SBATCH --job-name=imageanalysisgast\n#SBATCH --time=5:00:00\n#SBATCH --cpus-per-task=5\n#SBATCH --mail-type=end\n#SBATCH --array=1-1%5\n#SBATCH --mail-user=judith.pineau@pasteur.fr\n#SBATCH --error first_batch.err\n#SBATCH --output output_file.out\n\nsrun ./Python_Ilastik/ilastik-1.4.0-Linux/run_ilastik.sh --headless \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--readonly\t\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--project=./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--stack_along=\"t\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--export_source=\"Probabilities Stage 2\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--ouput_format=\"hdf5\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--output_filename_format=\"{nickname}_{result_type}.h5\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--log_file= \"log.txt\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA1-1_1stitch.tif_z.tif.h5\n\n\necho \"This is array task ${SLURM_ARRAY_TASK_ID}, the sample name is ${nickname}.\" &gt;&gt; output.txt\nexit 0\n</code></pre>\n<p>I already had access to the.out files actually, and for the python run all they printed was what I asked them to:</p>\n<pre><code class=\"lang-python\">\n\n./run_ilastik.sh --headless --project=\"./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp\" --export_source=\"Probabilities Stage 2\" --stack_along=\"t\" --log_file \"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/log_results.txt\" --output_filename_format=\"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/{nickname}_results.h5\" \"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5./221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA10-1_1stitch.tif_z.tif.h5\"\n0\n\n\n./run_ilastik.sh --headless --project=\"./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp\" --export_source=\"Probabilities Stage 2\" --stack_along=\"t\" --log_file \"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/log_results.txt\" --output_filename_format=\"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/{nickname}_results.h5\" \"./TL_BrightField_IsmaProject/Test_HemiBra_hdf5./221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA1-1_1stitch.tif_z.tif.h5\"\n0\n</code></pre>\n<p>(actually I don\u2019t know what the 0 at the end is, but the rest is the output from the \u201cprint(g)\u201d</p>\n<p>Weirdly, it doesn\u2019t seem to save a log file, I don\u2019t know why. I tried with log_file=\u2026 and log file \u2026 (space of equal sign), and I never see anything. In the python script, I create a folder in the directory and this works, so I don\u2019t think it\u2019s an authorization problem (?).</p>\n<p>Also, exit status is code 0, so it\u2019s not indicating any issue.</p>\n<p>I tried to check if the node on the cluster can see my ilastik and my model, so I checked by going into the directory and typing ls -l, and it can see it (I also made sure it had execution permission).</p>\n<p>And I will look into the link you put, my python is definitely still a bit messy and a patchwork from bits of code I find here and there, I should definitely make it a bit nicer and human-friendly !</p>\n<p>Cheers</p>\n<p>Judith</p>", "<aside class=\"quote no-group\" data-username=\"Judith_Pineau\" data-post=\"3\" data-topic=\"78122\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/judith_pineau/40/68638_2.png\" class=\"avatar\"> Judith Pineau:</div>\n<blockquote>\n<p>Weirdly, it doesn\u2019t seem to save a log file, I don\u2019t know why. I tried with log_file=\u2026 and log file \u2026 (space of equal sign), and I never see anything. In the python script, I create a folder in the directory and this works, so I don\u2019t think it\u2019s an authorization problem (?).</p>\n</blockquote>\n</aside>\n<p>just really quick, by <code>...</code> I meant that you\u2019d have to put a filename there that you could find again\u2026 Just to clarify that you\u2019ve used <code>...</code> here also as a placeholder\u2026</p>", "<p>Oh yes, it\u2019s just that it was a bit long to put in the middle of a sentence ! I tried writing just \u201clog_results.txt\u201d (so it would be saved in the current directory, like my .out and .err), or putting the same address as my results directory ! Sorry it wasn\u2019t clear</p>", "<p>Okay, sorry, I didn\u2019t have time to properly read your last replies before.</p>\n<p>In your direct invocation, you have <code>--log_file= \"log.txt\" \\</code> with a space after the eqals sign, that will not be interpreted correctly, I think (so that should be removed).<br>\nThe <code>--stack-along</code> argument is not needed (doesn\u2019t make sense without a \u201cpattern\u201d and I think your data is a single dataset in the hdf5 file).</p>\n<p>could you maybe do an <code>ls ./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/results/</code> <em>at the end</em> of your script, to verify that no output has been written? (that is for the python version, for the direct invocation, could you <code>ls .</code> (file is written to the working directly I think).</p>", "<p>Hi Dominik !</p>\n<p>Don\u2019t apologize, you are already so fast in helping out, it\u2019s really great to have such support !<br>\nSo I removed the space after the equal sign, and I removed the --stack_along (I thought it was necessary for timelapse, I didn\u2019t get that it was only if the images were in separated files).<br>\nI tried direct invocation, then <code>ls .</code>, and it gave me the list of files in my main directory (which is the working directory here), where I could find the .out and .err files saved from my previous attempt for example, but no trace of the log file.<br>\nActually, the job end so quickly that it feels like it\u2019s never really \u201cinvoking\u201d (not sure what the right word is here\u2026) Ilastik at all. Which would explain the total absence of log file.<br>\nBut then it\u2019s strange that I do not get any error message saying for example that it cannot find Ilastik when I call it\u2026<br>\nI will try to re-download Ilastik and re-put it there, maybe some problem during copy to the server?<br>\nIf you have any other idea or suggestion, I\u2019m all ears <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Okay, so:<br>\nI re-downloaded Ilastik 1.4.0 for Linux, and un-compressed it from the terminal on the cluster server (which is linux-based).<br>\nThen I:</p>\n<ul>\n<li>gave access (execution rights) to run_ilastik.sh</li>\n<li>gave access (execution rights) to the file <code>ilastik-1.4.0-Linux/bin/python</code> because there was an output saying that it was denied permission<br>\nThis is the bash code I tried to run (doing <code>sbatch name_of_my_code.sh</code>) :</li>\n</ul>\n<pre><code class=\"lang-python\">#!/bin/bash\n\n#SBATCH --job-name=imageanalysisgast\n#SBATCH --time=5:00:00\n#SBATCH --cpus-per-task=5\n#SBATCH --mail-type=end\n#SBATCH --array=1-1%5\n#SBATCH --error first_batch.err\n#SBATCH --output output_file.out\n\nsrun ./Python_Ilastik/ilastik-1.4.0-Linux/run_ilastik.sh --headless \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--readonly \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--project=\"./TL_BrightField_IsmaProject/Ilastik_model/Model20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2/20230209_TL_BF_Segmentation_OptimZ_Context_Hdf5_2.ilp\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--export_source=\"Probabilities Stage 2\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--ouput_format=\"hdf5\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--output_filename_format=\"{nickname}_{result_type}.h5\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t--log_file=\"log.txt\" \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t./TL_BrightField_IsmaProject/Test_HemiBra_hdf5/221104_Ol_10x_Timelapse_Isma_BF_48h__20221105_001_WIA1-1_1stitch.tif_z.tif.h5\n\n\necho \"This is array task ${SLURM_ARRAY_TASK_ID}, the sample name is ${nickname}.\" &gt;&gt; output.txt\nexit 0\n</code></pre>\n<p>And now\u2026 it is running ! I am pretty sure it was coming from something stupid - I did the unzipping on my windows desktop, then copied to the server, and I guess there were some glitch when you do it using WinZip on a Windows computer that made some files unusable.</p>\n<p>Anyway, I now have my probability output <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"> !!!<br>\nHowever, I still do not have a log.txt file, and in my .err file it\u2019s written</p>\n<pre><code class=\"lang-python\">WARNING 2023-03-07 17:20:57,922 opConservationTracking 2050886 140187458860864 Could not find any ILP solver\nWARNING 2023-03-07 17:20:57,984 opStructuredTracking 2050886 140187458860864 Could not find any ILP solver\nWARNING 2023-03-07 17:20:57,986 structuredTrackingWorkflow 2050886 140187458860864 Could not find any learning solver. Tracking will use flow-based solver (DPCT). Learning for tracking will be disabled!\nWARNING 2023-03-07 17:21:03,210 newAutocontextWorkflow 2050886 140187458860864 Unused command-line args: ['--ouput_format=hdf5', '--log_file=log.txt']\nWARNING arraytypes.py(1271): FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\nWARNING arraytypes.py(1277): FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n</code></pre>\n<p>so for some reason, it doesn\u2019t want to use my log_file argument.<br>\nBut now that I\u2019m able to get this to run, maybe it\u2019s not important <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>So, turns out my problem was likely mostly coming from a stupid unzipping bug, but thank you so so much for the help and pointers and advices to improve my coding !<br>\nHopefully I won\u2019t run into any more trouble <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThanks !!!</p>", "<p>Great that it works now!</p>\n<p>For the log, the argument should be <code>logfile</code> without underscore as far as I can see from the code. On my Windows machine, it also works with just <code>log</code>, though I\u2019m not sure why <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Awesome that it works now <a class=\"mention\" href=\"/u/judith_pineau\">@Judith_Pineau</a> - i didn\u2019t know unzipping on windows could have this kind of side-effects - good to know.<br>\nAnd thanks <a class=\"mention\" href=\"/u/btbest\">@btbest</a> for the <code>--logfile</code>\u2026 idk how I hallucinated the underscore <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I had let my Ilastik analysis on the side for a few weeks to take care of other projects, but I tried with logfile today and it works perfectly <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThank you !!</p>"], "78123": ["<p>Hi</p>\n<p>I\u2019ve been only using DeepLabCut for a short period and have successfully used the GUI to perform a single animal project.</p>\n<p>However, I was hoping to create a multi-animal project to track 2 individuals with a single body part.</p>\n<p>I\u2019ve spent a couple of weeks trying to work it out, including following instructions and forum discussions, but I can\u2019t seem to figure it out.</p>\n<p>To begin, I\u2019ve been using the GUI to create a multi-animal project, extract frames, label frames (20 frames), create training dataset, and train network (30K iterations - apparently fewer iterations are required for multi-animal projects\u2026?). Then, I used coding in the terminal to evaluate network, and with my limited knowledge, the performance looks ok (pixel error below 2 [results in uploaded file] and RMSE below 5 [most RMSE scores were 1-3 with one body part scoring a 5 for a single frame]).</p>\n<p>I also performed the create_video_with_all_detections command and from what I can tell the targets in the full.mp4 video look fine.</p>\n<p>I tried the extract_outlier_frames command (thinking I could remove less accurate frames) but I couldn\u2019t figure it out\u2026? (just read you can\u2019t extract_outlier_frames until after stitch_tracklets\u2026?)</p>\n<p>Anyway, everything looks ok, so I moved to analysing_video with auto-track = FALSE (because I couldn\u2019t work out why an h5 file wasn\u2019t been generated when auto-track = TRUE\u2026?) and executed the convert_ detections2tracklets command with a prompt indicating that tracklets were created. However, when I performed the stitch_tracklets command I receive a message saying tracklets are empty\u2026?</p>\n<p>From what I\u2019ve read, tracklets are empty indicates the training has not been accurate. But, from my understanding, the metrics look OK\u2026 but I guess I\u2019m wrong\u2026?</p>\n<p>From what I\u2019ve read, I might need to perform further training (does that mean more iterations in training?) or include more body parts, or label more frames\u2026 (or all the above?)</p>\n<p>Please find my command information attached.</p>\n<p>Any help will be greatly appreciated <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/cwGzxnHD3P81ENMfkVMqRy6MALO.txt\">060323 Script.txt</a> (8.8 KB)</p>", "<p>You cannot use maDLC with one bodypart per animal. The tracklets are empty because there are no tracklets. There has to be at least a pair of bodyparts per animal.</p>", "<p>Thank you for your prompt reply <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Sounds good - do I need to create a skeleton as well?</p>\n<p>Is 2 the minimum number of body parts or would 4 (for example) be better?</p>\n<p>Thanks</p>\n<p>Grant</p>", "<p>More is generally better. You have to make one for visualization, but on the training part it will make the fully redundant skeleton on it\u2019s own</p>", "<p>Thank you.</p>\n<p>I\u2019ll add more body parts in my next attempt.</p>\n<p>I\u2019ll let you know how I go during the week.</p>\n<p>Have a great day</p>", "<p>Hi again,</p>\n<p>I\u2019ve created a new project with a video to track 3 bodyparts on 2 individuals - everything went well with stitches_tracklets. Now, I cant work out the file paths for the video and pickle files for the refine_tracklets commands.</p>\n<p>I checked the help and it reads:</p>\n<p>config: str<br>\nFull path of the config.yaml file.</p>\n<p>pickle_or_h5_file: str<br>\nFull path of either the pickle file obtained after calling<br>\ndeeplabcut.convert_detections2tracklets, or the h5 file written after<br>\nrefining the tracklets a first time. Note that refined tracklets are<br>\nalways stored in the h5 format.</p>\n<p>video: str<br>\nFull path of the corresponding video.<br>\nIf the video duration and the total length of the tracklets disagree<br>\nby more than 5%, a message is printed indicating that the selected<br>\nvideo may not be the right one.</p>\n<p>But I wasn\u2019t following which files were created when I performed the convert_detections2tracklets command. I can see pickle files and new full-mp4 video were created in the videos folder, but I cant work out what files I should be inputting for the refine_tracklets command.</p>\n<p>Thanks, again for your help <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>The pickle or h5 files that end with \u201c_el\u201d or whatever tracker you chose.</p>", "<p>Thank you, Konrad.</p>\n<p>I double checked the user manual and I saw the figure highlighting that file ending with \u2018el\u2019 was the one I was after</p>\n<p>The help also mentions to use the corresponding video file. Is the video file different to the originally uploaded video file?</p>\n<p>I\u2019m really sorry to ask basic questions but I\u2019m super eager to get things right, and I\u2019m sure it will be second nature shortly <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thanks, again</p>", "<p>Yup, the path to the video you analyzed (not the labeled one)</p>", "<p>Thank you</p>\n<p>I\u2019ll give it a go tomorrow <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi Konrad,</p>\n<p>Thanks for your help yesterday.</p>\n<p>I was able to activate the refine tracklets GUI <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>However, I quickly noticed that the 3 bodyparts were identified on the wrong individual throughout the whole video.</p>\n<p>I watched the tutorial <a href=\"https://www.youtube.com/watch?v=bEuBKB7eqmk\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DeepLabCut 2.2: how to use the refine tracklets GUI! - YouTube</a> about \u2018flagging\u2019 the section of the video where the swap occurs, but because the individual labels are wrong throughout the video, I first tried to create a flag window from the start to the end. However, I kept getting this error:</p>\n<p>deeplabcut.refine_tracklets(<br>\n\u2026: config_path,<br>\n\u2026: h5_pickle,<br>\n\u2026: analysed_video,<br>\n\u2026: )<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\cbook_<em>init</em>_.py\u201d, line 304, in process<br>\nfunc(*args, **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\widgets.py\u201d, line 247, in <br>\nreturn self.<em>observers.connect(\u2018clicked\u2019, lambda event: func(event))<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\gui\\tracklet_toolbox.py\u201d, line 532, in flag_frame<br>\nax.fill_between(<br>\nFile \"C:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib_<em>init</em></em>.py\", line 1472, in inner<br>\nreturn func(ax, *map(sanitize_sequence, args), **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5431, in fill_between<br>\nreturn self._fill_between_x_or_y(<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5420, in _fill_between_x_or_y<br>\npts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\numpy\\ma\\core.py\u201d, line 3254, in <strong>getitem</strong><br>\nmout = _mask[indx]<br>\nIndexError: invalid index to scalar variable.</p>\n<p>I also tried with a smaller window just to see if I could get it to work, but I kept getting the same error.</p>\n<p>I also pressed the flag button twice at the end of my defined window, and a second red vertical line popped up, but no other shaded area defining the flag window popped up.</p>\n<p>Anyway, I lasso the 3 bodyparts on the individuals following the tutorial and selected the other individual name on the top of the GUI and I got this error:</p>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\cbook_<em>init</em>_.py\u201d, line 304, in process<br>\nfunc(*args, **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\gui\\tracklet_toolbox.py\u201d, line 684, in on_pick<br>\nself.ax_slider.lines.clear()<br>\nAttributeError: \u2018ArtistList\u2019 object has no attribute \u2018clear\u2019</p>\n<p>I saw in a previous post a similar issue: <a href=\"https://forum.image.sc/t/how-to-use-refine-tracklets-gui/65589\" class=\"inline-onebox\">How to use refine tracklets GUI</a></p>\n<p>which you suggested updating matplotlib version. I did this and my current version is 3.7.1 (below) but I keep encountering the same error</p>\n<p>pip show matplotlib<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nName: matplotlib<br>\nVersion: 3.7.1<br>\nSummary: Python plotting package<br>\nHome-page: <a href=\"https://matplotlib.org\" rel=\"noopener nofollow ugc\">https://matplotlib.org</a><br>\nAuthor: John D. Hunter, Michael Droettboom<br>\nAuthor-email: <a href=\"mailto:matplotlib-users@python.org\">matplotlib-users@python.org</a><br>\nLicense: PSF<br>\nLocation: c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages<br>\nRequires: contourpy, cycler, fonttools, importlib-resources, kiwisolver, numpy, packaging, pillow, pyparsing, python-dateutil<br>\nRequired-by: deeplabcut, filterpy, imgaug<br>\nNote: you may need to restart the kernel to use updated packages.</p>\n<p>I also checked how to restart kernel and used the following command which was suggested here <a href=\"https://stackoverflow.com/questions/37751120/restart-ipython-kernel-with-a-command-from-a-cell:\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">jupyter - Restart ipython Kernel with a command from a cell - Stack Overflow</a></p>\n<p>import os<br>\nos._exit(00)</p>\n<p>Any further help will be greatly appreciated <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thanks</p>", "<p>Hi Konrad,</p>\n<p>Thanks for your help yesterday.</p>\n<p>I was able to activate the refine tracklets GUI <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>However, I quickly noticed that the 3 bodyparts were identified on the wrong individual throughout the whole video.</p>\n<p>I watched the tutorial <a href=\"https://www.youtube.com/watch?v=bEuBKB7eqmk\" rel=\"noopener nofollow ugc\">DeepLabCut 2.2: how to use the refine tracklets GUI! - YouTube</a> about \u2018flagging\u2019 the section of the video where the swap occurs, but because the individual labels are wrong throughout the video, I first tried to create a flag window from the start to the end. However, I kept getting this error:</p>\n<p>deeplabcut.refine_tracklets(<br>\n\u2026: config_path,<br>\n\u2026: h5_pickle,<br>\n\u2026: analysed_video,<br>\n\u2026: )<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\cbook_<em>init</em><em>.py\u201d, line 304, in process<br>\nfunc(*args, **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\widgets.py\u201d, line 247, in<br>\nreturn self.<em>observers.connect(\u2018clicked\u2019, lambda event: func(event))<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\gui\\tracklet_toolbox.py\u201d, line 532, in flag_frame<br>\nax.fill_between(<br>\nFile \"C:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib</em><em>init</em></em>.py\", line 1472, in inner<br>\nreturn func(ax, *map(sanitize_sequence, args), **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5431, in fill_between<br>\nreturn self._fill_between_x_or_y(<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5420, in _fill_between_x_or_y<br>\npts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\numpy\\ma\\core.py\u201d, line 3254, in <strong>getitem</strong><br>\nmout = _mask[indx]<br>\nIndexError: invalid index to scalar variable.</p>\n<p>I also tried with a smaller window just to see if I could get it to work, but I kept getting the same error.</p>\n<p>I also pressed the flag button twice at the end of my defined window, and a second red vertical line popped up, but no other shaded area defining the flag window popped up.</p>\n<p>Anyway, I lasso the 3 bodyparts of a particular individual following the tutorial and selected the other individual name on the top of the GUI and I got this error:</p>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\cbook_<em>init</em>_.py\u201d, line 304, in process<br>\nfunc(*args, **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\gui\\tracklet_toolbox.py\u201d, line 684, in on_pick<br>\nself.ax_slider.lines.clear()<br>\nAttributeError: \u2018ArtistList\u2019 object has no attribute \u2018clear\u2019</p>\n<p>I saw in a previous post a similar issue: <a href=\"https://forum.image.sc/t/how-to-use-refine-tracklets-gui/65589\">How to use refine tracklets GUI</a></p>\n<p>which you suggested updating matplotlib version. I did this and my current version is 3.7.1 (below) but I keep encountering the same error</p>\n<p>pip show matplotlib<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nName: matplotlib<br>\nVersion: 3.7.1<br>\nSummary: Python plotting package<br>\nHome-page: <a href=\"https://matplotlib.org/\" rel=\"noopener nofollow ugc\">https://matplotlib.org</a><br>\nAuthor: John D. Hunter, Michael Droettboom<br>\nAuthor-email: <a href=\"mailto:matplotlib-users@python.org\">matplotlib-users@python.org</a><br>\nLicense: PSF<br>\nLocation: c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages<br>\nRequires: contourpy, cycler, fonttools, importlib-resources, kiwisolver, numpy, packaging, pillow, pyparsing, python-dateutil<br>\nRequired-by: deeplabcut, filterpy, imgaug<br>\nNote: you may need to restart the kernel to use updated packages.</p>\n<p>I also checked how to restart kernel and used the following command which was suggested here <a href=\"https://stackoverflow.com/questions/37751120/restart-ipython-kernel-with-a-command-from-a-cell:\" rel=\"noopener nofollow ugc\">jupyter - Restart ipython Kernel with a command from a cell - Stack Overflow</a></p>\n<p>import os<br>\nos._exit(00)</p>\n<p>Any further help will be greatly appreciated <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thanks</p>", "<p>Can you downgrade to matplotlib == 3.5.1 or 3.6.1 and see if this helps. What is your python version?</p>", "<p>I\u2019m going to be away from my office computer for the weekend, but I\u2019ll downgrade the matplotlib version first thing Monday morning and let you know what happens</p>\n<p>My Python version is 3.8.16</p>\n<p>Is there something I should do with the Python version as well?</p>", "<p>No, 3.8, 3.9 should be fine</p>", "<p>Hi Konrad,</p>\n<p>I downgraded matplotlib to 3.5.1 (and 3.6.1) and I get the same error when I perform the \u2018flag\u2019 function in the refine_tracklets GUI.</p>\n<p>Please found output below.</p>\n<p>What else should I try to get it to work?</p>\n<p>Thanks for your help</p>\n<p>(base) C:\\WINDOWS\\system32&gt;conda activate deeplabcut</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;ipython<br>\nPython 3.8.16 | packaged by co(deeplabcut) C:\\WINDOWS\\system32&gt;ipython<br>\nPython 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>\nType \u2018copyright\u2019, \u2018credits\u2019 or \u2018license\u2019 for more information<br>\nIPython 8.10.0 \u2013 An enhanced Interactive Python. Type \u2018?\u2019 for help.</p>\n<p>In [1]: pip install matplotlib==3.6.1<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nCollecting matplotlib==3.6.1<br>\nDownloading matplotlib-3.6.1-cp38-cp38-win_amd64.whl (7.2 MB)<br>\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.2/7.2 MB 2.7 MB/s eta 0:00:00<br>\nRequirement already satisfied: packaging&gt;=20.0 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (23.0)<br>\nRequirement already satisfied: python-dateutil&gt;=2.7 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (2.8.2)<br>\nRequirement already satisfied: pyparsing&gt;=2.2.1 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (3.0.9)<br>\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (1.0.7)<br>\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (1.4.4)<br>\nRequirement already satisfied: numpy&gt;=1.19 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (1.23.5)<br>\nRequirement already satisfied: cycler&gt;=0.10 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (0.11.0)<br>\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (4.38.0)<br>\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.6.1) (9.4.0)<br>\nRequirement already satisfied: six&gt;=1.5 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib==3.6.1) (1.16.0)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nInstalling collected packages: matplotlib<br>\nAttempting uninstall: matplotlib<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nFound existing installation: matplotlib 3.7.1<br>\nUninstalling matplotlib-3.7.1:<br>\nSuccessfully uninstalled matplotlib-3.7.1<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nSuccessfully installed matplotlib-3.6.1<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nNote: you may need to restart the kernel to use updated packages.</p>\n<p>In [2]: import os</p>\n<p>In [3]: os._exit(00)</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;conda activate deeplabcut</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;ipython<br>\nPython 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>\nType \u2018copyright\u2019, \u2018credits\u2019 or \u2018license\u2019 for more information<br>\nIPython 8.10.0 \u2013 An enhanced Interactive Python. Type \u2018?\u2019 for help.</p>\n<p>In [1]: import deeplabcut<br>\nLoading DLC 2.3.0\u2026</p>\n<p>In [2]: config_path = r\u2019C:\\Users\\WSA20210206\\Desktop\\assembly3-grant080323-2023-03-08\\config.yaml\u2019</p>\n<p>In [3]: h5_pickle = r\u2019C:\\Users\\WSA20210206\\Desktop\\assembly3-grant080323-2023-03-08\\videos\\assembly3DLC_dlcrnetms5_asse<br>\n\u2026: mbly3Mar8shuffle1_30000_el.h5\u2019</p>\n<p>In [4]: analysed_video = r\u2019C:\\Users\\WSA20210206\\Desktop\\assembly3-grant080323-2023-03-08\\videos\\assembly3DLC_dlcrnetms5<br>\n\u2026: _assembly3Mar8shuffle1_30000_full.mp4\u2019</p>\n<p>In [5]: deeplabcut.refine_tracklets(<br>\n\u2026: config_path,<br>\n\u2026: h5_pickle,<br>\n\u2026: analysed_video,<br>\n\u2026: )<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\cbook_<em>init</em>_.py\u201d, line 307, in process<br>\nfunc(*args, **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\widgets.py\u201d, line 220, in <br>\nreturn self.<em>observers.connect(\u2018clicked\u2019, lambda event: func(event))<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\gui\\tracklet_toolbox.py\u201d, line 532, in flag_frame<br>\nax.fill_between(<br>\nFile \"C:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib_<em>init</em></em>.py\", line 1423, in inner<br>\nreturn func(ax, *map(sanitize_sequence, args), **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5337, in fill_between<br>\nreturn self._fill_between_x_or_y(<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5326, in _fill_between_x_or_y<br>\npts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\numpy\\ma\\core.py\u201d, line 3254, in <strong>getitem</strong><br>\nmout = _mask[indx]<br>\n<strong>IndexError: invalid index to scalar variable</strong>.<br>\nOut[5]:<br>\n(&lt;deeplabcut.refine_training_dataset.tracklets.TrackletManager at 0x15109171820&gt;,<br>\n&lt;deeplabcut.gui.tracklet_toolbox.TrackletVisualizer at 0x151791d4fa0&gt;)</p>\n<p>In [6]:<br>\nDo you really want to exit ([y]/n)? y</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;conda activate deeplabcut</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;ipython<br>\nPython 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>\nType \u2018copyright\u2019, \u2018credits\u2019 or \u2018license\u2019 for more information<br>\nIPython 8.10.0 \u2013 An enhanced Interactive Python. Type \u2018?\u2019 for help.</p>\n<p>In [1]: pip install matplotlib==3.5.1<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nCollecting matplotlib==3.5.1<br>\nUsing cached matplotlib-3.5.1-cp38-cp38-win_amd64.whl (7.2 MB)<br>\nRequirement already satisfied: numpy&gt;=1.17 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (1.23.5)<br>\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (1.4.4)<br>\nRequirement already satisfied: cycler&gt;=0.10 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (0.11.0)<br>\nRequirement already satisfied: pyparsing&gt;=2.2.1 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (3.0.9)<br>\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (9.4.0)<br>\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (4.38.0)<br>\nRequirement already satisfied: python-dateutil&gt;=2.7 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (2.8.2)<br>\nRequirement already satisfied: packaging&gt;=20.0 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from matplotlib==3.5.1) (23.0)<br>\nRequirement already satisfied: six&gt;=1.5 in c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib==3.5.1) (1.16.0)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nInstalling collected packages: matplotlib<br>\nAttempting uninstall: matplotlib<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nFound existing installation: matplotlib 3.6.1<br>\nUninstalling matplotlib-3.6.1:<br>\nSuccessfully uninstalled matplotlib-3.6.1<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nSuccessfully installed matplotlib-3.5.1<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nWARNING: Ignoring invalid distribution -atplotlib (c:\\programdata\\anaconda3\\envs\\deeplabcut\\lib\\site-packages)<br>\nNote: you may need to restart the kernel to use updated packages.</p>\n<p>In [2]: import os</p>\n<p>In [3]: os._exit(00)</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;conda activate deeplabcut</p>\n<p>(deeplabcut) C:\\WINDOWS\\system32&gt;ipython<br>\nPython 3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]<br>\nType \u2018copyright\u2019, \u2018credits\u2019 or \u2018license\u2019 for more information<br>\nIPython 8.10.0 \u2013 An enhanced Interactive Python. Type \u2018?\u2019 for help.</p>\n<p>In [1]: import deeplabcut<br>\nLoading DLC 2.3.0\u2026</p>\n<p>In [2]: config_path = r\u2019C:\\Users\\WSA20210206\\Desktop\\assembly3-grant080323-2023-03-08\\config.yaml\u2019</p>\n<p>In [3]: h5_pickle = r\u2019C:\\Users\\WSA20210206\\Desktop\\assembly3-grant080323-2023-03-08\\videos\\assembly3DLC_dlcrnetms5_asse<br>\n\u2026: mbly3Mar8shuffle1_30000_el.h5\u2019</p>\n<p>In [4]: analysed_video = r\u2019C:\\Users\\WSA20210206\\Desktop\\assembly3-grant080323-2023-03-08\\videos\\assembly3DLC_dlcrnetms5<br>\n\u2026: _assembly3Mar8shuffle1_30000_full.mp4\u2019</p>\n<p>In [5]: deeplabcut.refine_tracklets(<br>\n\u2026: config_path,<br>\n\u2026: h5_pickle,<br>\n\u2026: analysed_video,<br>\n\u2026: )<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\cbook_<em>init</em>_.py\u201d, line 287, in process<br>\nfunc(*args, **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\widgets.py\u201d, line 227, in <br>\nreturn self.<em>observers.connect(\u2018clicked\u2019, lambda event: func(event))<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\gui\\tracklet_toolbox.py\u201d, line 532, in flag_frame<br>\nax.fill_between(<br>\nFile \"C:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib_<em>init</em></em>.py\", line 1412, in inner<br>\nreturn func(ax, *map(sanitize_sequence, args), **kwargs)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5252, in fill_between<br>\nreturn self._fill_between_x_or_y(<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\matplotlib\\axes_axes.py\u201d, line 5241, in _fill_between_x_or_y<br>\npts = np.row_stack([np.column_stack([ind[where], dep1[where]]),<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\envs\\deeplabcut\\lib\\site-packages\\numpy\\ma\\core.py\u201d, line 3254, in <strong>getitem</strong><br>\nmout = _mask[indx]<br>\n<strong>IndexError: invalid index to scalar variable.</strong></p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf.jpeg\" data-download-href=\"/uploads/short-url/sC9qOuXvMU11b5QP8psrhqo01vV.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_375x500.jpeg\" alt=\"image\" data-base62-sha1=\"sC9qOuXvMU11b5QP8psrhqo01vV\" width=\"375\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c88cedc0f71f04a4d47955d27d7a0dedb87285bf_2_750x1000.jpeg 2x\" data-dominant-color=\"627D98\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">3024\u00d74032 3.46 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hi Konrad,</p>\n<p>I thought I would send you a photo of the files I have in the video folder for a project.</p>\n<p>Can you please confirm which files I should be using for stitch_tracklets and refine_tracklets?</p>\n<p>Maybe this might help</p>\n<p>Thanks</p>\n<p>Grant</p>", "<p><code>assembly3</code> video and <code>_el.h5</code>. Can you reinstall it normally from the env and not ipython console, just to be sure? Though the new error seems to point to not having a second flag? <code>IndexError: invalid index to scalar variable.</code> means that you\u2019re trying to get an index from a single value (not a list/array etc.)</p>", "<p>Thanks, Konrad.</p>\n<p>Just to be clear: you want me to load the DeepLabCut environment (conda activate deeplabcut) and call the refine_tracklets command without calling ipython?</p>\n<p>With the index error, you\u2019re also saying I don\u2019t need to put brackets around the el.h5 file and assembly3 file when calling the refine_tracklets command?</p>\n<p>Makes sense because they are not lists\u2026 only single files for each component</p>\n<p>Thanks</p>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/+1.png?v=12\" title=\":+1:\" class=\"emoji only-emoji\" alt=\":+1:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>The paths to a video and the <code>h5</code> file should be strings, not lists of strings.</p>\n<p>From what you copied it looks like you\u2019ve run the installation of matplotlib from withing the ipython console not in the conda env (might be weird formatting - it\u2019s better if you use triple <code> </code> ` and input code inside, to keep the formatting of the output from the terminal. I just wanted to see if the installation went ok.</p>"], "78130": ["<p>Hi,<br>\nI try to set the measurements exported in the xls files : What ever I choose the xls files contains always the same :<br>\nDetection # Surface|x|y|z|t min intensity max intensity  average intensity</p>\n<p>I would like to get the Area (in \u00b5m) of the ROI but it never saves it in batch mode.<br>\nDoing the export individually works !!!<br>\nAny advice ?</p>", "<p>Hello. Can you elaborate a bit on how to reproduce the issue?</p>", "<p>Sorry Jean-Yves it is a mistake. It was a question about Icy spot detector !!!</p>\n<p>But I will have a question about trackmate.</p>"], "78132": ["<p>Hi,</p>\n<p>I try to set the measurements exported in the xls files : What ever I choose the xls files contains always the same :<br>\nDetection # Surface|x|y|z|t min intensity max intensity average intensity</p>\n<p>I would like to get the Area (in \u00b5m) of the ROI but it never saves it in batch mode.<br>\nDoing the export individually works !!!<br>\nAny advice ?</p>", "<p>Hi <a class=\"mention\" href=\"/u/eric_denarier\">@Eric_Denarier</a></p>\n<p>The <em>Spot Detector</em> plugin only has a fixed export type (the one you described) and when used in batch you can only use that one. I guess when you speak about doing the export individually you mean using the ROI export tool which is not part of the <em>Spot Detector</em> plugin. If you want a custom export for batch mode then you need to use <em>Protocols</em> plugin which allow design graphical analysis workflow and doing batches.<br>\nYou can take example on this protocol for instance :<br>\n<a href=\"https://icy.bioimageanalysis.org/protocol/batch-spots-detections-and-statistics/\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://icy.bioimageanalysis.org/protocol/batch-spots-detections-and-statistics/</a></p>\n<p>You can access online protocols directly from the Icy top search bar by taping its name (just try \u201cbatch spot\u201d here for instance).</p>\n<p>Hope that helps !</p>\n<p>\u2013 Stephane</p>", "<aside class=\"quote group-team\" data-username=\"Stephane\" data-post=\"2\" data-topic=\"78132\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/stephane/40/361_2.png\" class=\"avatar\"> Stephane Dallongeville:</div>\n<blockquote>\n<p>you need to use <em>Protocols</em> plugin which allow design graphical analysis workflow and doing batches.<br>\nYou can take example on this protocol for instance :</p>\n</blockquote>\n</aside>\n<p>I will try this. Merci Stephane</p>"], "78133": ["<p>I\u2019m playing with GPU in pyclesperanto on my Mac with Apple M1 Pro chip. In macOS-13.2.1-arm64-arm-64bit. environment, when I try to select device by</p>\n<pre><code class=\"lang-auto\">cle.select_device(dev_type='gpu')\n</code></pre>\n<p>I get</p>\n<pre><code class=\"lang-auto\">---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[51], line 2\n      1 # Select GPU for processing\n----&gt; 2 cle.select_device(dev_type='gpu')\n\nFile ~/mambaforge/envs/cellpose-env/lib/python3.9/site-packages/pyclesperanto_prototype/_tier0/_device.py:75, in select_device(name, dev_type, score_key, device_index)\n     71 except:\n     72     pass\n---&gt; 75 device = filter_devices(name, dev_type, score_key)[device_index]\n     76 if name is not None and name not in device.name:\n     77     warnings.warn(f\"No OpenCL device found with {name} in their name. Using {device.name} instead.\")\n\nIndexError: list index out of range\n</code></pre>\n<p>The same command in macOS-10.16-x86_64-i386-64bit. environment gives</p>\n<pre><code class=\"lang-auto\">&lt;Apple M1 Pro on Platform: Apple (2 refs)&gt;\n</code></pre>\n<p>As expected.</p>\n<p>Is this a feature, or a bug? I\u2019m just starting with GPUs and I got confused by this behaviour.<br>\nThanks!</p>\n<p>Edit: Now I realise this might actually be coming from the fact that the arm64 environment is set up by mamba, and the x86 environment by conda\u2026</p>", "<p>There\u2019s already a related thread on this topic, so be sure to check out</p><aside class=\"quote quote-modified\" data-post=\"16\" data-topic=\"77523\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/not-seeing-mac-gpu-with-cle-available-device-names/77523/16\">Not seeing Mac GPU with cle.available_device_names()</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a> \nFor me Cell 1 outputs: \n['Apple M1'] \nand cell 3 (the device = cpu one) gives an error: \nIndexError                                Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 device = cle.select_device(dev_type='cpu')\n      2 device\n\nFile ~/Dev/miniforge3/envs/cle-test/lib/python3.10/site-packages/pyclesperanto_prototype/_tier0/_device.py:72, in select_device(name, dev_type, score_key)\n     68 except:\n     69     pass\n---&gt; 72 device = filter_devices(name, dev_typ\u2026\n  </blockquote>\n</aside>\n\n<p>Can you post the output to:</p>\n<pre><code class=\"lang-auto\">import pyclesperanto_prototype as cle\ncle.available_device_names()\n</code></pre>\n<p>I get:<br>\n<code>['Apple M1']</code><br>\nand I can select it using:<br>\n<code>cle.select_device('Apple')</code></p>"], "78144": ["<p>Dear all, I try to find a way to unlarge a ROI only in a certain direction; in the direction of  an other ROI (ellipse).  I try EDM of each ROI and mathematic operation between them , it should work but I don\u2019t find the good operation. thanks if you any idea to solve the problem simply ?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9f06d1d10ee67b0983d22a23b7b53892d09f6db.png\" data-download-href=\"/uploads/short-url/v5YIKYAmTH5wewYTCZ0vsPUaSvN.png?dl=1\" title=\"Result of C1-Untitled\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9f06d1d10ee67b0983d22a23b7b53892d09f6db.png\" alt=\"Result of C1-Untitled\" data-base62-sha1=\"v5YIKYAmTH5wewYTCZ0vsPUaSvN\" width=\"500\" height=\"500\" data-dominant-color=\"070707\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Result of C1-Untitled</span><span class=\"informations\">512\u00d7512 1.61 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e087ea68a786700eaf9fcd4111c6d81a1697a4c.png\" data-download-href=\"/uploads/short-url/fHoIJUF4Sf2qZltBWXVqkndHwSw.png?dl=1\" title=\"Result of C1-Untitled\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e087ea68a786700eaf9fcd4111c6d81a1697a4c.png\" alt=\"Result of C1-Untitled\" data-base62-sha1=\"fHoIJUF4Sf2qZltBWXVqkndHwSw\" width=\"500\" height=\"500\" data-dominant-color=\"080808\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Result of C1-Untitled</span><span class=\"informations\">512\u00d7512 1.79 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Try a morphological closing of radius 27.  Unfortunately the oval is too close to the image border, so it generates that undesired extension. Make sure the image is larger and the ellipse is far from the border.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d016d4b390eb66f94cc8b9bc215ae6bd8071c9ab.png\" data-download-href=\"/uploads/short-url/tGQebkXm7fHt5SelT9FFruATUEj.png?dl=1\" title=\"closing\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d016d4b390eb66f94cc8b9bc215ae6bd8071c9ab.png\" alt=\"closing\" data-base62-sha1=\"tGQebkXm7fHt5SelT9FFruATUEj\" width=\"500\" height=\"500\" data-dominant-color=\"0D0D0D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">closing</span><span class=\"informations\">512\u00d7512 2.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Could you explain how do you do that with ImageJ (morphology plugin ?)  on the ellipse ROI ? This image is just an exemple/simulation on what I want do on a real image containing a lot of folicles (ellipse) with marginal zone (ROI inside in white)  that I want to supplement, the distance between the marginal  zone and the folicles will be different for each object\u2026so the radius you utilize have to be adapted automatically for each object\u2026 not a  simple problem isn\u2019t it ?<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/9/e9dc84e9491696381ab30fefd4a202fae6b033bf.jpeg\" alt=\"Clipboard\" data-base62-sha1=\"xmPI4gRXoWfDeov11fFmFkLFMHJ\" width=\"587\" height=\"417\"></p>", "<p>Closing is maximum filter fiollowed by minimum filter of the same size.</p>"], "37185": ["<h3>Sample image and/or macro code</h3>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"nofollow noopener\">minimal working example</a> of your macro code.</li>\n</ul>\n<p>(If this is your first post, you may not be able to attach images, in which case make a post with a link or a first post without the image attachment and a second one with)</p>\n<h3>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<p>Converting to bigtiff by running :</p>\n<blockquote>\n<p>Blockquote<br>\nbfconvert -noflat -overwrite -bigtiff ID01_Wk7d3_1D_Section01.vsi ID01_Wk7d3_1D_Section01.ome.tif<br>\nBlockquote</p>\n</blockquote>\n<p>throws the error:</p>\n<blockquote>\n<p>Blockquote<br>\nSeries 0: converted 1/1 planes (100%)<br>\nException in thread \u201cmain\u201d loci.formats.FormatException: Buffer is too small; expected 560640 bytes, got 249318 bytes.<br>\nat loci.formats.FormatWriter.checkParams(FormatWriter.java:476)<br>\nat loci.formats.out.TiffWriter.saveBytes(TiffWriter.java:229)<br>\nat loci.formats.out.OMETiffWriter.saveBytes(OMETiffWriter.java:218)<br>\nat loci.formats.out.OMETiffWriter.saveBytes(OMETiffWriter.java:208)<br>\nat loci.formats.FormatWriter.saveBytes(FormatWriter.java:132)<br>\nat loci.formats.ImageWriter.saveBytes(ImageWriter.java:252)<br>\nat loci.formats.tools.ImageConverter.convertPlane(ImageConverter.java:801)<br>\nat loci.formats.tools.ImageConverter.testConvert(ImageConverter.java:719)<br>\nat loci.formats.tools.ImageConverter.main(ImageConverter.java:1096)<br>\nBlockquote</p>\n</blockquote>\n<h3>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?<br>\nGetting an ome.tif file</li>\n</ul>\n<h3>Challenges</h3>\n<ul>\n<li>\n<p>What stops you from proceeding?<br>\nThe exception thrown</p>\n</li>\n<li>\n<p>What have you tried already?<br>\nThe command bfconvert as mentioned above</p>\n</li>\n<li>\n<p>Have you found any related forum topics? If so, cross-link them.<br>\nNo</p>\n</li>\n<li>\n<p>What software packages and/or plugins have you tried?</p>\n</li>\n</ul>\n<blockquote>\n<p>Blockquote<br>\nbfconvert -version<br>\nVersion: 6.4.0<br>\nBuild date: 11 March 2020<br>\nBlockquote</p>\n</blockquote>\n<p><a class=\"attachment\" href=\"/uploads/short-url/wdlVm9cjHAdhHL2XaGr5AoQ1gfW.zip\">ID01_Wk7d3_1D_Section01.zip</a> (314.3 KB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/serban\">@serban</a>,</p>\n<p>welcome to the image.sc forum and thanks for your clear and detailed bug report. I was able to  reproduce the issue successfully and tracked the origin of the bug which was introduced in Bio-Formats 6.4.0.</p>\n<p>I have opened a <a href=\"https://github.com/ome/bioformats/pull/3553\">Pull Request</a> to address this regression and hope to have it released in an upcoming patch release of Bio-Formats. In the meantime, a workaround would be to use Bio-Formats 6.3.1 which should convert your sample file without issue.</p>\n<p>Best,<br>\nSebastien</p>", "<p>Hi Sebastien,</p>\n<p>I tested with 6.3.1 and it was working.<br>\nHowever, I tried it also with the latest 6.5.0 (released yesterday) and the error is still present there.</p>\n<p>Thanks,</p>\n<p>Serban</p>", "<p>Hi Serban.</p>\n<p>yes the fix has been opened too late for inclusion into the <a href=\"https://forum.image.sc/t/release-of-bio-formats-6-5-0/37266\">Bio-Formats 6.5.0 release</a> so the error persistence is expected. The next Bio-Formats release, either 6.5.1 or 6.6.0, should fix the regression.</p>\n<p>Best,<br>\nSebastien</p>", "<p>Hi,</p>\n<p>I am also having this issue using bioformats 6.7.0 when I run the command. Please can you advise?</p>\n<p>bfconvert -crop \u201c100,100,500,500\u201d -pyramid-resolutions 1 P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff test_pyr_bfconvert.ome.tiff<br>\nbf.sh HERE Here<br>\njava  -Xmx2G -Dhttp.proxyHost= -Dhttp.proxyPort= -cp /gpfs2/well/leedham/users/brr908/conda/skylake/envs/hyperion/bin:/gpfs2/well/leedham/users/brr908/conda/skylake/envs/hyperion/bin/bioformats_package.jar: loci.formats.tools.ImageConverter -crop 100,100,500,500 -pyramid-resolutions 1 P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff test_pyr_bfconvert.ome.tiff<br>\nOutput file test_pyr_bfconvert.ome.tiff exists.<br>\nDo you want to overwrite it? ([y]/n)<br>\ny<br>\nP264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff<br>\nOMETiffReader initializing P264MoFibroblastMPRXB84sScan1_SAMPLE_1_ROI_1.ome.tiff<br>\nReading IFDs<br>\nPopulating metadata<br>\n[OME-TIFF] \u2192 test_pyr_bfconvert.ome.tiff [OME-TIFF]<br>\nSwitching to BigTIFF (by file size)<br>\nSeries 0: converted 8/51 planes (15%)<br>\nSeries 0: converted 16/51 planes (31%)<br>\nSeries 0: converted 24/51 planes (47%)<br>\nSeries 0: converted 34/51 planes (66%)<br>\nSeries 0: converted 42/51 planes (82%)<br>\nSeries 0: converted 46/51 planes (90%)<br>\nSeries 0: converted 51/51 planes (100%)<br>\nException in thread \u201cmain\u201d loci.formats.FormatException: Buffer is too small; expected 1327104000 bytes, got 250000 bytes.<br>\nat loci.formats.FormatWriter.checkParams(FormatWriter.java:476)<br>\nat loci.formats.out.TiffWriter.saveBytes(TiffWriter.java:229)<br>\nat loci.formats.out.OMETiffWriter.saveBytes(OMETiffWriter.java:218)<br>\nat loci.formats.out.OMETiffWriter.saveBytes(OMETiffWriter.java:208)<br>\nat loci.formats.FormatWriter.saveBytes(FormatWriter.java:132)<br>\nat loci.formats.ImageWriter.saveBytes(ImageWriter.java:252)<br>\nat loci.formats.tools.ImageConverter.convertPlane(ImageConverter.java:800)<br>\nat loci.formats.tools.ImageConverter.testConvert(ImageConverter.java:718)<br>\nat loci.formats.tools.ImageConverter.main(ImageConverter.java:1095)</p>"], "78146": ["<p>I\u2019m trying to add a scale bar to this 3D rendered image. In the 2D composite channel, the scale bar says the approximate length across is 153 microns:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74.png\" data-download-href=\"/uploads/short-url/Abgb9oSPvzJmdAkFxAUjwjX7RRO.png?dl=1\" title=\"Screenshot 2023-03-06 at 11.38.41 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74_2_579x500.png\" alt=\"Screenshot 2023-03-06 at 11.38.41 AM\" data-base62-sha1=\"Abgb9oSPvzJmdAkFxAUjwjX7RRO\" width=\"579\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74_2_579x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74_2_868x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/d/fd9422ce9bb44a7a240810409c7ff1501ec03d74.png 2x\" data-dominant-color=\"0B051F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-06 at 11.38.41 AM</span><span class=\"informations\">878\u00d7758 88.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>But when I 3D render the bot, and zoom in and out, then take snapshots, the scale bar length is changing with the change of zoom.</p>\n<p>When I zoom farther away, the scale bar length decreases:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c7565a9d1b95ecdd87b170b2371f2f771c0fd24.png\" alt=\"Screenshot 2023-03-06 at 11.40.51 AM\" data-base62-sha1=\"hL0JaotpNdxwy7kacVCDGVjqChK\" width=\"422\" height=\"318\"></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a96d9a0f4b04cdb237342d2adf1b48e6d28dd90c.png\" alt=\"Screenshot 2023-03-06 at 11.40.45 AM\" data-base62-sha1=\"oaPvP0JIDhMym9lJlJqM1rIiDGQ\" width=\"568\" height=\"472\"></p>\n<p>When I zoom closer, the scale bar length increases:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181.png\" data-download-href=\"/uploads/short-url/p5vAO7b3LSSsymFHcOvbDBDrKU1.png?dl=1\" title=\"Screenshot 2023-03-06 at 11.40.55 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181_2_625x500.png\" alt=\"Screenshot 2023-03-06 at 11.40.55 AM\" data-base62-sha1=\"p5vAO7b3LSSsymFHcOvbDBDrKU1\" width=\"625\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181_2_625x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afd5c9734b0772476c6c2cd8a0be9dfbcb760181.png 2x\" data-dominant-color=\"401B31\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-06 at 11.40.55 AM</span><span class=\"informations\">866\u00d7692 217 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Does anyone know why this is happening? And how to fix it/add scale bars for 3D rendered images in an accurate way?</p>", "<p>Try changing to an orthographic camera. If the camera is perspective, it sort of mimics depth in the image. The  scalebar will have the length of the depth it is sitting in, which might not be the same depth as the thing you want to measure.</p>", "<p>Hi <a class=\"mention\" href=\"/u/cindy_zhu\">@Cindy_Zhu</a>,</p>\n<p>How do you render the image?</p>\n<p>If you want quite some flexibility for 3D reconstruction I would use <a href=\"https://github.com/bene51/3Dscript/wiki\">3Dscript</a> for the rendering which allows you even animation and it adds a scale bar by default automatically. Because one problem in your different zoom versions seems to be the actual quality of the image, since the number of pixel changes. In 3Dscript you can zoom freely as needed and the scale bar even adapts to the zoom automatically.</p>\n<p>I just think it does not show the text in addition, but that could be typed on top using an overlay.  This is how I did it in this example image.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42.jpeg\" data-download-href=\"/uploads/short-url/lV9EXoEMMZd7PXVkZjDE6cSPEsi.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_500x500.jpeg\" alt=\"image\" data-base62-sha1=\"lV9EXoEMMZd7PXVkZjDE6cSPEsi\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99a3a519cc943513659c993388c77ae12c95eb42_2_1000x1000.jpeg 2x\" data-dominant-color=\"0B0B02\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1000\u00d71000 72.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Is the orthographic camera something through ImageJ, or would changing the imaging device of the sample help with this issue?</p>", "<p>Nothing to do with the imaging device, just the way it is shown.</p>\n<p>What are you using for displaying the 3D image?</p>\n<p>I thought the \u201c3D viewer\u201d had the option to switch between orthographic and perspective, but it seems not, sorry.</p>", "<p>I\u2019m just using the 3D viewer plugin once the composite has been generated.</p>", "<p>HI <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a><br>\nI just downloaded the 3D script plugin, but it\u2019s asking me to convert it to either an 8- or 16-bit image, and the plugin looks like this:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08.png\" data-download-href=\"/uploads/short-url/qVhpJSfQlbrLVVADigNkeJjO8Tm.png?dl=1\" title=\"Screenshot 2023-03-09 at 3.01.04 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_244x500.png\" alt=\"Screenshot 2023-03-09 at 3.01.04 PM\" data-base62-sha1=\"qVhpJSfQlbrLVVADigNkeJjO8Tm\" width=\"244\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_244x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_366x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcb22259786f48c93cf0fd16c2b9a819b128dc08_2_488x1000.png 2x\" data-dominant-color=\"D4DAE2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-09 at 3.01.04 PM</span><span class=\"informations\">794\u00d71624 79.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is this the plugin that you are working off of as well?</p>", "<p>Hi <a class=\"mention\" href=\"/u/cindy_zhu\">@Cindy_Zhu</a>,</p>\n<p>Yes that is the correct one and also yes, the image needs to be 8- or 16-bit.<br>\nHow are your channes or your complete image saved?<br>\nSince it is fluorescence it should be a grayscale image of a bit-depth between 8 and 32-bit.<br>\nI would discourage from dsaving those data as color images (meaning RGB, as labelled at the top of the images when opened in InageJ).<br>\nIn case of multi-channel images, they are best saved as composite images with all channels being kept physically separate.<br>\nIn such cases you can convert the channels for display reasons to 8 or 16 bit using <code>&gt;Image &gt;Type</code></p>", "<p>Is it possible for multi-channels to be added, such that the final composite is still grey scale 8- or 16-bit, and have this stack be imported into the 3D script? Or would adding them cause loss of depth data that would alter the addition of scale bars?</p>", "<p>Hi <a class=\"mention\" href=\"/u/cindy_zhu\">@Cindy_Zhu</a>,</p>\n<p>You can actually work with 5D data in 3Dscript (while only 4 dimensions, including channels and volume will be displayed at first).</p>\n<p>Just in case there is some confusion:</p>\n<p>The amount of channels does not have any influence on the bit depth of those.<br>\nIf you imaged with a specific bit-depth and save the raw data correctly (preferable in the original file format of the imaging software) then all channels are kept physically separate and in the original bit-depth. Just if you export images as Tiff files, some software combines the channels in an RGB color image and then your data is in most cases messed up and does not serve for analysis or reconstruction in 3Dscript.</p>\n<p>The z-slices are independent of the channels, so you will not loose anything.</p>\n<p>Premise is, that your data are existing as Hyperstack showing 1 separate slider for each individual dimension (C=channels, Z=volume, \u25ba=time)</p>"], "55634": ["<p>Dear all, Dear <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a>,<br>\nHello,</p>\n<p>I am using a Jython script to track cells. So, I have already done segmentation for the cells by a Python code and prepared a csv file of all the spots and also would like to import the csv file to TrackMate. I know it is possible to use \u201cTrackMate CSV importer\u201d manually, but I\u2019m interested to import my csv file to the Jython script and then track the cells automatically, since I have many image files for tracking cells, so automation is key.</p>\n<p>I have also used \u201cimport csv\u201d and could open the csv file in the Jython script and read the segmentation data (spots) there, but I could not find any good way to build the \u201cSpotCollection()\u201d.<br>\nI wonder if there is any command line for \u201c TrackMate CSV importer\u201d to be used inside a Jython script?</p>\n<p>Thanks a lot for any help/suggestions!</p>", "<p>Hello <a class=\"mention\" href=\"/u/setareh\">@Setareh</a></p>\n<p>I made this script:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/tinevez/TrackMate-CSVImporter/blob/6312f11b715a3946579e232ca1eec164dbef9592/scripts/CsvToTrackMate.py\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/tinevez/TrackMate-CSVImporter/blob/6312f11b715a3946579e232ca1eec164dbef9592/scripts/CsvToTrackMate.py\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/tinevez/TrackMate-CSVImporter/blob/6312f11b715a3946579e232ca1eec164dbef9592/scripts/CsvToTrackMate.py\" target=\"_blank\" rel=\"noopener\">tinevez/TrackMate-CSVImporter/blob/6312f11b715a3946579e232ca1eec164dbef9592/scripts/CsvToTrackMate.py</a></h4>\n\n\n    <pre><code class=\"lang-py\">\"\"\"\nA Jython script that parse arguments and uses them to configure a CSV to TrackMate \nimporter. \n\nThis script must be called from Fiji (to have everything on the class path), for instance\nin headless mode. Here is an example of a call from the command line:\n\n./ImageJ-macosx --headless   ../../../TrackMate-CSVImporter/scripts/CsvToTrackMate.py \n\t--csvFilePath=\"../../../TrackMate-CSVImporter/samples/data.csv\" \n\t--imageFilePath=\"../../../TrackMate-CSVImporter/samples/171004-4mins-tracking.tif\"\n\t --xCol=1 \n\t --radius=2 \n\t --yCol=2 \n\t --zCol=3 \n\t --frameCol=0\n\t --targetFilePath=\"../../../TrackMate-CSVImporter/samples/data.xml\"\n\"\"\"\n\nfrom fiji.plugin.trackmate.importer.csv import TrackMateImporter\nfrom java.io import File\n</code></pre>\n\n\n  This file has been truncated. <a href=\"https://github.com/tinevez/TrackMate-CSVImporter/blob/6312f11b715a3946579e232ca1eec164dbef9592/scripts/CsvToTrackMate.py\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>This is a good starting point but it was meant to be used from the command line.</p>\n<p>I could try to come up with an example this week-end, if you would be kind enough to send a CSV file and a small image for me to play with.</p>\n<p>Also, if you can wait for the end of the summer we will release a new version of TrackMate that can store segmentation results in 2D.</p>", "<p>Hi, <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a></p>\n<p>Thank you so much for your help.<br>\nSo by this script, let me do a bit more effort to prepare  a new script. Then I\u2019ll send it to you for your comments on that.</p>\n<p>Thanks a lot.</p>", "<p>Hello, <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a> <a class=\"mention\" href=\"/u/setareh\">@Setareh</a><br>\nCould you please share your new script, I\u2019m also stuck on this problem.<br>\nThanks a lot.</p>"], "12628": ["<p>Hey I\u2019m using a macbook pro mid 2012 running 10.1.4 with 16GB of RAM.  Every time I try to open the latest Stable (2.2.1)  release from April 18, 2016 I get this error message.<br>\n<img src=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/0/0c0a33efdefec211ae1877b382873db19a528a02.png\" width=\"422\" height=\"154\"></p>\n<p>This is what the console shows<br>\n28/04/2016 12:25:37.889 sharedfilelistd[401]: [default] [&lt;CFString 0x7fff7640de00 [0x7fff764c5440]&gt;{contents = \u201ccom.apple.LSSharedFileList.RecentApplications\u201d}] List write failed invalid info items: (null) properties: (null)<br>\n28/04/2016 12:25:37.889 sharedfilelistd[401]: -[ListStore writeListItems:properties:withListIdentifier:notificationHander:] [com.apple.LSSharedFileList.RecentApplications] List write failed invalid info items: (null) properties: (null)<br>\n28/04/2016 12:25:40.389 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.551 CellProfiler[9482]: Plugin directory doesn\u2019t point to valid folder: /Applications/CellProfiler.app/Contents/Resources/plugins<br>\n28/04/2016 12:25:40.591 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjvm<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjvm<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]: Failed to find libjvm<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjli<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjli<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to find libjli<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 274, in start_thread<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Exception: Javabridge failed to find JVM library<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to create Java VM<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/<strong>boot</strong>.py\u201d, line 355, in <br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:     _run()<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/<strong>boot</strong>.py\u201d, line 336, in _run<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     exec(compile(source, path, \u2018exec\u2019), globals(), globals())<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/CellProfiler.py\u201d, line 4, in <br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     cellprofiler.<strong>main</strong>.main()<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/cellprofiler/<strong>main</strong>.py\u201d, line 176, in main<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     cp_start_vm()<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/cellprofiler/utilities/cpjvm.py\u201d, line 167, in cp_start_vm<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     max_heap_size = heap_size)<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 312, in start_vm<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     raise RuntimeError(\u201cFailed to start Java VM\u201d)<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]: RuntimeError: Failed to start Java VM<br>\n28/04/2016 12:25:40.733 CellProfiler[9482]: CellProfiler Error<br>\n28/04/2016 12:25:40.733 CellProfiler[9482]: 2016-04-28 12:25:40.732 CellProfiler[9482:471412] CellProfiler Error</p>\n<p>I thought it was Java, so I updated it to Java SE Runtime Environment 8u92 downloaded from here <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html\" rel=\"nofollow noopener\">http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html</a> . It doesn\u2019t specify 64x for OSX like the others on the list but I believe it is. After I have done this, restarted the computer and everything I get the same error message.</p>\n<p>Any idea what is causing this?</p>\n<p>THanks</p>", "<p>I have the exact same problem on OS X 10.11.4 and 10.10.5 running JRE 1.8.91 and Python 2.7.10.</p>", "<p>We have other reports like this.  Though it works for some (me, for example).  It is possible you have two Java installations and the wrong one might being called(?).  Can you both please verify your Java version:</p>\n<ol>\n<li>open Terminal</li>\n<li>type \u201cjava -version\u201d (without the quotes)</li>\n</ol>\n<p>Mine reports this and CP 2.2.0 works on my Mac:</p>\n<blockquote>\n<p>java version \"1.8.0_60\"<br>\nJava\u2122 SE Runtime Environment (build 1.8.0_60-b27)<br>\nJava HotSpot\u2122 64-Bit Server VM (build 25.60-b23, mixed mode)</p>\n</blockquote>\n<p>Ensure that the first line has \u201c1.8\u201d and the last line says \u201c64-bit\u201d.  Paste the output here if you like.</p>", "<p>Here is what I get.</p>\n<p>java version \u201c1.6.0_65\u201d<br>\nJava\u2122 SE Runtime Environment (build 1.6.0_65-b14-468-11M4833)<br>\nJava HotSpot\u2122 64-Bit Server VM (build 20.65-b04-468, mixed mode)</p>\n<p>How can I get it to call the right version of Java?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0.png\" data-download-href=\"/uploads/short-url/mPcbPsnmwKKrBMhgaB4LbFDX9Zu.png?dl=1\" title=\"Screen Shot 2016-04-28 at 15.35.45.png\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0_2_503x500.png\" width=\"503\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0_2_503x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2016-04-28 at 15.35.45.png</span><span class=\"informations\">594\u00d7590 37.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When I look at JRE settings on the preferences panel, it shows 1.8, so I have no idea what\u2019s going on</p>", "<p>I also get Java version \u201c1.6.0_65\u201d but that\u2019s because we both have installed leagcy java for OS X which is required for other programs like Adobe Photoshop CS6 for example.</p>\n<p><a href=\"https://support.apple.com/kb/DL1572?locale=en_US\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://support.apple.com/kb/DL1572?locale=en_US</a></p>\n<p>I also have Java version  1.8.91 installed.</p>", "<p>That\u2019s right, I also have the legacy (version 6 I think?) for OSX installed, I think its required for older versions of CP.</p>", "<p>Okay, it works for me now. You need to install Java SE Development Kit (JDK)</p>\n<p>You can download it here:</p>\n<p><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>", "<p>Thanks henber! Totally worked!</p>", "<p><img src=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/c/c040fddb7269010bab814941b3d60708ddb5d49b.png\" width=\"451\" height=\"57\"></p>\n<p>Could possibly link to <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\" rel=\"nofollow noopener\">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a> in addition on the download page to help others out.</p>", "<p>No problem.</p>\n<p>Should actually say:<br>\nRequirements: Java SE Development Kit 8 (64-bit)<br>\nDevelopment kit also includes Runtime Enviroment.</p>\n<p>Have a nice day =)</p>", "<p>ava version \u201c19.0.2\u201d 2023-01-17</p>\n<p>Java\u2122 SE Runtime Environment (build 19.0.2+7-44)</p>\n<p>Java HotSpot\u2122 64-Bit Server VM (build 19.0.2+7-44, mixed mode, sharing)</p>", "<p>Hey I\u2019m using a macbook pro 2.3 GHz 8-Core Intel Core i9 Intel UHD Graphics 630 1536 MB ventura 13.2.1 .  Every time I try to install bioformats.  I get this error message</p>\n<p>(tuto-env) (base) titan@Titans-MacBook-Pro Documents % pip install python-bioformats<br>\nCollecting python-bioformats<br>\nUsing cached python_bioformats-4.0.7-py3-none-any.whl (40.6 MB)<br>\nCollecting python-javabridge==4.0.3<br>\nUsing cached python-javabridge-4.0.3.tar.gz (1.3 MB)<br>\nPreparing metadata (setup.py) \u2026 error<br>\nerror: subprocess-exited-with-error</p>\n<p>\u00d7 python setup.py egg_info did not run successfully.<br>\n\u2502 exit code: 1<br>\n\u2570\u2500&gt; [9 lines of output]<br>\nCould not find Java JRE compatible with x86_64 architecture<br>\nTraceback (most recent call last):<br>\nFile \u201c\u201d, line 2, in <br>\nFile \u201c\u201d, line 34, in <br>\nFile \u201c/private/var/folders/4_/n6nzlmcd4b719txgbffmwsgr0000gn/T/pip-install-73y8gsxo/python-javabridge_35e6b47ca93d435cb6c56bb3134a6689/setup.py\u201d, line 412, in <br>\next_modules=ext_modules(),<br>\nFile \u201c/private/var/folders/4_/n6nzlmcd4b719txgbffmwsgr0000gn/T/pip-install-73y8gsxo/python-javabridge_35e6b47ca93d435cb6c56bb3134a6689/setup.py\u201d, line 98, in ext_modules<br>\nraise Exception(\u201cJVM not found\u201d)<br>\nException: JVM not found<br>\n[end of output]</p>\n<p>note: This error originates from a subprocess, and is likely not a problem with pip.<br>\nerror: metadata-generation-failed</p>\n<p>\u00d7 Encountered error while generating package metadata.<br>\n\u2570\u2500&gt; See above for output.</p>\n<p>note: This is an issue with the package mentioned above, not pip.<br>\nhint: See above for details</p>", "<p>Looks like your env doesn\u2019t have java available? pip can\u2019t install that.<br>\nIt looks like you might be using conda to manage the env? if so, try to install openjdk from conda.</p>"], "78165": ["<p>I was wondering if it was possible to import registrations made using QuickNii into ABBA. The idea is to manually order the sections in QuickNii as I feel the UI is better for this, but take advantage of ABBA\u2019s automated affine and spline transforms, and ideally export back into VisuAlign for finetuning (I need very precise registrations). I guess this would rely on being able to coerce the quicknii json output into a form ABBA can interpret. Not sure if this is possible but seems like it should work, any suggestions or advice?</p>", "<p>Hello <a class=\"mention\" href=\"/u/paschamber\">@paschamber</a> and welcome to the forum!</p>\n<blockquote>\n<p>I was wondering if it was possible to import registrations made using QuickNii into ABBA.</p>\n</blockquote>\n<p>Not right now, but part of the code is already there: I\u2019ve written this for DeepSlice compatibility:</p>\n<aside class=\"onebox githubfolder\" data-onebox-src=\"https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h3><a href=\"https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii\" target=\"_blank\" rel=\"noopener\">ijp-imagetoatlas/src/main/java/ch/epfl/biop/quicknii at master \u00b7...</a></h3>\n\n  <p><a href=\"https://github.com/BIOP/ijp-imagetoatlas/tree/master/src/main/java/ch/epfl/biop/quicknii\" target=\"_blank\" rel=\"noopener\">master/src/main/java/ch/epfl/biop/quicknii</a></p>\n\n  <p><span class=\"label1\">2D sections to 3D atlas registration with Fiji and QuPath - ijp-imagetoatlas/src/main/java/ch/epfl/biop/quicknii at master \u00b7 BIOP/ijp-imagetoatlas</span></p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>If you have a demo dataset to share, I could try to see how feasible it is. (Note that ABBA is working optimally with multiresolution file formats. I do not know if quicknii is supporting that.)</p>\n<blockquote>\n<p>and spline transforms, and ideally export back into VisuAlign for finetuning</p>\n</blockquote>\n<p>Exporting splines transform for visualign is going to be more complex I\u2019m afraid.</p>\n<blockquote>\n<p>(I need very precise registrations)</p>\n</blockquote>\n<p>Did you have a look at BigWarp ? You can manually edit the spline registration already in ABBA, if that helps: next to last timestamp on youtube tutorial (<a href=\"https://www.youtube.com/watch?v=sERGONVw4zE&amp;t=2534s\" class=\"inline-onebox\">Aligning Serial Brain Sections to the Allen Brain Atlas with ABBA - YouTube</a>). I believe it\u2019s more of less the same functionality than VisuAlign.</p>\n<p>I opened an issue a while ago about compatibility with Quicknii, but closed it because nobody seemed to be interested so far. I\u2019ve reopened it (<a href=\"https://github.com/BIOP/ijp-imagetoatlas/issues/75\" class=\"inline-onebox\">Make QuickNII exporter for sending DeepSlice corrected dataset \u00b7 Issue #75 \u00b7 BIOP/ijp-imagetoatlas \u00b7 GitHub</a>).</p>\n<p>It\u2019s not on my priority list right now, but maybe I find some time in the coming months to look at it. If you can share a simple quicknii-like dataset, that would make things easier.</p>\n<p>Cheers,</p>\n<p>Nicolas</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>,</p>\n<p>Happy to share the data and would appreciate you looking at it.<br>\n<a href=\"https://wormhole.app/1R9qn#H7m_0vO7RUXfVvpwNTEiUg\" rel=\"noopener nofollow ugc\">Here is a link</a> to one dataset we already aligned (images downsampled by 10x) it contains images as well as json files from QuickNii (filebuilderoutput_aligned.json) and VisuAlign (\u2026_visualign.json). The sections are not the prettiest.<br>\nIf you meant a dataset that hasn\u2019t been aligned or if full resolution images would be more helpful I also have a few of those.</p>\n<p>The multiresolution file compatibility is one of really nice things about ABBA. I did look at BigWarp, but since I\u2019m already familiar with Visualign I was hesitant to jump in. But may just have to get used to it.</p>\n<p>Thanks for your help,<br>\nPascal</p>"], "78167": ["<p>I have some images generated with 2D numerical simulations that represent fractures. They often cross-cut each other like in this example:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/5/75dc65f1bcbff3346349845d61172f918849a1bd.png\" alt=\"image\" data-base62-sha1=\"gOE423r47f043yTZom2pPEdSwZn\" width=\"144\" height=\"186\"></p>\n<p>I want to perform a topological analysis: extract nodes and branches and get statistics about them. Basically, the type of data that <code>Analyze Skeleton (2D/3D)</code> gives. My workflow so far is as follows:</p>\n<ol>\n<li>Split channels,</li>\n<li>Keep the red channel and</li>\n<li>Convert to binary:</li>\n</ol>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/4322841eea2b0779212e409185e3dfb41f177fa5.png\" alt=\"image\" data-base62-sha1=\"9zTWkSGKn7rPM7jXdbTUYt0YLL7\" width=\"144\" height=\"186\"></p>\n<ol start=\"4\">\n<li>Apply <code>Morphological filters &gt; Closing</code> (with \u201cDisk\u201d as element shape) to close some holes and make my shape more regular:</li>\n</ol>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/5/0586c211cc9d9a48c2f468dc7dd0ce2ddaec82d2.png\" alt=\"image\" data-base62-sha1=\"MT6wrT0sOtqddM2S4wfydo2TWq\" width=\"144\" height=\"186\"></p>\n<ol start=\"5\">\n<li>At this point, I use <code>skeletonize</code>:</li>\n</ol>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/7/e7fbd9abdcd73612204fce3093e345384966df3c.png\" alt=\"image\" data-base62-sha1=\"x6dSHYlES610QWllLel6WybRfMo\" width=\"144\" height=\"186\"></p>\n<ol start=\"6\">\n<li>and run <code>Analyze Skeleton (2D/3D)</code>.</li>\n</ol>\n<p>I\u2019m quite happy with the type of data that I get this way (information about junctions, branches, etc). However, I don\u2019t like the way <code>skeletonize</code> deals with some of the junctions. For example, a human eye would treat the top one as a single junction with four branches, caused by the intersection of two lines going <em>top left</em> \u2192 <em>bottom right</em>  and  <em>top right</em> \u2192 <em>bottom left</em> respectively. Ideally, I\u2019d like to get this kind of skeleton:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2f88afd5313c6d5ed640bdb0ee8fe07e8026937.png\" alt=\"image\" data-base62-sha1=\"nfHQLalOmvEkRMEe3dNurbbM1Uj\" width=\"144\" height=\"186\"></p>\n<p><small>(branches and nodes sketched by hand).</small></p>\n<p>Any advice on how to get something like the last image? Maybe I need to pre-process my image better? Is there a way to simplify paths?</p>\n<p>Happy to give other tools a go. This <a href=\"https://github.com/danvk/extract-raster-network\" rel=\"noopener nofollow ugc\">python code</a> (<em>Extract Raster Network</em>) looks really interesting, for example. It looks like it can merge nodes that are close to each other.</p>\n<p>Thanks!</p>", "<p>Hi <a class=\"mention\" href=\"/u/juliafed\">@juliafed</a><br>\nI get this with the attached interactive macro.<br>\nAppreciate any feedback. Thanks in advance.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c.png\" data-download-href=\"/uploads/short-url/3Lg1sIYj8WGQjv7CNIAnasT4wO0.png?dl=1\" title=\"Selectionner_les_ points_de jonctions\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c_2_382x500.png\" alt=\"Selectionner_les_ points_de jonctions\" data-base62-sha1=\"3Lg1sIYj8WGQjv7CNIAnasT4wO0\" width=\"382\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c_2_382x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c_2_573x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a5e19b9b768e1a1a15481c17aa8dfef9944476c.png 2x\" data-dominant-color=\"141212\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Selectionner_les_ points_de jonctions</span><span class=\"informations\">735\u00d7960 70 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-auto\">macro \"How to capture two cross-cutting lines \"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\n//--------------------------------\n// Start processing\nimg=getImageID();\nnewImage(\"1\", \"RGB black\", 154, 190, 1);\nselectImage(img);\n//---------------------------------\n//rectify the luminescence and enlarge study image\nrun(\"Duplicate...\", \"title=temp\");\nrun(\"Polynomial Shading Corrector\", \"degree_x=2 degree_y=2 regularization=2\");\nrun(\"Select All\");\nrun(\"Copy\");\nclose(\"temp\");\nselectWindow(\"1\");\nrun(\"Paste\");\nrun(\"Select None\");\n//---------------------------------\n// Locate the object of study\nrun(\"Unsharp Mask...\", \"radius=5 mask=0.60\");\nrun(\"Minimum...\", \"radius=4\");\nrun(\"Maximize\");\nrun(\"Find Maxima...\", \"prominence=20 output=[Point Selection]\");\nroiManager(\"Add\");\nroiManager(\"Show None\");\nrun(\"8-bit\");\nrun(\"Ridge Detection\", \"line_width=3.5 high_contrast=230 low_contrast=87 extend_line method_for_overlap_resolution=NONE sigma=1.51 lower_threshold=3.06 upper_threshold=7.99 minimum_line_length=5 maximum=1000\");\nroiManager(\"Show All\");\n//---------------------------------\nrun(\"Set Measurements...\", \"display redirect=None decimal=2\");\nroiManager(\"Measure\");\nTable.sort(\"X\");\n//---------------------------------\nfor ( i = nResults() - 1; i &gt;= 0; i--) {\nselectWindow(\"1\");\nTable.setSelection(i,i);\n//setTool(\"multipoint\");\nrun(\"Point Tool...\", \"type=Circle color=Red size=XXXL label counter=0\");\nmakePoint(getResult(\"X\",i),getResult(\"Y\",i), \"extra large red circle\");\nmsg=\"Look at the red circle.\\nShould the point be taken account?\";\nif(getBoolean(msg) == 1){\n//setTool(\"multipoint\");\nrun(\"Point Tool...\", \"type=Circle color=White size=XXXL label counter=0\");\nmakePoint(getResult(\"X\",i),getResult(\"Y\",i), \"extra large white circle\");\nroiManager(\"Add\");}\nelse\nTable.deleteRows(i, i) ;\n}\nupdateResults();\n//---------------------------------\nroiManager(\"Select\", 0);\nroiManager(\"Delete\");\nroiManager(\"Show None\");\nroiManager(\"Show All\");\n// End of processing\n//--------------------------------\nexit();\n}type or paste code here\n</code></pre>\n<p>From the results you can draw your lines.</p>\n<p>An alternative for your situation is to use openCv:</p><aside class=\"onebox stackexchange\" data-onebox-src=\"https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc\">\n  <header class=\"source\">\n\n      <a href=\"https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc\" target=\"_blank\" rel=\"noopener nofollow ugc\">stackoverflow.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https://stackoverflow.com/users/3168934/jo%c3%a3o-david\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img alt=\"Jo&amp;#227;o David\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/a/da9e578f5f7eaf2f5ffd37624d2279032e5f88f5.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n  </a>\n\n<h4>\n  <a href=\"https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc\" target=\"_blank\" rel=\"noopener nofollow ugc\">How to find the junction points or segments in a skeletonized image Python OpenCV?</a>\n</h4>\n\n<div class=\"tags\">\n  <strong>python, image, opencv, image-processing</strong>\n</div>\n\n<div class=\"date\">\n  asked by\n  \n  <a href=\"https://stackoverflow.com/users/3168934/jo%c3%a3o-david\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    Jo&amp;#227;o David\n  </a>\n  on <a href=\"https://stackoverflow.com/questions/72164740/how-to-find-the-junction-points-or-segments-in-a-skeletonized-image-python-openc\" target=\"_blank\" rel=\"noopener nofollow ugc\">08:10PM - 08 May 22 UTC</a>\n</div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I like having feedback.</p>", "<p>To make a long story short, the problem is that the orientation detection filters have a hard time resolving the orientations at the junction. The filters need to be longer in order to resolve the orientation.</p>\n<p>I developed an adaptive method below, but it may be doing too much.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/mkitti/AdaptiveResolutionOrientationSpace\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/mkitti/AdaptiveResolutionOrientationSpace\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/5/b5f8d9a55399b13b070327c2f4e0277b60bec701.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://github.com/mkitti/AdaptiveResolutionOrientationSpace\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - mkitti/AdaptiveResolutionOrientationSpace: Adaptive Resolution...</a></h3>\n\n  <p>Adaptive Resolution Orientation Space performs multi-orientation analysis across angular resolutions to segment complex filamentous networks - GitHub - mkitti/AdaptiveResolutionOrientationSpace: Ad...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>See the following article for details:<br>\nKittisopikul M, Vahabikashi A, Shimi T, Goldman RD, Jaqaman K. Adaptive multiorientation resolution analysis of complex filamentous network images. Bioinformatics. 2020 Dec 22;36(20):5093-5103. doi: 10.1093/bioinformatics/btaa627. PMID: 32653917; PMCID: PMC8453242.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pubmed.ncbi.nlm.nih.gov/32653917/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce19e2f5f546bf5a36c511a7b75edc28c9d99197.png\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https://pubmed.ncbi.nlm.nih.gov/32653917/\" target=\"_blank\" rel=\"noopener nofollow ugc\">PubMed</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_500x500.png\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86cd93ed10e93506e53abf8d9e8a573741580fed_2_1000x1000.png 2x\" data-dominant-color=\"3975AA\">\n\n<h3><a href=\"https://pubmed.ncbi.nlm.nih.gov/32653917/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Adaptive multiorientation resolution analysis of complex filamentous network...</a></h3>\n\n  <p>Supplementary information is available at Bioinformatics online.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hi <a class=\"mention\" href=\"/u/markkitt\">@markkitt</a><br>\nNice documentation. Thank you for these links.</p>"], "43352": ["<p>Hi all!<br>\n\u201cGraphical abstract\u201d\u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/481b6574a7f67b23fb9e3fa1b1f8a13fd6fccfe1.png\" data-download-href=\"/uploads/short-url/ahT4BdAEBjGyEsU1oyZpKCtHL5T.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/481b6574a7f67b23fb9e3fa1b1f8a13fd6fccfe1_2_690x366.png\" alt=\"image\" data-base62-sha1=\"ahT4BdAEBjGyEsU1oyZpKCtHL5T\" width=\"690\" height=\"366\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/481b6574a7f67b23fb9e3fa1b1f8a13fd6fccfe1_2_690x366.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/481b6574a7f67b23fb9e3fa1b1f8a13fd6fccfe1_2_1035x549.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/481b6574a7f67b23fb9e3fa1b1f8a13fd6fccfe1.png 2x\" data-dominant-color=\"CECCCE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1315\u00d7699 662 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>New post on cluster/neighborhood analysis, though I will mostly be focusing on the clustering aspect. The focus here is on using <a href=\"https://gitlab.com/gernerlab/cytomap/-/tree/master/StandaloneInstaller\">CytoMAP</a>, a great MATLAB (but does not require a paid MATLAB license, so no fear! ) based tool that can be used in conjunction with <a href=\"https://qupath.readthedocs.io/en/latest/\">QuPath</a> to perform some great analysis, while still being fairly straightforward to use. I think it is particularly amazing how powerful the analyses can be with entirely open source solutions (and there may be other options, all of this can likely be done in R, or possibly HistoCAT, though I am less familiar with those options. Someone else can create those guides!).</p>\n<p>That said, there is one script involved this time (<strong>QP 0.2.2+</strong>), but it will not be too bad, I promise! The basic steps are these:</p>\n<ol>\n<li>\n<a href=\"https://forum.image.sc/t/there-and-back-again-a-guide-to-qupath-and-cytomap-cluster-analysis/43352/2\">Generate some objects in QuPath that have measurements.</a> These measurements have to be meaningful, or else the cluster analysis will not generate good results. Garbage in, garbage out. Or in more concrete terms, if you use H-DAB color measurments on an H&amp;E image, do not expect your cluster results to be any good.</li>\n</ol>\n<details>\n<summary>\nImage: Well, some of these cell borders certainly do not look right</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/93c90e86c3756720f2f18701e0fd62eb49425efd.png\" alt=\"image\" data-base62-sha1=\"l5mWUJvYIScKQtIYPxEeJciuy6h\" width=\"624\" height=\"470\"><br>\nThe data from the cytoplasmic and cell measurements will certainly not be reflective of the true cells in all cases.</p>\n</details>\n<ol start=\"2\">\n<li>Export these measurements through the handy-dandy <a href=\"https://qupath.readthedocs.io/en/latest/docs/tutorials/exporting_measurements.html#via-the-measurement-exporter\"><em>Measure-&gt;Export measurements</em></a> interface, <a href=\"https://qupath.readthedocs.io/en/latest/docs/tutorials/exporting_measurements.html#via-scripting\">or write a script</a>.</li>\n<li>\n<a href=\"https://forum.image.sc/t/there-and-back-again-a-guide-to-qupath-and-cytomap-cluster-analysis/43352/3\">Run <strong>CytoMAP 1.4.19 or later</strong> and import</a> those measurements right on in!</li>\n<li>\n<a href=\"https://forum.image.sc/t/there-and-back-again-a-guide-to-qupath-and-cytomap-cluster-analysis/43352/4\">Create clusters, tSNE plots</a>, a little bit of neighborhood analysis, whatever it is you want to do.</li>\n<li>\n<a href=\"https://forum.image.sc/t/there-and-back-again-a-guide-to-qupath-and-cytomap-cluster-analysis/43352/5\">Export the results from CytoMAP</a>.</li>\n<li>Use a script to import the results from CytoMAP back into the correct images and objects in QuPath.</li>\n<li>Use the Measurement Maps or perhaps some other tools to <a href=\"https://forum.image.sc/t/there-and-back-again-a-guide-to-qupath-and-cytomap-cluster-analysis/43352/6\">inspect the results of your analysis within the original images</a>.</li>\n</ol>\n<p>The fact that you still have to use CSV files as a go between for the two programs might be somewhat off-putting, but it also nicely gives you a \u201chard copy\u201d of the results that you could use in other analysis software.</p>\n<p>One more update - the same workflow can be used for other CSV based transfers:</p>\n<aside class=\"quote no-group\" data-username=\"Sasha_Tkachev\" data-post=\"41\" data-topic=\"43352\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/sasha_tkachev/40/37175_2.png\" class=\"avatar\"> Sasha Tkachev:</div>\n<blockquote>\n<p>Thanks so much for putting this together <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> ! As a comment, this approach also works quite well when doing an analysis in <a href=\"https://giottosuite.readthedocs.io/en/latest/\">Giotto Suite R package</a> or in <a href=\"https://squidpy.readthedocs.io/en/stable/\">Squidpy</a>, so there is a choice of ST tools to use!</p>\n</blockquote>\n</aside>\n<p>Plus <a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/43\">code for converting objects in R</a>.</p>", "<p>Alright, let\u2019s begin! I will be using a demo project similar to the last one that I hosted, and can be found here:<br>\n<a href=\"https://drive.google.com/file/d/1CiybTDId-XdflNcttiCq-UxNPz4se3UX/view?usp=sharing\">Updated Google Drive Link for QuPath 3.2</a></p>\n<details>\n<summary>\nOld link - V0.2.3</summary>\n<p><s><a href=\"https://drive.google.com/file/d/1uVJN-poGAsgVJl8i-DzbDA6rSxRCbrBw/view?usp=sharing\">https://drive.google.com/file/d/1uVJN-poGAsgVJl8i-DzbDA6rSxRCbrBw/view?usp=sharing</a></s></p>\n</details>\n<p>It includes:<br>\nThe LuCa image from<br>\nData file with some cells and measurements<br>\nSeveral scripts including the Import from CytoMAP script, and a visualization script.<br>\nSeveral csv files representing results at different stages of the guide.<br>\nA folder of colormaps that can be used by adding them to your User directory/colormaps folder (future post)</p>\n<p>You will want <a href=\"https://github.com/qupath/qupath/releases\">QuPath 0.2.2 or later installed</a>. I specifically <a href=\"https://qupath.readthedocs.io/en/latest/docs/advanced/stardist.html?highlight=stardist#stardist\">used a version with Tensorflow</a> so that I could use StarDist to generate my cell outlines, but that is not necessary. You will also need to have <a href=\"https://gitlab.com/gernerlab/cytomap/-/wikis/Installation-Guide\">CytoMAP 1.4.9 or later installed, which can be found here</a>.</p>\n<h3>\n<a name=\"h-1-generate-some-data-containing-objects-to-analyze-gigo-warning-1\" class=\"anchor\" href=\"#h-1-generate-some-data-containing-objects-to-analyze-gigo-warning-1\"></a>1.\tGenerate some data containing objects to analyze. GIGO warning.</h3>\n<p>Well, the objects have already been generated in the demo project, and there is only one image, but I will be treating this demo as if there are multiple images that are part of a project, and my recommendations about what to export will be based on the assumption that everything is run for a multi-image project. It works just fine on a single image, though.</p>\n<p>Remember that the measurements generated must be meaningful; while we are essentially treating these values as flow cytometry data, they very much are not flow cytometry data! Mean intensities will be inaccurate due to not tracing the outline of the cell exactly, and will be impacted by how exactly the cell was sliced in the tissue section. Take this into consideration with your conclusions! You may have better luck generating intensity sums per cell than means if you need a large cytoplasmic expansion.</p>\n<h3>\n<a name=\"h-2-export-the-measurements-2\" class=\"anchor\" href=\"#h-2-export-the-measurements-2\"></a>2.\tExport the measurements</h3>\n<p>I will show the <em>Measure-&gt;Export measurements</em> dialog here, but if you are script savvy, you can also script this.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1aebae0cf3d63e3ea863520e1fc4bd30db48a900.png\" data-download-href=\"/uploads/short-url/3Q9m4Gjctbhy1QqnZkZUbzgppsY.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1aebae0cf3d63e3ea863520e1fc4bd30db48a900.png\" alt=\"image\" data-base62-sha1=\"3Q9m4Gjctbhy1QqnZkZUbzgppsY\" width=\"437\" height=\"375\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1aebae0cf3d63e3ea863520e1fc4bd30db48a900_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">594\u00d7509 16.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nMake sure all of the files you want to export are on the right side in the Selected window.<br>\n<strong>Output file</strong>: Name your csv file.<br>\n<strong>Export type</strong>: Choose Cells or Detections, usually.<br>\n<strong>Separator</strong>: I have only used CSV.<br>\n<strong>Columns to include(Optional)</strong>: While this says optional, I would STRONGLY recommend it be mandatory for you, in this case. You really do not want to scroll through dozens of measurements you will never use every time you want to do something in CytoMAP. Worse, many of those measurements would be harmful to your analysis if you simply \u201cinclude everything.\u201d<br>\nClick Populate, and then include at least these base measurements.</p>\n<details>\n<summary>\nColumns necessary to include</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1238ba1f9c4eb96bed1910482fa7e4999396b35e.png\" alt=\"image\" data-base62-sha1=\"2Bc89SetDRu92G3IPmZEF9361hs\" width=\"477\" height=\"288\"></p>\n</details>\n<p>After those, you can include any additional measurements you want your clustering to be based off of. If you want to do neighborhood analysis based off of Classes you created in QuPath, include \u201cClass\u201d as well.<br>\nFor the exported CSV you will find in the demo folder, you can see that I included some Nucleus size/shape measurements, and one measurement per signal channel, based on the localization of that signal (either nuclear or cytoplasmic).</p>\n<h2>\n<a name=\"warning-3\" class=\"anchor\" href=\"#warning-3\"></a>Warning!!!</h2>\n<details>\n<summary>\nIf you upgrade to version 1.4.19+, you should no longer have to worry about this warning - Null values can be removed during the CytoMAP import process</summary>\n<p>At this step, I would always check the csv file for any missing values. CytoMAP does not handle those well at all, especially when you try to Standardize your data. As in it will hang and leave you wondering what has gone wrong. I just ran into the issue again, so I wanted to put an extra warning here. My steps to resolve were to open the .csv file in Excel, use CTRL+SHIFT+arrow keys to select the entire data set, then CTRL+F to get to Find. Then use \u201cReplace\u201d instead to replace all instances of nothing (do not put anything into the \u201cFind what\u201d field) with 0.</p>\n</details>\n<h1>\n<a name=\"double-warning-4\" class=\"anchor\" href=\"#double-warning-4\"></a>Double Warning!!</h1>\n<p>If running neighborhood analysis downstream, you MUST EXPORT INDIVIDUAL CSV FILES PER IMAGE. You will need to use scripting for this as of QuPath 0.2.3, as it will default to exporting one giant CSV file with the image in the first column.<br>\n<a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/10\">There is now a script to do this here, though you will want to edit the columns that are included!!</a></p>\n<p>TMA core analysis: Export per-Core CSV files <a href=\"https://forum.image.sc/t/exporting-mif-images-and-metadata-from-tma/67849/4\" class=\"inline-onebox\">Exporting mIF images and metadata from TMA - #4 by Mike_Nelson</a></p>\n<p>I recommend looking through <a href=\"https://qupath.readthedocs.io/en/latest/docs/tutorials/exporting_measurements.html#via-scripting\" class=\"inline-onebox\">Exporting measurements \u2014 QuPath 0.3.0 documentation</a><br>\nand for an example of such a script: <a href=\"https://forum.image.sc/t/qupath-saving-filtered-detectiontable-with-measurementexporter-for-current-image-only/41328/2\" class=\"inline-onebox\">QuPath: saving filtered DetectionTable with MeasurementExporter for current image only? - #2 by Research_Associate</a></p>", "<h3>3.\tRun CytoMAP</h3>\n<p>(<a href=\"https://gitlab.com/gernerlab/cytomap/-/wikis/Installation-Guide\" rel=\"noopener nofollow ugc\">or install</a>. The first time you install, you may need to download the MATLAB Runtime (~3GB) if you do not already have a MATLAB installation. Further updates to CytoMAP are incredibly fast and only take a few seconds)<br>\nThe interface should look something like this, showing the import option.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4fefd443073cbf1fb37eae55f2fff1914dcf058f.png\" data-download-href=\"/uploads/short-url/bp9ycUeEG6WzPOFGBbo2vamaf7h.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4fefd443073cbf1fb37eae55f2fff1914dcf058f_2_183x375.png\" alt=\"image\" data-base62-sha1=\"bp9ycUeEG6WzPOFGBbo2vamaf7h\" width=\"183\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4fefd443073cbf1fb37eae55f2fff1914dcf058f_2_183x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4fefd443073cbf1fb37eae55f2fff1914dcf058f_2_274x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4fefd443073cbf1fb37eae55f2fff1914dcf058f_2_366x750.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4fefd443073cbf1fb37eae55f2fff1914dcf058f_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">425\u00d7864 103 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div><br>\nSo far I have only tested the following on Windows 10 and 7 systems, but MATLAB seems to be an option for Windows, Mac and Linux.<br>\nOnce you complete the Import (or Load .csv or .mat in this case) nothing happens as it gets stuck at:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff4a0a04557202f650385f164c449c7775294667.png\" alt=\"image\" data-base62-sha1=\"AqonKvm3S5Dsbf7CPUpclkCRTJJ\" width=\"282\" height=\"85\"><br>\nOops. Apparently CytoMAP doesn\u2019t like the name of my demo file! If CytoMAP does not handle certain characters or combinations of characters, it will simply hang, so check the CPU usage if you run into loading bars that are not moving to make sure CytoMAP is working. I am not exactly sure what broke in this case, but replacing the name on the right with the name on the left solves the problem. If you do not have weirdly named files, this likely will not be an issue for you, as it has not been a problem for me in previous projects.</p>\n<details>\n<summary>\nImage: Fixing the oopsie</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d38d54d3dd142ec4c19a325fbb880135754a410.png\" alt=\"image\" data-base62-sha1=\"k9j5rYIG08rY2zx7nTrA6juc2qs\" width=\"624\" height=\"131\"></p>\n</details>\n<p>There are no two copies of CellData.csv in the demo folder, I will be using CellData2.csv for the rest of this guide as it has the adjusted image name.</p>\n<p>Next step, select the X,Y and Z coordinates for your objects. Selecting these is not terribly important unless you want to view your data in CytoMAP \u201cas if it were in an image.\u201d Since the main purpose of this for me has been viewing the data in QuPath, I do not worry about it much. Do choose values with numerical entries though. For Z there is a default \u201cDo not use Z\u201d option you can leave selected.</p>\n<details>\n<summary>\nImage: CytoMAP X and Y selection on import</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73543f8baa53940a60031dbe14a832cc7621bcba.png\" data-download-href=\"/uploads/short-url/gsfpqo9ZpwJ2Q8bP41Xqiv0FZXA.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73543f8baa53940a60031dbe14a832cc7621bcba.png\" alt=\"image\" data-base62-sha1=\"gsfpqo9ZpwJ2Q8bP41Xqiv0FZXA\" width=\"172\" height=\"375\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73543f8baa53940a60031dbe14a832cc7621bcba_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">299\u00d7649 12.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n</details>\n<p>In the next popup, select Load in the lower left.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/845306fed4befc6ed3198e5e3a797708871dc816.png\" alt=\"image\" data-base62-sha1=\"iSAUv8baHGrHQ41mzW601oUE6z4\" width=\"171\" height=\"59\"><br>\nNow CytoMAP returns to its default look, though with no indication the data is loaded. If you have not gotten stuck on any green partial loading bars, you should be good to proceed.</p>", "<h3>4.\tNow, on to analysis in CytoMAP.</h3>\n<p>I recommend going through <a href=\"https://gitlab.com/gernerlab/cytomap/-/wikis/home\" rel=\"noopener nofollow ugc\">the documentation here</a>, look at the options on the right side of the screen. While I intended this to be a few quick steps, it got a bit out of control.</p>\n<p>If you classified your cells in QuPath, you can use the \u201cAnnotate Clusters\u201d button to tell CytoMAP that it should treat that \u201cClass\u201d entry in your CSV as a list of clusters. One of the great recent changes is that even if you have 30+ classes, it will autofill them all for you!</p>\n<p>I will jump straight into the two options I use most \u201cCluster Cells\u201d and \u201cDimensionality Reduction.\u201d</p>\n<h2>Cluster Cells</h2>\n<p>When you first select Cluster Cells, you will get an interface like this.</p>\n<details>\n<summary>\nImage+description:Cluster cells interface</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc5e08e93040818ec43350c08a681519dcfee4dc.png\" data-download-href=\"/uploads/short-url/t9US6lwTlkReIiAxZvy7Ck0US16.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc5e08e93040818ec43350c08a681519dcfee4dc_2_597x500.png\" alt=\"image\" data-base62-sha1=\"t9US6lwTlkReIiAxZvy7Ck0US16\" width=\"597\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc5e08e93040818ec43350c08a681519dcfee4dc_2_597x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc5e08e93040818ec43350c08a681519dcfee4dc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc5e08e93040818ec43350c08a681519dcfee4dc.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc5e08e93040818ec43350c08a681519dcfee4dc_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7522 52.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Select All/All in <em>Use for Sorting</em> in the upper left. If you do not, CytoMAP will try to cluster zero cells. It does not work well.<br>\nNext select the measurements you want to cluster based on, and adjust any Weights (half hidden column \u201cWei\u2026\u201d) for measurements you think are more important. Be careful with this. Great power, great responsibility, all that. I would not include any positional data like the XYZ coordinates, or any strings in this case (maybe string data should not show up here?).</p>\n<p>At the bottom, starting from the left, I usually select \u201cStandardize\u201d rather than Raw MFI (mean fluorescent intensity), as I do not want to weight the results based on my brightest channels.<br>\n<strong>Normalize</strong> as is appropriate for your project, if you are not sure, the forum should be a good place to ask! I have never altered Color scheme.<br>\n<strong>Number of Regions</strong> is very important, and you can either use one of the built in auto-detection algorithms, or select your own. I generally start with the built in options, and then increase the number of base clusters to study what happens, and where the algorithm finds differences in the data. There is no single right answer; there are many right and wrong answers, however. Depending on how you normalized, you may find that clusters form \u201cper sample\u201d due to variations in staining, for example. You may want to merge those clusters for downstream analysis (overclustering).<br>\n<strong>Input Data Type</strong>: For the moment, use individual cells. Explore the other options once you are comfortable generating neighborhood data.<br>\n<strong>Select Algorithm</strong>: I have not explored the variations in these quite enough to really say when one in particular should be used. Try them all!</p>\n<h2>Huge note that I missed earlier, there is a menu at the top where you can File-&gt;Save or Load model.</h2>\n</details>\n<p>Example first run.</p>\n<details>\n<summary>\nImage+description:Cluster cells first run</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7e59158fd8ef81ad79853745f7b7977d1bc2732.png\" data-download-href=\"/uploads/short-url/qePdS7Ui7CKVCYYuiiMqpP7EJ5U.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e59158fd8ef81ad79853745f7b7977d1bc2732_2_597x500.png\" alt=\"image\" data-base62-sha1=\"qePdS7Ui7CKVCYYuiiMqpP7EJ5U\" width=\"597\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e59158fd8ef81ad79853745f7b7977d1bc2732_2_597x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7e59158fd8ef81ad79853745f7b7977d1bc2732.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7e59158fd8ef81ad79853745f7b7977d1bc2732.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e59158fd8ef81ad79853745f7b7977d1bc2732_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7522 51.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n</details>\n<p>I usually name the model (the results) after the settings I used. The model above, for example, was called \u201cStandardize DB NN 1\u201d. If you do not include information about the settings you use, I do not believe they are stored anywhere else, so it can be hard to replicate an analysis on another or larger data set.<br>\nOnce you run it, this may take some time, so check your CPU utilization if you are worried that the program has stopped.<br>\nIn Windows:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/60837d825910cf509d64d3d16206527c6d3e28af.png\" alt=\"image\" data-base62-sha1=\"dLNykGmndoCNBZvFvH9yplAkQ2z\" width=\"312\" height=\"94\"><br>\nIf you chose to allow an automatic detection of the number of clusters, you should get both some sort of plot and a Figure on completion of the analysis.<br>\nThere is way too much to go over here, so I recommend the official documentation, but I want to point out that the Y axis is inverted as QuPath starts Y values from 0 at the top of the image.</p>\n<details>\n<summary>\nImage: Cluster analysis results</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e660b422a417a054e18d587dd5ca27a7acbb68e6.png\" alt=\"image\" data-base62-sha1=\"wS10iy3PfkygOG2YKqp6J3fd0z4\" width=\"624\" height=\"258\"></p>\n</details>\n<p>In the Options in the lower left of the Figure, you can Invert the Y axis. The \u201cC-Axis\u201d options are your color map (which can also be changed in the Options).<br>\nThe cluster analysis we just ran results in:</p>\n<details>\n<summary>\nImage: Cluster analysis results colormap</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/2/62837db26812c82973c721ac4e0449c29c15f2cd.png\" alt=\"image\" data-base62-sha1=\"e3uvDvXo5BGYFPcblXPANIN4EmF\" width=\"624\" height=\"447\"></p>\n</details>\n<p>Ok. So. That is a thing. I feel like an optometrist is about to ask me which number is hidden in the picture. But, we can at least see that some of the clusters align with structures from the original image. What does it mean though? Well, thankfully, there are heatmaps! Go back to the original CytoMAP window and select <em>Extensions-&gt;cell_heatmaps.m</em><br>\nSince this is going a bit long, I will simply show the options I selected, and feel free to ask if you have any questions!</p>\n<details>\n<summary>\nImage: Heatmap options</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/244184e43b36fd450f085367369037ae58e101a0.png\" data-download-href=\"/uploads/short-url/5aJyU56g5xkgLiyqGQWCt2ZnS2Q.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/244184e43b36fd450f085367369037ae58e101a0_2_602x500.png\" alt=\"image\" data-base62-sha1=\"5aJyU56g5xkgLiyqGQWCt2ZnS2Q\" width=\"602\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/244184e43b36fd450f085367369037ae58e101a0_2_602x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/244184e43b36fd450f085367369037ae58e101a0.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/244184e43b36fd450f085367369037ae58e101a0.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/244184e43b36fd450f085367369037ae58e101a0_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7518 48.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n</details>\n<p>And the result is in!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d16a9275518c349844b6d105d11789168f5259e8.png\" data-download-href=\"/uploads/short-url/tSA7v7R9ROhpJqBVonIW72hLSwU.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d16a9275518c349844b6d105d11789168f5259e8_2_421x375.png\" alt=\"image\" data-base62-sha1=\"tSA7v7R9ROhpJqBVonIW72hLSwU\" width=\"421\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d16a9275518c349844b6d105d11789168f5259e8_2_421x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d16a9275518c349844b6d105d11789168f5259e8.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d16a9275518c349844b6d105d11789168f5259e8.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d16a9275518c349844b6d105d11789168f5259e8_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7555 75.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Right off, I can identify two clusters that are heavy in CK which represent the majority of the tumor area (6 and 9 and green and red in the previous XY map). The primary difference between the two seems to be the size of the nucleus. Is that useful? I am not sure, ask the biologists! However, we could include other measurements from QuPath at this point, like nearest CD8+ cell, or similar distance based measurements, which might tease out more differences between the clusters. <em>You can include data on the heatmap that was not used in the generation of the clusters!</em><br>\nA couple of other things that jumped out: The CD8 and FOXP3 clusters are relatively rare compared to most other clusters and PDL1 seems mostly associated with the CD68 positive cells, both of which might tell us something about the tumor microenvironment in this case.</p>\n<h2>Dimensionality Reduction, tSNE plots and manual gating!</h2>\n<p>I chose similar settings for the quick Dimensionality Reduction example as for the cluster analysis. There are also other options like PHATE, but I only included tSNE here for, hah, \u201cbrevity.\u201d</p>\n<p>Note that I did exclude the cluster analysis results, as I did not want them to bias the tSNE plot.<br>\nSettings choices for tSNE:</p>\n<details>\n<summary>\nImage: tSNE options</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb021f83dfba005613b3407cfa0799310978e223.png\" data-download-href=\"/uploads/short-url/sXTtsNKqenyaqzgrJWJsc18OQan.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb021f83dfba005613b3407cfa0799310978e223_2_601x500.png\" alt=\"image\" data-base62-sha1=\"sXTtsNKqenyaqzgrJWJsc18OQan\" width=\"601\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb021f83dfba005613b3407cfa0799310978e223_2_601x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb021f83dfba005613b3407cfa0799310978e223.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb021f83dfba005613b3407cfa0799310978e223.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb021f83dfba005613b3407cfa0799310978e223_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7519 48.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n</details>\n<p>From the MATLAB implementation: <a href=\"https://www.mathworks.com/help/stats/tsne.html#bvh3rti-2\" rel=\"noopener nofollow ugc\">https://www.mathworks.com/help/stats/tsne.html#bvh3rti-2</a><br>\nFun reading about tSNE variables (thanks <a class=\"mention\" href=\"/u/cstoltzfus\">@cstoltzfus</a> !): <a href=\"https://distill.pub/2016/misread-tsne/\" rel=\"noopener nofollow ugc\">https://distill.pub/2016/misread-tsne/</a></p>\n<p>I chose the default settings and, after a short wait, obtained the following plot.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/040a2c88cf30f7672b5030d20ce00638935d4399.png\" alt=\"image\" data-base62-sha1=\"zJHRC6TAVRNxrxDGrTNmgsWiKZ\" width=\"468\" height=\"342\"><br>\nA tSNE plot of QuPath data! Dreams do come true!</p>\n<p>The one change I did make was changing the color axis to the previously discovered clusters, so I could see how they matched up. I can see clusters 9 and 6 on the left, which would be the tumor region, and a fairly well separated cluster 5 on the right. Cluster 5 was my FOXP3 cells, but why are there cluster 2, 3 and 4 cells mixed in? I wonder where and what those are?</p>\n<p>That is where <strong>Gating</strong> comes in. There are two ways I could go about this, either selecting the entire cluster on the right, or trying to grab only the non-cluster 5 cells in that tSNE cluster. I will simply create a gate or two and bring that data back into QuPath.</p>\n<details>\n<summary>\nFirst, click Show Table and change All/All to All Cells, as shown. Then Exit Table and use the annotation options to draw some gates.</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/c/2c3c2f419eca666b816ded7f643c9456ca37185c.png\" alt=\"image\" data-base62-sha1=\"6jjX7Dz3mzC2qk96KTEUFJZo6PW\" width=\"647\" height=\"473\"></p>\n</details>\n<details>\n<summary>\nThe useful buttons, from left to right:</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/1/014a4df958b19fd435b8a18995f9e32c02a09b68.png\" alt=\"image\" data-base62-sha1=\"bpFO3m8CgRft1DvRyiHLvfezck\" width=\"183\" height=\"45\"><br>\n<strong>Clear plot.</strong><br>\n<strong>Refresh image</strong>: This is useful if you run a new cluster analysis in another window, and want to view those results in your tSNE plot.<br>\n<strong>Square gate</strong>: Draw a square gate.<br>\n<strong>Polygon gate</strong>: Draw a polygon gate, close it by clicking on the first point a second time.<br>\n<strong>Save Last Chosen Gate</strong>: This is very important. It does not save ALL gates, it only saves the last gate selected. But if you never save the gates, you cannot do anything with the gates, like import them back into QuPath.</p>\n</details>\n<p>When you click on a button to create a gate, it will first ask you to name it.</p>\n<details>\n<summary>\nThis is my gate. There are many like it, but this one is mine</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b08560f39b4d38395e43e2a1b822ad9094e44be.png\" alt=\"image\" data-base62-sha1=\"jPWeayN4ZhmXOt7WIlIp8z3Gzdc\" width=\"198\" height=\"129\"></p>\n</details>\n<p>The cursor changes to a crosshairs, and you can start clicking to draw your gate.<br>\nEdges can be tricky and I recommend setting your first point far away from an edge. After completing your gate, you can always drag it around to capture points near the edge of the image. Dragging the gate will \u201cpush\u201d the edge.</p>\n<details>\n<summary>\nFinal gates; I also created a CK gate.</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/0960f458d05172e9ba780b22d34408a6e1355043.png\" alt=\"image\" data-base62-sha1=\"1kY1jGqIsBSC6f5TpKVEJ2Iolc7\" width=\"624\" height=\"431\"></p>\n</details>\n<p>A quick check of the heatmaps for those gates.</p>\n<details>\n<summary>\nHow do I create a heatmap for gates?</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcde1a3a4bf8c4c94c8fc92a9c74f7a90f60b872.png\" data-download-href=\"/uploads/short-url/qWNCeNA4ZsbboVfjNvJ9TBBNXcS.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcde1a3a4bf8c4c94c8fc92a9c74f7a90f60b872_2_604x500.png\" alt=\"image\" data-base62-sha1=\"qWNCeNA4ZsbboVfjNvJ9TBBNXcS\" width=\"604\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcde1a3a4bf8c4c94c8fc92a9c74f7a90f60b872_2_604x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcde1a3a4bf8c4c94c8fc92a9c74f7a90f60b872.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcde1a3a4bf8c4c94c8fc92a9c74f7a90f60b872.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcde1a3a4bf8c4c94c8fc92a9c74f7a90f60b872_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7516 52.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div><br>\nBy selecting the gates I want to analyze and the Phenotypes option within \u201cSelect What To Compare\u201d I can see that some of the tSNE FOXP3 cluster now includes CD68 and PD1 positive cells.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/a/9aad0e34d1aa793fc751abaa4e41961f2ced539b.png\" data-download-href=\"/uploads/short-url/m4kiBIjtcRISR2BcVp0mhhFQ6uL.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9aad0e34d1aa793fc751abaa4e41961f2ced539b_2_421x375.png\" alt=\"image\" data-base62-sha1=\"m4kiBIjtcRISR2BcVp0mhhFQ6uL\" width=\"421\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9aad0e34d1aa793fc751abaa4e41961f2ced539b_2_421x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/a/9aad0e34d1aa793fc751abaa4e41961f2ced539b.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/a/9aad0e34d1aa793fc751abaa4e41961f2ced539b.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9aad0e34d1aa793fc751abaa4e41961f2ced539b_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7555 76 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div><br>\nThere currently seems to be a bug when choosing <em>Select Heatmap Type</em>, if you attempt to choose <em>Combined heatmap for all samples</em>. Individual will work.</p>\n<p>Remembering that this is not flow data, what we may be seeing here is interactions between macrophages and FOXP3 positive cells.</p>\n</details>", "<h3>\n<a name=\"h-5-export-the-data-from-cytomap-1\" class=\"anchor\" href=\"#h-5-export-the-data-from-cytomap-1\"></a>5.\tExport the data from CytoMAP</h3>\n<p>Fairly straightforward: <em>File-&gt;Export full data tables as csv</em></p>\n<details>\n<summary>\nTo import back into QuPath, the export settings should look like this.</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/8/089a9b5d0042fd68d0ed8905ab8eea0772622097.png\" data-download-href=\"/uploads/short-url/1e73W4biQ6FTmZ9L4RkUk5KjOxF.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/8/089a9b5d0042fd68d0ed8905ab8eea0772622097_2_447x500.png\" alt=\"image\" data-base62-sha1=\"1e73W4biQ6FTmZ9L4RkUk5KjOxF\" width=\"447\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/8/089a9b5d0042fd68d0ed8905ab8eea0772622097_2_447x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/8/089a9b5d0042fd68d0ed8905ab8eea0772622097.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/8/089a9b5d0042fd68d0ed8905ab8eea0772622097.png 2x\" data-dominant-color=\"F3F3F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">624\u00d7697 70.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</details>\n<p>Include all cells, and include the gates we created. Include the XY coordinates so match up the clusters with the correct objects in QuPath. Include the Image name so that if you have multiple images, you assign the objects to the correct XY coordinates in the correct images!<br>\nFinally, include any models you want, and check the <em>Include Gate Logicals</em> next to the <em>Export</em> button. Uncheck <em>Individual .csv for each cell</em>.<br>\nWhen you click Export, it will have you select a folder, NOT a file name. This can be a little disconcerting if you export multiple times, as you will need to either choose another folder or rename the original file. It will attempt to overwrite your original export otherwise, and it may be locked for editing! I chose the QuPath project folder.</p>\n<details>\n<summary>\nWhat the project folder looks like now.</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/5156b824517eedd5046e8c2d96ff3fe16047b589.png\" alt=\"image\" data-base62-sha1=\"bByta3E0n26PWs0LXEqBFSWzM9j\" width=\"545\" height=\"466\"></p>\n</details>\n<h3>\n<a name=\"h-6-time-to-import-all-that-data-back-into-qupath-2\" class=\"anchor\" href=\"#h-6-time-to-import-all-that-data-back-into-qupath-2\"></a>6.\tTime to import all that data back into QuPath!</h3>\n<p>This is where the first script comes in.<br>\nThe script is also included within the demo download.</p>\n<details>\n<summary>\n**Script:** Import a targeted CSV file exported from CytoMAP</summary>\n<pre><code class=\"lang-groovy\">/*\nThe image name in the CSV file must match the image name in the project. If you rename one, you must rename the other. REQUIRES IMAGES TO HAVE PIXEL SIZE METADATA.\nOtherwise, go through the script and change any instances of microns to px, and remove the pixelsize sections :(\n\nScript to import a CSV file exported from CytoMAP 1.4.9 or later.\nExpected input:\nCSV file\nX,Y coordinates in first two columns\nImage name in third column\nAll other columns are cluster or gate data\nVersion 3! Much faster! Improved logic to not fail when certain objects are not found (in case you delete a cell or have an empty image). Improved cell catching.\nMichael Nelson, September 2020\n*/\ndef project = getProject()\n\ndef file = Dialogs.promptForFile(null)\n\n// Create BufferedReader\ndef csvReader = new BufferedReader(new FileReader(file));\n\n\n//The rest of the script assumes the X coordinate is in column1, Y in column2, and all other columns are to be imported.\nrow = csvReader.readLine() // first row (header)\nmeasurementNames = row.split(',')\nlength = row.split(',').size()\n//measurementNames -= 'X'\n//measurementNames -= 'Y'\nprint measurementNames\nprint \"Adding results from \" + file\nprint \"This may take some time, please be patient\"\ncsv = []\nSet imageList = []\nwhile ((row = csvReader.readLine()) != null) {\n    toAdd = row.split(',')\n    imageList.add(toAdd[2])\n    csv &lt;&lt; toAdd\n}\nint t = 0\nint z = 0\n\nprint imageList\nimageList.each{ image-&gt;\n    entry = project.getImageList().find {it.getImageName() == image}\n\n    if (entry == null){print \"NO ENTRIES FOR IMAGE \"+ image;return;}\n    imageData = entry.readImageData()\n    hierarchy = imageData.getHierarchy()\n    pixelSize = imageData.getServer().getPixelCalibration().getAveragedPixelSizeMicrons()\n        \n    csvSubset = csv.findAll{it[2] == image}\n    //println(\"csv subset \"+csvSubset)\n    objects = hierarchy.getDetectionObjects()//.findAll{it.getPathClass() == getPathClass(\"Islet\")}\n    ob = new ObservableMeasurementTableData();\n    ob.setImageData(imageData,  objects);\n    \n    csvSubset.each{line-&gt;\n        x = line[0] as double\n        y = line[1] as double\n        object = PathObjectTools.getObjectsForLocation(hierarchy, x/pixelSize, y/pixelSize,  t, z,-1).find{it.isDetection()}\n        if (object == null){print \"ERROR, OBJECT NOT FOUND AT \"+x+\",\"+y;return}\n/*            if (round(ob.getNumericValue(object, \"Centroid X \u00b5m\")) != x || round(ob.getNumericValue(object, \"Centroid Y \u00b5m\")) != y){\n                object = objects.find{round(ob.getNumericValue(it, \"Centroid X \u00b5m\")) == x &amp;&amp; round(ob.getNumericValue(it, \"Centroid Y \u00b5m\")) == y}\n            }*/\n        i=3 //skip the X Y and Image entries\n        \n        while (i&lt;length){\n            //toAdd = row.split(',')[i] as double\n            object.getMeasurementList().putMeasurement(measurementNames[i], line[i] as double)\n            i++\n        }\n        objects.remove(object)\n    }\n    entry.saveImageData(imageData)\n}\nprint \"Done with all images!\"\n\ndef round(double number){\n    BigDecimal bd = new BigDecimal(number)\n    def result\n    if (number &lt; 100){\n        bd = bd.round(new MathContext(4))\n        result = bd.doubleValue()\n    }else if (number &lt; 1000){\n        result = number.round(2)\n    }else {result = number.round(1) }\n\n    return result\n}\ngetCurrentViewer().getHierarchy().setHierarchy(getProjectEntry().readHierarchy())\nfireHierarchyUpdate()\n\nimport qupath.lib.gui.measure.ObservableMeasurementTableData\nimport java.math.MathContext\n</code></pre>\n</details>\n<details>\n<summary>\nVariant script to import into QuPath a folder of CSV files that have been exported from CytoMAP</summary>\n<pre><code class=\"lang-auto\">/*\nThe image name in the CSV file must match the image name in the project. If you rename one, you must rename the other. REQUIRES IMAGES TO HAVE PIXEL SIZE METADATA.\nOtherwise, go through the script and change any instances of microns to px, and remove the pixelsize sections :(\n\nScript to import a folder of exported from CytoMAP 1.4.9 or later.\nExpected input:\nCSV file\nX,Y coordinates in first two columns\nImage name in third column\nAll other columns are cluster or gate data\nVersion 3! Much faster! Improved logic to not fail when certain objects are not found (in case you delete a cell or have an empty image). Improved logic to catch cells missed by fast method.\nMichael Nelson, September 2020\n*/\ndef project = getProject()\n\ndef folder = Dialogs.promptForDirectory(null)\nfolder.listFiles().each{file-&gt;\n    // Create BufferedReader\n    def csvReader = new BufferedReader(new FileReader(file));\n    \n    \n    //The rest of the script assumes the X coordinate is in column1, Y in column2, and all other columns are to be imported.\n    row = csvReader.readLine() // first row (header)\n    measurementNames = row.split(',')\n    length = row.split(',').size()\n    //measurementNames -= 'X'\n    //measurementNames -= 'Y'\n    print measurementNames\n    print \"Adding results from \" + file\n    print \"This may take some time, please be patient\"\n    csv = []\n    Set imageList = []\n    while ((row = csvReader.readLine()) != null) {\n        toAdd = row.split(',')\n        imageList.add(toAdd[2])\n        csv &lt;&lt; toAdd\n    }\n    int t = 0\n    int z = 0\n    \n    print imageList\n    imageList.each{ image-&gt;\n        entry = project.getImageList().find {it.getImageName() == image}\n    \n        if (entry == null){print \"NO ENTRIES FOR IMAGE \"+ image;return;}\n        imageData = entry.readImageData()\n        hierarchy = imageData.getHierarchy()\n        pixelSize = imageData.getServer().getPixelCalibration().getAveragedPixelSizeMicrons()\n            \n        csvSubset = csv.findAll{it[2] == image}\n        //println(\"csv subset \"+csvSubset)\n        objects = hierarchy.getDetectionObjects()//.findAll{it.getPathClass() == getPathClass(\"Islet\")}\n        ob = new ObservableMeasurementTableData();\n        ob.setImageData(imageData,  objects);\n        \n        csvSubset.each{line-&gt;\n            x = line[0] as double\n            y = line[1] as double\n            object = PathObjectTools.getObjectsForLocation(hierarchy, x/pixelSize, y/pixelSize,  t, z,-1).find{it.isDetection()}\n            if (object == null){print \"ERROR, OBJECT NOT FOUND AT \"+x+\",\"+y;return}\n/*                if (round(ob.getNumericValue(object, \"Centroid X \u00b5m\")) != x || round(ob.getNumericValue(object, \"Centroid Y \u00b5m\")) != y){\n            object = objects.find{round(ob.getNumericValue(it, \"Centroid X \u00b5m\")) == x &amp;&amp; round(ob.getNumericValue(it, \"Centroid Y \u00b5m\")) == y}\n        }*/\n            i=3 //skip the X Y and Image entries\n            \n            while (i&lt;length){\n                //toAdd = row.split(',')[i] as double\n                object.getMeasurementList().putMeasurement(measurementNames[i], line[i] as double)\n                i++\n            }\n            objects.remove(object)\n        }\n        entry.saveImageData(imageData)\n    }\n}\n\nprint \"Done with all images!\"\n\ndef round(double number){\n    BigDecimal bd = new BigDecimal(number)\n    def result\n    if (number &lt; 100){\n        bd = bd.round(new MathContext(4))\n        result = bd.doubleValue()\n    }else if (number &lt; 1000){\n        result = number.round(2)\n    }else {result = number.round(1) }\n\n    return result\n}\ngetCurrentViewer().getHierarchy().setHierarchy(getProjectEntry().readHierarchy())\nfireHierarchyUpdate()\n\nimport qupath.lib.gui.measure.ObservableMeasurementTableData\nimport java.math.MathContext\n</code></pre>\n</details>\n<p>Feel free to take a look at the CytoMAP_Sample_\u201dOriginal CSV File Name\u201d to see what you are working with, if you want.<br>\n<strong>UPDATE</strong><br>\nI have not adjusted the scripts here, but in 0.4.x it is possible to use the UUID for much cleaner imports, see <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>\u2019s <a href=\"https://forum.image.sc/t/importing-cell-types-back-into-qupath-by-object-id/76718/8\">script here</a>.</p>\n<p>Then, run the script, and target the appropriate CSV file. It SHOULD be as easy as that. If you run into any errors, please let me know, as I have only been able to test this on my own data (and this example). The only problem I had with this example is that I also had to rename the LuCa image within QuPath. The file names must match up exactly, or else the script does not know where to put the CytoMAP values.</p>\n<details>\n<summary>\nSample script output.</summary>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/5/759eae4c8a9f4e141171d88bed26425d14e46a24.png\" alt=\"image\" data-base62-sha1=\"gMvPQnCYJDgKBWEAhCqlVq4Z3SI\" width=\"624\" height=\"98\"></p>\n</details>\n<h2>\n<a name=\"ok-it-worked-but-where-are-the-measurements-nothing-showed-up-3\" class=\"anchor\" href=\"#ok-it-worked-but-where-are-the-measurements-nothing-showed-up-3\"></a>Ok, it worked, but where are the measurements? Nothing showed up!</h2>\n<p>Well, we wrote them to the data file, but QuPath is working off of the temporary data file. Couple of options here: \u201cCTRL+R\u201d with the main window selected will reload the data, and give you the measurements. If you have more than one image, switching to another image will load the measurements in the new image. You could also run the import script <em>with no images open in the first place</em> and everything will be fine.</p>", "<h3>7.\tGreat, but what can we do with this?</h3>\n<p>Cluster analysis results.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d188ce5e6f0a4417820428b475f2a8a1fb13fc.png\" alt=\"image\" data-base62-sha1=\"5fI75qzOJ6DT0v8PzNw5hACpG68\" width=\"624\" height=\"390\"></p>\n<p>FoxP3 gate from the tSNE plot.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/a/3a138839a0fb4dd93bccc4d7773c5226b66a01d3.png\" alt=\"image\" data-base62-sha1=\"8hLxwndSuTSwa3IZoGtXqi9Ifnl\" width=\"624\" height=\"387\"></p>\n<p>Since I have no classifications to maintain, I decided to quickly create a single measurement classifier using the gate.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d9f89a619fd4aa59dbf793c46a3666324e06410.png\" alt=\"image\" data-base62-sha1=\"kcR88yMtdZzUc33NMz4mRycmNpK\" width=\"624\" height=\"469\"></p>\n<p>At this point, if you do not have any QuPath classifications that you want to keep, you could create a classifier based on the cluster measurement. Once classified, it would be much easier to turn various clusters on and off through the Annotation menu. Even if you have classifications, you might consider <em>Duplicating</em> (Project tab, right click on an image) an image, and then reclassifying only the duplicate.</p>\n<p>With all of the unclassified cells turned off in the Annotations tab<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/b/abd569aaf25c6a93cb1b55981899a969df0adcd5.png\" alt=\"image\" data-base62-sha1=\"ow6SH7IxXK4fRoSU45TSMHrxWnj\" width=\"300\" height=\"76\"><br>\nit is much easier to inspect the remaining cells for CD68 and PD1.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2d685c6ae412b65addf9dc530eb7d20837802add.jpeg\" alt=\"image\" data-base62-sha1=\"6tH4S7fz6EOcImIiSTcOUKu8WS1\" width=\"624\" height=\"454\"><br>\nLooking at the white and yellow cells (most strongly positive for CD68), I can see that many of these FOXP3 cells do seem to have extensions of CD68 touching them, though not necessarily completely surrounding them. Looking at which cluster those cells are in, it turns out they are largely in Cluster 3, which was one of the more minor components of that gate. Clusters 2 and 4 end up looking fairly normal based on the measurements included, but who knows what further analysis might find!<br>\nThe cytokeratin gate, as a sanity check:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47e5982177d8a2171922ec2d4abdf4c894e6ef8e.png\" alt=\"image\" data-base62-sha1=\"ag1NPWO5STqNInjSMdU441VpgzI\" width=\"624\" height=\"358\"><br>\nYep, that gate was definitely the tumor area.</p>\n<p>I hope this is useful to someone out there! I appreciate any feedback, and blasted this out in a couple of hours, so please point out if anything is unclear. I plan to update it a bit further with the measurement map script (First image in this section that color codes the clusters, which seems easier than visualizing them by heatmap) in the next few days.</p>\n<p>Huge thanks to both <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> and <a class=\"mention\" href=\"/u/cstoltzfus\">@cstoltzfus</a> for making these two programs available for everyone to use! <a class=\"mention\" href=\"/u/cstoltzfus\">@cstoltzfus</a> made 3-4 updates in the last couple of weeks that made this all happen (sometimes within hours), so if anyone has questions for him about the software, I highly recommend asking. His response and update time has been phenomenal!</p>", "<p>Thank you for putting this together! This is really a cool use of CytoMAP and QuPath together.</p>", "<p>A second script can be used to fix the colors on a particular type of colormap to certain values, allowing users to go back and forth between the heatmaps and clusters in CytoMAP and the resulting cells or other objects in QuPath.</p>\n<details>\n<summary>\nScript: Force display colors to match color legend</summary>\n<pre><code class=\"lang-groovy\">//V3\n//STEP 1. Acquire a .TSV file that represents a color map of distinct colors\n//STEP 2. Make sure that the User  directory is set in Preferences/Extensions (gear icon, upper right)\n//STEP 3. Place the .TSV file into a \"colormaps\" folder within the directory\n//STEP 4. Change the file path below to match the colormap you wish to use.\n//STEP 5. Open the Measure-&gt;Measurement maps menu and select a grouping based measurement\n//STEP 6. Run the script and press the button at the top of the dialog to update the view\n//Change the measurement and press the button again to see a different grouping/neighborhood\n// Influenced by https://sashamaps.net/docs/resources/20-colors/\n\n/*\nMichael Nelson, September 2020\nMinor update May 2022\nWith assistance from Pete Bankhead: https://forum.image.sc/t/interacting-with-the-measurement-maps-dialog/43168/4?u=mike_nelson\n*/\nfileName = \"Rand_CL20.tsv\"\n//Display minimum. For grouping this will usually be 1.0, but for binary gates it should be 0.0\n//A future update might be smarter about handling this\nmin = 1.0\n//Update the list of colormaps available to QuPath with ones in the colormaps directory\nString userPath = PathPrefs.getUserPath();\nPath dirUser = Paths.get(userPath, \"colormaps\");\nColorMaps.installColorMaps(dirUser)\nfile = buildFilePath(userPath,\"colormaps\", fileName)\n\nif (!new File(file).exists()){\n    print \"Exiting as the input file does not exist. Either the Preferences-&gt;User directory is incorrect or the tsv file is not in the correct location.\"\n    print \"Attempted to open \" +file\n    return\n}\n\n//Look at the chosen fileName and the data to establish the size of the map\ndef csvReader = new BufferedReader(new FileReader(file));\nrandMap = []\nwhile ((row = csvReader.readLine()) != null) {\n    toAdd = row.split(\"\\t\")\n    toAddInts = []\n    //print toAdd\n    toAdd.each{\n        toAddInts &lt;&lt; it.toDouble()\n    }\n    randMap &lt;&lt; toAddInts\n}\nnumberOfColors = randMap.size()\n\n//Put together most of the dialog box. Standard JavaFX\nint col = 0\nint row = 0\n//int textFieldWidth = 120\nint labelWidth = 20\ndef gridPane = new GridPane()\ngridPane.setPadding(new Insets(10, 10, 10, 10));\ngridPane.setVgap(2);\ngridPane.setHgap(10);\nScrollPane scrollPane = new ScrollPane(gridPane)\nscrollPane.setFitToHeight(true);\nBorderPane border = new BorderPane(scrollPane)\nborder.setPadding(new Insets(15));\n//Place a button to update the Measurement Maps window\nButton setMapThresholds = new Button()\nsetMapThresholds.setText(\"Correct Colors\")\ngridPane.add( setMapThresholds, 2, row++, 1,1)\n\n\n//Create RGB patches in the dialog box to visualize which colors correspond to which group numbers\nfor (i=0; i&lt;numberOfColors;i++){\n    clusterNumber = new Label((i+1).toString())\n    clusterNumber.setId((i+1).toString())\n    //labels.add(cb)\n    gridPane.add( clusterNumber, col, row, 1,1)\n    rect = new Rectangle(25,(i+1)*10, 60,10)\n    int R = 255*randMap[i][0]\n    int G = 255*randMap[i][1]\n    int B = 255*randMap[i][2]\n\n    color = Color.rgb(R,G ,B )\n    rect.setFill(color)\n    //rectangles.add(rect)\n    gridPane.add( rect, 2, row++, 1,1)\n    \n}\n\nprint \"If you made it this far and get an error when pressing the 'Correct Colors' button, \"\nprint \"you probably do not have the Meausrement Maps dialog open and a Measurement selected\"\n\n//When the putton is pressed, update the viewer\nsetMapThresholds.setOnAction {\n    //Use the name provided at the beginning of the script, without the .tsv, to target the color map\n    colormap = ColorMaps.getColorMaps().get(GeneralTools.getNameWithoutExtension(fileName))\n    \n    //Update the display with the colormap above, and the selected measurement (which should be a group).\n    def viewer = getCurrentViewer()\n    def options = viewer.getOverlayOptions()\n    def detections = getQuPath().getImageData().getHierarchy().getDetectionObjects()\n\n    //Two options here, the one that works but requires \"index\" or a specific measurement, and the one that tries to find out what the currently selected measurement in the dialog is. The latter fails.\n    name = options.getMeasurementMapper().measurement\n    def mapper = new MeasurementMapper(colormap,name, detections)\n    mapper.setDisplayMinValue(min)\n    mapper.setDisplayMaxValue(numberOfColors)\n    options.setMeasurementMapper(mapper)\n \n}\n\n\nPlatform.runLater {\n\n    def stage = new Stage()\n    stage.initOwner(QuPathGUI.getInstance().getStage())\n    stage.setScene(new Scene( border))\n    stage.setTitle(\"Cluster colormap\")\n    stage.setWidth(300);\n    stage.setHeight(500);\n    stage.setResizable(true);\n    stage.show()\n\n}\n\n\n\n\nimport qupath.lib.gui.tools.*\nimport java.nio.file.Paths;\nimport java.nio.file.Path;\nimport qupath.lib.gui.prefs.PathPrefs;\nimport javafx.application.Platform\nimport javafx.geometry.Insets\nimport javafx.scene.Scene\nimport javafx.geometry.Pos\nimport javafx.scene.control.Button\nimport javafx.scene.control.Label\nimport javafx.scene.control.ColorPicker\nimport javafx.scene.layout.BorderPane\nimport javafx.scene.layout.GridPane\nimport javafx.scene.control.ScrollPane\nimport javafx.scene.layout.BorderPane\nimport javafx.stage.Stage\nimport javafx.scene.input.MouseEvent\nimport javafx.beans.value.ChangeListener\nimport qupath.lib.gui.QuPathGUI\nimport qupath.lib.gui.tools.ColorToolsFX\nimport javafx.scene.shape.Rectangle\nimport javafx.scene.paint.Color\nimport qupath.lib.color.ColorMaps\n\n</code></pre>\n</details>\n<p>When you run this script, you should get a dialog that looks like what is shown below, with a single button.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/2/8201e1896bdb0abea5e0b3b6f10bbed10acc8868.png\" data-download-href=\"/uploads/short-url/iy66gYaHvDroTgbbVAINy0QmCCc.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8201e1896bdb0abea5e0b3b6f10bbed10acc8868_2_217x375.png\" alt=\"image\" data-base62-sha1=\"iy66gYaHvDroTgbbVAINy0QmCCc\" width=\"217\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8201e1896bdb0abea5e0b3b6f10bbed10acc8868_2_217x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8201e1896bdb0abea5e0b3b6f10bbed10acc8868_2_325x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/2/8201e1896bdb0abea5e0b3b6f10bbed10acc8868.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8201e1896bdb0abea5e0b3b6f10bbed10acc8868_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">423\u00d7729 13.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If you have a <em>Measure-&gt;Measurement map</em> open and selected, you will be able to click the \u201cCorrect Colors\u201d button to be able to see the cluster colors in a way that matches up with the list under the Correct Colors button. If you do not have a \u201cMeasurement map\u201d open and selected, the button will cause an <strong>error</strong>!!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/d/9d9947788eadf2e4e3905253b27f5bf7dd0db712.png\" data-download-href=\"/uploads/short-url/mubmrvPbTK0nkaAn7gIwbEX8yum.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9d9947788eadf2e4e3905253b27f5bf7dd0db712_2_690x311.png\" alt=\"image\" data-base62-sha1=\"mubmrvPbTK0nkaAn7gIwbEX8yum\" width=\"690\" height=\"311\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9d9947788eadf2e4e3905253b27f5bf7dd0db712_2_690x311.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9d9947788eadf2e4e3905253b27f5bf7dd0db712_2_1035x466.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9d9947788eadf2e4e3905253b27f5bf7dd0db712_2_1380x622.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9d9947788eadf2e4e3905253b27f5bf7dd0db712_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1807\u00d7817 1.23 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nNow we can see that clusters 9 (white) and 6 (gray) are dominant in the tumor (cytokeratin positive) areas.</p>\n<details>\n<summary>\nNow as a GIF!</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/d/9dc3d47e410ec29a65a86e5fefd3d6f2dc3431e6.gif\" data-download-href=\"/uploads/short-url/mvEwF3LocSCYSoXf0nkbb0YphCS.gif?dl=1\" title=\"Correct colors\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dc3d47e410ec29a65a86e5fefd3d6f2dc3431e6_2_690x394.gif\" alt=\"Correct colors\" data-base62-sha1=\"mvEwF3LocSCYSoXf0nkbb0YphCS\" width=\"690\" height=\"394\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dc3d47e410ec29a65a86e5fefd3d6f2dc3431e6_2_690x394.gif, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dc3d47e410ec29a65a86e5fefd3d6f2dc3431e6_2_1035x591.gif 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dc3d47e410ec29a65a86e5fefd3d6f2dc3431e6_2_1380x788.gif 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dc3d47e410ec29a65a86e5fefd3d6f2dc3431e6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Correct colors</span><span class=\"informations\">1791\u00d71024 1.7 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</details>\n<details>\n<summary>\nWithout using the correct colors button, or using other colormaps, you get images like these</summary>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/66314201f864d1ca1cf090e65e9516aa8e996740.png\" data-download-href=\"/uploads/short-url/eA2eEP7vkfwRuD2dxWyhmtHcpG0.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66314201f864d1ca1cf090e65e9516aa8e996740_2_690x285.png\" alt=\"image\" data-base62-sha1=\"eA2eEP7vkfwRuD2dxWyhmtHcpG0\" width=\"690\" height=\"285\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66314201f864d1ca1cf090e65e9516aa8e996740_2_690x285.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66314201f864d1ca1cf090e65e9516aa8e996740_2_1035x427.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66314201f864d1ca1cf090e65e9516aa8e996740_2_1380x570.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66314201f864d1ca1cf090e65e9516aa8e996740_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1802\u00d7745 1.13 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIn the first image, the Rand_CL20 colormap was chosen, but the colors used are stretched out between the dark green at the end of the color map and the teal at the beginning, with no easy way to see what all of the colors in between ended up as.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/d/cd66668f2d5a3a7415887bae4714beb54b62902b.png\" data-download-href=\"/uploads/short-url/tj3gYlsItcA7cj9SJfBaCHdMM8b.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd66668f2d5a3a7415887bae4714beb54b62902b_2_690x283.png\" alt=\"image\" data-base62-sha1=\"tj3gYlsItcA7cj9SJfBaCHdMM8b\" width=\"690\" height=\"283\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd66668f2d5a3a7415887bae4714beb54b62902b_2_690x283.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd66668f2d5a3a7415887bae4714beb54b62902b_2_1035x424.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd66668f2d5a3a7415887bae4714beb54b62902b_2_1380x566.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd66668f2d5a3a7415887bae4714beb54b62902b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1798\u00d7739 1.14 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nUsing one of the standard color maps, it can be very difficult to tell clusters apart, much less tell exactly which cluster is which.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2b64f2c3d194369778bee1c2a842b205905dbe58.png\" data-download-href=\"/uploads/short-url/6bSOlc7VEJ5v9kIMPrCOAtnjqE8.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b64f2c3d194369778bee1c2a842b205905dbe58_2_690x347.png\" alt=\"image\" data-base62-sha1=\"6bSOlc7VEJ5v9kIMPrCOAtnjqE8\" width=\"690\" height=\"347\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b64f2c3d194369778bee1c2a842b205905dbe58_2_690x347.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b64f2c3d194369778bee1c2a842b205905dbe58_2_1035x520.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2b64f2c3d194369778bee1c2a842b205905dbe58.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b64f2c3d194369778bee1c2a842b205905dbe58_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1375\u00d7693 1.09 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nJet (legacy) is slightly better in terms of cluster color separation, but still does not help much with identifying exactly which cluster is which.</p>\n</details>\n<p>Regardless of what colormap you choose, assuming you have adjusted the script and set up your QuPath user directory correctly, the Correct Colors button will use the colormap chosen (I recommend <strong>Rand_CL20</strong> to start!) in the script to overwrite the normal Measurement map display.</p>\n<p>To make the whole setup work, first set your User directory, if you have not already.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b1053e91caff516795359b37d70005b7c632dd4.png\" data-download-href=\"/uploads/short-url/hyFI4WuWW3kmrH9tbCXGkUzoO6E.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b1053e91caff516795359b37d70005b7c632dd4_2_428x375.png\" alt=\"image\" data-base62-sha1=\"hyFI4WuWW3kmrH9tbCXGkUzoO6E\" width=\"428\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b1053e91caff516795359b37d70005b7c632dd4_2_428x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b1053e91caff516795359b37d70005b7c632dd4_2_642x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b1053e91caff516795359b37d70005b7c632dd4.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b1053e91caff516795359b37d70005b7c632dd4_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">712\u00d7623 25.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nOpen Preferences from the gear in the upper right of QuPath (also in the upper right of the image above), and copy and paste a path into the provided space. Alternatively, double click in the empty space and navigate to the desired folder.</p>\n<p>Once you have a user directory, create a colormaps folder within that directory, or copy in the colormaps folder from the Demo folder included in the second post. There should be several sample colormaps within that folder, which you can delete or move to another folder if you like. QuPath <em>will</em> show <em>all</em> of the added maps, so if it gets messy, delete away!<br>\nIn the end, you should end up with something like this:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/418384cbcd7f88da5400bcc352f07f2502b83d9c.png\" alt=\"image\" data-base62-sha1=\"9lyOpOV8dRedU6FFUEdVYEgagOg\" width=\"337\" height=\"198\"></p>\n<p><strong>Edit the script</strong> above to point to the desired file. You can also get the script from the scripts folder in the Demo.</p>\n<p>That is pretty much it! Please ask if you have any problems or questions. Note that this will only be useful \u201cas is\u201d if you have 20 or fewer clusters. If you have more clusters, you may need to add more lines to the provided colormap, or create your own colormap by manipulating the provided .tsv files.</p>\n<p>P.S. You technically do not <em>need</em> to set up the User folder, as you can get away with simply editing the script to point to the desired .tsv file anywhere. If you like the results, though, you may want to explore creating and sharing your own colormaps which will be easier with this setup already in place.</p>", "<p>Minor update to the import script to avoid \u201cmisses\u201d that I could not pin down and a warning about missing fields in the QuPath CSV export.</p>", "<p>Wanted to quickly point out that there have been a number of improvements if you are working with multiple samples/files.</p>\n<details>\n<summary>\nFirst, I would use a script to export \"per sample\" csv files, something like this.</summary>\n<pre><code class=\"lang-auto\">// ======== Save Results =============\nimport qupath.lib.gui.tools.MeasurementExporter\nimport qupath.lib.objects.PathCellObject\nimport qupath.lib.objects.PathDetectionObject\n//New User Defined Entry- only for exporting certain classes of objects\n//exportClass = getPathClass(\"PD1 (Opal 650)\")\n\n\n// Get the list of all images in the current project\ndef project = getProject()\ndef entry = getProjectEntry()\nentryList = []\nentryList &lt;&lt; getProjectEntry()\n\ndef outputPath = buildFilePath(PROJECT_BASE_DIR, \"results\")\nmkdirs(outputPath)\n\n\n//NEW CODE - only needed if you want to exclude certain classes\n//removedObjects = getCellObjects().findAll{it.getPathClass() != exportClass}\n//removeObjects(removedObjects, true)\n//fireHierarchyUpdate()\ngetProjectEntry().saveImageData(getCurrentImageData())\nimageData = entry.readImageData()\n    \n// Separate each measurement value in the output file with a tab (\"\\t\")\ndef separator = \",\"\n\n// Choose the columns that will be included in the export\n\ndef columnsToExclude = new String[]{\"Name\", \"Class\",\"Parent\", \"ROI\" }\n\ndef exportType = PathDetectionObject.class\n\ndef name1 = entry.getImageName() +'.csv'\n//need a file here\ndef outputFile = new File( buildFilePath(outputPath, name1))\n\n// Create the measurementExporter and start the export\ndef exporter  = new MeasurementExporter()\n                  .imageList(entryList)            // Images from which measurements will be exported\n                  .separator(separator)                 // Character that separates values\n                  //.includeOnlyColumns()\n                  .excludeColumns(columnsToExclude) // Columns are case-sensitive\n                  .exportType(exportType)               // Type of objects to export\n                  .exportMeasurements(outputFile)        // Start the export process\n\n//Put stuff back in place!\n//addObjects(removedObjects)\n//getProjectEntry().saveImageData(getCurrentImageData())\n//fireHierarchyUpdate()\nprint \"Done\"\n</code></pre>\n</details>\n<p><a href=\"https://forum.image.sc/t/qupath-saving-filtered-detectiontable-with-measurementexporter-for-current-image-only/41328/2\">See here for the original.</a></p>\n<p>Once you have your pile of CSV files (and have checked them all for empty entries in your measurement lists! Ack!), you can use CytoMAP\u2019s Import Multiple Samples option.</p>\n<p>For neighborhood analysis, <a href=\"https://gitlab.com/gernerlab/cytomap/-/blob/master/StandaloneInstaller/CytoMAP_Installer_WindowsV1.4.11.exe\">update 1.4.11,</a> allows you to pull classifications from QuPath from all images, and use those to define your neighborhoods. Or simply use Clusters from within CytoMAP.</p>\n<p>*Edit for code not being formatted.</p>", "<p>22 posts were split to a new topic: <a href=\"/t/responses-to-cytomap-there-and-back-again-thread/49178\">Responses to CytoMap There and back again thread</a></p>", "<p>Another update thanks to <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>\u2019s improvement to the code importing the data from the MATLAB export: <a href=\"https://forum.image.sc/t/coders-block-fast-way-of-finding-the-cell-that-contains-an-xy-coordinate/49175/2\" class=\"inline-onebox\">Coder's block - fast way of finding the cell that contains an XY coordinate - #2 by petebankhead</a></p>\n<p>Main changes to the import script are logical checks to skip missing cells and missing images, and the dramatic speed improvements for large numbers of cells.<br>\n<a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/5\">The updated post, above, is linked here.</a> Along with descriptions of how to use it correctly!</p>\n<details>\n<summary>\nAnd in case you only want the new code, it is here!</summary>\n<pre><code class=\"lang-auto\">/*\nThe image name in the CSV file must match the image name in the project. If you rename one, you must rename the other. REQUIRES IMAGES TO HAVE PIXEL SIZE METADATA.\nOtherwise, go through the script and change any instances of microns to px, and remove the pixelsize sections :(\n\nScript to import a folder of exported from CytoMAP 1.4.9 or later.\nExpected input:\nCSV file\nX,Y coordinates in first two columns\nImage name in third column\nAll other columns are cluster or gate data\nVersion 3! Much faster! Improved logic to not fail when certain objects are not found (in case you delete a cell or have an empty image). Improved logic to catch cells missed by fast method.\nMichael Nelson, September 2020\n*/\nproject = getProject()\n\ndef folder = Dialogs.promptForDirectory(null)\nfolder.listFiles().each{file-&gt;\n    // Create BufferedReader\n    def csvReader = new BufferedReader(new FileReader(file));\n    \n    \n    //The rest of the script assumes the X coordinate is in column1, Y in column2, and all other columns are to be imported.\n    row = csvReader.readLine() // first row (header)\n    measurementNames = row.split(',')\n    length = row.split(',').size()\n    //measurementNames -= 'X'\n    //measurementNames -= 'Y'\n    print measurementNames\n    print \"Adding results from \" + file\n    print \"This may take some time, please be patient\"\n    csv = []\n    Set imageList = []\n    while ((row = csvReader.readLine()) != null) {\n        toAdd = row.split(',')\n        imageList.add(toAdd[2])\n        csv &lt;&lt; toAdd\n    }\n    int t = 0\n    int z = 0\n    \n    print imageList\n    imageList.each{ image-&gt;\n        entry = project.getImageList().find {it.getImageName() == image}\n    \n        if (entry == null){print \"NO ENTRIES FOR IMAGE \"+ image;return;}\n        imageData = entry.readImageData()\n        hierarchy = imageData.getHierarchy()\n        pixelSize = imageData.getServer().getPixelCalibration().getAveragedPixelSizeMicrons()\n            \n        csvSubset = csv.findAll{it[2] == image}\n        //println(\"csv subset \"+csvSubset)\n        objects = hierarchy.getDetectionObjects()//.findAll{it.getPathClass() == getPathClass(\"Islet\")}\n        ob = new ObservableMeasurementTableData();\n        ob.setImageData(imageData,  objects);\n        \n        csvSubset.each{line-&gt;\n            x = line[0] as double\n            y = line[1] as double\n            object = PathObjectTools.getObjectsForLocation(hierarchy, x/pixelSize, y/pixelSize,  t, z,-1).find{it.isDetection()}\n            if (object == null){print \"ERROR, OBJECT NOT FOUND AT \"+x+\",\"+y;return}\n/*                if (round(ob.getNumericValue(object, \"Centroid X \u00b5m\")) != x || round(ob.getNumericValue(object, \"Centroid Y \u00b5m\")) != y){\n            object = objects.find{round(ob.getNumericValue(it, \"Centroid X \u00b5m\")) == x &amp;&amp; round(ob.getNumericValue(it, \"Centroid Y \u00b5m\")) == y}\n        }*/\n            i=3 //skip the X Y and Image entries\n            \n            while (i&lt;length){\n                //toAdd = row.split(',')[i] as double\n                object.getMeasurementList().putMeasurement(measurementNames[i], line[i] as double)\n                i++\n            }\n            objects.remove(object)\n        }\n        entry.saveImageData(imageData)\n    }\n}\n\nprint \"Done with all images!\"\n\ndef round(double number){\n    BigDecimal bd = new BigDecimal(number)\n    def result\n    if (number &lt; 100){\n        bd = bd.round(new MathContext(4))\n        result = bd.doubleValue()\n    }else if (number &lt; 1000){\n        result = number.round(2)\n    }else {result = number.round(1) }\n\n    return result\n}\nimport qupath.lib.gui.measure.ObservableMeasurementTableData\nimport java.math.MathContext\n</code></pre>\n</details>\n<details>\n<summary>\nFIXED, should not be an issue in currently posted code</summary>\n<p>Quick note, for most cell detections this method works fine, but sometimes a detection\u2019s centroid is not inside of\u2026 well, itself:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/b/bbe04b506dfd06d5922a5dd687afbe9110d4f8ef.png\" alt=\"image\" data-base62-sha1=\"qO1PM1cb69qBV0AnUBIIajpkWRx\" width=\"137\" height=\"146\"><br>\nIn these cases the import will fail for that specific object. If this ends up being a signficant problem for your project, let me know and I can send you the old code which should work in those circumstances, but is much slower overall.<br>\nEDIT: Possible fix for oddly shaped cells in next post!</p>\n</details>", "<p>One more update. The above method works well when you only want to cluster your data across multiple images.</p>\n<p>Once you want to do neighborhood analysis, it will fail on account of CytoMAP not differentiating between the input images when all of the data is in a single CSV file, as is the default export for QuPath.</p>\n<p>The following script will handle importing a folder (full of <strong>only</strong> CSV files that were exported from CytoMAP) of files into QuPath.</p>\n<p>This version specifically takes a folder, and not a single file, so it is not strictly an update, but rather to be used when doing neighborhood analysis.</p>\n<details>\n<summary>\nScript to import a folder of CytoMAP CSV files into their correct QuPath images</summary>\n<pre><code class=\"lang-auto\">/*\nThe image name in the CSV file must match the image name in the project. If you rename one, you must rename the other. REQUIRES IMAGES TO HAVE PIXEL SIZE METADATA.\nOtherwise, go through the script and change any instances of microns to px, and remove the pixelsize sections :(\n\nScript to import a folder of exported from CytoMAP 1.4.9 or later.\nExpected input:\nCSV file\nX,Y coordinates in first two columns\nImage name in third column\nAll other columns are cluster or gate data\nVersion 3! Much faster! Improved logic to not fail when certain objects are not found (in case you delete a cell or have an empty image). Improved logic to catch cells missed by fast method.\nMichael Nelson, September 2020\n*/\nproject = getProject()\n\ndef folder = Dialogs.promptForDirectory(null)\nfolder.listFiles().each{file-&gt;\n    // Create BufferedReader\n    def csvReader = new BufferedReader(new FileReader(file));\n    \n    \n    //The rest of the script assumes the X coordinate is in column1, Y in column2, and all other columns are to be imported.\n    row = csvReader.readLine() // first row (header)\n    measurementNames = row.split(',')\n    length = row.split(',').size()\n    //measurementNames -= 'X'\n    //measurementNames -= 'Y'\n    print measurementNames\n    print \"Adding results from \" + file\n    print \"This may take some time, please be patient\"\n    csv = []\n    Set imageList = []\n    while ((row = csvReader.readLine()) != null) {\n        toAdd = row.split(',')\n        imageList.add(toAdd[2])\n        csv &lt;&lt; toAdd\n    }\n    int t = 0\n    int z = 0\n    \n    print imageList\n    imageList.each{ image-&gt;\n        entry = project.getImageList().find {it.getImageName() == image}\n    \n        if (entry == null){print \"NO ENTRIES FOR IMAGE \"+ image;return;}\n        imageData = entry.readImageData()\n        hierarchy = imageData.getHierarchy()\n        pixelSize = imageData.getServer().getPixelCalibration().getAveragedPixelSizeMicrons()\n            \n        csvSubset = csv.findAll{it[2] == image}\n        //println(\"csv subset \"+csvSubset)\n        objects = hierarchy.getDetectionObjects()//.findAll{it.getPathClass() == getPathClass(\"Islet\")}\n        ob = new ObservableMeasurementTableData();\n        ob.setImageData(imageData,  objects);\n        \n        csvSubset.each{line-&gt;\n            x = line[0] as double\n            y = line[1] as double\n            object = PathObjectTools.getObjectsForLocation(hierarchy, x/pixelSize, y/pixelSize,  t, z,-1).find{it.isDetection()}\n            if (object == null){print \"ERROR, OBJECT NOT FOUND AT \"+x+\",\"+y;return}\n/*                if (round(ob.getNumericValue(object, \"Centroid X \u00b5m\")) != x || round(ob.getNumericValue(object, \"Centroid Y \u00b5m\")) != y){\n            object = objects.find{round(ob.getNumericValue(it, \"Centroid X \u00b5m\")) == x &amp;&amp; round(ob.getNumericValue(it, \"Centroid Y \u00b5m\")) == y}\n        }*/\n            i=3 //skip the X Y and Image entries\n            \n            while (i&lt;length){\n                //toAdd = row.split(',')[i] as double\n                object.getMeasurementList().putMeasurement(measurementNames[i], line[i] as double)\n                i++\n            }\n            objects.remove(object)\n        }\n        entry.saveImageData(imageData)\n    }\n}\n\nprint \"Done with all images!\"\n\ndef round(double number){\n    BigDecimal bd = new BigDecimal(number)\n    def result\n    if (number &lt; 100){\n        bd = bd.round(new MathContext(4))\n        result = bd.doubleValue()\n    }else if (number &lt; 1000){\n        result = number.round(2)\n    }else {result = number.round(1) }\n\n    return result\n}\nimport qupath.lib.gui.measure.ObservableMeasurementTableData\nimport java.math.MathContext\n</code></pre>\n</details>\n<p>This pairs with <a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/10\">the above script to export a single CSV file per image</a>.<br>\nUpdate: Speed improvement mostly kept while error checking now fixes the problematic cells.</p>", "<p>Hi Mike,</p>\n<p>Nice update! Thanks. I have tested in a large image with 1.3 M cells, loosing only 100 cells more or less, but it was very fast.</p>\n<p>On the other hand, do you know if it possible convert my clusters in CytoMAP to my class in Qupath, directly? Not using classifier tools based on the colour maps.</p>\n<p>Many thanks</p>", "<p>Yes, you can use something like:</p>\n<pre><code class=\"lang-auto\">getCellObjects().each{\n    string = measurement(it, \"The Name of your export measurement from CytoMAP\").toString()\n    it.setPathClass(getPathClass(string))\n}\n</code></pre>\n<p>That was on the fly so there might be some errors, but I just wrote something similar at home for a CytoMAP presentation tomorrow.</p>\n<p>Also working on fixing the missing cells now. It will slow things down slightly, but it should only be a minor hiccup for those cells that are missed.</p>", "<p>Thanks for your rapid reply. It was perfect!<br>\nGood luck with your presentation</p>", "<p>Thanks, also mentioning this for other users that stumble across this, the Right click in the Annotations-class panel gives the option to \u201c<em>Populate from existing objects</em>\u201d, which is a nice time saver when you have different amounts of classes or neighborhoods - allowing you to recolor them much more easily.</p>", "<p>Ok, last bump for this, hopefully for a while. I believe the code has been updated in all places to import data much more quickly, and it should now catch those weird cells where the centroid is not inside of the cell itself. Hopefully with only a minor decrease in speed compared to the previous version.</p>\n<p>I would also like to point out that CytoMAP itself had two major QoL improvements for QuPath users in the last update. You can now invert ALL of the plots\u2019 Y axes at once (since QuPath has a different Y coordinate axis from what MATLAB expects) - handy for entire projects of images when CytoMAP has spit out 15-20 plots.<br>\nAlso, you no longer have to check the CSV files generated by QuPath for NULL values - that can be taken care of during the import process by an option at the bottom of the import dialog!<br>\nCytoMAP 1.4.19 can be found here : <a href=\"https://cstoltzfus.com/posts/CytoMAP%20V%201.4.19/\" class=\"inline-onebox\">CytoMAP Version 1.4.19 - Dr. Caleb R. Stoltzfus</a></p>", "<p>Continuing the discussion from <a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/5\">There and back again, QuPath&lt;==&gt;CytoMAP cluster analysis</a>:</p>\n<p>Hey Mike,<br>\ngreat script thank you so much! I successfully performed all steps until the import back to QuPath - while exporting my images they seem to flip, meaning the bottom right corner then is on the top right in CytoMAP - when trying to import gates back to QuPath I get thousands of these error messages:<br>\nINFO: ERROR, OBJECT NOT FOUND AT 2401.0,3523.5</p>\n<p>Do you have any idea what to do?</p>\n<p>Thank you,<br>\nCheers,<br>\nRichard</p>", "<p>Hi <a class=\"mention\" href=\"/u/sittnerr\">@sittnerr</a>,</p>\n<p>The flip in CytoMAP should only be apparent on the plot if you do not invert the Y axis - it is a known difference between how objects are normally plotted (0,0 in the lower left) and how QuPath and most image analysis software plots pixels (0,0 in the upper left).<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b945331192dc3582e76c35fabe2d4e47884853c.png\" alt=\"image\" data-base62-sha1=\"aMBATpU32Kw1nVjnRjoRyF4wkZ6\" width=\"325\" height=\"178\"><br>\nThe XY coordinates of the objects should not be affected by that, so the issue probably lies elsewhere. Can you confirm that your image has accurate pixel size metadata? That is one place others have had issues before - the import scripts require metadata to run as written. Otherwise, you will need to adapt them slightly as described in the script.<br>\nImage below<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/326ac5f8e1d6eb76696f123cd8db198a0bbc506d.png\" alt=\"image\" data-base62-sha1=\"7c0DkliWxSzsTQw1UuL8ZzuiobP\" width=\"669\" height=\"122\"></p>\n<p>Cheers,<br>\nMike</p>"], "78169": ["<p>Hello All,</p>\n<p>I am new to imagej and I am trying to create a binary stack from the image that I threshold. If I do it only for one image it works well, but when I try converting the stack to binary the image is converted to white and only the background is left black. I don\u2019t understand why this is happening.<br>\nI would really appreciate it if you can help me with this.</p>\n<p>I have also attached a link to the stack of 50 images and I am trying to segregate the black space/voids from the remaining portion.</p>\n<p><a href=\"https://drive.google.com/drive/folders/1d9ydyrI_KDrp2AIucB-P2WQKf4ncCtfC?usp=sharing\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://drive.google.com/drive/folders/1d9ydyrI_KDrp2AIucB-P2WQKf4ncCtfC?usp=sharing</a><br>\nI have also attached the images when I work with one slice and stack for referencing the issue.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/235c5161b6a049bdd92affbc135d941eea2e92a2.png\" data-download-href=\"/uploads/short-url/52Ov6kQA2hB3fB2Vh6X8huv70ie.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/235c5161b6a049bdd92affbc135d941eea2e92a2.png\" alt=\"image\" data-base62-sha1=\"52Ov6kQA2hB3fB2Vh6X8huv70ie\" width=\"690\" height=\"394\" data-dominant-color=\"BBBBBB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1161\u00d7664 49.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/vipul\">@Vipul</a>,</p>\n<p>Please try the following procedure:</p>\n<p>Use the threshold-adjuster (Image&gt;Adjust&gt;Threshold\u2026), select Stack Histogram to set your min/max threshold values, press \u201capply\u201d and deselect \u201cCalculate threshold for each image\u201d.</p>\n<p>Do you get what you want that way?</p>\n<p>Best,<br>\nVolker</p>"], "65882": ["<p>Dear all,</p>\n<p>I met a question about installing mgx plugin. In order to install the CNN plugin, I removed mgx and repeated the installation, following the procedure (<a href=\"https://morphographx.org/software/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Software \u2013 MorphoGraphX</a>). However, when I run mgx, the CNN plugin didn\u2019t show up and the ITK plugin also disappeared. Can anyone help me solve this? Thanks in advance. I paste the terminal recording below, hope this will be helpful.<br>\nqt5ct: using qt5ct plugin<br>\nWelcome to MorphoGraphX!<br>\nThrust host evaluation: OpenMP<br>\nThrust device evaluation: CUDA</p>\n<p>MorphoGraphX version: 2.0 revision: 1-256<br>\nCannot bind Moves forward to FRAME<br>\nqt5ct: D-Bus global menu: no<br>\nCuda driver version: 11.60<br>\nCuda runtime version: 11.60</p>\n<p>Cuda capable device found, device 0: NVIDIA GeForce GTX 1080<br>\nCompute capability: 6.1<br>\nTotal memory: 8116 Mb<br>\nMultiProcessors: 20<br>\nRes.threads per MultiProcessor: 2048<br>\nMax resident threads: 40960<br>\nCuda cores: 3840<br>\nClock rate: 1.797 GHz</p>\n<p>CUDA initialized, 7669MB memory available from 8116 total</p>\n<p>Memory reserved for Cuda: 1917 MB.<br>\nOpenMP Processors: 4<br>\nOpenMP Max threads: 4</p>\n<p>Base processes<br>\nprocesses: 3</p>\n<p>Loading system processes<br>\nadded 15 processes, total: 18</p>\n<p>Loading libraries from \u2018/usr/lib/MorphoGraphX/processes\u2019<br>\nLoaded library libmgxBase.so<br>\nadded 338 processes, total: 356<br>\nFailed to load library file mgxITK.so:<br>\nlibITKStatistics-5.0.so.1: cannot open shared object file: No such file or directory<br>\nLoaded library mgxPython.so<br>\nadded 1 processes, total: 357<br>\nFailed to load library file mgxUCNN.so:<br>\nlibmpi_cxx.so.20: cannot open shared object file: No such file or directory<br>\nLoaded library mgxUCellAtlas.so<br>\nadded 33 processes, total: 390<br>\nLoaded library mgxUDivisionAnalysis.so<br>\nadded 41 processes, total: 431<br>\nLoaded library mgxUImplicitDeformation.so<br>\nadded 43 processes, total: 474<br>\nLoaded library mgxULobynessMeasures.so<br>\nadded 10 processes, total: 484<br>\nLoaded library mgxUMGX2R.so<br>\nadded 5 processes, total: 489<br>\nLoaded library mgxUTypeRecognition.so<br>\nadded 15 processes, total: 504<br>\nLoaded library mgxUXPIWIT.so<br>\nadded 3 processes, total: 507</p>\n<p>Loading libraries from \u2018/home/leili/.mgx/processes\u2019</p>\n<p>Loading libraries from \u2018/home/leili/.local/share/MorphoGraphX/MorphoGraphX/processes\u2019<br>\nlibpng warning: iCCP: known incorrect sRGB profile<br>\nlibpng warning: iCCP: cHRM chunk does not match sRGB<br>\nCreated 498 processes, Warning, 507 registered<br>\nInvalid process tab/folder/name:<br>\nRendering OpenGL 4.6.0 NVIDIA 510.47.03 on NVIDIA GeForce GTX 1080/PCIe/SSE2<br>\nOpenGL maximum texture size: 32768<br>\nOpenGL error in file /usr/users/JIC_c1/rismith/Desktop/MGX/mgx_private/src/MorphoViewer.cpp on line 1341 for command : draw<br>\ninvalid value<br>\nReading Process Parameters<br>\nReading Tasks<br>\nMisc/System/Load_View Process running time: 0.22 s.</p>", "<p>Maybe you installed Cuda 11.6? Try uninstalling Cuda and re-installing 11.4:</p>\n<p><span class=\"math\"> sudo apt purge \"nvidia*\"\n</span> sudo apt purge \u201c<em>cuda</em>\u201d</p>\n<p>Then install Cuda 11.4 from the nVidia site.</p>\n<p>Then install MGX, ITK, and the CNN packages from the MGX website.</p>", "<p>Dear Richard,</p>\n<p>Thanks for your suggestion, but it is still unworkable. I reinstalled everything followed the order, but the CNN plugin still unappear. I pasted the terminal record below, hope it will be helpful. Thanks in advance.<br>\nqt5ct: using qt5ct plugin<br>\nWelcome to MorphoGraphX!<br>\nThrust host evaluation: OpenMP<br>\nThrust device evaluation: CUDA</p>\n<p>MorphoGraphX version: 2.0 revision: 1-256<br>\nCannot bind Moves forward to FRAME<br>\nqt5ct: D-Bus global menu: no<br>\nCuda driver version: 11.60<br>\nCuda runtime version: 11.60</p>\n<p>Cuda capable device found, device 0: NVIDIA GeForce GTX 1080<br>\nCompute capability: 6.1<br>\nTotal memory: 8116 Mb<br>\nMultiProcessors: 20<br>\nRes.threads per MultiProcessor: 2048<br>\nMax resident threads: 40960<br>\nCuda cores: 3840<br>\nClock rate: 1.797 GHz</p>\n<p>CUDA initialized, 7592MB memory available from 8116 total</p>\n<p>Memory reserved for Cuda: 1898 MB.<br>\nOpenMP Processors: 4<br>\nOpenMP Max threads: 4</p>\n<p>Base processes<br>\nprocesses: 3</p>\n<p>Loading system processes<br>\nadded 15 processes, total: 18</p>\n<p>Loading libraries from \u2018/usr/lib/MorphoGraphX/processes\u2019<br>\nLoaded library libmgxBase.so<br>\nadded 338 processes, total: 356<br>\nLoaded library mgxITK.so<br>\nadded 11 processes, total: 367<br>\nLoaded library mgxPython.so<br>\nadded 1 processes, total: 368<br>\nFailed to load library file mgxUCNN.so:<br>\nlibmpi_cxx.so.20: cannot open shared object file: No such file or directory<br>\nLoaded library mgxUCellAtlas.so<br>\nadded 33 processes, total: 401<br>\nLoaded library mgxUDivisionAnalysis.so<br>\nadded 41 processes, total: 442<br>\nLoaded library mgxUImplicitDeformation.so<br>\nadded 43 processes, total: 485<br>\nLoaded library mgxULobynessMeasures.so<br>\nadded 10 processes, total: 495<br>\nLoaded library mgxUMGX2R.so<br>\nadded 5 processes, total: 500<br>\nLoaded library mgxUTypeRecognition.so<br>\nadded 15 processes, total: 515<br>\nLoaded library mgxUXPIWIT.so<br>\nadded 3 processes, total: 518</p>\n<p>Loading libraries from \u2018/home/leili/.mgx/processes\u2019</p>\n<p>Loading libraries from \u2018/home/leili/.local/share/MorphoGraphX/MorphoGraphX/processes\u2019<br>\nlibpng warning: iCCP: known incorrect sRGB profile<br>\nlibpng warning: iCCP: cHRM chunk does not match sRGB<br>\nCreated 509 processes, Warning, 518 registered<br>\nInvalid process tab/folder/name:<br>\nRendering OpenGL 4.6.0 NVIDIA 510.47.03 on NVIDIA GeForce GTX 1080/PCIe/SSE2<br>\nOpenGL maximum texture size: 32768<br>\nReading Process Parameters<br>\nReading Tasks<br>\nMisc/System/Load_View Process running time: 0.13 s.<br>\nBye bye!</p>\n<p>Best regards.</p>\n<p>Lei LI.</p>", "<p>It still says Cuda 11.6 is installed, those purge statements should have gotten rid of it.</p>\n<p>The ITK is looking for 5.0 library, but the Ubuntu 20 version on the MGX site is 5.1.2. Maybe you installed the wrong version? What version of Ubuntu do you have?</p>", "<p>Dear Richard,</p>\n<p>The terminal is displayed as below:</p>\n<p>(base) leili@morpho1:~$ lsb_release -a<br>\nNo LSB modules are available.<br>\nDistributor ID:\tLinuxmint<br>\nDescription:\tLinux Mint 20<br>\nRelease:\t20<br>\nCodename:\tulyana</p>", "<p>It looks like ITK loaded properly after the reinstall.</p>\n<aside class=\"quote no-group\" data-username=\"lei_li\" data-post=\"3\" data-topic=\"65882\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/lei_li/40/30942_2.png\" class=\"avatar\"> lei_li:</div>\n<blockquote>\n<p>Loaded library mgxITK.so<br>\nadded 11 processes, total: 367</p>\n</blockquote>\n</aside>\n<p>For the cuda, here are the purge statements again, it looks like the formatting was messed up in the previous post:</p>\n<p>$ sudo apt purge \u201c*cuda*\u201d<br>\n$ sudo apt purge \u201cnvidia*\u201d</p>\n<p>You can go into to synaptic to double check there are no cuda packages left.</p>\n<p>And then reinstall from here:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://developer.nvidia.com/cuda-11-4-2-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=deb_local\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e6aed2df4e7105cb48698ca32bafc07fcb9f49e.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://developer.nvidia.com/cuda-11-4-2-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=deb_local\" target=\"_blank\" rel=\"noopener\" title=\"12:18PM - 20 October 2021\">NVIDIA Developer \u2013 20 Oct 21</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://developer.nvidia.com/cuda-11-4-2-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=deb_local\" target=\"_blank\" rel=\"noopener\">CUDA Toolkit 11.4 Update 2 Downloads</a></h3>\n\n  <p>Resources CUDA Documentation/Release NotesMacOS Tools Training Sample Code Forums Archive of Previous CUDA Releases FAQ Open Source PackagesSubmit a Bug</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hello,</p>\n<p>Hope you don\u2019t mind, I\u2019m poppping in this thread as I have a similar issue but only with CNN.<br>\nWe\u2019ve moved to linux system to use it but can\u2019t manage to install properly. Even after switching from cuda 11.6 to 11.4 some libraries can\u2019t be opened, sounds like a path issue?</p>\n<p>I precise that I\u2019m a complete beginner with linux system so I hope I didn\u2019t do anything wrong.</p>\n<p>Please find the terminal message below (with concerned libraries in bold):</p>\n<p>QApplication: invalid style override passed, ignoring it.<br>\nAvailable styles: Windows, Fusion<br>\nWelcome to MorphoGraphX!<br>\nThrust host evaluation: OpenMP<br>\nThrust device evaluation: CUDA</p>\n<p>MorphoGraphX version: 2.0 revision: 1-254<br>\nCannot bind Moves forward to FRAME<br>\nCuda driver version: 11.40<br>\nCuda runtime version: 11.40</p>\n<p>Cuda capable device found, device 0: Quadro RTX 6000<br>\nCompute capability: 7.5<br>\nTotal memory: 24197 Mb<br>\nMultiProcessors: 72<br>\nRes.threads per MultiProcessor: 2048<br>\nMax resident threads: 147456<br>\nCuda cores: 13824<br>\nClock rate: 1.77 GHz</p>\n<p>CUDA initialized, 23928MB memory available from 24197 total</p>\n<p>Memory reserved for Cuda: 5982 MB.<br>\nOpenMP Processors: 28<br>\nOpenMP Max threads: 28</p>\n<p>Base processes<br>\nprocesses: 3</p>\n<p>Loading system processes<br>\nadded 15 processes, total: 18</p>\n<p>Loading libraries from \u2018/usr/lib/MorphoGraphX/processes\u2019<br>\nLoaded library libmgxBase.so<br>\nadded 338 processes, total: 356<br>\nLoaded library mgxITK.so<br>\nadded 11 processes, total: 367<br>\n<strong>Failed to load library file mgxPython.so:</strong><br>\n<strong>libpython2.7.so.1.0: cannot open shared object file: No such file or directory</strong><br>\n<strong>Failed to load library file mgxUCNN.so:</strong><br>\n<strong>libmpi_cxx.so.40: cannot open shared object file: No such file or directory</strong><br>\nLoaded library mgxUCellAtlas.so<br>\nadded 33 processes, total: 400<br>\nLoaded library mgxUDivisionAnalysis.so<br>\nadded 41 processes, total: 441<br>\n<strong>Failed to load library file mgxUHDF.so:</strong><br>\n<strong>libhdf5_serial.so.103: cannot open shared object file: No such file or directory</strong><br>\nLoaded library mgxUImplicitDeformation.so<br>\nadded 43 processes, total: 484<br>\nLoaded library mgxULobynessMeasures.so<br>\nadded 10 processes, total: 494<br>\n<strong>Failed to load library file mgxUMGX2R.so:</strong><br>\n<strong>libR.so: cannot open shared object file: No such file or directory</strong><br>\nLoaded library mgxUTypeRecognition.so<br>\nadded 15 processes, total: 509<br>\nLoaded library mgxUXPIWIT.so<br>\nadded 3 processes, total: 512</p>\n<p>Loading libraries from \u2018/home/XXXXX/.mgx/processes\u2019</p>\n<p>Loading libraries from \u2018/home/XXXXX/.local/share/MorphoGraphX/MorphoGraphX/processes\u2019<br>\nlibpng warning: iCCP: known incorrect sRGB profile<br>\nlibpng warning: iCCP: cHRM chunk does not match sRGB<br>\nCreated 503 processes, Warning, 512 registered<br>\nInvalid process tab/folder/name:<br>\nRendering OpenGL 3.1 Mesa 21.2.6 on llvmpipe (LLVM 12.0.0, 256 bits)<br>\nOpenGL maximum texture size: 16384<br>\nReading Process Parameters<br>\nReading Tasks<br>\nMisc/System/Load_View Process running time: 0.245 s.</p>\n<p>Best wishes,<br>\nSovanna</p>", "<p>Hi Sovanna,</p>\n<p>You should be able to fix the errors by installing packages as follows:</p>\n<p>libpython2.7.so.1.0 - install libpython2.7-dev (this will make the run python process available)<br>\nlibmpi_cxx.so.40 - install libopenmpi3 (this should make the CNN processes available)<br>\nlibhdf5_serial.so.103 - install libhdf5-103 (this will enable saving to/from HDF5 files)<br>\nlibR.so - install r-cran-rinside (this will also install R and make the R processes available)</p>\n<p>Cheers,<br>\nRichard</p>", "<p>Hi Richard,</p>\n<p>Thanks I installed everything and CNN now appears into mgx process but I still have issues:</p>\n<p>First is one still related to one library:<br>\nFailed to load library file mgxUHDF.so:<br>\nlibhdf5_cpp.so.103: cannot open shared object file: No such file or directory<br>\nEDIT: I fixed this point by installing libhdf5-cpp-103</p>\n<p>Second is a message error when I tried to run the BasselCombinedUNet.pt file, which is this:<br>\nStack/CNN/UNet3D Prediction::run Error running torch model: The following operation failed in the TorchScript interpreter.<br>\nTraceback of TorchScript, serialized code (most recent call last):<br>\nFile \u201ccode/<strong>torch</strong>/pytorch3dunet/unet3d/model.py\u201d, line 284, in forward<br>\ninput46 = torch._convolution(input45, <em>64, None, [1, 1, 1], [1, 1, 1], [1, 1, 1], False, [0, 0, 0], 1, False, False, True)<br>\ninput47 = torch.relu</em>(input46)<br>\ninput48 = torch._convolution(input47, _66, _67, [1, 1, 1], [0, 0, 0], [1, 1, 1], False, [0, 0, 0], 1, False, False, True)<br>\n~~~~~~~~~~~~~~~~~~ &lt;\u2014 HERE<br>\nreturn torch.sigmoid(input48)</p>\n<p>Traceback of TorchScript, original code (most recent call last):<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/nn/modules/conv.py(480): forward<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/nn/modules/module.py(525): _slow_forward<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/nn/modules/module.py(539): <strong>call</strong><br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/pytorch3dunet-1.2.5-py3.7.egg/pytorch3dunet/unet3d/model.py(133): forward<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/nn/modules/module.py(525): _slow_forward<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/nn/modules/module.py(539): <strong>call</strong><br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py(997): trace_module<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py(858): trace<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/plantseg/predictions/predict.py(63): <strong>call</strong><br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/site-packages/plantseg/pipeline/raw2seg.py(69): raw2seg<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/concurrent/futures/thread.py(57): run<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/concurrent/futures/thread.py(80): _worker<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/threading.py(870): run<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/threading.py(926): _bootstrap_inner<br>\n/home/rismith/.conda/envs/plant-seg/lib/python3.7/threading.py(890): _bootstrap<br>\nRuntimeError: CUDA error: an illegal memory access was encountered</p>\n<p>Looks like it\u2019s related to something called PyTorch?</p>\n<p>Cheers,<br>\nSovanna</p>", "<p>Was having similar error related to libmpi_cxx.so.20 on Ubuntu 18.04.</p>\n<p>Ran this command which installs the related  libraries.</p>\n<pre><code class=\"lang-auto\">sudo apt-get install libomp-dev\n</code></pre>\n<p>now able to run MGX smoothly with CUDA.</p>", "<p>Followup question about the Cuda version installed.  Do both the driver and runtime version need to be Cuda version 11.4?</p>\n<p>Not sure how it happened, but they are different for me:</p>\n<p>MorphoGraphX version: 2.0 revision: 1-256<br>\nCannot bind Moves forward to FRAME<br>\nCuda driver version: 12.10<br>\nCuda runtime version: 11.40</p>\n<p>I assume that\u2019s why I\u2019m getting the following error?</p>\n<p>Failed to load library file mgxUCNN.so:<br>\nlibc10.so: cannot open shared object file: No such file or directory</p>", "<p>That file should be in the MGX CNN addon package. When you type locate libc10.so what do you get? It should get installed to /usr/lib/MorphoGraphX.</p>\n<p>How are you starting MorphoGraphX? If starting from the command line you need to use \u201cmgx\u201d not \u201cMorphoGraphX\u201d as the startup script sets the library paths.</p>"], "57692": ["<p>Hello,<br>\nI would like to know if it is possible to call the new version of trackMate (v7) through a macro? Especially, is it possible to choose the detector type (for example Threshold detector) in the macro?</p>\n<p>Thanks in advance!</p>", "<p>Hello <a class=\"mention\" href=\"/u/acolin\">@Acolin</a></p>\n<p>Here is what we can tell about TrackMate + IJ macros:</p>\n<p>You can call a subset of TM functionalities with macros. For instance:</p>\n<pre><code class=\"lang-auto\">run('TrackMate', \"use_gui=false \"\n    + \"save_to=[/Users/tinevez/Desktop/TrackMateSaveTest.xml] \"\n    + \"export_to=[/Users/tinevez/Desktop/TrackMateExportTest.xml] \"\n    + \"display_results=true \"\n    + \"radius=2.5 \"\n    + \"threshold=50.1 \"\n    + \"subpixel=false \"\n    + \"median=false \"\n    + \"channel=1 \"\n    + \"max_frame_gap=0\" )\n</code></pre>\n<p>will</p>\n<ul>\n<li>run TrackMate on the image currently open,</li>\n<li>save the results to a TM file in <code>/Users/tinevez/Desktop/TrackMateSaveTest.xml</code>\n</li>\n<li>export the results to simple XML file in <code>/Users/tinevez/Desktop/TrackMateExportTest.xml</code>\n</li>\n<li>not show the GUI</li>\n<li>but show the results on the image.</li>\n</ul>\n<p>But these commands restrict the choice of detector and linker. They are always</p>\n<ul>\n<li>the LoG detector</li>\n<li>the Simple LAP tracker.</li>\n</ul>\n<p>(The logic that implements TrackMate macro support is here: <a href=\"https://github.com/fiji/TrackMate/blob/5ed5bb09e3471cd8ce83cc7ab405d045072ce5ba/src/main/java/fiji/plugin/trackmate/TrackMateRunner.java\">https://github.com/fiji/TrackMate/blob/5ed5bb09e3471cd8ce83cc7ab405d045072ce5ba/src/main/java/fiji/plugin/trackmate/TrackMateRunner.java</a>)</p>\n<p><a class=\"mention\" href=\"/u/imagejan\">@imagejan</a> made a script that makes TrackMate macro-recordable (<a href=\"https://imagej.net/plugins/trackmate/scripting#making-trackmate-macro-recordable-with-a-64-line-script\" class=\"inline-onebox\">Scripting TrackMate</a>).<br>\nBut you cannot change the detector either.</p>\n<p>The best solution is to use Jython scripting:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://imagej.net/plugins/trackmate/scripting#a-full-example\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e21227d5d291733059a786bc090795df86cd9e1.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://imagej.net/plugins/trackmate/scripting#a-full-example\" target=\"_blank\" rel=\"noopener\">ImageJ Wiki</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bdf44c48f1e1cadc2d4efd5a472476ff9aaa536d.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://imagej.net/plugins/trackmate/scripting#a-full-example\" target=\"_blank\" rel=\"noopener\">Scripting TrackMate</a></h3>\n\n  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<aside class=\"quote group-team\" data-username=\"tinevez\" data-post=\"2\" data-topic=\"57692\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tinevez/40/21_2.png\" class=\"avatar\"> tinevez:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/imagejan\">@imagejan</a> made a script that makes TrackMate macro-recordable (<a href=\"https://imagej.net/plugins/trackmate/scripting#making-trackmate-macro-recordable-with-a-64-line-script\">Scripting TrackMate</a>).<br>\nBut you cannot change the detector either</p>\n</blockquote>\n</aside>\n<p>You can add a detector choice to the script like this:</p>\n<pre><code class=\"lang-groovy\">#@ String (choices={\"LoG detector\", \"DoG detector\"}) detectorChoice\n\nimport fiji.plugin.trackmate.Settings\nimport fiji.plugin.trackmate.detection.LogDetectorFactory\nimport fiji.plugin.trackmate.detection.DogDetectorFactory\n\nsettings = new Settings()\nswitch (detectorChoice) {\n\tcase \"LoG detector\":\n\t\tsettings.detectorFactory = new LogDetectorFactory()\n\t\tbreak\n\tcase \"DoG detector\":\n\t\tsettings.detectorFactory = new DogDetectorFactory()\n\t\tbreak\n}\n\n// ...\n</code></pre>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/207329f7be8544db5607c2dc80a2d47408f0ab26.png\" alt=\"image\" data-base62-sha1=\"4D4120grOSydLB0LYcyfKj0XqoC\" width=\"284\" height=\"127\"></p>\n<p>\u2026 but you still have to hard-code the available choices in the script.</p>\n<hr>\n<p><a class=\"mention\" href=\"/u/tinevez\">@tinevez</a> It should also not be too difficult to enhance TrackMate to provide a widget that provides a choice of all <code>SpotDetectorFactory</code> classes available at runtime, such that it\u2019s possible to e.g. write:</p>\n<pre><code class=\"lang-groovy\">#@ SpotDetectorFactory detector\n// ...\nsettings.detectorFactory = detector\n</code></pre>\n<p>\u2026 and get a drop-down choice for all. Would you be interested?</p>", "<p>Totally!</p>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sunny.png?v=10\" title=\":sunny:\" class=\"emoji\" alt=\":sunny:\"></p>", "<p>Hello,<br>\nI would like to know whether there are more detailed subsets of TrackMate functionalities with macros ?<br>\nFor example:</p>\n<ol>\n<li>Setting \u201cLinking max distance\u201d and \u201cGap-closing max distance\u201d (figure 1)</li>\n<li>Export \u201cAll spots table\u201d (figure 2)</li>\n</ol>\n<p>Thank you so much !<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5.jpeg\" data-download-href=\"/uploads/short-url/xRqHgGg2FFnufSb116NEUdA77Xn.jpeg?dl=1\" title=\"figure 1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5_2_303x499.jpeg\" alt=\"figure 1\" data-base62-sha1=\"xRqHgGg2FFnufSb116NEUdA77Xn\" width=\"303\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5_2_303x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed51ee75a278caa9fc3f5b19c89021908eb81be5.jpeg 2x\" data-dominant-color=\"EAEAEA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">figure 1</span><span class=\"informations\">336\u00d7554 32.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f883160e69e24a1f41c4c464d3227c0460f48af.jpeg\" data-download-href=\"/uploads/short-url/fUEN36oZ3IizJN1vzcXsraSxruD.jpeg?dl=1\" title=\"figure 2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f883160e69e24a1f41c4c464d3227c0460f48af.jpeg\" alt=\"figure 2\" data-base62-sha1=\"fUEN36oZ3IizJN1vzcXsraSxruD\" width=\"457\" height=\"500\" data-dominant-color=\"F2F2F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">figure 2</span><span class=\"informations\">464\u00d7507 60.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello.</p>\n<p>No actually. To go beyond the features you mention you need to use Jython to script TrackMate.</p>", "<p>Regarding scripting Jython &amp; more flexible parameters, how about this approach:</p>\n<ul>\n<li>Whenever you save a TrackMate xml, it includes all the config options you set</li>\n<li>I wrote a small <a href=\"https://github.com/leogolds/MicroscopyPipeline/blob/3346facb359cfb448e01514a45aaa80fa58381f1/containers/TrackMate/read_settings_and_process_tiff_stack.py#L38\" rel=\"noopener nofollow ugc\">script</a> to load this config file and run TrackMate headlessly<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf.png\" data-download-href=\"/uploads/short-url/assJZyhPxUAvFVJS0fpyx7ztuzB.png?dl=1\" title=\"Screenshot_20230314-132630\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_225x500.png\" alt=\"Screenshot_20230314-132630\" data-base62-sha1=\"assJZyhPxUAvFVJS0fpyx7ztuzB\" width=\"225\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_225x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_337x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/494d6d4ca10fde0bda3ba1982fcfed32b32b58bf_2_450x1000.png 2x\" data-dominant-color=\"1E2127\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot_20230314-132630</span><span class=\"informations\">1080\u00d72400 290 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div>\n</li>\n<li>By <a href=\"https://github.com/leogolds/MicroscopyPipeline/tree/main/containers/TrackMate\" rel=\"noopener nofollow ugc\">containerizing</a> Fiji+TrackMate with the script, you get a fairly flexible way to operate TrackMate</li>\n</ul>\n<p>I like to have a few commonly used .xml files on a repo for reuse. As to generating the config files, notice you can click the save xml button before you do any tracking. That way you don\u2019t waste space on the actual tracking info and just get the settings you used.</p>\n<p><a href=\"https://github.com/leogolds/MicroscopyPipeline/blob/3346facb359cfb448e01514a45aaa80fa58381f1/models/trackmate/basic_settings.xml\" rel=\"noopener nofollow ugc\">Here\u2019s</a> an example of the most basic config file (label image detector, simple LAP).</p>\n<p>Leo</p>\n<p>P.s. shameless plug</p>"], "78185": ["<p>I\u2019m having trouble opening tiles of RGB24 format .ome.tif pyramidal files. This problem only occurs with RGB images of some formats like ome.tif while, 8 bit, &amp; 16 bit images open up fine. What is the format bytes are returned by the OpenBytes() function for RGB tiles? Attached an image showing the incorrect image when using the bytes directly to create a bitmap.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3.jpeg\" data-download-href=\"/uploads/short-url/1inMOrtFYVMNMFXVkjcWVzA9A1d.jpeg?dl=1\" title=\"Capture\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_690x335.jpeg\" alt=\"Capture\" data-base62-sha1=\"1inMOrtFYVMNMFXVkjcWVzA9A1d\" width=\"690\" height=\"335\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_690x335.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_1035x502.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/09162a7426a9eb71881c6d0cb69a44baf31d50b3_2_1380x670.jpeg 2x\" data-dominant-color=\"656565\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture</span><span class=\"informations\">1911\u00d7928 398 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Are you writing the file yourself ? How do you set and read the tile size ? Do you read the \u2018isinterleaved\u2019 property ?</p>\n<p>Can you share a file ?</p>", "<p>Thank you for the interleaving suggestion. Looks like the issue is with non-interleaved images. So far I haven\u2019t found a solution thought I will have to do some more searching. I am not writing the files myself, sharing any files is difficult due to their size.</p>", "<p>Hi Erik, if you are still having issues could you also provide some sample code showing how you are reading the data and creating the output image?</p>", "<p>Yes I\u2019m still having trouble. I found the example at <a href=\"https://github.com/melissalinkert/bioformats-examples/blob/4a07150286ef810ebdd1976942cf232890520bb3/src/main/java/TiledReaderWriter.java\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">bioformats-examples/TiledReaderWriter.java at 4a07150286ef810ebdd1976942cf232890520bb3 \u00b7 melissalinkert/bioformats-examples \u00b7 GitHub</a> however it doesn\u2019t seem to work for non-interleaved RGB images. Giving the resulting image I posted. Here is the C# code I\u2019m using which works for interleaved images.</p>\n<pre><code class=\"lang-auto\">byte[] bytesr = b.imRead.openBytes(b.Coords[coord.Z, 0, coord.T], tilex, tiley, sx, sy);\nBitmap bm = new Bitmap(b.file, sx, sy, PixelFormat, bytesr, coord, p, littleEndian);\n</code></pre>", "<p><a class=\"mention\" href=\"/u/dgault\">@dgault</a> So far I have found a workaround for this issue which partially works returning a grayscale image when dealing with RGB tiles. By doing the following to the bytes from OpenBytes().</p>\n<pre><code class=\"lang-auto\">int strplane = 0;\n            if (RGBChannelCount == 1)\n            {\n                if (b.bitsPerPixel &gt; 8)\n                    strplane = sx * 2;\n                else\n                    strplane = sx;\n            }\n            bytes = b.imRead.openBytes(b.Coords[coord.Z, coord.C, coord.T], tilex, tiley, sx, sy);\n            if (b.file.EndsWith(\"tif\") || b.file.EndsWith(\"ndpi\"))\n            {\n                byte[] rb = new byte[strplane * sy];\n                Bitmap[] bfs = new Bitmap[3];\n                for (int y = 0; y &lt; sy; y++)\n                {\n                    for (int x = 0; x &lt; strplane; x++)\n                    {\n                        rb[((strplane) * y) + x] = bytes[((strplane) * y) + x];\n                    }\n                }\n                if (b.bitsPerPixel == 8)\n                {\n                    return new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, rb, new ZCT(0, 0, 0), p, littleEndian);\n                }\n                else\n                {\n                    return new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, rb, new ZCT(0, 0, 0), p, littleEndian);\n                }\n            }\n</code></pre>", "<p>I found a workaround that works for RGB tiles.</p>\n<pre><code class=\"lang-auto\">byte[] bytesr = b.imRead.openBytes(b.Coords[coord.Z, 0, coord.T], tilex, tiley, sx, sy);\n            bool interleaved = b.imRead.isInterleaved();\n            if (!interleaved)\n            {\n                byte[] rb = new byte[strplane * sy];\n                byte[] gb = new byte[strplane * sy];\n                byte[] bb = new byte[strplane * sy];\n                int ind = 0;\n                for (int y = 0; y &lt; sy; y++)\n                {\n                    for (int st = 0; st &lt; strplane; st++)\n                    {\n                        rb[((strplane) * y) + st] = bytesr[((strplane) * y) + st];\n                        ind++;\n                    }\n                }\n                byte[] bytes = new byte[ind];\n                Array.Copy(bytesr,ind,bytes,0,ind);\n                for (int y = 0; y &lt; sy; y++)\n                {\n                    int x = 0;\n                    for (int st = 0; st &lt; strplane; st++)\n                    {\n                        int i = ((strplane) * y) + x;\n                        gb[i] = bytes[((strplane) * y) + st];\n                        x++;\n                    }\n                }\n                Array.Copy(bytesr, ind * 2, bytes, 0, ind);\n                for (int y = 0; y &lt; sy; y++)\n                {\n                    int x = 0;\n                    for (int st = 0; st &lt; strplane; st++)\n                    {\n                        int i = ((strplane) * y) + x;\n                        bb[i] = bytes[((strplane) * y) + st];\n                        x++;\n                    }\n                }\n                Bitmap[] bms = new Bitmap[3];\n                if (b.bitsPerPixel == 8)\n                {\n                    bms[2] = new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, rb, new ZCT(0, 0, 0), p, littleEndian);\n                    bms[1] = new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, gb, new ZCT(0, 0, 0), p, littleEndian);\n                    bms[0] = new Bitmap(b.file, sx, sy, PixelFormat.Format8bppIndexed, bb, new ZCT(0, 0, 0), p, littleEndian);\n                    return Bitmap.RGB8To24(bms);\n                }\n                else\n                {\n                    bms[2] = new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, rb, new ZCT(0, 0, 0), p, littleEndian);\n                    bms[1] = new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, gb, new ZCT(0, 0, 0), p, littleEndian);\n                    bms[0] = new Bitmap(b.file, sx, sy, PixelFormat.Format16bppGrayScale, bb, new ZCT(0, 0, 0), p, littleEndian);\n                    return Bitmap.RGB16To48(bms);\n                }\n            }\n</code></pre>", "<p>Thanks Erik, thats great that you got it sorted!</p>"], "72044": ["<p>Hi everyone,<br>\nI am using bioformats 6.10.1. This issue is potentially similar to other issues with line scans from Olympus microscopes (<a href=\"https://github.com/ome/bioformats/issues/3242\" rel=\"noopener nofollow ugc\">3242</a> and <a href=\"https://github.com/ome/bioformats/pull/3349\" rel=\"noopener nofollow ugc\">3349</a> and <a href=\"https://github.com/ome/bioformats/issues/3779\" rel=\"noopener nofollow ugc\">3779</a>), but this is for a Zeiss microscope (czi file). I have acquired line scans with 100,000 time points, and when I use bioformats either in ImageJ or in Matlab to read the czi file, only about 3300 lines are read-in as data. The rest are set to zeros. This does not happen with the native Zen reader\u2026all of the data are there in the file.</p>\n<p>The data that are properly read-in seem to be in arbitrary places. Every file I\u2019ve checked has the first line read-in properly, then another line soon after (like time point <span class=\"hashtag\">#4</span>), then after that every 30 or so lines. These correspond to their proper lines in the Zen reader (ie, bioformats is setting line 2 to zeros rather than taking line 2 and putting it into line 4, etc.)</p>\n<p>Here is a working example file (32 Mb):</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.dropbox.com/s/gqihxtaof2e66yb/linescan_1_exp.czi?dl=0\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/0279ebc78567aad1d35e85b3a72fc71ec7ce568f.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.dropbox.com/s/gqihxtaof2e66yb/linescan_1_exp.czi?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">Dropbox</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0dff56437732419ff48c9a41c3bf8f37f45d0185.png\" class=\"thumbnail onebox-avatar\" width=\"160\" height=\"160\">\n\n<h3><a href=\"https://www.dropbox.com/s/gqihxtaof2e66yb/linescan_1_exp.czi?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">linescan_1_exp.czi</a></h3>\n\n  <p>Shared with Dropbox</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>To even attempt to open the file in Matlab with bioformats 6.10.1, I had modify line 152 of bfopen. That line previously was \u201ccolorMaps = cell(numImages);\u201d but that resulted in pre-allocating a cell of size 100,000 x 100,0000, which was too big for memory (and likely not what was originally intended with that line). I changed it to \u201ccolorMaps = cell(numSeries,numImages);\u201d, which fits what the rest of the code apparently was trying to do. That is a separate bug report, though.</p>\n<p>I appreciate any help with this!<br>\nGreg</p>", "<p>Thanks Greg for providing the sample file. I was able to reproduce the issue you described with the latest Bio-Formats. I have opened a new GitHub Issue to track this bug: <a href=\"https://github.com/ome/bioformats/issues/3881\" class=\"inline-onebox\">Zeiss CZI: Line scans missing data \u00b7 Issue #3881 \u00b7 ome/bioformats \u00b7 GitHub</a></p>\n<p>I have also opened a separate issue for the reported MatLab bug: <a href=\"https://github.com/ome/bioformats/issues/3882\" class=\"inline-onebox\">MatLab: Bfopen preallocating cell of wrong size \u00b7 Issue #3882 \u00b7 ome/bioformats \u00b7 GitHub</a></p>", "<p>Hi, I was wondering if any progress has been made on this issue? I am in need of analyzing my time courses that have the dropped data. I understand that the community has a priority list and might not be able to get to my personal bug <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">  I am just wondering to see if I need to pursue a fix myself or if it would be better to wait.</p>\n<p>Thank you!</p>", "<p>Hi Greg, unfortunately there is still no fix in place for <a href=\"https://github.com/ome/bioformats/issues/3525\">this issue</a>. If you are able to work on a fix for the reader we would love to help you include that in Bio-Formats. You can see some links to docs on how to contribute <a href=\"https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/CONTRIBUTING.md\">here</a>, I can offer guidance and help with testing and integration if needed.</p>", "<p>Thank you. I\u2019ll see if I can hire someone to my team that can work on it over the next few months.</p>"], "78190": ["<p>Hello team.<br>\nI installed qupath on a distant server (calculation cluster), and I am running it through command line, with the help of this documentation: <a href=\"https://qupath.readthedocs.io/en/stable/docs/advanced/command_line.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Command line \u2014 QuPath 0.4.3 documentation</a>.</p>\n<p>It works perfectly, and confirm that your work on this softawre is amazing.</p>\n<p>But I am kind of stuck, I can\u2019t use extensions. I didn\u2019t find a way to set the extension folder path, and I have the import error \u2018import.ext.StarDist.StaDist2D\u2019.<br>\nI tried to open the distant program on my local machine to set this parameter (the absolute path on distant server), but it just set it on my local machine. I tried to put my extension folder (or directly files inside) on the QuPath app folder. Nothing worked.<br>\nI don\u2019t see a way to set this parameter as a command line argument.</p>\n<p>Do you see a way to set it modifying a file or something?</p>\n<p>Thank you very much for your solid work.<br>\nHave a nice day.</p>", "<p>Hi <a class=\"mention\" href=\"/u/dam\">@Dam</a> you could try setting the property directly through a script:</p>\n<pre><code class=\"lang-groovy\">qupath.lib.gui.prefs.PathPrefs.userPathProperty().set('/path/to/user/path/')\n</code></pre>\n<p>This should then be retained in the preferences. Note that you need to give the user path; \u2018extensions\u2019 is a subdirectory inside that.</p>\n<p>I haven\u2019t tried it in a scenario like the one you describe, but it\u2019s my best guess at a workaround.</p>", "<p>Hello Pete,</p>\n<p>Thank you very much for your response.</p>\n<p>I launched a script with just a line as you said:</p>\n<pre><code class=\"lang-auto\">qupath.lib.gui.prefs.PathPrefs.userPathProperty().set('/path/to/user/path/')\n</code></pre>\n<p>with the command :</p>\n<pre><code class=\"lang-auto\">declare qupath=\"$HOME/Program_files/QuPath/bin/QuPath.sh\"\ndeclare groovy_script1=\"$path/QP_Proj/scripts/0-Set_Preferences.groovy\"\n$qupath script $groovy_script1 --save\n</code></pre>\n<p>And I got a successful response:</p>\n<pre><code class=\"lang-auto\">23:06:13.847 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Refreshing extensions in /gpfs/home/user/qupath/extensions\n23:06:13.880 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Added extension: /gpfs/home/user/qupath/extensions/qupath-extension-imagecombiner-0.2.3.jar\n23:06:13.892 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Added extension: /gpfs/home/user/qupath/extensions/qupath-extension-stardist-0.4.0.jar\n23:06:13.902 [main] [INFO ] qupath.lib.gui.ExtensionClassLoader - Added extension: /gpfs/home/user/qupath/extensions/qupath-extension-cellpose-0.6.1.jar\n\n</code></pre>\n<p>And now when I launch a script my extenstion file is taken in account.</p>\n<p>Thank you very much for the fast response!</p>"], "78197": ["<p>Good day to everyone.<br>\nI am currently conducting a project to analyze the integral and differential uniformity values of gamma camera images acquired via a QC test. Is there a way to produce these results in ImageJ without using plugins? I am hoping to obtain the values for:</p>\n<p>Integral uniformity<br>\nX differential uniformity<br>\nY differential uniformity</p>\n<p>This is my first time using ImageJ. Thank you in advance.</p>"], "78199": ["<p>The images are stored in a 3D stacked tiff. And when I run it on cellfinder, I just meet this error and don\u2019t konw how to process(Fig1):</p>\n<p>I also tried it on napari with cell detection plugin, but it was just stuck like this(Fig2):<br>\nThanks a lot for your help!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218.png\" data-download-href=\"/uploads/short-url/xlysw4FhbGzkfWhkqeFu3vza65a.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_690x250.png\" alt=\"image\" data-base62-sha1=\"xlysw4FhbGzkfWhkqeFu3vza65a\" width=\"690\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_690x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_1035x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/9/e9b78774e8c173d51c1b793cb259cbebca03f218_2_1380x500.png 2x\" data-dominant-color=\"151616\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">3516\u00d71276 371 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b.png\" data-download-href=\"/uploads/short-url/lf0b5IIoQCCp2gVqfNaARwMFsJd.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_690x190.png\" alt=\"image\" data-base62-sha1=\"lf0b5IIoQCCp2gVqfNaARwMFsJd\" width=\"690\" height=\"190\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_690x190.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_1035x285.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94dfcfaab415fac5caf65dd4a86776cc0536775b_2_1380x380.png 2x\" data-dominant-color=\"222222\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">3815\u00d71055 196 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/amaranth123\">@amaranth123</a>,</p>\n<p>The cellfinder command-line tool doesn\u2019t support 3D tiffs. However, the napari plugin should work. How big is the image?</p>\n<p>Could you try creating a new conda environment and installing everything from scratch, then try again?</p>\n<p>Thanks,<br>\nAdam</p>", "<p>Hi. Adam,<br>\nI did just as you said here, but it was still stuck with such error while it was received for 3D filtering on napari :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54.png\" data-download-href=\"/uploads/short-url/nLwDpxqBfWSjpvuV3F0BXovUQte.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_690x358.png\" alt=\"image\" data-base62-sha1=\"nLwDpxqBfWSjpvuV3F0BXovUQte\" width=\"690\" height=\"358\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_690x358.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_1035x537.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a691556011eee2dfc2923338cddb7f31a2692f54_2_1380x716.png 2x\" data-dominant-color=\"191A16\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">3816\u00d71984 505 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Could you describe your data (what is the image, how big is it, what resolution etc), and also attach a screenshot of the cell detection parameters?</p>", "<p>yean thanks,</p>\n<p>my data are 2 dirs with 1233 .tiff(12.1 GB in total) respectively.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256.jpeg\" data-download-href=\"/uploads/short-url/oT0ELG3gAdaorVSMKLtW2ALnnfw.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_690x363.jpeg\" alt=\"image\" data-base62-sha1=\"oT0ELG3gAdaorVSMKLtW2ALnnfw\" width=\"690\" height=\"363\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_690x363.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_1035x544.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae6c168e571cbaf019ffea8e83b00983b77c7256_2_1380x726.jpeg 2x\" data-dominant-color=\"1F2227\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71011 71.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c.png\" data-download-href=\"/uploads/short-url/eNOcuzIEaTd7st8ciaA406XY74E.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_690x343.png\" alt=\"image\" data-base62-sha1=\"eNOcuzIEaTd7st8ciaA406XY74E\" width=\"690\" height=\"343\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_690x343.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_1035x514.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67bfd7d877ce8844be1242364daf9dd2aa25554c_2_1380x686.png 2x\" data-dominant-color=\"EADFD2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">3671\u00d71829 295 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2.jpeg\" data-download-href=\"/uploads/short-url/jhJPxLoqZ3TWyxhQ6wX3OYugQ8i.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_690x364.jpeg\" alt=\"image\" data-base62-sha1=\"jhJPxLoqZ3TWyxhQ6wX3OYugQ8i\" width=\"690\" height=\"364\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_690x364.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_1035x546.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/7/872aa5e157bed5b9a00fea3799c3cfbd9b7cd6b2_2_1380x728.jpeg 2x\" data-dominant-color=\"1F2227\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71013 69.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Could you share the data so I can take a look myself?</p>", "<p>sure,thanks a lot for your help. I will email my data to you.</p>", "<p>The last error you posted seems to be the same as <a href=\"https://forum.image.sc/t/cellfinder-issue-exception-in-thread-thread-1/78332/3\">this issue</a>, so could you try increasing the <code>Number of free CPUs</code> option?</p>\n<p>Would you mind posting all related issues in the same forum thread? It would make it easier for me (and others) to keep track. Thanks!</p>", "<p>Hey <a class=\"mention\" href=\"/u/amaranth123\">@amaranth123</a> just as an exercise I tried running cellfinder with your data. I converted the data to directories of single-plane tiffs and ran cellfinder with:</p>\n<pre><code class=\"lang-auto\">cellfinder -s signal -b background -o cellfinder_output -v 5 5 5 --orientation sal --atlas allen_mouse_25um\n</code></pre>\n<p>Cell detection seems ok with the default parameters, I\u2019m sure this can be improved with a bit of playing around though. I very quickly (1 minute) generated some training data and re-trained the network (2 epochs), and the results seem fairly promising.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2.png\" data-download-href=\"/uploads/short-url/yuennKmDOdyURxXdOotDcu4Auz0.png?dl=1\" title=\"cf\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2_2_552x500.png\" alt=\"cf\" data-base62-sha1=\"yuennKmDOdyURxXdOotDcu4Auz0\" width=\"552\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2_2_552x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2_2_828x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1b4c62cc69eb4a35657af2cbeb236117a16c6d2.png 2x\" data-dominant-color=\"2F2F2E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cf</span><span class=\"informations\">874\u00d7791 308 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The registration however isn\u2019t good, I think because there is very little autofluorescence in these images. It\u2019s slightly better using the signal channel than the background though. Could be improved with some tweaking though I expect.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2d5cda4c2f34a80537848f0fa0eb1f01f5af3a.png\" data-download-href=\"/uploads/short-url/t8eALXjwRvmAGxy2OG8V1IMDMTw.png?dl=1\" title=\"bg\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2d5cda4c2f34a80537848f0fa0eb1f01f5af3a.png\" alt=\"bg\" data-base62-sha1=\"t8eALXjwRvmAGxy2OG8V1IMDMTw\" width=\"690\" height=\"414\" data-dominant-color=\"2E2A2B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">bg</span><span class=\"informations\">1530\u00d7920 47.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I\u2019m not sure why you\u2019re having so many issues running the tools on your machine, do you have another computer to try?</p>", "<p>Hello,</p>\n<p>I sincerely appreciate all your help!</p>\n<p>It is really strange, could you please share you envs and pkgs in your .conda file through email?</p>\n<p>I am really doubting there are some wrongs for my installation but I should do it according to your website\u2026 I have retried it on another computer but just met the same issues\u2026</p>\n<p>Thanks again\uff01</p>", "<p>And if it is convenient for you, could you please share figures to show how you nominate these tiffs images and how you write you path while running cellfinder??</p>\n<p>Thanks a lot!</p>", "<p>Hey, I don\u2019t have the environment to hand, but I ran:</p>\n<pre><code class=\"lang-bash\">conda create --name ENV_NAME python=3.10 -y\nconda activate ENV_NAME\npip install cellfinder\n</code></pre>\n<p>I converted the image using <a href=\"https://imagej.net/software/fiji/\">FIJI</a> by:</p>\n<ul>\n<li>Creating a directory to save the images (one called <code>signal</code>, and one <code>background</code>)</li>\n<li>Opening FIJI</li>\n<li>Dragging the 3D image onto the main FIJI window</li>\n<li>Saving the image as a series of 2d tiffs by <code>File</code> \u2192 <code>Save as</code> \u2192 <code>Image Sequence</code> and choosing the appropriate directory (leaving the other options as default)</li>\n</ul>\n<p>I then ran cellfinder as per the instructions passing in the data directories, e.g.:</p>\n<pre><code class=\"lang-auto\">cellfinder -s signal -b background -o output -v 5 5 5 --orientation sal\n</code></pre>\n<p>Hope this helps.<br>\nAdam</p>", "<p>Hi, Adam:</p>\n<p>Thanks a lot for all your help!!!</p>\n<p>I really found where my issues are!! Cellfinder requried directories of tif images but not tiff images!!!<br>\nI preivous transformed my data into tiff images through the plugin of Fiji. and now when I transformed them into tif images, all the issues are sloved!!</p>\n<p>Thanks a lot\uff01\uff01</p>"], "78204": ["<p>I\u2019m using Mac M1 Pro and Cellpose via both Jupyter and its GUI. I\u2019d like to speed up my workflow (currently 1 image segmentation ~ 10 s) by running Cellpose on my GPU. I\u2019m aware CUDA is designed for NVidia GPUs, though from <a href=\"https://forum.image.sc/t/cellpose-on-macos-m1-pro-apple-silicon-arm64/68018\">this thread</a> it seems like there are ways to get Cellpose running on M1 Macs using Metal Performance Shaders (MPS). I\u2019ve tried replicating the steps listed there, but I always end with Cellpose running on CPU.</p>\n<pre><code class=\"lang-auto\">from cellpose import models, core\ncore.use_gpu()\n</code></pre>\n<p>always gives me False.</p>\n<pre><code class=\"lang-auto\">from cellpose.io import logger_setup\nlogger_setup();\n</code></pre>\n<p>gives</p>\n<pre><code class=\"lang-auto\">2023-03-07 12:55:10,463 [INFO] WRITING LOG OUTPUT TO /Users/bocan/.cellpose/run.log\n2023-03-07 12:55:10,474 [INFO] \ncellpose version: \t2.2 \nplatform:       \tdarwin \npython version: \t3.10.9 \ntorch version:  \t1.13.1\n</code></pre>\n<p>Running the cellpose model itself also prints</p>\n<pre><code class=\"lang-auto\">[INFO] TORCH CUDA version not installed/working.\n[INFO] &gt;&gt;&gt;&gt; using CPU\n[INFO] WARNING: MKL version on torch not working/installed - CPU version will be slightly slower.\n</code></pre>\n<pre><code class=\"lang-auto\">import torch\nprint(torch.backends.mps.is_available())\nprint(torch.backends.mps.is_built())\n</code></pre>\n<p>returns both True.</p>\n<p>I also noticed that</p>\n<pre><code class=\"lang-auto\">print(*torch.__config__.show().split(\"\\n\"), sep=\"\\n\")\n</code></pre>\n<p>prints</p>\n<pre><code class=\"lang-auto\">... LAPACK_INFO=accelerate, TORCH_VERSION=1.13.1, USE_CUDA=0, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=OFF, USE_ROCM=OFF, \n</code></pre>\n<p>so there might be problem with MKL(DNN)?<br>\nThis seems to be way over my beginner skills so I\u2019d really appreciate any insight and recommendations.<br>\nThanks a ton!</p>", "<p>I\u2019m not sure we need another thread for the same topic.<br>\nAs discussed in that thread, the released cellpose does not appear to fully use MPS to use GPU on macOS arm64. You can try my fork where I\u2019ve gotten it to work see my recent post:</p><aside class=\"quote quote-modified\" data-post=\"35\" data-topic=\"68018\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/cellpose-on-macos-m1-pro-apple-silicon-arm64/68018/35\">Cellpose on MacOS M1 Pro (Apple Silicon arm64)</a> <a class=\"badge-wrapper  bullet\" href=\"/c/development/5\"><span class=\"badge-category-bg\" style=\"background-color: #F7941D;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for development (i.e., programming) questions about scientific image software.\">Development</span></a>\n  </div>\n  <blockquote>\n    So I\u2019ve not had any luck with released cellpose to use MPS. <a class=\"mention\" href=\"/u/jluethi\">@jluethi</a> can chime in as well. \nI made a fork that works for me. You can try it, like Joel did. His post has good details: \n\nFor him I think it worked for inference but not training. \nAlso you may want to use Nightly PyTorch (see <a href=\"http://PyTorch.org\" rel=\"noopener nofollow ugc\">PyTorch.org</a>) \nAnyhow, it\u2019s on my TODO to figure out/upstream my changes. \nAbout video tutorials, I\u2019m not sure how to make those\u2014I\u2019m very much a beginner myself, I\u2019ve just had an M1 mac for a while I guess.\n  </blockquote>\n</aside>\n<p>\nOtherwise, you need to be patient. Apple is still rapidly iterating MPS support in PyTorch, so it\u2019s totally understandable that the cellpose folks haven\u2019t gotten it all ironed out.</p>"], "78206": ["<p>This is a cross-post from the Scientific Python Discourse. The original post can be <a href=\"https://discuss.scientific-python.org/t/would-you-like-to-see-a-imageio-community-meetings-mailing-list-forum/667/1\" rel=\"noopener nofollow ugc\">found here</a>.</p>\n<hr>\n<p><strong>TL:DR</strong> <a href=\"https://github.com/imageio/imageio\" rel=\"noopener nofollow ugc\">ImageIO</a> is looking for a place to build a community for people that have an interest or stake in reading/writing image data in Python. If you are interested, head over to the <a href=\"https://discuss.scientific-python.org/t/would-you-like-to-see-a-imageio-community-meetings-mailing-list-forum/667/1\" rel=\"noopener nofollow ugc\">Scientific Python Discourse</a> and let us know your preferred format.</p>\n<p>I am the maintainer of <a href=\"https://github.com/imageio/imageio\" rel=\"noopener nofollow ugc\">ImageIO</a>, a library that makes it easy to read/write image and video data and that powers many other libraries including scikit-image, napari, cellprofiler, moviepy, etc.</p>\n<p>While there is a lot of interest in the library (currently ~ 13M downloads/month), it doesn\u2019t yet have an obvious place for discussion, feedback, and community beyond GitHub Issues. I want to create this place and am currently figuring out which format to choose. If you are interested in joining (e.g., because you are actively using ImageIO, or because you are curious about the library) please do head over to the <a href=\"https://discuss.scientific-python.org/t/would-you-like-to-see-a-imageio-community-meetings-mailing-list-forum/667/1\" rel=\"noopener nofollow ugc\">discussion thread</a> and let me know which format would work well for you <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78211": ["<p>Continuing the discussion from <a href=\"https://forum.image.sc/t/stardist-error-message-version-issue/75803/4\">Stardist error message - version issue?</a>:</p>\n<p>Dear all,</p>\n<p>I am quite new to this forum and Bioimage Analysis in general and I have trouble with the StarDist Plugin in Fiji.<br>\nI have been trying to use the StarDist Plugin in Fiji for the segmentation of DAPI-stained nuclei in my IF images (1560.43x1710.88 microns, 16-bit, 176 MB, TIFF)</p>\n<p>When I follow the usual path (Plugins&gt;StarDist&gt;StarDist 2D) the software either crashes completely or shows the following error message/exception:</p>\n<pre><code class=\"lang-auto\">java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)\n\t... 10 more\nCaused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n\t... 4 more\nCaused by: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.nio.Buffer.&lt;init&gt;(Buffer.java:199)\n\tat java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:281)\n\tat java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:289)\n\tat java.nio.MappedByteBuffer.&lt;init&gt;(MappedByteBuffer.java:89)\n\tat java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:162)\n\tat org.tensorflow.Tensor.buffer(Native Method)\n\tat org.tensorflow.Tensor.buffer(Tensor.java:570)\n\tat org.tensorflow.Tensor.writeTo(Tensor.java:473)\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:171)\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\njava.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)\n\t... 10 more\nCaused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n\t... 4 more\nCaused by: java.lang.IllegalArgumentException: Negative capacity: -991022080\n\tat java.nio.Buffer.&lt;init&gt;(Buffer.java:199)\n\tat java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:281)\n\tat java.nio.ByteBuffer.&lt;init&gt;(ByteBuffer.java:289)\n\tat java.nio.MappedByteBuffer.&lt;init&gt;(MappedByteBuffer.java:89)\n\tat java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:162)\n\tat org.tensorflow.Tensor.buffer(Native Method)\n\tat org.tensorflow.Tensor.buffer(Tensor.java:570)\n\tat org.tensorflow.Tensor.writeTo(Tensor.java:473)\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:171)\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n[INFO] CSBDeep plugin exit (took 17145 milliseconds)\n[INFO] Using default TensorFlow version from JAR: TF 1.15.0 CPU\n[INFO] Loading TensorFlow model GenericNetwork_aea3be563cb56b8824f53a8c2382aaa5 from source file file:/C:/Users/AG_SCH~1/AppData/Local/Temp/stardist_model_7870431095929648409.zip\n[INFO] Shape of input tensor: [-1, -1, -1, 1]\n[INFO] Shape of output tensor: [-1, -1, -1, 33]\n[INFO] Normalize .. \n[INFO] Dataset type: 32-bit signed float, converting to FloatType.\n[INFO] Dataset dimensions: [9179, 10064]\n[INFO] INPUT NODE: \n[INFO] Mapping of tensor input: \n[INFO]    datasetAxes:[X, Y]\n[INFO]    nodeAxes:[(Time, -1), (Y, -1), (X, -1), (Channel, 1)]\n[INFO]    mapping:[2, 1, 0, 3]\n[INFO] OUTPUT NODE: \n[INFO] Mapping of tensor output: \n[INFO]    datasetAxes:[X, Y]\n[INFO]    nodeAxes:[(Time, -1), (Y, -1), (X, -1), (Channel, 33)]\n[INFO]    mapping:[2, 1, 0, 3]\n[INFO] Complete input axes: [X, Y, Time, Channel]\n[INFO] Tiling actions: [TILE_WITH_PADDING, TILE_WITH_PADDING, TILE_WITHOUT_PADDING, NO_TILING]\n[INFO] Dividing image into 1 tile(s)..\n[INFO] Size of single image tile: [9184, 10064, 1, 1]\n[INFO] Final image tiling: [1, 1, 1, 1]\n[INFO] Network input size: [9184, 10064, 1, 1]\n[INFO] Processing tile 1..\njava.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)\n\t... 10 more\nCaused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n\t... 4 more\nCaused by: java.lang.NegativeArraySizeException\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:170)\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\njava.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:110)\n\tat de.csbdresden.csbdeep.network.DefaultModelExecutor.run(DefaultModelExecutor.java:71)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tileAndRunNetwork(GenericCoreNetwork.java:594)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.tryToTileAndRunNetwork(GenericCoreNetwork.java:567)\n\tat de.csbdresden.csbdeep.commands.GenericCoreNetwork.mainThread(GenericCoreNetwork.java:469)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)\n\tat java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005)\n\t... 10 more\nCaused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:110)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.call(DefaultNetwork.java:51)\n\tat java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)\n\t... 4 more\nCaused by: java.lang.NegativeArraySizeException\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:170)\n\tat net.imagej.tensorflow.Tensors.imgFloat(Tensors.java:262)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.DatasetTensorFlowConverter.tensorToDataset(DatasetTensorFlowConverter.java:70)\n\tat de.csbdresden.csbdeep.network.model.tensorflow.TensorFlowNetwork.execute(TensorFlowNetwork.java:359)\n\tat de.csbdresden.csbdeep.network.model.DefaultNetwork.lambda$call$0(DefaultNetwork.java:102)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n[INFO] CSBDeep plugin exit (took 41176 milliseconds)\n</code></pre>\n<p>I increased the number of tiles to 2 in \u201cAdvanced Options\u201d as suggested with previous crashing problems with large images, see <a href=\"https://forum.image.sc/t/error-with-stardist-plugin-on-a-large-image/56465\" class=\"inline-onebox\">Error with stardist plugin on a large image</a>. The segmentation runs without crashing but is rather incomplete as shown below\u2026</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c.jpeg\" data-download-href=\"/uploads/short-url/4PqJsz82JmHSOkmyCSb4ywiDMcc.jpeg?dl=1\" title=\"Screenshot StarDist tiles\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_199x374.jpeg\" alt=\"Screenshot StarDist tiles\" data-base62-sha1=\"4PqJsz82JmHSOkmyCSb4ywiDMcc\" width=\"199\" height=\"374\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_199x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_298x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21d906bbe59e417df013f89257472c1edee8021c_2_398x748.jpeg 2x\" data-dominant-color=\"EBEBEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot StarDist tiles</span><span class=\"informations\">525\u00d7985 89.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e.jpeg\" data-download-href=\"/uploads/short-url/sWYSxNUfMrcd4F7GRvxo6VWJWDY.jpeg?dl=1\" title=\"Label Image_2tiles\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_342x374.jpeg\" alt=\"Label Image_2tiles\" data-base62-sha1=\"sWYSxNUfMrcd4F7GRvxo6VWJWDY\" width=\"342\" height=\"374\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_342x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_513x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cae7b5159add322874f8fd8fb3ac5e01c390032e_2_684x748.jpeg 2x\" data-dominant-color=\"0F0F0F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Label Image_2tiles</span><span class=\"informations\">1920\u00d72105 233 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Does anyone have any ideas on how to solve this problem?</p>\n<p>Thank you very much in advance</p>\n<p>Theresa</p>", "<p>Hi <a class=\"mention\" href=\"/u/theresa_p\">@Theresa_P</a>, this reminds me of these previous discussions:</p>\n<aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"56465\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/niederle/40/7123_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/error-with-stardist-plugin-on-a-large-image/56465\">Error with stardist plugin on a large image</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hi, \nI\u2019ve already reported the error on the stardist-github and got the recommendation to ask here as it might be a Fiji specific error (<a href=\"https://github.com/CSBDeep/CSBDeep_website/issues/6\" rel=\"noopener nofollow ugc\">this one was mentioned</a> ) \nI tried to run the StarDist 2D plugin on a 5108x4308 16-bit tiff and got the following error: \n[INFO] Reading available sites from https://imagej.net/\n[INFO] Successfully retrieved OPTIONS.\n[INFO] Successfully called PropFind, directory exists: .\n[INFO] Successfully locked db.xml.gz.lock.\n[INFO] Successfully uploaded to db.xml.gz.lock\n\u2026\n  </blockquote>\n</aside>\n\n<aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"60373\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/lguerard/40/14201_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/issue-with-stardist-w-or-w-o-trackmate-with-large-images/60373\">Issue with StarDist (w/ or w/o TrackMate) with large images</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hi all, \nLinked to <a href=\"https://forum.image.sc/t/issue-with-trackmate-and-stardist-on-big-dataset/57976\">this topic</a>, I wanted to report that I also encountered a similar issue today without the use of the Imglib2-imaris-bridge. \nAs you can see from the screenshot, the detector somehow stopped after a while: \n <a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b42c46143b7f72f422db78b8468d686970d946d.jpeg\" data-download-href=\"/uploads/short-url/aJMRcKqKkTVSWsexIr3vgTQXd0N.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\">[image]</a> \nData is not that big, it\u2019s 3.3Gb\u2026 \nI also tried running StarDist alone and it is also showing the top of the picture so I guess it is really linked to the StarDist people (which is why I created this new topic). \nAny ideas why ? Thanks !\n  </blockquote>\n</aside>\n\n<p>Please try to increase the number of tiles and see if the problem goes away. (Not a great solution, but might be working.)</p>", "<p>Hello <a class=\"mention\" href=\"/u/uschmidt83\">@uschmidt83</a>,</p>\n<p>it seems I simply had to increase the tiles even further than what I did before, so not only to 2 or 4 tiles.<br>\nNow, when using 8 tiles, the segmentation works fine.<br>\nThank you very much!</p>"], "78214": ["<p>Dear Qupath user,</p>\n<p>I try to find a way to extract cells objects based on two thershold ?  We develop some mask colocalisation that gives the identification obtect based on the mask number (Ch1=1, Ch2=2 so Ch1+Ch2 = 3). This measure is created on each detections. I would like to extract the Ch2 object that gives 2.In fact that is a bit more complicated because I have 4 channels.  I can put a low or high threshold to extract the two others values  but not the value  in between \u2026<br>\nThanks for your help.</p>\n<p>Cheers, Mathieu</p>", "<p>I think to create mid-level masks you run multiple thresholders. In your case, run 2 and above, and then run below 3 within that.</p>", "<p>So you mean doing that in two times yes correct it should work , it is a bit long with 4 channels possibilities (15 levels) \u2026I will try thanks</p>", "<p>If the images are small enough, it might be easier to do in ImageJ, or through a script calling ImageJ.</p>\n<p>There might be some other pixel level functions that might work better if you search the forum, but I don\u2019t remember them off the top of my head.</p>", "<p>of course this is a big image that is why I use Qupath\u2026but I can try</p>", "<p>Yep, here\u2019s an example of using a ImageJ based script that might require downsampling on large images to find objects of unique values <a href=\"https://forum.image.sc/t/import-external-masks-in-qupath/75516/5\" class=\"inline-onebox\">Import external masks in QuPath - #5 by sophia1</a></p>\n<p>Usually intended for masks of cell objects.</p>", "<p>I don\u2019t entirely understand the question, but I\u2019m throwing this thread out there in case it\u2019s helpful:</p>\n<aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"71579\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/smcardle/40/17938_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579\">Script for generating double threshold classifier</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hi All, \nHere\u2019s a script I wrote that will hopefully be useful to some people. \nNormally, if I need to find double-positive regions, I run two pixel classifiers, then use Java geometry functions to find the object intersections. This works, but if there are many small detections, it can be quite slow. The script below uses ImageOps to apply a threshold to two different channels and writes it as a single pixel classifier. Then, the \u201ccolocalized\u201d objects can be created directly through normal QuPa\u2026\n  </blockquote>\n</aside>\n"], "37255": ["<p>I have stained for collagen in my tissues with Sirius Red stain, I have attached one of the images below. Qupath seems to be adapted to quantify histological images where cells are also stained.</p>\n<p>Is it possible to quantify image stianing when cells are not stained but tissue components such as collagen?</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/3/63f6dbef6393b6860d0da8eaa5557932df796658.png\" alt=\"grafik\" data-base62-sha1=\"egka5OxyYz7aD3YQ9zHhmCFZdGg\" width=\"564\" height=\"406\"></p>", "<p>If by quantification you mean area, that is fairly straightforward using either the pixel classifier or simple thresholding tool in M10/M11, or the Positive Pixel detection in 0.1.2. I have never seen anyone trying to differentiate between the two areas you have indicated, and from the quality of image provided can\u2019t see differences.</p>\n<p>It may be that a pixel classifier could pick them up if there are texture differences, but I don\u2019t think a simple thresholder would suffice to separate the two types of tissue.</p>\n<p><a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> and <a class=\"mention\" href=\"/u/zbigniew_mikulski\">@Zbigniew_Mikulski</a> have more experience with PSR and might have other suggestions.</p>", "<p>Thanks a lot for your helpful response. I want to see the area of the image occupied by the fibrotic areas which I have enclosed in a selection with red borders.  Fibrotic areas as well as connective tissues near airways have higher collagen content but the latter is normal. I am not able to set a colour threshold to separate the fibrotic and normal areas.</p>\n<p>My objective is to define areas of fibrosis, have appttern and apply to all other images I have from the same staining experiment.</p>", "<aside class=\"quote no-group\" data-username=\"ghu_gha\" data-post=\"3\" data-topic=\"37255\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ghu_gha/40/29764_2.png\" class=\"avatar\"> ghu_gha:</div>\n<blockquote>\n<p>Fibrotic areas as well as connective tissues near airways have higher collagen content but the latter is normal. I am not able to set a colour threshold to separate the fibrotic and normal areas</p>\n</blockquote>\n</aside>\n<p>Yes, with that problem, you would need to try the pixel classifier, and make sure you try out many of the alternative \u201cFeatures\u201d possible, and run it at very high resolution. It will be\u2026 slow.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/1/11ef71055d98cd2795565b008c3c94be0bb72a81.png\" data-download-href=\"/uploads/short-url/2yF7jFtch4GKnxEhDY4BgT603qF.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/1/11ef71055d98cd2795565b008c3c94be0bb72a81.png\" alt=\"image\" data-base62-sha1=\"2yF7jFtch4GKnxEhDY4BgT603qF\" width=\"504\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/1/11ef71055d98cd2795565b008c3c94be0bb72a81_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">606\u00d7601 29.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>You might also try Weka pixel classifier through FIJI, or Ilastik.</p>", "<p>Thanks a lot.So I can run the pixel classifier on the fibrotic areas and then apply the criteria to all my other slides?</p>", "<p>That should currently be possible through scripting.</p><aside class=\"quote quote-modified\" data-post=\"7\" data-topic=\"30960\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/d/a587f6/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-v0-2-0-m5-now-available/30960/7\">QuPath v0.2.0-m5 now available</a> <a class=\"badge-wrapper  bullet\" href=\"/c/announcements/10\"><span class=\"badge-category-bg\" style=\"background-color: #AB9364;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for announcements, such as new software releases and upcoming community events.\">Announcements</span></a>\n  </div>\n  <blockquote>\n    Actually, after doing some spelunking into the source code I came up with this script which seems to get the job done! \nimport qupath.lib.gui.ml.PixelClassifierTools\n\n//Need a full image annotation to begin with\ncreateSelectAllObject(true)\ndef annotations = getAnnotationObjects()\n\n//Define pixel classifier\ndef project = getProject()\ndef classifier = project.getPixelClassifiers().get('2019-11-04 Pixel Model')\n\n//Define image data\ndef imageData = getCurrentImageData()\n\n//Convert pixel classifier t\u2026\n  </blockquote>\n</aside>\n", "<p>I\u2019m going to jump in here and recommend superpixels. I love them for acellular regions like this. I find that they are better at \u201cseeing\u201d large scale features than the pixel classifier. <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> has a great description of them <a href=\"https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/19\">here</a>.</p>\n<p>The general workflow would be to 1) generate superpixels, 2) calculate features on those regions, including basic intensity features and haralick measurements and anything else you think might be helpful, 3) train an object classifier to identify fibrotic vs non-fibrotic regions, 4) classify all the objects in all images in your project, 5) (optional) use \u201ctile classifications to annotations\u201d to merge the detection objects into annotation objects (<a href=\"https://github.com/qupath/qupath/blob/e488e4e0aaa38dd950e050734e41fa0b3b2ab722/CHANGELOG.md\" rel=\"nofollow noopener\">with a recent bug fix in m10</a>), 6) measure the area of the classified objects.</p>", "<p>Thank you so much. In step 5 of the workflow you have kindly proposed, I need to classify  all objects in all images in my project. Can this be done automatically or I have to door manually?</p>", "<p>This might help as well.</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"29515\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-guide-creating-classified-annotation-areas-using-slics/29515\">QuPath Guide: Creating classified annotation areas using SLICs</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Quick tutorial on generating annotation level areas from SLICs. \nImage courtesy of La Jolla Institute of Immunology Microscopy Core shared image resource. \nNot going to go into a whole lot of details here as the entire method will likely be obsolete once the Pixel Classifier is fully functional. \n\nGenerate your primary area with an annotation, either <a href=\"https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/9\">simple tissue detection</a>, createSelectAllObject() or something else like drawing a square.\n\n<a href=\"https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/19\">Generate your SLICs</a>. There isn\u2019t any one right answer fo\u2026\n  </blockquote>\n</aside>\n\n<p>I\u2019m not sure if this was the bug fix <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> was mentioning, but there may be some issues with the old detection classifier in M11.</p><aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"37259\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/saving-loading-training-objects-for-cell-classifier-in-qupath-m11/37259/2\">Saving/loading training objects for cell classifier in QuPath M11</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    This morning I wrote a little bit about what \u2018deprecated\u2019 means in this context at <a href=\"https://qupath.readthedocs.io/en/latest/docs/reference/faqs.html#why-are-some-commands-marked-as-deprecated\" class=\"inline-onebox-loading\">https://qupath.readthedocs.io/en/latest/docs/reference/faqs.html#why-are-some-commands-marked-as-deprecated</a> \nI also created an enhancement request regarding training the classifiers from multiple images <a href=\"https://github.com/qupath/qupath/issues/462\">here</a>. \nI completely agree this is necessary; the fact it doesn\u2019t yet exist is a big part of the reason why the \u2018old\u2019 classifier approach remains for now. They have only deprecated and not removed because I understa\u2026\n  </blockquote>\n</aside>\n", "<p>After you have trained and saved a classifier, you can write a script to apply it. In m11, using the new object classifier, the command is simply:</p>\n<p>runObjectClassifier(\u201cNameOfClassifier\u201d);</p>\n<p>Once you have a script to do the whole workflow (generating super pixels through classification), you can use \u201cRun for Project\u201d to do all the images.</p>\n<p>Note: I highly recommend training your classifier on a combined training image (Classify &gt; Training Images &gt; Create Combined Training Image) for better accuracy across a project.</p>", "<p>Thank you very mych. But when I do Classify &gt; Training Images &gt; Create Combined Training Image I get the error message : \u201cNo suitable annotations found in the current project\u201d.</p>", "<p>Create combined training image requires annotations of the class chosen to exist throughout the project. Have you created annotations with the same assigned class in several of your images?</p>\n<p>One further thing to note when creating those\u2026 it will only collect <em>saved</em> annotations. Drawing them is not sufficient. I have occasionally omitted the last image in a set when using that feature as I forget to save the current data set before Creating the combined training image.</p>", "<p>Please which steps do I need to \u201ctrain an object classifier to identify fibrotic vs non-fibrotic regions\u201d?</p>\n<p>This is how I tried to do it:</p>\n<ul>\n<li>\n<p>to set annotations on one loaded images:</p>\n<ul>\n<li>\n<p>polygon &gt; annotations &gt; right click &gt; set properties &gt; name  = \u201cfibrotic\u201d</p>\n</li>\n<li>\n<p>polygon &gt; annotations &gt; right click &gt; set properties &gt; name  = \u201cnon-fibrotic\u201d</p>\n</li>\n</ul>\n<p>I defined on the same slide image 5 fibrotic and 5 non-fibrotic zones</p>\n</li>\n<li>\n<p>To train an object classifier to identify fibrotic vs non-fibrotic regions</p>\n<ul>\n<li>\n<p>Classify &gt; Object classification &gt; Train object classifier &gt; save &amp; apply</p>\n</li>\n<li>\n<p>Then I get the error: java.lang.NullPointerException</p>\n</li>\n</ul>\n</li>\n</ul>", "<p>You may want to look into the classification steps mentioned in general to understand how QuPath works. You simply changed the name of the annotations, not their class. The name is only there for your benefit, the program doesn\u2019t really make use of it.<br>\n<a href=\"https://qupath.readthedocs.io/en/latest/docs/tutorials/cell_classification.html#annotate-regions-containing-different-cell-types\" class=\"onebox\" target=\"_blank\">https://qupath.readthedocs.io/en/latest/docs/tutorials/cell_classification.html#annotate-regions-containing-different-cell-types</a></p>", "<p>There\u2019s also step-by-step instructions for building an object classifier <a href=\"https://www.youtube.com/playlist?list=PLlGXRBscPbCD89fRULm4peopF57qugciN\" rel=\"nofollow noopener\">on Youtube</a>.</p>", "<p>Thank you so much for your kind help. I wish to ask for a little clarification. So if I got you well,  is this how the work flow from scratch will be:?</p>\n<ol>\n<li>\n<p>Create project</p>\n</li>\n<li>\n<p>Add images</p>\n</li>\n<li>\n<p>build an object classifier</p>\n</li>\n<li>\n<p>generate superpixels like this:<br>\nAnalyze-&gt;Region identification-&gt;Tiles &amp;<br>\nsuperpixels-&gt;SLIC superpixel<br>\nsegmentation.</p>\n</li>\n<li>\n<p>calculate features on those regions, including basic intensity features and haralick measurements and anything else you think might be helpful,</p>\n</li>\n<li>\n<p>train an object classifier to identify fibrotic vs non-fibrotic regions,</p>\n</li>\n<li>\n<p>classify all the objects in all images in your project,</p>\n</li>\n<li>\n<p>(optional) use \u201ctile classifications to annotations\u201d to merge the detection objects into annotation objects (<a href=\"https://github.com/qupath/qupath/blob/e488e4e0aaa38dd950e050734e41fa0b3b2ab722/CHANGELOG.md\" rel=\"nofollow noopener\">with a recent bug fix in m10</a>),</p>\n</li>\n<li>\n<p>measure the area of the classified objects.</p>\n</li>\n</ol>\n<p>Thank you in advance</p>", "<ol start=\"3\">\n<li>isn\u2019t correct as you wouldn\u2019t have any objects to build the classifier with at that point. Also it is redundant with 6.</li>\n</ol>", "<p>It has been a while, but I wonder if you were able to measure fibrosis or interstitial cellular areas in lung specimens.</p>\n<p>I am working with lungs from mice and trying to measure (in the area) the amount of cellular interstitium/fibrosis as a result of radiation damage, in contrast to alveoli from healthy controls. Two images are below. I have tried to train Pixel Classifier for interstitium and for empty alveoli and tried to calculate % and area of each Category, but have been unsuccessful.</p>\n<p>THanks for any help</p>\n<p>Fabio</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/6/96657539a9083be0458ef308203589cb4eddb490.jpeg\" data-download-href=\"/uploads/short-url/lssZzCUo6LhBE667J3XezOx4snu.jpeg?dl=1\" title=\"901a\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/6/96657539a9083be0458ef308203589cb4eddb490.jpeg\" alt=\"901a\" data-base62-sha1=\"lssZzCUo6LhBE667J3XezOx4snu\" width=\"539\" height=\"500\" data-dominant-color=\"D5B1D7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">901a</span><span class=\"informations\">773\u00d7716 143 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/44c25802762f6a66d62c0c2a01665f0162b1500c.jpeg\" data-download-href=\"/uploads/short-url/9OgQyjawpAF8PAj79I9encSvoVS.jpeg?dl=1\" title=\"905 normal\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/44c25802762f6a66d62c0c2a01665f0162b1500c.jpeg\" alt=\"905 normal\" data-base62-sha1=\"9OgQyjawpAF8PAj79I9encSvoVS\" width=\"690\" height=\"489\" data-dominant-color=\"E3D0E2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">905 normal</span><span class=\"informations\">1009\u00d7716 131 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Fabio you can do it if you create a thresholder:</p>\n<p>(If you use scanned slides) Classified the tissue with Analyze&gt;deprecrated&gt;simple tissue detection. And select the threshold useful for your purpose</p>\n<p>Select your ROI and create a thresholder that you can run in your images/slides Classify&gt;Pixel classification&gt;create thresholder<br>\nModify the threshold and sigma values for your suitable values select e.g above threshold negative and below threshold positive.<br>\nWhen you run the measure it will provide you with the total area of parenchyma (positive) and air (negative) and their percentages regarding the ROI<br>\nFor your image you should work from a threshold of 220ish and try what suits you best</p>"], "78217": ["<p>Hello,</p>\n<p>I want to ask if there is a way to get the underlying array from a dataset object. I am reading dataset from</p>\n<pre><code class=\"lang-java\">@Parameter\nprivate Dataset currentData;\n</code></pre>\n<p>using</p>\n<pre><code class=\"lang-java\">final Img&lt;T&gt; image = (Img&lt;T&gt;)currentData.getImgPlus();\n</code></pre>\n<p>The image has several channels, which I want to combine subsets of (typically sum), to get new images.</p>"], "61834": ["<p>Hi, there.<br>\nI am looking for a way to look up an image stack which is composed of several hundreds of slices from a perspective or plane with a user-defined angle, like 15, 45, 65 degrees from the z axis.</p>\n<p>However, I only know there is ready available approach to reslice the stack from the yz, zy, or xy plane, all of which are 90 degrees to the x,y,z axis. Can the slices of other crossing angles be generated using ImageJ?</p>\n<p>Thank you in advance!</p>", "<p>Hi <a class=\"mention\" href=\"/u/vioc\">@vioc</a> ,</p>\n<p>Do you want to view your resliced volume? Or do you want to create a new image stack that is resliced?</p>\n<p>For viewing try <a href=\"https://imagej.net/plugins/bdv/\">BigDataViewer</a>.<br>\nLeft-click + drag lets you reslice arbitrarily.  <code>X,Y,Z</code> and the arrow keys let you have some more fine-grained / precise control.  See the <a href=\"https://imagej.net/plugins/bdv/#basic-navigation\">Basic navigation section</a>.</p>\n<p>If you want to export the resliced result, <a href=\"https://imagej.net/plugins/bigwarp#table-of-transformation-types\">bigwarp with the rigid transform type</a> could be a good option, but the way to specify the transform is a bit different (landmark points) and perhaps not exactly what you want.</p>\n<p>John</p>", "<p>Hi <a class=\"mention\" href=\"/u/vioc\">@vioc</a>. Have you tried Radial Reslice on Fiji?</p>", "<p>Of course, but with this feature only the reslice at 90 degrees is capable\u2026</p>", "<p>It works! I\u2019m impressed at the convenient manipulation and fast reconstruction that comes with BigDataViewer. Thank you for sharing with us.</p>", "<p>Hi, that\u2019s a good question, bigdata viewer works at viewing the stack but I did not get how to export as a tiff stack. Could anyone help to imply how?</p>", "<p>Hello <a class=\"mention\" href=\"/u/liangy10\">@liangy10</a>,</p>\n<p>There\u2019s one option explained in <a href=\"https://forum.image.sc/t/save-one-tiff-projection-from-a-current-view-in-bdv/33965/20\" class=\"inline-onebox\">Save one tiff projection from a current view in BDV - #20 by NicoKiaru</a> .</p>\n<p>I think the interface is slightly outdated (the update site to activate is PTBIOP), but let me know if you get stuck.</p>\n<p>Cheers,</p>\n<p>Nicolas</p>", "<p>Thank you Nicolas for the help. I installed the Current BDV view to ImagePlus, but when it was opened, another window of BigDataViewer popped up without any image stack. Any idea?</p>", "<p><a class=\"mention\" href=\"/u/liangy10\">@liangy10</a> I just had the same issue as you.</p>\n<p>I wanted to save an image stack from an arbitrary plane orientation in BigDataViewer. Found this thread, and the <a href=\"https://forum.image.sc/t/save-one-tiff-projection-from-a-current-view-in-bdv/33965/20\">linked solution</a> that <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a> shared doesn\u2019t seem to work if you do the following:</p>\n<ol>\n<li>Close everything, open your image stack of interest</li>\n<li>\u2018Open Current Image\u2019 in BigDataViewer Plugin menu to produce a BDV window (and navigate to your arbitrary plane orientation)</li>\n<li>\u2018Current BDV View To ImagePlus (Basic)\u2019 from BDV playground</li>\n</ol>\n<p>That loads a blank BDV window (even though one is already open) and doesn\u2019t produce any data when you start the function.</p>\n<p>Alternatively, if you follow <a href=\"https://forum.image.sc/t/save-one-tiff-projection-from-a-current-view-in-bdv/33965/36\">the post slightly further down</a>:</p>\n<ol>\n<li>Close everything, open your TIFF stack of interest</li>\n<li><strong>\u2018Open Image\u2019 in ImagePlus BigDataViewer menu</strong></li>\n<li>\u2018BDV - Show Sources (new Bdv window)\u2019 in BDV playground (and navigate to your arbitrary plane orientation)</li>\n<li>\u2018Current BDV View To ImagePlus (Basic)\u2019 from BDV playground</li>\n</ol>\n<p>That doesn\u2019t open a blank window, and it produces the intended result of an image stack in the new plane orientation.</p>"], "78219": ["<p><a class=\"mention\" href=\"/u/wayne\">@Wayne</a> <a class=\"mention\" href=\"/u/dgault\">@dgault</a> <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a></p>\n<p>If have this image and the display settings for the first channel are such that <code>min = 10000</code> and <code>max = 17000</code>. You can see this is you open the image in Fiji.</p>\n<p>My question is: how can I extract those values programatically from the ImagePlus?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/sfu7Y9uYwCR30qNJSnyasrEl9rU.zip\">2023_01_18\u2013Extract_8hr_1-1.zip</a> (2.2 MB)</p>\n<p>The code that is used here does not seem to return those values:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122\" target=\"_blank\" rel=\"noopener\">BIOP/bigdataviewer-image-loaders/blob/93c22096aa267828c7a314ab430a0eb5a2da5800/src/main/java/ch/epfl/biop/bdv/img/imageplus/ImagePlusToSpimData.java#L122</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-java\">\n      <ol class=\"start lines\" start=\"112\" style=\"counter-reset: li-counter 111 ;\">\n          <li>\t\tfinal int numTimepoints = imp.getNFrames();</li>\n          <li>\t\tfinal int numSetups = imp.getNChannels();</li>\n          <li>\n          </li>\n<li>\t\t// create setups from channels</li>\n          <li>\t\tfinal HashMap&lt;Integer, BasicViewSetup&gt; setups = new HashMap&lt;&gt;(numSetups);</li>\n          <li>\t\tfor (int s = 0; s &lt; numSetups; ++s) {</li>\n          <li>\t\t\tfinal BasicViewSetup setup = new BasicViewSetup(s, String.format(imp</li>\n          <li>\t\t\t\t\t.getTitle() + \" channel %d\", s + 1), size, voxelSize);</li>\n          <li>\t\t\tsetup.setAttribute(new Channel(s + 1));</li>\n          <li>\t\t\tDisplaysettings ds = new Displaysettings(s + 1);</li>\n          <li class=\"selected\">\t\t\timp.setPositionWithoutUpdate(s+1,1,1);</li>\n          <li>\t\t\tds.min = imp.getDisplayRangeMin();</li>\n          <li>\t\t\tds.max = imp.getDisplayRangeMax();</li>\n          <li>\t\t\tif (imp.getType() == ImagePlus.COLOR_RGB) {</li>\n          <li>\t\t\t\tds.isSet = false;</li>\n          <li>\t\t\t}</li>\n          <li>\t\t\telse {</li>\n          <li>\t\t\t\tds.isSet = true;</li>\n          <li>\t\t\t\tLUT[] luts = imp.getLuts();</li>\n          <li>\t\t\t\tLUT lut = luts.length&gt;s ? luts[s]:luts[0];</li>\n          <li>\t\t\t\tds.color = new int[] { lut.getRed(255), lut.getGreen(255), lut.getBlue(</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>and it would be great to fix this.</p>\n<p>Thank you for your insights!</p>", "<p>Hi <a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a>,</p>\n<p>You need to go via the ImageProcessor. The following should work, at least it does for me on your example image:</p>\n<pre><code class=\"lang-auto\">...\nImageProcessor ip = imp.getProcessor();\nip.getMin();\nip.getMax();\n...\n</code></pre>\n<p>The following minimal working example should show this:</p>\n<pre><code class=\"lang-auto\">import ij.ImagePlus;\nimport ij.WindowManager;\nimport ij.process.ImageProcessor;\n\npublic class Test {\n\n\tpublic static void main(String[] args) {\n\n\t\tImagePlus imp = new ImagePlus(\"pat/to/the/image/\"); //or WindowManager.getCurrentImage();\n\t\tImageProcessor ip = imp.getProcessor();\n\t\tSystem.out.println(ip.getMin());\n\t\tSystem.out.println(ip.getMax());\n\n\t}\n\n}\n</code></pre>", "<p><a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a> would you have time to change the code accordingly in your repo?</p>", "<p>Hi <a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a>,</p>\n<p>This bug will be fixed in the next daily build. In the meantime, you can work around it by using ImageProcessor.getMin() and ImageProcessor.getMax(), as in this example:</p>\n<pre><code class=\"lang-auto\">imp = IJ.openImage(IJ.getDir(\"downloads\")+\"2023_01_18--Extract_8hr_1-1.tif\");\nip = imp.getProcessor();\nmin = ip.getMin();\nmax = ip.getMax();\nIJ.log(\"display range: \"+min+\"-\"+max);\n</code></pre>", "<p><a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a> : thanks to <a class=\"mention\" href=\"/u/wayne\">@Wayne</a> updating ImageJ should be sufficient.</p>\n<p>Otherwise I can modify the code, but I\u2019d like to have it working for multiple channels. Currently the code suggested only works for the first image processor. Could you make the code work for multi-channel images ?</p>", "<p>I guess we could then also just wait for the fix!<br>\nThank you <a class=\"mention\" href=\"/u/wayne\">@Wayne</a>!</p>", "<p>Hi <a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a>, <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a> ,<a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>,</p>\n<p>Here is a version of the script that gets the display ranges of all the channels:</p>\n<pre><code class=\"lang-auto\">imp = IJ.openImage(IJ.getDir(\"downloads\")+\"2023_01_18--Extract_8hr_1-1.tif\");\nfor (c=1; c&lt;=imp.getNChannels(); c++) {\n     imp.setC(c);\n     ip = imp.getProcessor();\n     min = ip.getMin();\n     max = ip.getMax();\n     IJ.log(c+\": \"+min+\"-\"+max);\n}\n</code></pre>\n<p>Update to the ImageJ 1.54d2 daily build and you can use this slightly simpler version:</p>\n<pre><code class=\"lang-auto\">imp = IJ.openImage(IJ.getDir(\"downloads\")+\"2023_01_18--Extract_8hr_1-1.tif\");\nfor (c=1; c&lt;=imp.getNChannels(); c++) {\n     imp.setC(c);\n     min = imp.getDisplayRangeMin();\n     max = imp.getDisplayRangeMax();\n     IJ.log(c+\": \"+min+\"-\"+max);\n}\n</code></pre>", "<p>Awesome <a class=\"mention\" href=\"/u/wayne\">@Wayne</a>! Thanks a lot!<br>\nJust to be sure, does that also work if the <code>ImagePlus</code> is a <code>CompositeImage</code>?</p>", "<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"8\" data-topic=\"78219\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian Tischer:</div>\n<blockquote>\n<p>Just to be sure, does that also work if the <code>ImagePlus</code> is a <code>CompositeImage</code>?</p>\n</blockquote>\n</aside>\n<p>Yes.</p>\n<p>Here is an even simpler way to get the display ranges of the channels in a composite image:</p>\n<blockquote>\n<p>imp = IJ.openImage(IJ.getDir(\u201cdownloads\u201d)+\u201c2023_01_18\u2013Extract_8hr_1-1.tif\u201d);<br>\nluts = imp.getLuts();<br>\nfor (i=0; i&lt;luts.length; i++)<br>\nIJ.log((i+1)+\u201c: \u201c+luts[i].min+\u201d-\u201d+luts[i].max);</p>\n</blockquote>", "<aside class=\"quote no-group\" data-username=\"Wayne\" data-post=\"9\" data-topic=\"78219\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/w/0ea827/40.png\" class=\"avatar\"> Wayne Rasband:</div>\n<blockquote>\n<p>even simpler way to get the display ranges of the channels</p>\n</blockquote>\n</aside>\n<p>Thanks a lot! I tried that and did not manage to get the correct results\u2026<br>\nCould it be that this did not work before your bug fix?</p>", "<p>It should work without the bug fix.</p>\n<p>This is the output from the script I get</p>\n<pre><code class=\"lang-auto\">1: 10009-17752.560117302055\n2: 0-65535\n3: 0-65535\n4: 0-65535\n</code></pre>\n<p>when running the script using ImageJ 1.54c (pre bugfix), which is the same as what is shown by the Image&gt;Show Info command:</p>\n<pre><code class=\"lang-auto\">Display ranges\n  1: 10009-17752.5601\n  2: 0-65535\n  3: 0-65535\n  4: 0-65535\n</code></pre>"], "78225": ["<p>Hi,</p>\n<p>I have a script that, first, uses cellpose to detect cells. These are converted to annotations. Then stardist is run on these annotations before converting them back to detections to create \u201ccells\u201d.</p>\n<pre><code class=\"lang-auto\">import qupath.ext.stardist.StarDist2D\nimport qupath.ext.biop.cellpose.Cellpose2D\n\nvar imageData = getCurrentImageData()\nvar pathObjects = getSelectedObjects()\n\npathModel = 'cyto2'\ndef cellpose = Cellpose2D.builder(pathModel)\n        .channels('Channel 2')\n        .normalizePercentiles(1,99)\n        .pixelSize( 0.25 )\n        .diameter(0.0)\n        .measureShape()\n        .measureIntensity()\n        .build()\n\ncellpose.detectObjects(imageData, pathObjects)\n\ndef detections = getDetectionObjects()\ndef newAnnotations = detections.collect {\n    return PathObjects.createAnnotationObject(it.getROI(), it.getPathClass())\n    }    \nremoveObjects(detections, true)\naddObjects(newAnnotations)\n\npathModel = 'C:/Program Files/QuPath/dsb2018_heavy_augment.pb'\nvar stardist = StarDist2D.builder(pathModel)\n        .threshold(0.5)             \n        .channels('Channel 1')\n        .normalizePercentiles(1, 99) \n        .pixelSize(0.5) \n\t    .cellExpansion(0)\n        .measureShape()\n        .measureIntensity() \n        .includeProbability(true)\n        .build()\n\nselectAnnotations()\nstardist.detectObjects(imageData, pathObjects)\n\ndef newDetections = newAnnotations.collect {\n    return PathObjects.createDetectionObject(it.getROI(), it.getPathClass())\n    }\n    \nremoveObjects(newAnnotations, true)\naddObjects(newDetections)\n\nfireHierarchyUpdate()\nprintln(\"Done!\")\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f.jpeg\" data-download-href=\"/uploads/short-url/oQ1q1gGCKbpM8seSiMER94f3bpt.jpeg?dl=1\" title=\"Capture\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_322x250.jpeg\" alt=\"Capture\" data-base62-sha1=\"oQ1q1gGCKbpM8seSiMER94f3bpt\" width=\"322\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_322x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_483x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/e/ae15a103551c07d97787c4032ae97a39d7e5146f_2_644x500.jpeg 2x\" data-dominant-color=\"2E0E02\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture</span><span class=\"informations\">1006\u00d7779 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is there any way I can convert these into actual qupath cells? Or at least have the nuclei as \u201cchildren\u201d to the cellpose detections? Ideally I would want to remove the nuclear ROI from the cytoplasm so they are separate compartments. I can\u2019t figure out how to do this\u2026 Any advice is welcome!</p>", "<p>You\u2019ll need to create new objects, and I\u2019m not sure how easy it will be to merge the measurement lists so: <a href=\"https://forum.image.sc/t/transferring-segmentation-predictions-from-custom-masks-to-qupath/43408/12\" class=\"inline-onebox\">Transferring segmentation predictions from Custom Masks to QuPath - #12 by petebankhead</a></p>\n<p>The function to take two ROIs to create a cell is <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/objects/PathObjects.html#createCellObject(qupath.lib.roi.interfaces.ROI,qupath.lib.roi.interfaces.ROI,qupath.lib.objects.classes.PathClass,qupath.lib.measurements.MeasurementList)\" class=\"inline-onebox\">PathObjects (QuPath 0.4.0)</a><br>\nWhich I found by searching for \u201ccreatecell\u201d in the javadocs.</p>\n<aside class=\"quote\" data-post=\"124\" data-topic=\"58901\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/cellpose-in-qupath-qupath-extension-cellpose/58901/124\">Cellpose in QuPath - QuPath Extension Cellpose</a> <a class=\"badge-wrapper  bullet\" href=\"/c/announcements/10\"><span class=\"badge-category-bg\" style=\"background-color: #AB9364;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for announcements, such as new software releases and upcoming community events.\">Announcements</span></a>\n  </div>\n  <blockquote>\n    I haven\u2019t had the opportunity to try it yet, but I was thinking that I could use StarDist to get the nuclei and Cellpose to get the cytoplasm, get their ROIs, and then create cell objects with createCellObject(roiCell, roiNucleus, pathClass, measurements) (<a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/objects/PathObjects.html#createCellObject(qupath.lib.roi.interfaces.ROI,qupath.lib.roi.interfaces.ROI,qupath.lib.objects.classes.PathClass,qupath.lib.measurements.MeasurementList)\" rel=\"noopener nofollow ugc\">javadocs</a>). This would probably take a loooooong time to run if you have a lot of cells in your image though.\n  </blockquote>\n</aside>\n", "<aside class=\"quote no-group\" data-username=\"ChrisStarling\" data-post=\"1\" data-topic=\"78225\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png\" class=\"avatar\"> Chris Starling:</div>\n<blockquote>\n<p>These are converted to annotations. Then stardist is run on these annotations before converting them back to detections to create \u201ccells\u201d.</p>\n</blockquote>\n</aside>\n<p>I believe you don\u2019t have to convert the CellPose detection objects into annotation objects, although I have not verified this.</p>\n<p>StarDist should accept any PathObjects, including detections when you run <code>stardist.detectObjects(imageData, pathObjects)</code></p>\n<p>This should save you a lot of time.</p>", "<p>Thanks, I didn\u2019t realise that stardist accepted detections as inputs. After completing cellpose I just <code>selectDetections()</code> and then run stardist. This also means the nuclei belong to the cellpose parent.</p>\n<p>I am still struggling to code the <code>createCellObject()</code> to combine the two\u2026</p>", "<p>If anyone would find it useful, I\u2019ve managed to get it working using some scripts from <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> and <a class=\"mention\" href=\"/u/romainguiet\">@romainGuiet</a>. Also thanks to <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> and <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a> !</p>\n<p>Here it is:</p>\n<pre><code class=\"lang-auto\">import qupath.ext.stardist.StarDist2D\nimport qupath.ext.biop.cellpose.Cellpose2D\nimport qupath.lib.objects.PathObjects\nimport qupath.lib.analysis.features.ObjectMeasurements\n\nclearDetections()\n\n// Set some variables\nvar imageData = getCurrentImageData()\nvar server = getCurrentServer()\nvar pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    createSelectAllObject(true)\n    }\nvar cal = server.getPixelCalibration()\nvar downsample = 1.0\n\n// Run cellpose\npathModel = 'cyto'\ndef cellpose = Cellpose2D.builder(pathModel)\n        .channels('Channel 2')\n        .normalizePercentiles(1,99)\n        .pixelSize( 0.25 )\n        .diameter(0.0)\n        .build()\ncellpose.detectObjects(imageData, pathObjects)\n\n// Save cytoplasm\ndef cytoplasms = getDetectionObjects()\nselectDetections()\n\n// Run stardist\npathModel = 'C:/Program Files/QuPath/dsb2018_heavy_augment.pb'\nvar stardist = StarDist2D.builder(pathModel)\n        .threshold(0.5)             \n        .channels('Channel 1')\n        .normalizePercentiles(1, 99) \n        .pixelSize(0.5) \n        .cellExpansion(0)\n        .measureShape()\n        .measureIntensity() \n        .includeProbability(true)\n        .build()\nstardist.detectObjects(imageData, pathObjects)\n\nclearSelectedObjects()\n\n// Save nuclei\ndef nuclei = getDetectionObjects()\n\n// Start with a clean slate\nclearDetections()\n\n// Create cells\ncells = []\ncytoplasms.each{ cytoplasm -&gt;\n    nuclei.each{ nucleus -&gt;      \n        if ( cytoplasm.getROI().contains( nucleus.getROI().getCentroidX() , nucleus.getROI().getCentroidY())){\n            cells.add(PathObjects.createCellObject(cytoplasm.getROI(), nucleus.getROI(), getPathClass(\"Tissue\"), null ));\n            }\n        }\n    }\naddObjects(cells)\n\n// Add measurements\ndef measurements = ObjectMeasurements.Measurements.values() as List\ndef compartments = ObjectMeasurements.Compartments.values() as List\ndef shape = ObjectMeasurements.ShapeFeatures.values() as List\ndef cells = getCellObjects()\nfor ( cell in cells ) {\n    ObjectMeasurements.addIntensityMeasurements( server, cell, downsample, measurements, compartments )\n    ObjectMeasurements.addCellShapeMeasurements( cell, cal,  shape )\n    }\n\n// Finished!\nfireHierarchyUpdate()\nprintln(\"Done!\")\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2.jpeg\" data-download-href=\"/uploads/short-url/rgDehKIiTJ5r5nBXdqBxCglM7Au.jpeg?dl=1\" title=\"Capture2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_345x242.jpeg\" alt=\"Capture2\" data-base62-sha1=\"rgDehKIiTJ5r5nBXdqBxCglM7Au\" width=\"345\" height=\"242\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_345x242.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_517x363.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/f/bf1c053d2ffcefed945b7ac312f29c1e73f05dc2_2_690x484.jpeg 2x\" data-dominant-color=\"141B19\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture2</span><span class=\"informations\">881\u00d7618 44.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78229": ["<p>I am trying to run the Find Peaks function of the BAR plugin on a profile plot, in batch mode.</p>\n<p>For example with a code like this:</p>\n<pre><code class=\"lang-auto\">setBatchMode(true);\n\nn = roiManager(\"count\");\nfor (i = 0; i &lt; n; i++) {\n    \troiManager(\"select\", i);\n        Roi.getBounds(x, y, width, height);\n        makeLine(x, y, width, height);\n        run(\"Plot Profile\");\n        run(\"Find Peaks\", \"min._peak_amplitude=1 min._peak_distance=0 min._value=[] max._value=[] exclude list\");\n\t    peaks_idx = Table.getColumn(\"X1\");\n\t    selectWindow(\"Plot Values\");\n\t    run(\"Close\");\n   \t    Array.sort(peaks_idx);\n}\nsetBatchMode(false);\n</code></pre>\n<p>The code works without batch mode but it causes an exception. I suspects it is because the plot profile is not shown. Does anyone know a way around this?</p>", "<p>Batch mode will suppress the display of any images (including IJ plots), so it won\u2019t work unless you keep including <code>setBatchMode('show')</code> calls <em>before</em> displaying plots. I am not sure that will make the macro run any faster.<br>\nI\u2019m not sure what exactly you are trying to do, but once the \u2018Find Peaks\u2019 plot is displayed it becomes the frontmost image, and thus the plot profile will no longer be obtained from the initial image parsed by the macro.</p>\n<p>I would expect something like this instead:</p>\n<pre><code class=\"lang-javascript\">\nid = getImageID(); // id of active image\nmaximaXcoordHolder = newArray() // storing array\nmaximaYcoordHolder = newArray() // storing array\n\nn = roiManager(\"count\");\nfor (i = 0; i &lt; n; i++) {\n\t// we must activate initial image to ensure plot profile runs on it\n\tselectImage(id);\n\t// obtain profile\n\troiManager(\"select\", i);\n\tRoi.getBounds(x, y, width, height);\n\tmakeLine(x, y, width, height);\n\trun(\"Plot Profile\");\n\t// Plot profile is now frontmost window. Store its title\n\tprofileTitle = getTitle();\n\trun(\"Find Peaks\", \"min._peak_amplitude=1 min._peak_distance=0 min._value=[] max._value=[] exclude list\");\n\t// Find maxima profile is now frontmost window. Store its title\n\tpeaksTitle = getTitle();\n\t// extract maxima coordinates from the \"Plot Values\" Table also open\n\tmaximaXcoord = Table.getColumn(\"X1\");\n\tmaximaYcoord = Table.getColumn(\"Y1\");\n\t// close windows no longer in use\n\tselectWindow(profileTitle);\n\trun(\"Close\");\n\tselectWindow(peaksTitle);\n\trun(\"Close\");\n\t// store coordinates in a common array\n\tmaximaXcoordHolder = Array.concat(maximaXcoord, maximaXcoordHolder);\n\tmaximaYcoordHolder = Array.concat(maximaYcoord, maximaYcoordHolder);\n}\n\n// close residual windows and show result\nselectWindow(\"Plot Values\");\nrun(\"Close\")\nTable.showArrays(\"Maxima Coords\", maximaXcoordHolder, maximaYcoordHolder); \n</code></pre>\n<p>That being said, you probably don\u2019t need \u2018Find Peaks\u2019 at all for this!? \u2018Find Peaks\u2019 was created to extract maxima and minima from plotted data. If you are parsing images, you could use built-in macro functions that will work in batch mode out of the box:</p>\n<pre><code class=\"lang-javascript\">(...)\nmakeLine(x, y, width, height);\nprofile = getProfile();\nmaxIndices = Array.findMaxima(profile, 10);\nprint(\"Maxima (descending strength):\");\nfor (i= 0; i &lt; maxIndices.length; i++) {\n\tx = maxIndices[i];\n\ty = profile[x];\n\tprint(\"x= \", x, \" y= \", y);\n}\n</code></pre>", "<p>Thanks you! This is exactly what I was after. I suspected that the function wrapped some native ImageJ functions but couldn\u2019t figure out which one. Your second suggestion is perfectly fine.</p>"], "78233": ["<p>Je suis d\u00e9butant en traitement d\u2019images et je travaille actuellement sur l\u2019analyse de l\u2019\u00e9coulement de mat\u00e9riaux h\u00e9t\u00e9rog\u00e8nes dans une buse et je souhaite fournir dans mon rapport des informations sur la longueur et l\u2019orientation (tenseur d\u2019orientation) des fibres. J\u2019ai donc effectu\u00e9 une segmentation des images \u00b5CT avec les plugins <strong>Trainable Weka</strong> afin de ne conserver que la phase fibreuse de mon composite.<br>\nEst-ce que quelqu\u2019un pourrait m\u2019expliquer comment calculer la distribution de longueur et la distribution d\u2019orientation des fibres pour des images 3D avec Fiji ?<br>\nDe plus, comment puis-je obtenir les composantes du tenseur d\u2019orientation des fibres ? Je joins une image .gif.</p>\n<p>Merci beaucoup pour votre temps, Cordialement,</p>\n<p>Achille<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/14cdd5b740c62fc4b7da7baa4bdd22c81f059f4c.gif\" alt=\"Fibrous\" data-base62-sha1=\"2Y2y0peo3ZIWiUdjRwsYIKPRQba\" width=\"210\" height=\"210\" class=\"animated\"><a href=\"https://drive.google.com/file/d/1wDNhbyom7A0QWp9RXQsvzypytHcte_Wg/view?usp=share_link\" rel=\"noopener nofollow ugc\">https://drive.google.com/file/d/1wDNhbyom7A0QWp9RXQsvzypytHcte_Wg/view?usp=share_link</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/achille1\">@Achille1</a></p>\n<pre><code class=\"lang-auto\">//---------------------\nn=nSlices;\nrun(\"Set Measurements...\", \"feret's display redirect=None decimal=2\");\nfor(i=1;i&lt;=n;i++) {\n   setSlice(i);\nrun(\"Analyze Particles...\", \"display exclude slice\");\n}\nTable.deleteColumn(\"FeretX\");\nTable.deleteColumn(\"FeretY\");\nTable.deleteColumn(\"MinFeret\");\nTable.update\n//---------------------\n</code></pre>\n<p>Feedback is appreciated. \u2014&gt; Obviously, my macro does not give any information in the volume.(sorry)</p>", "<p>Salut <a class=\"mention\" href=\"/u/mathew\">@Mathew</a></p>\n<p>Merci pour ton aide, je vais essayer avec ton code.</p>"], "78234": ["<p>Hi <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a></p>\n<p>I was trying to run the <a href=\"https://clij.github.io/clij2-docs/reference_translationTimelapseRegistration\" rel=\"noopener nofollow ugc\">time lapse registration</a> in the CLIJ2 library, but it\u2019s giving me an error. I was wondering if you know what the problem could be.</p>\n<p>Here is the macro code I am running:</p>\n<pre><code class=\"lang-auto\">run (\"Close All\");\nopen(\"D:/image.tif\");\nrun(\"CLIJ2 Macro Extensions\", \"cl_device=\");\nExt.CLIJ2_clear();\ninput = getTitle();\nExt.CLIJ2_push(input);\nclose();\n \nExt.CLIJx_translationTimelapseRegistration(input, output);\n\nExt.CLIJ2_pull(output);\nExt.CLIJ2_clear();\n</code></pre>\n<p>And here is the error:</p>\n<pre><code class=\"lang-auto\">java.lang.ClassCastException: java.lang.String cannot be cast to net.haesleinhuepf.clij.clearcl.ClearCLBuffer\n\tat net.haesleinhuepf.clijx.registration.TranslationTimelapseRegistration.executeCL(TranslationTimelapseRegistration.java:23)\n\tat net.haesleinhuepf.clij.macro.CLIJHandler.lambda$handleExtension$0(CLIJHandler.java:163)\n\tat net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:97)\n\tat net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:28)\n\tat net.haesleinhuepf.clij.macro.CLIJHandler.handleExtension(CLIJHandler.java:53)\n\tat ij.macro.ExtensionDescriptor.dispatch(ExtensionDescriptor.java:288)\n\tat ij.macro.Functions.doExt(Functions.java:5096)\n\tat ij.macro.Functions.getStringFunction(Functions.java:279)\n\tat ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)\n\tat ij.macro.Interpreter.getString(Interpreter.java:1498)\n\tat ij.macro.Interpreter.doStatement(Interpreter.java:336)\n\tat ij.macro.Interpreter.doStatements(Interpreter.java:267)\n\tat ij.macro.Interpreter.run(Interpreter.java:163)\n\tat ij.macro.Interpreter.run(Interpreter.java:93)\n\tat ij.macro.Interpreter.run(Interpreter.java:107)\n\tat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)\n\tat ij.IJ.runMacro(IJ.java:160)\n\tat ij.IJ.runMacro(IJ.java:149)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)\n\tat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)\n\tat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)\n\tat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:164)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\njava.lang.NullPointerException\n\tat net.haesleinhuepf.clij.CLIJ.convert(CLIJ.java:461)\n\tat net.haesleinhuepf.clij.CLIJ.show(CLIJ.java:358)\n\tat net.haesleinhuepf.clij.macro.CLIJHandler.pullFromGPU(CLIJHandler.java:253)\n\tat net.haesleinhuepf.clij2.plugins.Pull.executeCL(Pull.java:24)\n\tat net.haesleinhuepf.clij.macro.CLIJHandler.lambda$handleExtension$0(CLIJHandler.java:163)\n\tat net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:97)\n\tat net.haesleinhuepf.clij.clearcl.util.ElapsedTime.measure(ElapsedTime.java:28)\n\tat net.haesleinhuepf.clij.macro.CLIJHandler.handleExtension(CLIJHandler.java:53)\n\tat ij.macro.ExtensionDescriptor.dispatch(ExtensionDescriptor.java:288)\n\tat ij.macro.Functions.doExt(Functions.java:5096)\n\tat ij.macro.Functions.getStringFunction(Functions.java:279)\n\tat ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)\n\tat ij.macro.Interpreter.getString(Interpreter.java:1498)\n\tat ij.macro.Interpreter.doStatement(Interpreter.java:336)\n\tat ij.macro.Interpreter.doStatements(Interpreter.java:267)\n\tat ij.macro.Interpreter.run(Interpreter.java:163)\n\tat ij.macro.Interpreter.run(Interpreter.java:93)\n\tat ij.macro.Interpreter.run(Interpreter.java:107)\n\tat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)\n\tat ij.IJ.runMacro(IJ.java:160)\n\tat ij.IJ.runMacro(IJ.java:149)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)\n\tat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)\n\tat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)\n\tat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:164)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n</code></pre>\n<p>Thank you!<br>\nVed</p>", "<p>Hi <a class=\"mention\" href=\"/u/vedsharma\">@vedsharma</a> ,</p>\n<p>I just quickly checked the problem and I can reproduce the error. The issue not obvious to me unfortunately. As I can\u2019t fix it immediately, I recommend using this macro instead:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm\" target=\"_blank\" rel=\"noopener\">clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm</a></h4>\n\n\n      <pre><code class=\"lang-ijm\">// CLIJ example macro: motionCorrection,ijm\n//\n// This macro shows how to do motion correction \n// image stack where slices might be shifted to \n// each other in the GPU.\n//\n// Author: Robert Haase\n// December 2018\n// ---------------------------------------------\nrun(\"Close All\");\n\n// define move to correct\nfile = \"https://github.com/clij/clij-docs/raw/master/src/main/resources/motion_correction_Drosophila_DSmanila1.tif\";\n//file = \"C:/structure/code/clij-docs/src/main/resources/motion_correction_Drosophila_DSmanila1.tif\";\n\n// define identifiers for intermediate results in the GPU\ninputStack = \"inputStack\";\nslice = \"slice\";\nshifted = \"shifted\";\nbinary = \"binary\";\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection.ijm\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>It seems to me pretty much the same algorithm, just a macro-version of it.</p>\n<p>Let me know if this works for you!</p>\n<p>Best,<br>\nRobert</p>", "<p>Hi <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a></p>\n<p>Thank you for your quick reply. I checked the macro and it works without any error, so that\u2019s great!</p>\n<p>BUT, the results I am getting are very different than what I get from <a href=\"http://bigwww.epfl.ch/thevenaz/stackreg/\" rel=\"noopener nofollow ugc\">stackreg</a>. Basically, I am happy with the stackreg results but it\u2019s slow for my large time-lapse (many GBs), so I am looking for an option to run stackreg or a similar algorithm on GPU. Any suggestions?</p>\n<p>Ved</p>", "<aside class=\"quote no-group\" data-username=\"vedsharma\" data-post=\"3\" data-topic=\"78234\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/vedsharma/40/43754_2.png\" class=\"avatar\"> Ved Sharma:</div>\n<blockquote>\n<p>BUT, the results I am getting are very different than what I get from <a href=\"http://bigwww.epfl.ch/thevenaz/stackreg/\">stackreg</a>.</p>\n</blockquote>\n</aside>\n<p>Yes, I know. I compared it with stackreg before:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm\" target=\"_blank\" rel=\"noopener\">clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm</a></h4>\n\n\n      <pre><code class=\"lang-ijm\">// CLIJ example macro: motionCorrection_compare_stackreg.ijm\n//\n// This macro compares performance of motion correction\n// on the GPU with StackReg. You need to install the\n// BIG-EPFL update site in order to test it.\n//\n// Author: Robert Haase\n// June 2019\n// ---------------------------------------------\nrun(\"Close All\");\n\n// define move to correct\nfile = \"https://github.com/clij/clij-docs/raw/master/src/main/resources/motion_correction_Drosophila_DSmanila1.tif\";\n//file = \"C:/structure/code/clij-docs/src/main/resources/motion_correction_Drosophila_DSmanila1.tif\";\n\n// define identifiers for intermediate results in the GPU\ninputStack = \"inputStack\";\nslice = \"slice\";\nshifted = \"shifted\";\nbinary = \"binary\";\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/clij/clij2-docs/blob/78be595cea9a75c04d0b1e9daf96cec9e6f8835e/src/main/macro/motionCorrection_compare_stackreg.ijm\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<aside class=\"quote no-group\" data-username=\"vedsharma\" data-post=\"3\" data-topic=\"78234\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/vedsharma/40/43754_2.png\" class=\"avatar\"> Ved Sharma:</div>\n<blockquote>\n<p>I am looking for an option to run stackreg or a similar algorithm on GPU. Any suggestions?</p>\n</blockquote>\n</aside>\n<p>Reimplementing stackreg for the GPU might be quite complicated. I looked into the code back in the days in 2019. To be honest, I\u2019m afraid I couldn\u2019t do it; or it would take me a substantial amount of time\u2026</p>"], "78235": ["<p>Hi there,</p>\n<p>I\u2019m using Batch counting plugin to count cfos. It was working pretty good for couple batch counting and now it shows an error</p>\n<p>Error:\t\tEmpty array in line 221:<br>\n(called from line 369)<br>\n(called from line 158)<br>\n(called from line 80)</p>\n<pre><code>\tlastx = updatedX [ updatedX . length - 1 &lt;]&gt; ; \n</code></pre>\n<p>Line 221 from the source is</p>\n<p>lastx = updatedX[updatedX.length - 1];<br>\nlasty = updatedY[updatedY.length - 1];<br>\nfor (i = updatedX.length; i &lt; 250; i++) {<br>\nupdatedX = append(updatedX, lastx);<br>\nupdatedY = append(updatedY, lasty);</p>\n<p>Any suggestions to resolve this would be appreciated. Thanks</p>"], "78247": ["<p>Hey, everyone!</p>\n<p>I am brand new to everything around this forum and the topics discussed here, so please bear with me. <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Is there one image analysis software that I could have on my mobile device to scan the results of a lateral flow assay, such as the ones we use now for covid?</p>\n<p>Maybe this already exists but I was wondering how would you guys approach this issue.<br>\nHow to normalize for light conditions? How to measure the line intensity? Is that something that can be done with a smart phone camera?</p>\n<p>I will be happy to chat with whoever finds this application interesting!</p>\n<p>Thank you for your time!</p>"], "78249": ["<p>I\u2019ve constructed a pixel classifier that segments H&amp;E images into four different classes.<br>\nHere\u2019s an example of what the classifier produces (it actually is an annotation mask I labelled though\u2026):<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717.png\" data-download-href=\"/uploads/short-url/6btJAj7bZDtL6ZSxonevWRUr9wH.png?dl=1\" title=\"2022S 0280579050101_0_label\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_380x500.png\" alt=\"2022S 0280579050101_0_label\" data-base62-sha1=\"6btJAj7bZDtL6ZSxonevWRUr9wH\" width=\"380\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_380x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_570x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b593e6b99f5faeaa6cfca6ecf64521d32bf8717_2_760x1000.png 2x\" data-dominant-color=\"5F6798\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">2022S 0280579050101_0_label</span><span class=\"informations\">2169\u00d72848 51.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>What I want is to import the masks and to create annotation objects at QuPath, so that I can do something more with the classification result (it actually is a screenshot during annotation process; of course I\u2019m going to create a new QuPath project containing annotated ROIs, not WSIs).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d.jpeg\" data-download-href=\"/uploads/short-url/a8QAXmnJpCbH5S1hhui10jLlgQJ.jpeg?dl=1\" title=\"\ucea1\ucc98.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d_2_384x499.jpeg\" alt=\"\ucea1\ucc98.PNG\" data-base62-sha1=\"a8QAXmnJpCbH5S1hhui10jLlgQJ\" width=\"384\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d_2_384x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d_2_576x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/4715cb8898a8a8429c838b412f745792dd82716d.jpeg 2x\" data-dominant-color=\"91669C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\ucea1\ucc98.PNG</span><span class=\"informations\">592\u00d7769 138 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I discovered <a href=\"https://gist.github.com/petebankhead/f807395f5d4f4bf0847584458ab50277\" rel=\"noopener nofollow ugc\">Script to import binary masks &amp; create annotations (see also QuPath-Export binary masks.groovy) \u00b7 GitHub</a>, but the example doesn\u2019t quite seem to be relevant for me\u2026 any kind of tip or suggestion would be greately appreciated! thanks!</p>\n<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding?</li>\n<li>What have you tried already?</li>\n<li>Have you found any related forum topics? If so, cross-link them.</li>\n<li>What software packages and/or plugins have you tried?</li>\n</ul>", "<p>Maybe look into other examples of importing objects/masks with different pixel values per object.</p><aside class=\"quote quote-modified\" data-post=\"5\" data-topic=\"75516\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/8491ac/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/import-external-masks-in-qupath/75516/5\">Import external masks in QuPath</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    ok got it now working and also changed the script so that one can use it for the complete project: \ndef server = getCurrentServer()\ndef name_server = server.getMetadata()[\"name\"]\n\ndef directoryPath = '/your_directory_path/HistoQC/histoqc_output_534243234523/'+ name_server + '/' // TO CHANGE\n\n//clearAllObjects()\n//double downsample = 32 // TO CHANGE (if needed)\nImagePlane plane = ImagePlane.getDefaultPlane()\n\nFile folder = new File(directoryPath);\nFile[] listOfFiles = folder.listFiles();\n\ncurrent\u2026\n  </blockquote>\n</aside>\n<p>\nIf your mask images are actually RGB though, and not normal mask images, that could be an issue.</p>", "<p>def imp = IJ.openImage(maskDir+imgName+\u2018.png\u2019)<br>\ndef ip = imp.getProcessor()<br>\nFloatProcessor foatIP = TypeConverter(ip).convertRGBToFloat()</p>\n<p>produces the following error\u2026</p>\n<p>ERROR: MissingMethodException at line 27: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.TypeConverter() is applicable for argument types: (ij.process.ColorProcessor) values: [ip[width=1836, height=3307, bits=24, min=0.0, max=255.0]]</p>\n<p>would it be much better to convert RGB masks to grayscale before brining them to QuPath?</p>", "<p>It\u2019s just that there are already ways of doing that. I\u2019m not sure how easy it is to pick out values in RGB. Although I think there was something a while back about storing millions of labels as an RGB image due to the bit depth.</p>", "<p>ImagePlus imgPlus = new ImagePlus(maskDir+imgName+\u2018.png\u2019)<br>\nImageProcessor OrigIP= imgPlus.getProcessor()</p>\n<p><strong>FloatProcessor ip = new TypeConverter(OrigIP, true).convertRGBToFloat()</strong></p>\n<p>int n = ip.getStatistics().max<br>\ndef objects = RoiLabeling.labelsToConnectedROIs(ip, n)</p>\n<p>def rois = objects.collect {<br>\nif(it == null)<br>\nreturn<br>\nreturn IJTools.convertToROI(it, 0, 0, downsample, plane);<br>\n}<br>\nrois = rois.findAll{null!=it}</p>\n<p>def pathObjects = rois.collect{<br>\nreturn PathObjects.createAnnotationObject(it)<br>\n}<br>\naddObjects(pathObjects)</p>\n<p>worked for me. But since there\u2019s something wrong with the RGB to grayscale conversion (small area are ignored and blended with the surrounding), I\u2019m going to create grayscale masks and to apply the script. Thank you for your suggestion!</p>"], "78255": ["<p>Dear all, I recently received some MMI-format files from a research partner which were taken using MetaSystems\u2019 Metafer and Zeiss microscope on chromosomal metaphase phases. However, I am now faced with the problem of what to do with these files as they cannot be opened by regular image viewers. Our research team does not have access to any equipment related to Metafer.</p>\n<p>Please advise on any tools or solutions that could help me convert these files into a more universally compatible format such as JPEG. Any suggestions would be greatly appreciated.</p>\n<p>Thank you in advance.</p>"], "68018": ["<p>I just spent some time trying to get cellpose to run on my Mac with an M1Pro chip.</p>\n<p>Here are some fixes that may help save time.<br>\nI use conda miniforge running macOS ARM64.</p>\n<p>I couldnt get cellpose plus gui running due to a problem with pyqt<br>\nSo this needs to run on x86-64.<br>\nTo do this on miniforge I use this command:</p>\n<p>CONDA_SUBDIR=osx-64 conda create -n rosetta1 python</p>\n<p>once activated I could install cellpose with:</p>\n<p>python -m pip install \u2018cellpose[gui]\u2019</p>\n<p>And this seems to work.</p>\n<p>To run cellpose on arm64:<br>\nI first install Napari and pyqt and generate the environment</p>\n<p>conda create -y -n napari-env -c andfoy python=3.8 pyqt napari python.app</p>\n<p>Then you need imagecodecs\u2026</p>\n<p>conda install -c conda-forge imagecodecs</p>\n<p>Then cellpose\u2026</p>\n<p>python -m pip install cellpose</p>\n<p>This seems to work as well.<br>\nPytorch does run on the Mac M1 GPU since a couple of weeks ago,<br>\nbut this doesn\u2019t work yet for cellpose due to problems with cuda as<br>\nfar as I understand. So at the moment, this is only running on the CPU.</p>\n<p>I am only a Python beginner, so if anybody has better suggestions please let me know!</p>", "<p>Hi <a class=\"mention\" href=\"/u/helfrid\">@helfrid</a><br>\nGreat to see a thread on cellpose on M1 Apple Silicon arm64.<br>\nNice overview. By chance, i\u2019ve been playing with cellpose lately as well on my M1.<br>\nI\u2019ve been able to get the GUI to work in my arm64 env:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/0/30c9f4cc6134ace91c55d7ef908b1ec5c2e8b9b1.jpeg\" data-download-href=\"/uploads/short-url/6XBBGwN0TlEdoIQW8gfiwJhHGVz.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/30c9f4cc6134ace91c55d7ef908b1ec5c2e8b9b1_2_576x500.jpeg\" alt=\"image\" data-base62-sha1=\"6XBBGwN0TlEdoIQW8gfiwJhHGVz\" width=\"576\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/30c9f4cc6134ace91c55d7ef908b1ec5c2e8b9b1_2_576x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/30c9f4cc6134ace91c55d7ef908b1ec5c2e8b9b1_2_864x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/30c9f4cc6134ace91c55d7ef908b1ec5c2e8b9b1_2_1152x1000.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/30c9f4cc6134ace91c55d7ef908b1ec5c2e8b9b1_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2624\u00d72274 680 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nBasically, I installed napari from conda-froge, which brought along pyqt5 and then also installed <code>pyqtgraph</code> and <code>imagecodecs</code> using conda-forge.<br>\npyqt5 &amp; imagecodecs is not available arm64 from pypi (via pip) so that has to be via conda-forge\u2014I use mamba. BTW, you don\u2019t need to use the <code>-c andfoy</code> channel anymore, pyqt5 for arm64 is now in the regular conda-forge channel.<br>\nFor PyTorch, I installed the <code>torch</code> nightly using pip, but the 1.11 release also has wheels on pip, so it\u2019s no issue.<br>\n<code>cellpose</code> also installed fine using pip.<br>\n<code>cellpose-napari</code> also installs fine using pip and runs perfectly on the samples:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aa3ba25dceded2583edb41738be935c2616b847.jpeg\" data-download-href=\"/uploads/short-url/1w7yCOx3Ofh7FD8rrIePVAG8f9Z.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aa3ba25dceded2583edb41738be935c2616b847_2_690x491.jpeg\" alt=\"image\" data-base62-sha1=\"1w7yCOx3Ofh7FD8rrIePVAG8f9Z\" width=\"690\" height=\"491\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aa3ba25dceded2583edb41738be935c2616b847_2_690x491.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aa3ba25dceded2583edb41738be935c2616b847_2_1035x736.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aa3ba25dceded2583edb41738be935c2616b847_2_1380x982.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aa3ba25dceded2583edb41738be935c2616b847_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2436\u00d71734 467 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Regarding GPU support, I actually started testing that. Cellpose kinda hard-codes CUDA, so it\u2019s not enough to just have the new PyTorch with MPS. Cellpose will need some code changes. I\u2019ve gotten it working, but there was some bug in one of the ops. I hope to try again when I find some time and will make a PR.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a> Thats really useful! I was hoping someone would reply with better solutions! It would be amazing if you could get cellpose to run with Metal and share the solution!</p>", "<p>For anyone else following, here\u2019s the simplest\u2014I think\u2014way to get arm64 cellpose on M1 Apple Silicon:</p>\n<ol>\n<li>Use conda-forge conda or mamba (miniforge) for arm64 (the later, mamba, is typically <em>much</em> faster): <a href=\"https://github.com/conda-forge/miniforge\" class=\"inline-onebox\">GitHub - conda-forge/miniforge: A conda-forge distribution.</a>\n</li>\n</ol>\n<p>If you arn\u2019t sure if you have arm64 conda, use:<br>\n<code>conda info | grep platform</code> which should return <code>osx-arm64</code></p>\n<ol start=\"2\">\n<li><code>create -y -n cellpose-env python=3.9 pyqt imagecodecs pyqtgraph</code></li>\n<li><code>conda activate cellpose-env</code></li>\n<li>\n<code>pip install cellpose</code><br>\nNow you should have the GUI too: <code>python -m cellpose</code><br>\n(Note: <code>pyqtgraph</code> can also be installed using <code>pip</code> if you prefer.)</li>\n</ol>\n<p>If you want <code>napari</code> and <code>cellpose-napari</code> you can install <code>napari</code> using <code>pip</code> or conda/mamba. And you can install the plugin from the napari GUI or using pip:<br>\n<code>pip install cellpose-napari</code></p>\n<p>BTW be aware downloading the models the first time can take a bit of time, making it seem like the plugin isn\u2019t working!</p>\n<p>Edit: I took the liberty of editing the title of the thread to make it broader and, hopefully, easier to find. Plus I added <code>cellpose</code> and <code>macos</code> as keywords.</p>", "<p><a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a>, I can confirm that your instructions work fine!</p>\n<aside class=\"quote no-group\" data-username=\"psobolewskiPhD\" data-post=\"2\" data-topic=\"68018\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\"> Peter Sobolewski:</div>\n<blockquote>\n<p>Regarding GPU support, I actually started testing that. Cellpose kinda hard-codes CUDA, so it\u2019s not enough to just have the new PyTorch with MPS. Cellpose will need some code changes. I\u2019ve gotten it working, but there was some bug in one of the ops. I hope to try again when I find some time and will make a PR.</p>\n</blockquote>\n</aside>\n<p>I wonder how far you came. I managed to make it use the MPS and had to fix a few things but hit a final bug in PyTorch (similar to <a href=\"https://github.com/pytorch/pytorch/issues/78429\" rel=\"noopener nofollow ugc\">this one</a>). However, even if that was solved some operations are not yet implemented and I get messages like \u201cThe operator xxx is not currently supported on the MPS backend\u201d. One can ignore them and fall back to CPU, but I guess performances would take a big hit (or even become worse than on CPU).</p>", "<p>Hi <a class=\"mention\" href=\"/u/guiwitz\">@guiwitz</a><br>\nYeah, that\u2019s about where I got. I could get it to run and use MPS, with<br>\n<code>%env PYTORCH_ENABLE_MPS_FALLBACK=1</code> to cover missing ops, but still got something exploding in the flow/label calculations after the torch.<br>\nI somehow nuked my changes though so I can\u2019t share what I did\u2014git noob strikes again <img src=\"https://emoji.discourse-cdn.com/twitter/pensive.png?v=12\" title=\":pensive:\" class=\"emoji\" alt=\":pensive:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>I wonder how bad the fallback is, at least there shouldn\u2019t be memory transfers on M1. For tensorflow the apple tensorflow-metal plugin makes a huge difference.</p>", "<p>Hello, thank you for your instruction. I have arm64 conda and I could create an environment successfully and install cellpose but when I want to install GUI, \u2018python -m cellpose\u2019 or run cellpose notebooks, I have this error in the terminal:<br>\n\u2018ModuleNotFoundError: No module named \u2018fastremap\u2019\u2019.</p>\n<p>I try to install \u2018fastremap\u2019 by \u2018pip install fastremap\u2019 on cellpose-env which I get:</p>\n<p>'Requirement already satisfied: fastremap in ./miniforge3/envs/cellpose-env/lib/python3.9/site-packages (1.13.2)</p>\n<p>Requirement already satisfied: numpy in ./miniforge3/envs/cellpose-env/lib/python3.9/site-packages (from fastremap) (1.22.4)\u2019</p>\n<p>I really don\u2019t know what to do. Can you please help me to solve this issue?</p>", "<p>I just checked and indeed <code>python -m cellpose</code> with  <code>fastremap 1.13.2</code> doesn\u2019t work for me.<br>\nWhen I wrote those instructions, the version was 1.12.2 and installing that:<br>\n<code>pip install fastremap==1.12.2</code> does make things work again.</p>\n<p>Very odd.</p>\n<p>I\u2019m not sure if this is a M1 issue or not, but probably it\u2019s something to report to the cellpose devs.<br>\nEdit: I think it\u2019s not a cellpose issue. It looks like <code>fastremap</code> now has Universal2 macOS binaries on pypi for python 3.9. So perhaps there is some issue with that binary. I will try and do some more detective work this evening.</p>\n<p>Edit2: it turned out to be an issue with the python 3.9 universal2 binary upload. See issue here:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/seung-lab/fastremap/issues/31\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/seung-lab/fastremap/issues/31\" target=\"_blank\" rel=\"noopener\">github.com/seung-lab/fastremap</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/seung-lab/fastremap/issues/31\" target=\"_blank\" rel=\"noopener\">fastremap 1.13.2 arm64 macOS: ModuleNotFoundError: No module named 'fastremap'</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-07-16\" data-time=\"02:15:06\" data-timezone=\"UTC\">02:15AM - 16 Jul 22 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/psobolewskiPhD\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"psobolewskiPhD\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/5/252e840432703c8e2923871b838b1f1269bb48be.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          psobolewskiPhD\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          installation\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Setting up a fresh arm64 python 3.9 conda on macOS 12.4 M1 and doing:\n`pip inst<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">all fastremap` installs `fastremap-1.13.2-cp39-cp39-macosx_10_9_universal2.whl`\nIt's awesome that there is now a fat binary for macOS, but...\nit doesn't import:\n```python\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) \n[Clang 13.0.1 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import fastremap\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'fastremap'\n```\nIt is installed:\n```\n pip show fastremap                                              (test-env) \u2500\u256f\nName: fastremap\nVersion: 1.13.2\nSummary: Remap, mask, renumber, unique, and in-place transposition of 3D labeled images. Point cloud too.\nHome-page: https://github.com/seung-lab/fastremap/\nAuthor: William Silversmith\nAuthor-email: ws9@princeton.edu\nLicense: LGPLv3\nLocation: /Users/piotrsobolewski/Dev/miniforge3/envs/test-env/lib/python3.9/site-packages\nRequires: numpy\nRequired-by: \n```\nInstalling 1.12.2 which builds from source on python 3.9 or installing 1.13.0 which also has a universal2 wheel works.\n\nEdit:\nIf I install using conda:\n`fastremap                 1.13.2           py39hc86685f_0    conda-forge`\nit imports and appears to work fine...\nSo it's not a 1.13.2 bug per se, but something with the build on pypi?\nEdit2:\nSeems like it, because using `--no-binary` and building 1.13.2 from source works fine.\nEdit3:\nI don't see where those universal2 wheels are coming from, as CI seems to only build x86:\nhttps://github.com/seung-lab/fastremap/runs/7034737842?check_suite_focus=true#step:4:573\nEdit4:\nThey're certainly bugged though:\n&lt;img width=\"580\" alt=\"image\" src=\"https://user-images.githubusercontent.com/76622105/179336217-462ac6c9-8a07-4600-89f4-d3a5479515af.png\"&gt;\n&lt;8 kb?\nEdit5:\nThe file size of 3.8 and 3.10 wheels is fine and both work fine in fresh 3.8 and 3.10 conda envs. So the issue is just with a broken 3.9 macos-universal2 wheel upload.</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nShould be resolved: the bad binary was pulled, so it builds from source. A new binary is in the works.</p>", "<p>Perfect, it works for me too.<br>\nAnd would you please let me know if this issue will affect training a model, I mean it may reduce accuracy or something like that?</p>\n<p>\u2018[INFO] TORCH CUDA version not installed/working\u2019</p>\n<p>I\u2019m asking that because, in this new instruction for  M1 Mac, I didn\u2019t expect to get this issue in the terminal again. Actually, it\u2019s crucial for me to use Cellpose.V2 to train a model for the specific dataset that I have.</p>\n<p>Thank you so much.</p>", "<p>So, essentially no Macs these days have access to CUDA. CUDA is an nVidia technology and no modern Mac as an nVidia GPU\u2014for whatever reason.<br>\nBaseline, at present, cellpose on macOS does not use GPU acceleration for anything: training or inference.<br>\nIf you have pytorch 1.12 installed there are some optimizations for CPU for M1, including use of Apple\u2019s Accelerate library. Also, there is the beginning of the MPS backend, which uses the Apple Metal library for GPU acceleration. See:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/5/75bff5469a36bcf10d57f28a3f077a7dd869e43a.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\" target=\"_blank\" rel=\"noopener\">pytorch.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/388;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54a0c14bf7ab94745720fb088e757d2f13e630ad_2_690x388.webp\" class=\"thumbnail\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54a0c14bf7ab94745720fb088e757d2f13e630ad_2_690x388.webp, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54a0c14bf7ab94745720fb088e757d2f13e630ad_2_1035x582.webp 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54a0c14bf7ab94745720fb088e757d2f13e630ad.webp 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54a0c14bf7ab94745720fb088e757d2f13e630ad_2_10x10.png\"></div>\n\n<h3><a href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\" target=\"_blank\" rel=\"noopener\">Introducing Accelerated PyTorch Training on Mac</a></h3>\n\n  <p>In collaboration with the Metal engineering team at Apple, we are excited to announce support for GPU-accelerated PyTorch training on Mac. Until now, PyTorch training on Mac only leveraged the CPU, but with the upcoming  PyTorch v1.12 release,...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p><a class=\"mention\" href=\"/u/guiwitz\">@guiwitz</a> and I have both started to play with it, but cellpose will need some PRs to get it to work. That said, not all torch ops are fully implemented. You can follow this tracking issue for progress:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/pytorch/pytorch/issues/77764\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/pytorch/pytorch/issues/77764\" target=\"_blank\" rel=\"noopener\">github.com/pytorch/pytorch</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/pytorch/pytorch/issues/77764\" target=\"_blank\" rel=\"noopener\">General MPS op coverage tracking issue</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-05-18\" data-time=\"18:12:47\" data-timezone=\"UTC\">06:12PM - 18 May 22 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/albanD\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"albanD\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/2292c1858f868193ff60fb13275713d1700b4185.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          albanD\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          feature\n        </span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          triaged\n        </span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          module: mps\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">### This issue is to have a centralized place to list and track work on adding s<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">upport to new ops for the MPS backend.\nThere are a very large number of operators in pytorch and so they are not all implemented yet for the MPS backends as it is still in the prototype phase. We will be prioritizing adding new operators based on user feedback.\n\nIf you want to work on adding support for such op, feel free to comment below to get assigned one. Please avoid pickup up an op that is already being worked on or that already has a PR associated with it.\n\nTODO: add link to the wiki for details on how to add these ops and example PRs\n\nOps that have MPS backend implementations:\n- [ ] `aten::_index_put_impl_` (worked on by @DenisVieriu97)\n\nOps not supported by MPS (will require either to use the CPU fallback system or a custom Metal kernel):\n- [ ] `aten::bitwise_xor.Tensor_out`\n\nNot categorized ops:\n- [x] `aten::amax.out` (#79682)\n- [ ] `aten::index.Tensor` (WIP: @DenisVieriu97)\n- [ ] `aten::index.Tensor_out` (WIP: @DenisVieriu97)\n- [ ] `aten::cumsum.out`\n- [ ] `torch.bincount`\n- [ ] `nn.Conv3D`\n- [ ] `aten::slow_conv3d_forward` \n- [ ] `aten::_slow_conv2d_forward` (@malfet could be due to incompatibility with torchvision)\n- [ ] `aten::slow_conv_transpose2d.out` (@malfet could be due to incompatibility with torchvision)\n- [x] `aten::eye.m_out` (https://github.com/pytorch/pytorch/pull/78408)\n- [ ] `aten::multinomial` (https://github.com/pytorch/pytorch/pull/80760 ) \n- [x] `aten::flip` (#80214)\n- [x] `aten::equal` https://github.com/pytorch/pytorch/pull/80195\n- [ ] `aten::upsample_nearest1d.out` (worked on by @dhruvak3 )\n- [ ] `aten::_unique2`\n- [x] `aten::_local_scalar_dense`\n- [ ] `aten::grid_sampler_2d`\n- [x] `aten::l1_loss_backward.grad_input` (#80010)\n- [ ] `aten::kl_div_backward` (worked on by @dhruvak3 )\n- [x] ` aten::linspace.out` https://github.com/pytorch/pytorch/pull/78570\n- [x]  `aten::arange.out` https://github.com/pytorch/pytorch/pull/78789\n- [x] `aten::adaptive_max_pool2d` https://github.com/pytorch/pytorch/pull/78410\n- [x] `aten::count_nonzero.dim_IntList`\n- [ ] ` aten::lgamma.out`\n- [ ] `aten::linalg_householder_product`\n- [x] `aten::softplus.out` (https://github.com/pytorch/pytorch/pull/78930)\n- [ ] `aten::_ctc_loss`\n- [ ] `aten::index_add.out` https://github.com/pytorch/pytorch/pull/79935\n- [x] `aten::normal` (#80297)\n- [x] `aten::glu.out` (#79866)\n- [ ] `aten::sort.values_stable`\n- [ ] `aten::native_group_norm_backward`\n- [ ] `linalg_solve`\n- [x] `aten::native_layer_norm_backward` https://github.com/pytorch/pytorch/pull/79189\n- [ ] `aten::avg_pool3d.out`\n- [ ] `aten::erfinv.out`\n- [ ] `aten::logical_and.out` (#80216)\n- [ ] `aten::_cdist_forward`\n- [ ] `aten::softplus_backward.grad_input` (#79873)\n- [ ] `aten::linalg_qr.out`\n- [ ] `aten::bitwise_and.Tensor_out`\n- [ ] `aten::histc`\n- [ ] `aten::triangular_solve.X`\n- [ ] `aten::nonzero`\n- [ ] `aten::multilabel_margin_loss_forward`</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Anyhow, when I last tested it wasn\u2019t exactly worth it for the simple cellpose test images\u2014for inference anyways, I didn\u2019t do training. But there is some hope that it provides a real boost, much like the metal plugin does for tensorflow.</p>", "<p>Thank you so much, Peter. It was really helpful.</p>", "<p>Sorry this took so long, but I\u2019ve re-released a complete set of binaries including universal2 for fastremap.</p>", "<p>The <a href=\"https://cellpose.readthedocs.io/en/latest/installation.html#dependencies\" rel=\"noopener nofollow ugc\">Cellpose installation documentation</a> now refers to<br>\npsobolewskiPhD\u2019s solution and adds the following command:</p>\n<pre><code class=\"lang-auto\">python -m cellpose --dir path --gpu_device mps --use_gpu\n</code></pre>\n<p>However, when I run it I receive the following output:</p>\n<pre><code class=\"lang-auto\">2022-12-16 17:46:13,149 [INFO] WRITING LOG OUTPUT TO /Users/rehanzuberi/.cellpose/run.log\n2022-12-16 17:46:13,156 [INFO] TORCH CUDA version not installed/working.\n2022-12-16 17:46:13,157 [INFO] &gt;&gt;&gt;&gt; using GPU\n2022-12-16 17:46:13,157 [INFO] &gt;&gt;&gt;&gt; using CPU\n2022-12-16 17:46:13,158 [INFO] &gt;&gt;&gt;&gt; running cellpose on 3 images using chan_to_seg GRAY and chan (opt) NONE\n2022-12-16 17:46:13,158 [INFO] &gt;&gt;&gt;&gt; using CPU\n2022-12-16 17:46:13,158 [INFO] &gt;&gt; cyto &lt;&lt; model set to be used\nzsh: segmentation fault  python -m cellpose --dir images_test --gpu_device mps --use_gpu --verbose\n</code></pre>", "<p>Yeah, the release of cellpose with MPS support seems incomplete. CUDA is still hard coded in a few places.<br>\nYou can try my experimental branch where I made some more tweaks:</p><aside class=\"onebox githubfolder\" data-onebox-src=\"https://github.com/psobolewskiPhD/cellpose/tree/feature/add_MPS_device\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/psobolewskiPhD/cellpose/tree/feature/add_MPS_device\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h3><a href=\"https://github.com/psobolewskiPhD/cellpose/tree/feature/add_MPS_device\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - psobolewskiPhD/cellpose at feature/add_MPS_device</a></h3>\n\n  <p><a href=\"https://github.com/psobolewskiPhD/cellpose/tree/feature/add_MPS_device\" target=\"_blank\" rel=\"noopener nofollow ugc\">feature/add_MPS_device</a></p>\n\n  <p><span class=\"label1\">a generalist algorithm for cellular segmentation with human-in-the-loop capabilities</span></p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nI see a marked speed up on this script based off the cellpose examples:<a href=\"https://gist.github.com/psobolewskiPhD/80d4b18ba78b49eec6a2398153347a83\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Test script for cellpose MPS branch \u00b7 GitHub</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a>, thank you so much for your help on this! I tried your experimental branch, but am getting the following</p>\n<pre><code class=\"lang-auto\">python -m cellpose --gpu_device mps --use_gpu                         (cellpose-env)\nusage: __main__.py [-h] [--use_gpu] [--gpu_device GPU_DEVICE] [--check_mkl] [--dir DIR] [--look_one_level_down]\n                   [--img_filter IMG_FILTER] [--channel_axis CHANNEL_AXIS] [--z_axis Z_AXIS] [--chan CHAN] [--chan2 CHAN2]\n                   [--invert] [--all_channels] [--pretrained_model PRETRAINED_MODEL] [--unet] [--nclasses NCLASSES] [--no_resample]\n                   [--net_avg] [--no_interp] [--no_norm] [--do_3D] [--diameter DIAMETER] [--stitch_threshold STITCH_THRESHOLD]\n                   [--fast_mode] [--flow_threshold FLOW_THRESHOLD] [--cellprob_threshold CELLPROB_THRESHOLD]\n                   [--anisotropy ANISOTROPY] [--exclude_on_edges] [--save_png] [--save_tif] [--no_npy] [--savedir SAVEDIR]\n                   [--dir_above] [--in_folders] [--save_flows] [--save_outlines] [--save_ncolor] [--save_txt] [--train]\n                   [--train_size] [--test_dir TEST_DIR] [--mask_filter MASK_FILTER] [--diam_mean DIAM_MEAN]\n                   [--learning_rate LEARNING_RATE] [--weight_decay WEIGHT_DECAY] [--n_epochs N_EPOCHS] [--batch_size BATCH_SIZE]\n                   [--min_train_masks MIN_TRAIN_MASKS] [--residual_on RESIDUAL_ON] [--style_on STYLE_ON]\n                   [--concatenation CONCATENATION] [--save_every SAVE_EVERY] [--save_each] [--verbose]\n__main__.py: error: argument --gpu_device: invalid int value: 'mps'\n</code></pre>\n<p>No rush though, the CPU version is slow but works well enough <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Sorry! I\u2019ve never used the command line options, just from a script, so I bet they\u2019re not being set correctly. Maybe try just <code>--use-gpu</code>?</p>", "<p>Just came across this thread, this is super cool <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a> ! Very nice to be able to run cellpose models using the GPU on my Mac locally! For some 3D example data, I came down from 20 minutes to below 1 minute inference time <img src=\"https://emoji.discourse-cdn.com/twitter/smile.png?v=12\" title=\":smile:\" class=\"emoji\" alt=\":smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thanks a lot for sharing this, I hope this makes it back into the main cellpose, this is super useful!</p>\n<p>If anyone else comes across this:<br>\nYou can just run <code>python -m cellpose</code> to start the GUI and it automatically uses the GPU then.</p>\n<p>Also, here is a workflow to set this up that worked for me:</p>\n<pre><code class=\"lang-auto\">conda create --name cellpose-dev python=3.9 -y\nconda activate cellpose-dev\nconda install napari\ngit clone https://github.com/psobolewskiPhD/cellpose.git\ncd cellpose \ngit fetch\ngit switch feature/add_MPS_device \nconda install imagecodecs -y\npip install -e .\n\npython -m cellpose\n</code></pre>\n<p>(installing napari so I don\u2019t need to worry about things like pyqt5 installation anymore)</p>", "<p>Glad it works for you!<br>\nHave you checked out the main branch of cellpose?</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/MouseLand/cellpose/issues/523\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/issues/523\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/MouseLand/cellpose</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/MouseLand/cellpose/issues/523\" target=\"_blank\" rel=\"noopener nofollow ugc\">M1 apple</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-06-04\" data-time=\"23:08:56\" data-timezone=\"UTC\">11:08PM - 04 Jun 22 UTC</span>\n      </div>\n\n        <div class=\"date\">\n          closed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2023-01-12\" data-time=\"02:02:56\" data-timezone=\"UTC\">02:02AM - 12 Jan 23 UTC</span>\n        </div>\n\n      <div class=\"user\">\n        <a href=\"https://github.com/zyazdani-92\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"zyazdani-92\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e30e351200e5d314fe8ee620b14c8b6b31e8bb.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          zyazdani-92\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          enhancement\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Hello,\n\nI trained a model in GUI (Cellpose-V2) and I'd like to use it on a pip<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">eline in the Jupyter notebook.\nI'm facing this error: TORCH CUDA version not installed/working.\n\nI used `conda install pytorch cudatoolkit=10.1 -c pytorch` but it didn't work. \n\nWould you please help me to fix this issue?\nI have a MacBook air M1</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nI\u2019ve not had time, but maybe it\u2019s also working on MPS?<br>\nAlso, do you have <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code> set?</p>", "<p>Ah, interesting. No, I did not try the main branch yet. I came here via the cellpose documentation that recommends using your approach described in this forum thread (see <a href=\"https://cellpose.readthedocs.io/en/latest/installation.html#m1-mac-installation\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Installation \u2014 cellpose 0.7.2 documentation</a>). Good idea to try this though!</p>\n<p>Also, I did not set the <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code> option set.</p>", "<p>OK, then I think you might just be getting CPU, which with recent PyTorch is using Accelerate Apple library for various math things like matrix multiply, which will use the AMX, which is a pretty big performance boost over the totally un-accelerated CPU torch.</p>\n<p>BTW conda-forge has a numpy build that also uses Accelerate for BLAS and it\u2019s pretty <img src=\"https://emoji.discourse-cdn.com/twitter/fire.png?v=12\" title=\":fire:\" class=\"emoji\" alt=\":fire:\" loading=\"lazy\" width=\"20\" height=\"20\"> compared to OpenBLAS.<br>\nSee also: <a href=\"https://stackoverflow.com/questions/67587455/accelerate-framework-uses-only-one-core-on-mac-m1\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">c - Accelerate framework uses only one core on Mac M1 - Stack Overflow</a></p>"], "78259": ["<p>Hello!</p>\n<p>Has someone ever accessed the <a href=\"https://docs.openmicroscopy.org/ome-model/6.3.1/developers/screen-plate-well.html#wellsample\" rel=\"noopener nofollow ugc\">\u201cindex\u201d field for Well Samples</a> using the Java API?</p>\n<p>I cannot find it in the DataObject or the IObject.<br>\nI might be blind though!</p>", "<p>Good morning, <a class=\"mention\" href=\"/u/pierre.pouchin\">@pierre.pouchin</a>.</p>\n<p>In the OMERO objects, the index is the location in the list/array of well samples held by the Well object, e.g.:</p>\n<pre><code class=\"lang-auto\">            List&lt;WellSample&gt; samples = well.copyWellSamples();\n            int number = well.sizeOfWellSamples();\n            WellSample sample = well.getWellSample(2);\n</code></pre>\n<p>~J.</p>", "<p>Ok, thanks!</p>\n<p>Therefore if a well sample is retrieved from an image, it should be checked against its parent well to get the index?</p>", "<p>Yes. If you don\u2019t have the index already (<code>for (int i=0; i&lt;sizeOfWellSamples()...</code>, etc.) then you\u2019ll need to use <code>indexOf</code>-logic.</p>\n<p>~J.</p>"], "22967": ["<p>Hi!</p>\n<p>Could you help clarify what the deconvolutional layer outputs?<br>\nI would have expected it to have the same dimensions as the input image for each filter of the deconvolutional layer, with the pixel with the highest value being the predicted joint location (like a probability distribution over pixels). However, it seems that the shapes of input and prediction are different.</p>\n<p>Thank you</p>", "<p>Hi <a class=\"mention\" href=\"/u/bramn22\">@bramn22</a>,</p>\n<p>Welcome to the forum.</p>\n<p>I\u2019m not a deeplabcut expert, but I know some things about CNNs.<br>\n\u201cDeconvolutional layer\u201d is a <a href=\"https://github.com/tensorflow/tensorflow/issues/256#issuecomment-162257789\" rel=\"nofollow noopener\">confusing name</a> for it.  Better is a \u201ctransposed convolution.\u201d</p>\n<p>The <a href=\"https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers\" rel=\"nofollow noopener\">top answer for this stackoverflow post</a> explains it well.</p>\n<p>And includes this nice visualization (which I\u2019ll include here)<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24111d3912a5147b1be9e1617fb9ee368491b8ba.gif\" alt data-base62-sha1=\"593R3Q9gxzjFS4WaOxk9GSmClF8\" width=\"344\" height=\"386\"></p>\n<p>If its not clear after checking that out, post back!<br>\nJohn</p>", "<p>Hi <a class=\"mention\" href=\"/u/bogovicj\">@bogovicj</a>,</p>\n<p>Thank you for your response.<br>\nI understand that the deconvolution layer performs a similar operation to upsampling and a standard convolutional layer, but I am still confused as to what the output of the deeplabcut model is. Is it correct that this is a probability distribution of the same dimensions as the input image (scaled down) where the \u201cpixel\u201d with the highest probability is selected as a body part location corresponding to the current deconvolutional filter?</p>", "<p>Hi <a class=\"mention\" href=\"/u/bramn22\">@bramn22</a>,</p>\n<aside class=\"quote no-group\" data-post=\"3\" data-topic=\"22967\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bramn22/40/17108_2.png\" class=\"avatar\"> bramn22:</div>\n<blockquote>\n<p>s it correct that this is a probability distribution of the same dimensions as the input image (scaled down) where the \u201cpixel\u201d with the highest probability is selected as a body part location corresponding to the current deconvolutional filter?</p>\n</blockquote>\n</aside>\n<p>I think you\u2019ve got the right idea, but I\u2019ll just rephrase.  The output of the network will be <code>N</code> images, where <code>N</code> is the number of markers.  Each of those output images corresponds to one marker, and you  can interpret the value at a certain pixel in that image to be the probability of finding the corresponding marker at that point.</p>\n<p>Fig 7 in <a href=\"https://arxiv.org/pdf/1804.03142.pdf\" rel=\"nofollow noopener\">this paper</a> visualizes this idea.</p>\n<p>John</p>", "<p>Thank your very much <a class=\"mention\" href=\"/u/bogovicj\">@bogovicj</a> . I understand that the output of the network will be N images, where N is the number of markers. But I have a small question, what shape of one images? Is it the same as input image shape?</p>", "<p><a class=\"mention\" href=\"/u/fengzhiheng\">@FengZhiheng</a> ,</p>\n<aside class=\"quote no-group\" data-username=\"FengZhiheng\" data-post=\"5\" data-topic=\"22967\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fengzhiheng/40/42465_2.png\" class=\"avatar\"> FengZhiheng:</div>\n<blockquote>\n<p>what shape of one images? Is it the same as input image shape?</p>\n</blockquote>\n</aside>\n<p>Yes, usually the output images are the same size / shape as the input image.</p>"], "78265": ["<p>Dear all,</p>\n<p>maybe this topic already popped up somewhere, so I apologize for me being na\u00efve.</p>\n<p>I am analyzing a series of images in *.czi format (from Carl Zeiss Confocal microscope, acquired with ZEN software (2012)) and I am interested in collecting the X and Y data of the grayscale intensity, therefore I upload the picture, I draw a line along the region of interest (for the ones who understand, it is the coil series of the protoxylem stained with PI dye) which is a \u201cspiral-like\u201d structure, appearing white on a black background, and I \u201cPlot Profile\u201d. This latter action gives me X= distance in micron and Y= grayscale.</p>\n<p>Now, generally, the value of the distance (micron) between 0 and the first point is 0.104, but when I use a specific series of images, though that they are taken in the same manner, in the same conditions, with the same characteristics, the distance becomes 0.415 automatically.</p>\n<p>My collaborator and I were wondering if there is any way to adjust this sampling by hand, so that also this latter series of samples have the same distance of the others (all 0.104).</p>\n<p>Any comment or tip will be highly appreciated!</p>", "<p>Hi ValeEle,</p>\n<p>When opening the images, could you look at the image and pixel size?</p>\n<p>For image size, look at the top of the image.  For Pixel size, go to Image&gt;Properties.  Are they the same?</p>\n<p>Sincerely,</p>\n<p>Matthieu</p>", "<p>When I open the pic, on the border of the image I have 212.55 x 212.55 microns (512x512) and the pixel is 0.4151329.</p>\n<p>Thanks for helping</p>", "<p>Is that for all the images?<br>\nPlot profile will plot grey values in Y pixel by pixel, if pixel size changes between images, values in X will reflect this change.</p>\n<p>Sincerely,</p>\n<p>Matthieu</p>", "<p>this makes a lot of sense! So, to explain, I have a pool of samples belonging to different categories (A, B, C, D, E\u2026) in condition 1 and condition 2 (so the entire pool is 2x categories) and regardless of the time when they were collected, they tend to have the distance (in micron) = 0.104 circa.<br>\nIn the same category and treatment (i.e. A, condition 1) I have - let\u2019s say - 16 samples, which are generally consistent [distance (in micron) = 0.104 circa].<br>\nIn another case, instead, samples from a specific category, regardless the conditions, have distance (in micron) = 0.415 circa.<br>\nSo when it comes to comparing them, we have a problem and the possible \u2018normalizations\u2019 we could do downstream the data acquisition could end up in fabricated and imprecise data.</p>\n<p>DO you think that re-setting the pixel size before analyzing the images can help?</p>\n<p>Thank you very much</p>", "<p>Hi,</p>\n<p>Resizing the pixel size is unwise.  The issue is upstream when the pictures were taken.  Difference is a factor of 4 -either zoom during acquisition or lens\u2026</p>\n<p>Best thing I\u2019d do is to retake the pictures with large pixels to match those with smaller pixels.  You could fudge it by rescaling the images with smaller pixels (Image&gt; Scale\u2026) by a factor of 0.25, but you will then lose all the details in these images.</p>\n<p>Sorry.</p>", "<p>Don\u2019t be sorry, you helped me a lot!</p>\n<p>Thank you!</p>"], "78267": ["<p>Hi!</p>\n<p>I\u2019m having problem with \u201crefine tracklets\u201d\u2026</p>\n<p>This is my step.<br>\nMax gap of missing data to fill = 0 (this video has 6300 frames)<br>\n1.run stitching<br>\n2.Launch track refnement GUI and fixed all frames.<br>\n3.Filter tracks(+.csv)<br>\n4. Merge dataset. Then met error.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a.png\" data-download-href=\"/uploads/short-url/1Fb3z6RQNGbFav2gZ5HqtZI5pqO.png?dl=1\" title=\"Screenshot from 2023-03-08 19-56-26\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a_2_690x112.png\" alt=\"Screenshot from 2023-03-08 19-56-26\" data-base62-sha1=\"1Fb3z6RQNGbFav2gZ5HqtZI5pqO\" width=\"690\" height=\"112\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a_2_690x112.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0ba9ce0eb63c48e255a1a2850c0ff6ff499e0b1a.png 2x\" data-dominant-color=\"3F1C34\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-08 19-56-26</span><span class=\"informations\">991\u00d7161 29.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Any help is appreciated!!</p>\n<p>My PC:<br>\nubuntu 22.04, cuda11.7, driver 525.85.12<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5.png\" data-download-href=\"/uploads/short-url/a8E0hEQbjWdci868k8Orabtcb6l.png?dl=1\" title=\"Screenshot from 2023-03-09 13-08-02\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5_2_547x500.png\" alt=\"Screenshot from 2023-03-09 13-08-02\" data-base62-sha1=\"a8E0hEQbjWdci868k8Orabtcb6l\" width=\"547\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5_2_547x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/470feb0318794235584ad1c14c15ce1c69c1b5c5.png 2x\" data-dominant-color=\"3B1830\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-09 13-08-02</span><span class=\"informations\">721\u00d7658 81.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nconda environment<br>\nmatplotlib=3.5.1<br>\npython=3.8</p>\n<p>deeplabcut=2.3.0</p>", "<p>Also I can not use label frames GUI during extract outlier frames\u2026<br>\nIt seems my folder is not recognized. There are extracted frames in folder.</p>\n<p>ValueError: cannot reindex on an axis with duplicate labels</p>", "<p>When I drop label folder to GUI.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png\" data-download-href=\"/uploads/short-url/ch6Uq9sHI0X6tvhKx2jKUNQKtsZ.png?dl=1\" title=\"Screenshot from 2023-03-09 14-23-41\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png\" alt=\"Screenshot from 2023-03-09 14-23-41\" data-base62-sha1=\"ch6Uq9sHI0X6tvhKx2jKUNQKtsZ\" width=\"390\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png 2x\" data-dominant-color=\"442239\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-09 14-23-41</span><span class=\"informations\">738\u00d7944 169 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png\" data-download-href=\"/uploads/short-url/ch6Uq9sHI0X6tvhKx2jKUNQKtsZ.png?dl=1\" title=\"Screenshot from 2023-03-09 14-23-41\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png\" alt=\"Screenshot from 2023-03-09 14-23-41\" data-base62-sha1=\"ch6Uq9sHI0X6tvhKx2jKUNQKtsZ\" width=\"390\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/5609438780472b5217d75540782cbba0da27de29_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/5609438780472b5217d75540782cbba0da27de29.png 2x\" data-dominant-color=\"442239\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-09 14-23-41</span><span class=\"informations\">738\u00d7944 169 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8.png\" data-download-href=\"/uploads/short-url/nFTchsFcCFD1QdE6I8azjEEziMo.png?dl=1\" title=\"Screenshot from 2023-03-09 14-23-58\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8_2_390x500.png\" alt=\"Screenshot from 2023-03-09 14-23-58\" data-base62-sha1=\"nFTchsFcCFD1QdE6I8azjEEziMo\" width=\"390\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5ee3c14930b1118025c3b7711ac5005f42dcca8.png 2x\" data-dominant-color=\"45233B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-09 14-23-58</span><span class=\"informations\">738\u00d7944 173 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3.png\" data-download-href=\"/uploads/short-url/ndLewlbDLUfvaF2RCs9AVbIfTs7.png?dl=1\" title=\"Screenshot from 2023-03-09 14-24-11\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3_2_390x500.png\" alt=\"Screenshot from 2023-03-09 14-24-11\" data-base62-sha1=\"ndLewlbDLUfvaF2RCs9AVbIfTs7\" width=\"390\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3_2_390x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3_2_585x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/2/a2c03ed6bc67e50810568007d005530b9e3b17d3.png 2x\" data-dominant-color=\"44223A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-09 14-24-11</span><span class=\"informations\">738\u00d7944 171 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/709ecbde770b3662bfd890bd56ad5192fdfa7b83.png\" data-download-href=\"/uploads/short-url/g4hHaGPR54pbP31RV1awbf9gHRh.png?dl=1\" title=\"Screenshot from 2023-03-09 14-24-21\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/709ecbde770b3662bfd890bd56ad5192fdfa7b83.png\" alt=\"Screenshot from 2023-03-09 14-24-21\" data-base62-sha1=\"g4hHaGPR54pbP31RV1awbf9gHRh\" width=\"690\" height=\"192\" data-dominant-color=\"45243B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-09 14-24-21</span><span class=\"informations\">744\u00d7208 35.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Regarding the error from refine_tracklets GUI, I made a pull request to fix this, it\u2019s pending now (the line can be removed from the code to make it work - it\u2019s just making an icon in a popup window).</p>\n<p>Can you explain step by step what you\u2019re doing that causes the second error?</p>", "<p>I apologize for any inconvenience caused that I was unable to reproduce that error.<br>\nHowever, I encountered a similar issue(I think) and here are the steps:</p>\n<p>Load my project, Extract outline frames</p>\n<ol>\n<li>\n<p>Click \u201cExtract frames\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469.png\" data-download-href=\"/uploads/short-url/wPa1scoFXyV1DcvDfHl7A8iwGd.png?dl=1\" title=\"click extract frames\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469_2_690x479.png\" alt=\"click extract frames\" data-base62-sha1=\"wPa1scoFXyV1DcvDfHl7A8iwGd\" width=\"690\" height=\"479\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469_2_690x479.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469_2_1035x718.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03b5e7a19c5d22a49baa34ccefd16df825e0d469.png 2x\" data-dominant-color=\"2D303D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">click extract frames</span><span class=\"informations\">1278\u00d7889 118 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>Click \u201cLabeling GUI\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002.png\" data-download-href=\"/uploads/short-url/pACMfYzgciyhKq7LXDgleOqgR9w.png?dl=1\" title=\"click labeling GUI\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002_2_690x479.png\" alt=\"click labeling GUI\" data-base62-sha1=\"pACMfYzgciyhKq7LXDgleOqgR9w\" width=\"690\" height=\"479\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002_2_690x479.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002_2_1035x718.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b35a3a319491ab579b77aaddb97055a5ad371002.png 2x\" data-dominant-color=\"262129\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">click labeling GUI</span><span class=\"informations\">1278\u00d7889 134 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>Drop the folder located inside folder \u201clabeled-data\u201d  into the labele GUI and click \u201cOK\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c.png\" data-download-href=\"/uploads/short-url/yWoIBLwhtDkyjuv06MzNYcBfo5e.png?dl=1\" title=\"Ok\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c_2_690x351.png\" alt=\"Ok\" data-base62-sha1=\"yWoIBLwhtDkyjuv06MzNYcBfo5e\" width=\"690\" height=\"351\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c_2_690x351.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c_2_1035x526.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4e3df9d67e4bb3c032f3b03d7fa75d658b6307c.png 2x\" data-dominant-color=\"2B2D30\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Ok</span><span class=\"informations\">1247\u00d7635 71.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44.png\" data-download-href=\"/uploads/short-url/ywO7x2RuxysqhHAJw1ojjEW1VDm.png?dl=1\" title=\"after click OK\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_690x334.png\" alt=\"after click OK\" data-base62-sha1=\"ywO7x2RuxysqhHAJw1ojjEW1VDm\" width=\"690\" height=\"334\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_690x334.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_1035x501.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1ff5591d76b073e90bea360e0e8de4e304f9c44_2_1380x668.png 2x\" data-dominant-color=\"3F1C34\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">after click OK</span><span class=\"informations\">1408\u00d7682 157 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n</ol>\n<hr>\n<hr>\n<p>Regarding the issue with, Refine tracklets, I wanted to share with you the steps I took and the results I obtained today. I have completed the correction of all 6300 frames and followed these steps:</p>\n<ol>\n<li>\n<p>Launch track refinement GUI<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1.png\" data-download-href=\"/uploads/short-url/fzeJUHG6jI0mayykk9dzRBoi0j7.png?dl=1\" title=\"ref gui\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_690x423.png\" alt=\"ref gui\" data-base62-sha1=\"fzeJUHG6jI0mayykk9dzRBoi0j7\" width=\"690\" height=\"423\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_690x423.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_1035x634.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6d1c546c9f1f6f003370698726a519486b3141b1_2_1380x846.png 2x\" data-dominant-color=\"3F424D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ref gui</span><span class=\"informations\">1587\u00d7974 131 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>Fix some small jitters and click \u201csave\u201d</p>\n</li>\n<li>\n<p>After clicking \u201cMerge dataset\u201d, there are sometimes issues with the deeplabcut GUI crashing, and other times an error message appears.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd.png\" data-download-href=\"/uploads/short-url/3L4KKwGTevlXpinvlSiDcVXiF7D.png?dl=1\" title=\"merge dataset(refine)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd_2_660x499.png\" alt=\"merge dataset(refine)\" data-base62-sha1=\"3L4KKwGTevlXpinvlSiDcVXiF7D\" width=\"660\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd_2_660x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd_2_990x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/a/1a58d72862b784dae5f7378d9eb31835114028fd.png 2x\" data-dominant-color=\"34303F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">merge dataset(refine)</span><span class=\"informations\">1296\u00d7981 199 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n</ol>\n<p>I hope my reply will be helpful to you.</p>", "<p>It looks like it\u2019s trying to open the csv file as if it was an image. Can you try to just open the folder from the dropdown menu? The second error like I said, you can follow to this line in the code and comment it out or remove completely</p>", "<p>I apologize deeply for my delayed response!</p>\n<p>I encountered an error message even when using the dropdown menu too.<br>\nInitially, I thought the error was random, but later discovered that it was caused by two files in the folder named \u201cmachinelables.csv\u201d and \u201cmachinelables-iter0.h5\u201d. After deleting these two files, I was able to drag the folder into the labeling GUI without any issues.</p>\n<p>May I ask if deleting these two files will have any impact on the entire project?<br>\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>\nRegarding the issue of DLC GUI crashing, the entire GUI crashes when I click on \u201cSave All Layers\u201d, and my labels are not saved.<br>\nThere are only two layers, \u201cimages\u201d and \u201cCollectedData\u201d in the labeling GUI.</p>", "<p>You should save the point layer only (<code>CollectedData</code>), shouldn\u2019t crash then.</p>\n<p>A little weird that the <code>machinelabels</code> files would casue those issues, it should just load to point layers in that case. Deleting the files is fine. Especially if you already corrected them and merged into the dataset.</p>", "<p>Thank you for your response. Yes, it is possible to save only one layer.</p>\n<p>I have also come across a point of confusion. After analyzing the video, the video obtained from using \u201canalyze videos\u201d seems to be different from the video obtained from using \u201ccreate videos\u201d. I am unsure if I have missed any parameters.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/a/9a403698426f973d612b66865081a75bc8906f98.png\" data-download-href=\"/uploads/short-url/m0z6CfLY2vjgEr7rlO1jLCsDVfa.png?dl=1\" title=\"ID FULL P1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_254x500.png\" alt=\"ID FULL P1\" data-base62-sha1=\"m0z6CfLY2vjgEr7rlO1jLCsDVfa\" width=\"254\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_254x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_381x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/a/9a403698426f973d612b66865081a75bc8906f98_2_508x1000.png 2x\" data-dominant-color=\"A2A488\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ID FULL P1</span><span class=\"informations\">508\u00d71000 188 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Furthermore, the first ten frames of the .csv file obtained through \u201canalyze videos\u201d are blank, just like the video obtained in \u201ccreate videos\u201d.</p>", "<p>By the way, is there a solution available for the issue with deeplabcut not being able to label all body parts, as shown in the figure?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997.png\" data-download-href=\"/uploads/short-url/pXLX9Q3jQi7ZVTlZcJHXEqO7DV5.png?dl=1\" title=\"ID FULL P2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_254x500.png\" alt=\"ID FULL P2\" data-base62-sha1=\"pXLX9Q3jQi7ZVTlZcJHXEqO7DV5\" width=\"254\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_254x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_381x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/5/b5f8173ca4675d1e23302b47f268d2cf60f60997_2_508x1000.png 2x\" data-dominant-color=\"A2A488\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ID FULL P2</span><span class=\"informations\">508\u00d71000 257 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Also, despite having over 1k frames of training data, the results from shuffle 1 to 5 have not shown significant improvement. Since shuffle 2, I have manually selected frames with unusual body states.</p>", "<p>Are you using only 3 bodyparts per animal? Using more would definitely improve performance. Also, are you potentially running with <code>identity_only</code> set to <code>True</code>?</p>", "<p>Thank you for your suggestion. Yes, I only use 3 bodyparts now. I will try to use 7 bodyparts.<br>\nIf I want to obtain the coordinates of all frames without any blank frames, does this mean that I should label all body parts, even if they are occluded?</p>\n<p>Are you referring to \u201cAssemble with ID only\u201d?<br>\nI think l set it as true in shuffle1-4, and false in shuffle5.</p>", "<p>I assume you don\u2019t know the identity of the animals so you shouldn\u2019t assemble with identity. More bodyparts will definitely help</p>"], "78268": ["<p>Hi everyone,</p>\n<p>Since I now have a new graphic card (RTX3080), I removed the deeplabcut environment and installed it again. It also automatically updated Deeplabcut to version 2.3.1. When I try to load my existed project I am having an error and the project wont be loaded. When I try to create a new project, I get the same error message again. I tried to re-install Deeplabcut and conda environment several times but did not work. Here is the error message I got when I load the project:</p>\n<p>FileNotFoundError: [Errno 2] No such file or directory: \u2018C:\\Users\\Zoology_office_05\\Anaconda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\modelzoo\\models.json\u2019</p>\n<p>Any contribution would be appreciated. Thank you very much in advance</p>", "<p>I just updated to see if it would correct another error I have and am encountering the same problem. Very frustrating!</p>\n<p>The full output is as follows:</p>\n<p>(DEEPLABCUT) C:\\Users\\vamoro1&gt;python -m deeplabcut<br>\nLoading DLC 2.3.1\u2026<br>\nC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\statsmodels\\compat\\pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.<br>\nfrom pandas import Int64Index as NumericIndex<br>\nStarting GUI\u2026<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\window.py\u201d, line 396, in _open_project<br>\nself._update_project_state(<br>\nFile \u201cC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\window.py\u201d, line 383, in _update_project_state<br>\nself.add_tabs()<br>\nFile \u201cC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\window.py\u201d, line 474, in add_tabs<br>\nself.modelzoo = ModelZoo(<br>\nFile \u201cC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\tabs\\modelzoo.py\u201d, line 27, in <strong>init</strong><br>\nself._set_page()<br>\nFile \u201cC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\tabs\\modelzoo.py\u201d, line 46, in _set_page<br>\nsupermodels = parse_available_supermodels()<br>\nFile \u201cC:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\modelzoo\\utils.py\u201d, line 10, in parse_available_supermodels<br>\nwith open(json_path) as file:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018C:\\Users\\vamoro1\\.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\modelzoo\\models.json\u2019</p>", "<p>Either download the [.json] file from <a href=\"https://github.com/DeepLabCut/DeepLabCut/tree/main/deeplabcut/modelzoo\" rel=\"noopener nofollow ugc\">here (Github)</a> or the attachment to this post and place in the missing directory:</p>\n<blockquote>\n<p>&lt;C:\\your file directory structure&gt; \\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\modelzoo\\</p>\n</blockquote>\n<p><a class=\"attachment\" href=\"/uploads/short-url/7Sx5idp0DsxQ63yd9WBEsFEsTxv.json\">models.json</a> (108 Bytes)</p>", "<p>Thanks for sharing the file. The problem is also discussed here:</p>\n<p><a href=\"https://github.com/DeepLabCut/DeepLabCut/issues/2169\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">FileNotFoundError thrown in GUI after updating to 2.3.1 \u00b7 Issue #2169 \u00b7 DeepLabCut/DeepLabCut \u00b7 GitHub</a></p>", "<p>Hey, thanks a lot for the file. I am still very new to a lot of this and wanted to ask how you could go about placing the json file in the modelzoo.py?</p>", "<p>Just copy paste the file to your conda environemnt folder in the <code>lib\\site-packages\\deeplabcut\\modelzoo\\</code></p>", "<p>But modelzoo is a .py file. So it is not a folder where I can paste it.</p>", "<p>Are you sure you\u2019re on 2.3.1?</p>", "<p>Heya, checking a few days later now and I now have a model zoo folder so I managed to add the file to it. Thank you very much for the help.</p>"], "78270": ["<p>I\u2019m using MacBook Pro, running 3.10.9 Python version, x86_64 architecture,  0.4.17 napari version.<br>\nWhen I call <code>napari.Viewer()</code>, napari icon appears on the docker but no window is opened. It also raises this error:</p>\n<pre><code class=\"lang-auto\">---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 asdf = napari.Viewer()\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/viewer.py:67, in Viewer.__init__(self, title, ndisplay, order, axis_labels, show)\n     63 from .window import Window\n     65 _initialize_plugins()\n---&gt; 67 self._window = Window(self, show=show)\n     68 self._instances.add(self)\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_main_window.py:463, in Window.__init__(self, viewer, show)\n    460 self._unnamed_dockwidget_count = 1\n    462 # Connect the Viewer and create the Main Window\n--&gt; 463 self._qt_window = _QtMainWindow(viewer)\n    465 # connect theme events before collecting plugin-provided themes\n    466 # to ensure icons from the plugins are generated correctly.\n    467 _themes.events.added.connect(self._add_theme)\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_main_window.py:88, in _QtMainWindow.__init__(self, viewer, parent)\n     86 super().__init__(parent)\n     87 self._ev = None\n---&gt; 88 self._qt_viewer = QtViewer(viewer, show_welcome_screen=True)\n     89 self._quit_app = False\n     91 self.setWindowIcon(QIcon(self._window_icon))\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_viewer.py:217, in QtViewer.__init__(self, viewer, show_welcome_screen)\n    214 # This dictionary holds the corresponding vispy visual for each layer\n    215 self.layer_to_visual = {}\n--&gt; 217 self._create_canvas()\n    219 # Stacked widget to provide a welcome page\n    220 self._canvas_overlay = QtWidgetOverlay(self, self.canvas.native)\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_qt/qt_viewer.py:401, in QtViewer._create_canvas(self)\n    399 def _create_canvas(self) -&gt; None:\n    400     \"\"\"Create the canvas and hook up events.\"\"\"\n--&gt; 401     self.canvas = VispyCanvas(\n    402         keys=None,\n    403         vsync=True,\n    404         parent=self,\n    405         size=self.viewer._canvas_size[::-1],\n    406     )\n    407     self.canvas.events.draw.connect(self.dims.enable_play)\n    409     self.canvas.events.mouse_double_click.connect(\n    410         self.on_mouse_double_click\n    411     )\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/napari/_vispy/canvas.py:33, in VispyCanvas.__init__(self, *args, **kwargs)\n     31 self._last_theme_color = None\n     32 self._background_color_override = None\n---&gt; 33 super().__init__(*args, **kwargs)\n     34 self._instances.add(self)\n     36 # Call get_max_texture_sizes() here so that we query OpenGL right\n     37 # now while we know a Canvas exists. Later calls to\n     38 # get_max_texture_sizes() will return the same results because it's\n     39 # using an lru_cache.\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/scene/canvas.py:135, in SceneCanvas.__init__(self, title, size, position, show, autoswap, app, create_native, vsync, resizable, decorate, fullscreen, config, shared, keys, parent, dpi, always_on_top, px_scale, bgcolor)\n    130 # Set to True to enable sending mouse events even when no button is\n    131 # pressed. Disabled by default because it is very expensive. Also\n    132 # private for now because this behavior / API needs more thought.\n    133 self._send_hover_events = False\n--&gt; 135 super(SceneCanvas, self).__init__(\n    136     title, size, position, show, autoswap, app, create_native, vsync,\n    137     resizable, decorate, fullscreen, config, shared, keys, parent, dpi,\n    138     always_on_top, px_scale)\n    139 self.events.mouse_press.connect(self._process_mouse_event)\n    140 self.events.mouse_move.connect(self._process_mouse_event)\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/canvas.py:211, in Canvas.__init__(self, title, size, position, show, autoswap, app, create_native, vsync, resizable, decorate, fullscreen, config, shared, keys, parent, dpi, always_on_top, px_scale, backend_kwargs)\n    209 # Create widget now (always do this *last*, after all err checks)\n    210 if create_native:\n--&gt; 211     self.create_native()\n    213     # Now we're ready to become current\n    214     self.set_current()\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/canvas.py:228, in Canvas.create_native(self)\n    226 assert self._app.native\n    227 # Instantiate the backend with the right class\n--&gt; 228 self._app.backend_module.CanvasBackend(self, **self._backend_kwargs)\n    229 # self._backend = set by BaseCanvasBackend\n    230 self._backend_kwargs = None  # Clean up\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/backends/_qt.py:372, in QtBaseCanvasBackend.__init__(self, vispy_canvas, **kwargs)\n    369 self._initialized = False\n    371 # Init in desktop GL or EGL way\n--&gt; 372 self._init_specific(p, kwargs)\n    373 assert self._initialized\n    375 self.setMouseTracking(True)\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/backends/_qt.py:791, in CanvasBackendDesktop._init_specific(self, p, kwargs)\n    788 def _init_specific(self, p, kwargs):\n    789 \n    790     # Deal with config\n--&gt; 791     glformat = _set_config(p.context.config)\n    792     glformat.setSwapInterval(1 if p.vsync else 0)\n    793     # Deal with context\n\nFile ~/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/vispy/app/backends/_qt.py:283, in _set_config(c)\n    280 glformat.setAlphaBufferSize(c['alpha_size'])\n    281 if QT5_NEW_API or PYSIDE6_API:\n    282     # Qt5 &gt;= 5.4.0 - below options automatically enabled if nonzero.\n--&gt; 283     glformat.setSwapBehavior(glformat.DoubleBuffer if c['double_buffer']\n    284                              else glformat.SingleBuffer)\n    285 elif PYQT6_API:\n    286     glformat.setSwapBehavior(glformat.SwapBehavior.DoubleBuffer if c['double_buffer']\n    287                              else glformat.SwapBehavior.SingleBuffer)\n\nAttributeError: 'PySide6.QtGui.QSurfaceFormat' object has no attribute 'DoubleBuffer'\n</code></pre>\n<p>Any ideas what could be wrong? Missing a package? I\u2019ve been running napari from this environment before. Thanks!</p>", "<p>The issue is you are using PySide6 QT backend.<br>\nnapari does not yet officially support this, so I\u2019m not sure how you ended up in this situation.<br>\nAnyhow, the error you have here is in vispy and has actually been fixed. Likewise a number of Qt6 issues in napari have also recently been fixed, so support will be improved in the next release.</p>\n<p>As you are on x86 macOS, you can try to uninstall PySide6 and install instead pyside2 or pyqt5.</p>", "<p>Thank you, Peter! That was the cause.<br>\nI must have caused this situation when I was playing around cellpose and GPU.</p>"], "78271": ["<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/739ad55b634da7a6da8a91c0aa36df4b518467e0.jpeg\" alt=\"sytox_ImageJ\" data-base62-sha1=\"guGDzbFzBvKcGo0A1bgA6SWpoc0\" width=\"278\" height=\"254\"></p>\n<p>I have stained my tissue with the nuclei dye Sytox green to image cell nuclei (I have also tried DAPI with the same problems as Sytox green). I am trying to measure cell shape (especially interested in circularity and roundness), and because I need it to be fairly exact, drawing the outline around each cell is not possible. I have tried using the measurements plugin in ImageJ, but because the cells are so close together it does not recognize them as different nuclei. I have also tried CellProfiler, but just the example pipes from their website, which has the same problem as ImageJ. I don\u2019t need to be able to recognize every single cell in the image.</p>\n<p>I have tried to find posts on here already that answer this question, but because the cells are so close together, none of them seem exactly applicable. I would really appreciate suggestions of any ways that I might be able to get it to work.</p>\n<p>I have included an image of what I am trying to analyze. Thank you!</p>", "<p><a class=\"mention\" href=\"/u/meghanrb\">@meghanrb</a><br>\n<a href=\"https://imagej.net/imaging/watershed\" rel=\"noopener nofollow ugc\">Watershed seperation</a> looks like it will help with your issue.</p>\n<p>Christian</p>", "<p>If you still have trouble, you may want to try StarDist, it tends to handle fairly circular objects well. Though it can bias towards circularity. CellPose is another option.</p>\n<p>From a quick test on the cellpose website<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d.jpeg\" data-download-href=\"/uploads/short-url/c8lnd9OiGjODcKmTrgE6kcDBFbT.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_690x160.jpeg\" alt=\"image\" data-base62-sha1=\"c8lnd9OiGjODcKmTrgE6kcDBFbT\" width=\"690\" height=\"160\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_690x160.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_1035x240.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_1380x320.jpeg 2x\" data-dominant-color=\"504C4B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7447 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIt would be better if you had a monolayer of cells rather than multiple layers.</p>", "<p>In addition, <a href=\"https://github.com/stardist/stardist\">StarDist</a> should do a pretty good job in such a case.</p>\n<p>EDIT: sorry <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, didn\u2019t see that you had it included already in your post. I just saw the CellPose examples.</p>", "<p>Works really well for most</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e89ee6375099000cb5546f8de5715c3a1d50580.png\" alt=\"image\" data-base62-sha1=\"4m9Q6kIxoK5RbrHJE5ihWvmXzCo\" width=\"278\" height=\"254\"></p>\n<p>Here the parameter recording for the screenshot you posted. If the original is different, that needs adaption.</p>\n<pre><code class=\"lang-auto\">run(\"Command From Macro\", \"command=[de.csbdresden.stardist.StarDist2D], args=['input':'Clipboard', 'modelChoice':'Versatile (fluorescent nuclei)', 'normalizeInput':'true', 'percentileBottom':'1.0', 'percentileTop':'99.8', 'probThresh':'0.85', 'nmsThresh':'0.1', 'outputType':'ROI Manager', 'nTiles':'1', 'excludeBoundary':'2', 'roiPosition':'Automatic', 'verbose':'false', 'showCsbdeepProgress':'false', 'showProbAndDist':'false'], process=[false]\");\n</code></pre>", "<p>Both CellPose and and Stardist seem to be working fairly well, thank you! StarDist seems a bit easier for me at least because it can be run directly in ImageJ instead of Python (I don\u2019t have as much experience in Python as ImageJ).</p>", "<p>This is really great, thank you!</p>", "<p>Thank you for the suggestion! Watershed was okay for this image, but did not work as well for some of the other images I analyzed.</p>", "<p>Try using the software MIPAR. I suppose it can give you decent results.<br>\nBut it is a proprietary software.</p>"], "70085": ["<p>Hello fellow forum members,</p>\n<p>I have been able so far to get omero images as numpy arrays in scripts with either <a href=\"https://docs.openmicroscopy.org/omero/5.4.5/developers/Python.html#raw-data-access\" rel=\"noopener nofollow ugc\">a script like this</a> or with ezomero\u2019s <a href=\"https://thejacksonlaboratory.github.io/ezomero/ezomero.html#ezomero.get_image\" rel=\"noopener nofollow ugc\">get_image function</a>.</p>\n<p>However, if images are very large (still 2D, but having like 10k by 10k px or more), I get memory errors, which is understandable since these are indeed large images.</p>\n<pre><code class=\"lang-auto\">WARNING:omero.gateway:UnknownLocalException on &lt;class 'omero.gateway.OmeroGatewaySafeCallWrapper'&gt; to &lt;7a96ce1b-df68-479c-93ac-e10d3b0d06f2omero.api.RawPixelsStore&gt; getPlane((0, 0, 0), {})\nTraceback (most recent call last):\n  File \"/opt/omero/server/venv3/lib/python3.8/site-packages/omero/gateway/__init__.py\", line 4790, in __call__\n    return self.f(*args, **kwargs)\n  File \"/opt/omero/server/venv3/lib/python3.8/site-packages/omero_api_RawPixelsStore_ice.py\", line 1199, in getPlane\n    return _M_omero.api.RawPixelsStore._op_getPlane.invoke(self, ((z, c, t), _ctx))\nIce.UnknownLocalException: exception ::Ice::UnknownLocalException\n{\n    unknown = ConnectionI.cpp:1573: Ice::MemoryLimitException:\nprotocol error: memory limit exceeded:\nrequested 823353042 bytes, maximum allowed is 256000000 bytes (see Ice.MessageSizeMax)\n</code></pre>\n<p>Still, is it possible to get them in a more python friendly way (maybe as a dask array for example) ?<br>\nOr maybe increase the memory limitation?</p>\n<p>From <a href=\"https://forum.image.sc/t/absolute-limits-for-omero/37765/2\">here</a> and a few other posts, I believe that omero generates a tiled pyramid image for images larger than 3k by 3k, right?</p>\n<p>Best,<br>\nMarcelo</p>", "<p>Hi Marcelo,</p>\n<p>pyramid-level-wise, ezomero has <a href=\"https://thejacksonlaboratory.github.io/ezomero/ezomero.html?highlight=pyramid#ezomero.get_pyramid_levels\" rel=\"noopener nofollow ugc\"><code>get_pyramid_levels</code></a> and an optional argument on <code>get_image</code> for specifying a pyramid level.</p>\n<p>We <em>could</em> have ezomero return a dask array or you could do something like what <a href=\"https://forum.image.sc/t/omero-storage-ezomero-empanada-napari-dask/69471/2\">napari-omero is doing</a>. Unfortunately, I don\u2019t think either would change much for a single large plane, you would still need to load the plane in memory.</p>\n<p>For the future, the solution is \u201cuse Zarr\u201d, of course <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"> I\u2019m trying to think how we could use something like <code>ezomero.get_image</code> with <code>start_coords</code> and <code>axis_lenghts</code> to generate a dask array that could do lazy loading of parts of a single plane. I\u2019m no dask expert, so suggestions are welcome!</p>", "<p>Just reviving this thread hoping <a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a> or <a class=\"mention\" href=\"/u/s.besson\">@s.besson</a> can give any hints how to open large (whole-slide) images from Omero using Python? I presume solutions for opening large 2D images exist, it might just be hard to find online. Thanks!</p>", "<p>As shown in the exception above, trying to load too much pixel data from a single call in OMERO will hit the Ice Memory Limit (currently set to 250MB).  Probably the biggest hint when it comes to opening large 2D images (and this recommendation extends to any large images) is not to load entire planes at once but instead load the data in tiles. This recommendation is not really specific to OMERO and/or Python - see for instance <a href=\"https://bio-formats.readthedocs.io/en/latest/developers/wsi.html\" class=\"inline-onebox\">Working with whole slide images \u2014 Bio-Formats 6.12.0 documentation</a>.</p>\n<p>I\u2019ll leave the ezomero developers to comment but my understanding is that the <a href=\"https://thejacksonlaboratory.github.io/ezomero/ezomero.html#ezomero.get_image\">ezomero.get_image</a> API supports supplying <code>start_coords/axis_lengths </code> as inputs and these will call <code>getTiles</code> rather than <code>getPlane</code> under the hood. Assuming reasonable array dimensions, this might be sufficient to avoid the exception mentioned above.</p>", "<aside class=\"quote group-team\" data-username=\"s.besson\" data-post=\"4\" data-topic=\"70085\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/s.besson/40/8046_2.png\" class=\"avatar\"> S\u00e9bastien Besson:</div>\n<blockquote>\n<p>I\u2019ll leave the ezomero developers to comment but my understanding is that the <a href=\"https://thejacksonlaboratory.github.io/ezomero/ezomero.html#ezomero.get_image\" rel=\"noopener nofollow ugc\">ezomero.get_image</a> API supports supplying <code>start_coords/axis_lengths </code> as inputs and these will call <code>getTiles</code> rather than <code>getPlane</code> under the hood. Assuming reasonable array dimensions, this might be sufficient to avoid the exception mentioned above.</p>\n</blockquote>\n</aside>\n<p>That\u2019s correct - we don\u2019t have any way to e.g. automatically tile an image and return a tile generator or anything fancy like that, but using the optional <code>start_coords</code> and <code>axis_lengths</code> allows for retrieving a subsection of the image without getting the whole plane.</p>\n<p>I\u2019ll say that, in our use cases where this was necessary, our eventual solution was to have a separate Zarr copy of the slides and point to those directly. Depending on how many slides and how many concurrent requests you have for individual tiles, this CAN be a big resource drain on your OMERO server.</p>", "<p>One approach I have considered to this question is to create a <code>dask array</code> for a big tiled image that would lazily load the pixel data from OMERO when you try to access regions of the array.</p>\n<p>This is the approach used in <code>ome-zarr-py</code> when we want to pass a whole Plate as a single lazily-loaded image to <code>napari</code>.<br>\nIt stitches together the \u201ctiles\u201d, where a tile is a single Image from each Well, loaded with <code>getPlane()</code>, into a Plate:</p>\n<p>See <a href=\"https://github.com/ome/ome-zarr-py/blob/c1302e05998dfe2faf94b0f958c92888681f5ffa/ome_zarr/reader.py#L484\" class=\"inline-onebox\">ome-zarr-py/reader.py at c1302e05998dfe2faf94b0f958c92888681f5ffa \u00b7 ome/ome-zarr-py \u00b7 GitHub</a></p>\n<p>But for a single big image, we\u2019d stitch together individual tiles instead.</p>", "<p>Thanks everyone for your inputs on this! It seems that the solution sort of converges to me handling the tiling myself, right?</p>\n<p>I don\u2019t know if it makes sense, but I tried to make a dask block out of it. Also not sure if this is similar to what you have in ome-zarr <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>, I would be happy to use ome-zarr for this if suitable.</p>\n<p>I read somewhere that if a user loads a big tif image in OMERO, OMERO will turn it into a pyramidal structure. So I thought I could use the smallest level as a good tile size candidate and build a dask block with it. Thus, I tried expanding a bit the strategy used by <a href=\"https://github.com/tlambert03/napari-omero/blob/39092539905803abed861111c797ae114f92fba0/src/napari_omero/plugins/loaders.py#L112-L132\" rel=\"noopener nofollow ugc\">napari-omero</a></p>\n<pre><code class=\"lang-auto\"># Get pyramid levels\ntry:\n    levels = ezomero.get_pyramid_levels(conn, ids[0])\nexcept AttributeError:\n    levels = []\n  \nif levels:\n    # Define a tile shape\n    tile_shape = levels[-1]\n\n    nx_tiles, ny_tiles = np.ceil(np.array(levels[0]) / np.array(levels[-1])).astype(int)\n    \n    pixels = image.getPrimaryPixels()\n    \n    dtype = PIXEL_TYPES.get(pixels.getPixelsType().value, None)\n    get_tile = dask.delayed(lambda idx: pixels.getTile(*idx))\n  \n    def get_lazy_tile(zct):\n        return da.from_delayed(get_tile(zct), shape=tuple(tile_shape), dtype=dtype)\n    \n    nt, nc, nz, ny, nx = [getattr(image, f'getSize{x}')() for x in 'TCZYX']\n    # 5D stack: TCZXY\n    t_stacks = []\n    for t in range(nt):\n        c_stacks = []\n        for c in range(nc):\n            z_stack = []\n            for z in range(nz):\n                tile_stack = []\n                for x in range(nx_tiles):\n                    for y in range(ny_tiles):\n                        # list of [ (0,0,0,(x,y,w,h)), (1,0,0,(x,y,w,h)), (2,0,0,(x,y,w,h))... ]\n                        tile_stack.append(get_lazy_tile((z,c,t,(x*tile_shape[0], y*tile_shape[1], tile_shape[0], tile_shape[1])))) # x, y, width, height of tile\n                # Arrange dask block\n                data = [ [] for _ in range(nx_tiles) ]\n                for i, tile in enumerate(tile_stack):\n                    col_idx = i % nx_tiles\n                    data[col_idx].append(tile)\n                z_stack.append(da.block(data))\n            c_stacks.append(da.stack(z_stack))\n        t_stacks.append(da.stack(c_stacks))\n    im = da.stack(t_stacks)\n</code></pre>\n<p>Is this useful or unnecessary? After that, I thing I should have the image as a dask array.</p>\n<p>My goal would be to process this image and save it as zarr back to the server, but I don\u2019t know how to do that. Any ideas or links I should look for?</p>", "<p>This looks great (haven\u2019t tested it, but seems to make sense).</p>\n<p>I don\u2019t know if the smallest level is the best choice for tile_shape.</p>\n<p>You can ask OMERO for a preferred tile-shape for an image, which may be influenced by the way that the data is structured in the file and give better read performance but might not be optimal for viewing, eg. long thin strips instead of squares.</p>\n<p>Probably a good choice is a square of preferred size, e.g. 512 x 512 or 1024 x 1024.</p>\n<p>There\u2019s some code for creating a big tiled image from numpy tiles at <a href=\"https://github.com/ome/omero-py/pull/276\" class=\"inline-onebox\">createImageFromNumpySeq handles big images by will-moore \u00b7 Pull Request #276 \u00b7 ome/omero-py \u00b7 GitHub</a><br>\nbut it didn\u2019t get merged - see description for more info.<br>\nThat would create a new image in OMERO, but it wouldn\u2019t be in Zarr format on disk - it would just be Pixel-data on disk and all the metadata in OMERO, so not downloadable as a Zarr.</p>\n<p>The other option is to write it as a Zarr and then import it, either via the <code>omero import</code> command-line (which uses Bio-Formats to identify the files needed to upload) or if you want a python-only solution, there\u2019s some code at <a href=\"https://gitlab.com/openmicroscopy/incubator/omero-python-importer/-/blob/master/import.py\" class=\"inline-onebox\">import.py \u00b7 master \u00b7 openmicroscopy / incubator / omero-python-importer \u00b7 GitLab</a> which uploads a directory to a \u201cFileset\u201d and then imports it on the server.</p>\n<p>Hope that\u2019s useful?</p>", "<p>Install the Fiji application on your computer as described in this tutorial.<br>\nStart Fiji and go to Help &gt; Update. This starts the updater which looks for plugin updates online.<br>\nIn the ImageJ Updater dialog window, click on Manage Update Sites. Ensure that the boxes for the following plugins are checked:<br>\nJava-8<br>\nBio-Formats<br>\nOMERO 5.4<br>\nClick Close. This will take you back to the ImageJ Updater window.<br>\nClick Apply Changes.<br>\nRestart Fiji.</p>\n<p>Regards,<br>\nDiana</p>"], "78277": ["<p>Hello,<br>\nI am trying to calculate particle size and their number in a stack of images. I am a total beginner and used macro to create a mini code. I would like to have the results of all images in one csv with unique labels. But currently I am facing the problem, that ImageJ adds the outlines of the old images to the new one (s. attached image, yellow outlines and labels), so that images become more and more cluttered. Has someone an idea what to do?</p>\n<p>run(\u201c8-bit\u201d);<br>\nsetAutoThreshold(\u201cDefault dark no-reset\u201d);<br>\n//run(\u201cThreshold\u2026\u201d);<br>\nsetThreshold(114, 128, \u201craw\u201d);<br>\nrun(\u201cAnalyze Particles\u2026\u201d, \u201csize=200-Infinity display exclude include overlay add composite\u201d);<br>\nrun(\u201cFlatten\u201d);</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/hK2f6qOjBG0D1zhK1AfwRrbQuIF.tiff\">GOPR0876_19_17_OD-1.tiff</a> (751.6 KB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/ursu\">@Ursu</a> . We could help you better if you posted an original image. Is your stack a z stack or is it just a sequence of different images? If you have a z stack, you should use either the 3D Objects Counter or use the 3D Manager of 3D Image Suite. Analyze Particles works more for 2D images.</p>"], "78279": ["<p>Hello I have a simple problem that is blocking me. I am a real beginner and looking for help.</p>\n<p>My simple code was supposed to plot profile for each ROI, however it repeat the plot profile of the first ROI for n times.</p>\n<p>I hope that an expert person could easily spot the mistake.</p>\n<p>t=getTitle;<br>\nnumROIs = roiManager(\u201ccount\u201d);</p>\n<p>for(i=0; i&lt;numROIs;i++) // loop through ROIs<br>\n{<br>\nroiManager(\u201cSelect\u201d, i);<br>\nselectWindow(t);<br>\nrun(\u201cPlot Profile\u201d);<br>\nrun(\u201cFind Peaks\u201d, \u201cmin._peak_amplitude=20 min._peak_distance=10 min._value=<span class=\"chcklst-box fa fa-square-o fa-fw\"></span> max._value=<span class=\"chcklst-box fa fa-square-o fa-fw\"></span>\u201d);<br>\n}</p>", "<p>Hi Giulia,</p>\n<p>The issue is that you are selecting a ROI without first selecting the window, so Fiji selects the window and only sees a ROI that does not change.</p>\n<p>Do this:</p>\n<pre><code class=\"lang-auto\">t=getTitle;\nnumROIs = roiManager(\"count\");\n\nfor(i=0; i&lt;numROIs;i++) // loop through ROIs\n{\n\tselectWindow(t);\n\troiManager(\"Select\", i);\n\trun(\"Plot Profile\");\n}\n</code></pre>\n<p>Where did you find \u201cFind Peaks\u201d?  I don\u2019t have it on my Fiji?</p>\n<p>Sincerely,</p>\n<p>Matthieu</p>", "<p>Dear Matthieu,</p>\n<p>It worked smoothly. Thanks for the help.<br>\n\u201cFind Peaks\u201d is available in the BAR plugin.</p>\n<p>Thanks,<br>\nGiulia</p>"], "78280": ["<p>Hi,</p>\n<p>I\u2019m a beginner to image analysis, and have been working through Pete Bankhead\u2019s videos as well as parsing through various topics here in the community which have been really helpful . However, I\u2019ve now run into a problem where I\u2019m having difficulty trying to figure out how qupath is detecting cells. It seems that it is \u2018counting\u2019 cells below the intensity threshold set, and I\u2019m a bit at a loss as to how to get around it, as increasing the intensity threshold causes false negatives.</p>\n<p>I\u2019m using a Mac, running Qupath Version: 0.4.3</p>\n<p>I attach in my next post a screen shot from my visual stain editor, which I run before I run cell detection, as well as the cell detection screenshot and the false positive detection highlighted. Any helpful suggestions would be much appreciated!</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906.jpeg\" data-download-href=\"/uploads/short-url/iHsjTdGIhHFDXFXeazSugLMs9zE.jpeg?dl=1\" title=\"Screenshot 2023-03-08 at 14.45.50\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_690x448.jpeg\" alt=\"Screenshot 2023-03-08 at 14.45.50\" data-base62-sha1=\"iHsjTdGIhHFDXFXeazSugLMs9zE\" width=\"690\" height=\"448\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_690x448.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_1035x672.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/3/8310b250f168e11d31ffbce505a01366d110d906_2_1380x896.jpeg 2x\" data-dominant-color=\"E3E1E0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-08 at 14.45.50</span><span class=\"informations\">1920\u00d71247 218 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a.jpeg\" data-download-href=\"/uploads/short-url/aem0YdPwHy8V4ngsTzJll7nR47M.jpeg?dl=1\" title=\"Screenshot 2023-03-08 at 14.57.07\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_690x448.jpeg\" alt=\"Screenshot 2023-03-08 at 14.57.07\" data-base62-sha1=\"aem0YdPwHy8V4ngsTzJll7nR47M\" width=\"690\" height=\"448\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_690x448.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_1035x672.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b526c7aad86c4f64731e897f37a9eab24fcd9a_2_1380x896.jpeg 2x\" data-dominant-color=\"DFD5D4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-08 at 14.57.07</span><span class=\"informations\">1920\u00d71247 305 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>It looks like you are looking at the HTX OD sum on the left, and using the full Optical Density Sum, not HTX, on the right.</p>\n<p>To answer the bigger question though, at some point you have to accept that there is a middle ground between false negatives and false positives, and figure out some other method like classification or additional measurements to deal with errors.</p>", "<p>I assume you mean that the Haematoxylin mean for the cell you have highlighted is 0.0987 which is less than the threshold of 0.2 you have set.</p>\n<p>If you look on your cell detection parameters you are using the OD sum image, not the Haematoxylin OD image. The OD sum is the total of haematoxylin OD plus DAB OD plus residual OD so will probably be above your detection threshold.</p>", "<p>Right, I see.</p>\n<p>Thanks for pointing that out! That makes perfect sense now.</p>\n<p>And yes, <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, I do take your point that additional measurements may be necessary.</p>\n<p>Thanks again both for your prompt responses!</p>"], "78284": ["<p>Basically just the title! I am classifying objects using CPA Classifer, and for this manual training step it would be helpful to be able to adjust the colour balance of loaded object tiles such that each of the 4 channels being displayed here are nice and balanced, and background can be removed for viewing purposes. The view options only appear to let you adjust overall brightness of all channels together</p>\n<p>Using the log contrast stretch option helps to balance channels to some extent (in my case), but background in the images still makes manual viewing and classification fairly difficult. Previously I have run pipelines producing background-subtracted and/or intensity-normalised images which came up quite nicely through CPA, but the images here are just the raw images.</p>\n<p>Or would the only way be to have saved adjusted images/thumbnails separately and direct CPA to look at those? Thanks very much!</p>"], "78285": ["<p>Hi everyone,</p>\n<p>I would like to quantify how many lysosomes, within my total population of them, have or not have my protein of interest in their lumen. I think I should create a mask of the lysosomes (here in cyan) and then detect if there is signal (in the image in magenta).<br>\nI appreciate your help! Thank you</p>\n<p>Francesca</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/141e48611c04ba77e0fd75c2bc4bbc1428cf832e.png\" alt=\"Screen Shot 2023-03-08 at 10.51.37 AM\" data-base62-sha1=\"2RYqEohTgTz98IsjruYHUIABDae\" width=\"456\" height=\"274\"></p>", "<p>You could start by thresholding and making a mask of your lysosomes, do the same with your other channel, then do an AND operation with Process &gt; Image calculator. The resulting image will only contain objects that have both cyan and magenta signal. Then you can use Analyze Particles to count the objects in the resulting image.</p>", "<p>Thank you Mary! Should I do this on the sum projection? And splitting the channel first?</p>", "<p>I would first try the raw image data or on background-subtracted image data and you\u2019ll definitely need to split the channels first. I should have said so in my previous post. Please let me know how it goes.</p>", "<p>Thank you ! Should I use the &gt;Convert to mask, right ?</p>", "<p>You could use that or Image &gt; Adjust &gt; Threshold &gt; set a threshold with the Auto button &gt; click Apply. The latter also creates a mask after you\u2019ve set a threshold.</p>"], "78291": ["<p>Hello,</p>\n<p>I have a network that was training on over 2,000 images of 3 individuals. Unfortunately, three of the many videos were taken of just 2 individuals and when I try to analyze them, selecting [2] as the number of animals in the video, I get the following:</p>\n<blockquote>\n<p>C:\\Users\\vamoro1.conda\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\stitch.py:687: UserWarning: No optimal solution found. Employing black magic\u2026<br>\nwarnings.warn(\u201cNo optimal solution found. Employing black magic\u2026\u201d)</p>\n</blockquote>\n<p>The .h5 file is sparsely populated and the resulting video has no tracklets identified.</p>\n<p>My understanding from the paper and <a href=\"https://deeplabcut.github.io/DeepLabCut/docs/maDLC_UserGuide.html\" rel=\"noopener nofollow ugc\">Userguide</a> is this should be fine as it states:</p>\n<blockquote>\n<p>\u201cNote, once trained if you have a video with more or less animals, that is fine - you can have more or less animals during video analysis!\u201d</p>\n</blockquote>\n<p>Any help would be greatly appreciated as I\u2019d hate to train a new network for just a handful of videos.</p>\n<p>Thank you!</p>", "<p>So, I\u2019ve been able to get get around the [No optimal solution found] problem and the large gaps in tracking data to be filled by manually setting</p>\n<blockquote>\n<p>topktoretain: 2</p>\n</blockquote>\n<p>in inference_cfg.yaml. Which makes me question even more if the values in the GUI map actually to the underlying codebase at runtime.</p>\n<p>I still can\u2019t get a video created with the tracklets shown, though.</p>\n<p>If I open the .h5 in the tracklet GUI I can see that the network identified both animals flawlessly.</p>"], "78293": ["<p>How to quantify collagen staining/ trichome staining in Qupath?<br>\nHow to set the threshold?</p>", "<p>The easiest way, I would think, is to train a <a href=\"https://qupath.readthedocs.io/en/latest/docs/tutorials/pixel_classification.html\" rel=\"noopener nofollow ugc\">pixel classifier</a>. Set the image type as brightfield (other) then set the channel colour vectors for your nuclear stain, the picric acid and the red dye. Select areas in the image that represent good examples of the collagen and cytoplasm and the pixel classifier should be able to do the rest\u2026<br>\nThe documentation is quite thorough with videos to help you. Good luck! <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Thank you, Chris. That was very helpful</p>"], "78296": ["<p>Hi, I have 2 RTX 4090 setted up, v531 driver, 11.8 Cuda, and installed DLC using the conda yaml file. No<br>\nerror showing when training, however GPU was not used during training.</p>\n<p>code after training started:<br>\nStarting training\u2026<br>\n2023-03-09 02:00:38.669048: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100<br>\n2023-03-09 02:00:40.508583: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</p>\n<p>Any help will be greatly appreciated!!!</p>", "<p>Hey, I also have a very similar problem. Does your GPU memmory usage increase during the training? What is the size of your images?</p>", "<p>These are not errors, just logs. Is the training going?</p>"], "78298": ["<p>Hello everyone,</p>\n<p>I am working on analysis in QuPath for tumor/immune microenvironment study. I have recently been  phenotyping via scripting of a hierarchy using a simple threshold classifier. For example:</p>\n<pre><code class=\"lang-groovy\">import static qupath.lib.gui.scripting.QPEx.*\nimport qupath.lib.objects.PathObjects\nimport qupath.lib.objects.classes.PathClassFactory\nimport qupath.lib.objects.classes.PathClassTools\n\n*// set marker expression location*\n\n*measurement1 = \"CD4: Cell: Mean\" //CD4*\n\n*// CD4*\n*CD4Pos = getPathClass(\"CD4+\")*\n*CD4Neg = getPathClass(\"CD4-\")*\n\n*//Layer 2: classify CD4+/- in CD3e+*\n*selectObjects { p -&gt; p.getPathClass() == getPathClass(\"CD3e+\")}*\n*for (detection in getSelectedObjects()) {*\n*    m1 = measurement(detection, measurement1) *\n*    if ( m1 &gt;  meas01)*\n*        detection.setPathClass(CD4Pos)   *\n*  else *\n*  detection.setPathClass(CD4Neg)             *\n*}*\n*fireHierarchyUpdate()*\n</code></pre>\n<p>That\u2019s just a small portion of the total hierarchy, but hopefully you get the idea. Is it possible to put together a phenotyping hierarchy like this using a trained classifier (e.g. artificial neural network trained) instead of the simple threshold/binary method? I\u2019ve searched the forums but couldn\u2019t find anything about this specifically. Maybe I missed it!</p>\n<p>Thank you so much,</p>\n<p>Molly</p>", "<p>Hey <a class=\"mention\" href=\"/u/meethm\">@meethm</a>,<br>\nIt sounds like what you\u2019re looking for is a composite classifier? <a href=\"https://qupath.readthedocs.io/en/stable/docs/tutorials/multiplex_analysis.html#combine-the-classifiers\" class=\"inline-onebox\">Multiplexed analysis \u2014 QuPath 0.4.3 documentation</a></p>\n<p>This will do every combination of classifiers in the order you choose. It can become a bit overwhelming. To simplify the interpretation, I recommend not labeling the negatives of each class. When training, mark them as \u201cIgnore*\u201d. That way, there\u2019s simply fewer words to read and you\u2019re less likely to make mistakes.</p>", "<p>Hi Sara,</p>\n<p>Thank you for your response! I used the composite classifier before I started scripting. The problem with the composite classifier is that I get a bunch of junk in the output that I don\u2019t want to analyze. A simple example would be CD3e-CD8+, which isn\u2019t something I want to carry forward into neighborhood analysis because it\u2019s not a true CD8 T cell since it\u2019s missing the CD3e marker. With the hierarchical phenotyping, I can specify that that cell is phenotyped as \u201cOther.\u201d You can imagine that with 30 markers and numerous classifiers for one image that the amount of junk produced by applying the classifiers sequentially is astounding. The hierarchical phenotyping helps me avoid that problem, but I want to combine it with an object classifier instead of the single measurement classifier since I think the object classifiers can be a lot more accurate.</p>\n<p>I hope that makes sense! Maybe there\u2019s something about applying sequential classifiers that I don\u2019t know too.</p>", "<p>Hi <a class=\"mention\" href=\"/u/meethm\">@meethm</a> I edited your post to format the script as code so it is easier to read (you can do it with the <code>&lt;/&gt;</code> button when posting).</p>\n<p>There are some changes in v0.4.x that would make creating such a script easier \u2013 and you can avoid a lot of the <code>getPathClass()</code> stuff (even if it\u2019s still used internally). There\u2019s an example at <a href=\"https://github.com/qupath/qupath/pull/1094\" class=\"inline-onebox\">Improve working with measurements and classifications by petebankhead \u00b7 Pull Request #1094 \u00b7 qupath/qupath \u00b7 GitHub</a></p>\n<p>With the \u2018new\u2019 method you could identify all the cells with a particular classification like this:</p>\n<pre><code class=\"lang-groovy\">def cells = getCellObjects()\ndef cd8Pos = cells.findAll(cell -&gt; 'CD8' in cell.classifications)\n\nprintln \"${cd8Pos.size()}/${cells.size()} are CD8+ve\"\n</code></pre>\n<p>That sound find cells with the classification \u2018CD8\u2019, \u2018CD3: CD8\u2019, \u2018CD8: CD3\u2019\u2026 basically any set of classifications where \u2018CD8\u2019 is included. But it <em>wouldn\u2019t</em> get something like \u2018CD8+ve\u2019 because that\u2019s a whole other thing from QuPath\u2019s perspective.</p>\n<p>Do be a bit cautious though, and try to check it is behaving as you expect. It\u2019s a very new feature and as such there isn\u2019t really much documentation and I haven\u2019t explored its possibilities fully myself.</p>\n<p>For example, as a check you could add this to the end of the script above to count the actual classifications you\u2019ve got, all of which should contain \u2018CD8\u2019 somewhere:</p>\n<pre><code class=\"lang-groovy\">def countMap = cd8Pos.countBy(p -&gt; p.getPathClass())\nfor (def entry : countMap.entrySet()) {\n    println \"${entry.key} -&gt; ${entry.value}\"\n}\n</code></pre>", "<p>Ah, I understand. Well, fundamentally, QuPath doesn\u2019t know biology, so if you\u2019re going to have complicated biology-based rules, it\u2019s going to be a complicated script. I\u2019m going to make up some markers in the text below for clarity, hopefully it translates well enough to what you\u2019re doing.<br>\n[Tangent: it\u2019s worth mentioning that looking at unexpected classifications can yield interesting results, both in terms of finding out there are mistakes in your trained classifiers and in finding undiscovered cell types].</p>\n<p>How I would approach this is something like this:</p>\n<ol>\n<li>Run each of your major cell type classifiers (T cells, tumor, other immune, etc) individually. Use a line like<br>\n<code>def CD3cells=getDetectionObjects().findAll{'CD3' in it.classifications}</code> to record in a groovy object which are your T cells. Repeat for each major type.</li>\n<li>Generate a composite classifier for the cell subsets that can be multiple types (eg, make a single CD4/CD8/PD1 classifier; each T cell will eventually be some combination of all of these). Run the composite classifier. For each T cell, grab the subclassification into a list<br>\n<code>def CD3subclasses=CD3cells.collect{it.classifications}</code>\n</li>\n<li>Repeat steps 1 and 2 for each major cell type</li>\n<li>After you\u2019ve grabbed all the subclasses for each major type into an object, you can reset their classes to whatever you have saved as a last step:</li>\n</ol>\n<pre><code class=\"lang-auto\">CD3cells.eachWithIndex{cell,idx-&gt;\n    it.classifications = CD3subclasses[idx]\n    it.classifications += 'CD3'\n}\n</code></pre>\n<p>Problems with this method will arise when you inevitably have some combinations of major types that shouldn\u2019t exist (ie CD3+PanCK+ cells). You\u2019ll have to decide how you want to handle those instances (delete the troublesome cells? Have one phenotype always win?)</p>", "<aside class=\"quote no-group\" data-username=\"smcardle\" data-post=\"5\" data-topic=\"78298\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/smcardle/40/17938_2.png\" class=\"avatar\"> Sara McArdle:</div>\n<blockquote>\n<p>[Tangent: it\u2019s worth mentioning that looking at unexpected classifications can yield interesting results, both in terms of finding out there are mistakes in your trained classifiers and in finding undiscovered cell types].</p>\n</blockquote>\n</aside>\n<p>Yep.</p>\n<p>Related discussion here and some links. <a href=\"https://forum.image.sc/t/simplify-subcategories-by-hierarchical-object-classification/77554/2\" class=\"inline-onebox\">Simplify subcategories by hierarchical object classification - #2 by Mike_Nelson</a></p>", "<p>Thank you, everyone, for your input!</p>\n<p>Perhaps the word \u201cjunk\u201d was the wrong word. Certainly the composite classifiers can be useful for rare cell discovery and more. I found evidence of a rare cell type in our data set when I was using the composite classifiers earlier on. A better word might be \u201cnoise.\u201d There\u2019s only so much time in a work week, and rare cell discovery isn\u2019t the aim of our current study. Hence my need for more targeted phenotyping.</p>\n<p>I will try out these different methods and see what works. Thank you again, everyone!</p>"], "78300": ["<p>Hi,</p>\n<p>I am using a qupath (0.4.3) script (<a href=\"https://github.com/estellenassar/ABBA-QuPath-utility-scripts/blob/0e8c4a42e99efea0ffe57bee8b610e9ba14226e3/cells_in_regions.groovy\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ABBA-QuPath-utility-scripts/cells_in_regions.groovy at 0e8c4a42e99efea0ffe57bee8b610e9ba14226e3 \u00b7 estellenassar/ABBA-QuPath-utility-scripts \u00b7 GitHub</a>) to extract nuclei in brain regions annotations imported from ABBA and get the following error:</p>\n<p>Any help would be greatly appreciated!</p>\n<pre><code class=\"lang-auto\">INFO: 1 region detected (processing time: 0.04 seconds)\nINFO: Processing complete in 0.13 seconds\nINFO: Tasks completed!\nWARN: Unable to set parameter detectionImage with value NeuN (Opal 690)\nWARN: Unable to set parameter thresholdCompartment with value Cell: DAPI mean\nINFO: 97 nuclei detected (processing time: 3.71 seconds)\nINFO: 2335 nuclei detected (processing time: 11.89 seconds)\nINFO: 1184 nuclei detected (processing time: 12.41 seconds)\nINFO: 1286 nuclei detected (processing time: 12.51 seconds)\nINFO: 3614 nuclei detected (processing time: 12.61 seconds)\nINFO: 4059 nuclei detected (processing time: 12.69 seconds)\nINFO: 3480 nuclei detected (processing time: 13.25 seconds)\nINFO: 4509 nuclei detected (processing time: 13.28 seconds)\nINFO: 4391 nuclei detected (processing time: 13.49 seconds)\nINFO: 4273 nuclei detected (processing time: 13.64 seconds)\nINFO: 4062 nuclei detected (processing time: 13.82 seconds)\nINFO: 1296 nuclei detected (processing time: 14.11 seconds)\nINFO: 4327 nuclei detected (processing time: 15.02 seconds)\nINFO: 3703 nuclei detected (processing time: 15.09 seconds)\nINFO: 1109 nuclei detected (processing time: 15.83 seconds)\nINFO: 3928 nuclei detected (processing time: 15.92 seconds)\nINFO: 2911 nuclei detected (processing time: 12.53 seconds)\nINFO: 1176 nuclei detected (processing time: 16.42 seconds)\nINFO: 4059 nuclei detected (processing time: 16.44 seconds)\nINFO: 144 nuclei detected (processing time: 4.60 seconds)\nINFO: 3209 nuclei detected (processing time: 16.57 seconds)\nINFO: 3791 nuclei detected (processing time: 17.46 seconds)\nINFO: 1139 nuclei detected (processing time: 18.57 seconds)\nINFO: 864 nuclei detected (processing time: 18.63 seconds)\nINFO: 1439 nuclei detected (processing time: 19.96 seconds)\nINFO: 1718 nuclei detected (processing time: 6.98 seconds)\nINFO: 3547 nuclei detected (processing time: 8.42 seconds)\nINFO: 3658 nuclei detected (processing time: 8.55 seconds)\nINFO: 2154 nuclei detected (processing time: 9.05 seconds)\nINFO: 1945 nuclei detected (processing time: 10.13 seconds)\nINFO: Processing complete in 24.73 seconds\nINFO: Tasks completed!\nINFO: Loading 99 Allen Regions for TEL15_1_out_of_3.czi - Scene #01\nWARN: Unable to set parameter detection[Channel 4] with value 0.15\nWARN: Unable to set parameter detection[Channel 5] with value 0.2\nWARN: Unable to set parameter detection[Channel 6] with value -1.0\nWARN: Unable to set parameter detection[Channel 7] with value -1.0\nINFO: Processing complete in 27.73 seconds\nINFO: Tasks completed!\nERROR: com/google/gson/RuntimeTypeAdapterFactory\njava.lang.NoClassDefFoundError: com/google/gson/RuntimeTypeAdapterFactory\n    at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.getRealTransformAdapter(RealTransformDeSerializer.java:24)\n    at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.deserialize(RealTransformDeSerializer.java:49)\n    at ch.epfl.biop.qupath.transform.Warpy.getRealTransform(Warpy.java:362)\n    at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n    at QuPathScript.run(QuPathScript:66)\n    at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)\n    at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)\n    at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)\n    at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)\n    at qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    at java.base/java.lang.Thread.run(Unknown Source)\n  Caused by com.google.gson.RuntimeTypeAdapterFactory        at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.getRealTransformAdapter(RealTransformDeSerializer.java:24)\n        at ch.epfl.biop.qupath.transform.RealTransformDeSerializer.deserialize(RealTransformDeSerializer.java:49)\n        at ch.epfl.biop.qupath.transform.Warpy.getRealTransform(Warpy.java:362)\n        at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n        at QuPathScript.run(QuPathScript:66)\n        at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)\n        at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)\n        at qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)\n        at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)\n        at qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n        at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.base/java.lang.Thread.run(Unknown Source)\n</code></pre>\n<p>Script:</p>\n<pre><code class=\"lang-auto\">import static qupath.lib.gui.scripting.QPEx.* // For intellij editor autocompletion\nimport static ch.epfl.biop.qupath.atlas.allen.api.AtlasTools.*\n\nimport qupath.lib.objects.PathObjects\nimport qupath.lib.roi.ROIs\nimport qupath.lib.regions.ImagePlane\nimport qupath.lib.measurements.MeasurementList\nimport qupath.lib.objects.PathCellObject\n\nimport ch.epfl.biop.qupath.transform.*\nimport net.imglib2.RealPoint\n\nuseSmallArea = false;\nclearAllObjects();\n\n// create and select rectangle (code from https://qupath.readthedocs.io/en/stable/docs/scripting/overview.html#creating-rois)\n\nif (useSmallArea) {\n    int z = 0\n    int t = 0\n    def plane = ImagePlane.getPlane(z, t)\n    def roi = ROIs.createRectangleROI(7000, 8000, 100, 100, plane)\n    def annotation = PathObjects.createAnnotationObject(roi)\n    addObject(annotation)\n} else {\n    runPlugin('qupath.imagej.detect.tissue.SimpleTissueDetection2', '{\"threshold\": 1,  \"requestedPixelSizeMicrons\": 20.0,  \"minAreaMicrons\": 10000.0,  \"maxHoleAreaMicrons\": 1000000.0,  \"darkBackground\": true,  \"smoothImage\": false,  \"medianCleanup\": false,  \"dilateBoundaries\": false,  \"smoothCoordinates\": true,  \"excludeOnBoundary\": false,  \"singleAnnotation\": true}');\n}\n\nselectAnnotations();\n\n// run Positive Cell Detection\nrunPlugin('qupath.imagej.detect.cells.PositiveCellDetection', '{\"detectionImage\": \"NeuN (Opal 690)\",  \"requestedPixelSizeMicrons\": 0.5,  \"backgroundRadiusMicrons\": 35.0,  \"medianRadiusMicrons\": 1.5,  \"sigmaMicrons\": 2.0,  \"minAreaMicrons\": 40.0,  \"maxAreaMicrons\": 600.0,  \"threshold\": 0.5,  \"watershedPostProcess\": true,  \"cellExpansionMicrons\": 2.0,  \"includeNuclei\": false,  \"smoothBoundaries\": true,  \"makeMeasurements\": true,  \"thresholdCompartment\": \"Cell: DAPI mean\", \"thresholdPositive1\": 0.5,  \"thresholdPositive2\": 0.4,  \"thresholdPositive3\": 0.6,  \"singleThreshold\": true}');\n\n// select and delete rectangle (already selected but just in case)\nselectAnnotations();\nclearSelectedObjects();\n\n// load warped Allen regions\ndef imageData = getCurrentImageData();\ndef splitLeftRight = false; // 3.8.23 changed to false as true gives error\nloadWarpedAtlasAnnotations(imageData, splitLeftRight);\n\n// select all cells and insert them into hierarchy\n//clearSelectedObjects();\nselectCells();\n\ndef selectedObjects = getCurrentImageData().getHierarchy().getSelectionModel().getSelectedObjects();\ninsertObjects(selectedObjects);\n\n// run Subcellular Spot Detection\nrunPlugin('qupath.imagej.detect.cells.SubcellularDetection', '{\"detection[Channel 1]\": -1.0,  \"detection[Channel 2]\": 0.4,  \"detection[Channel 3]\": 0.3,  \"detection[Channel 4]\": 0.15,  \"detection[Channel 5]\": 0.2,  \"detection[Channel 6]\": -1.0,  \"detection[Channel 7]\": -1.0,  \"doSmoothing\": false,  \"splitByIntensity\": true,  \"splitByShape\": true,  \"spotSizeMicrons\": 0.5,  \"minSpotSizeMicrons\": 0.2,  \"maxSpotSizeMicrons\": 7.0,  \"includeClusters\": false}');\n\n// https://github.com/BIOP/qupath-biop-extensions/blob/d6eb0aec6766bd5b1f8aadd5d73a725fe25d77d9/src/test/resources/abba_scripts/importABBAResults.groovy\n// Get ABBA transform file located in entry path +\ndef targetEntry = getProjectEntry()\ndef targetEntryPath = targetEntry.getEntryPath();\n\ndef fTransform = new File (targetEntryPath.toString(),\"ABBA-Transform.json\")\n\nif (!fTransform.exists()) {\n    System.err.println(\"ABBA transformation file not found for entry \"+targetEntry);\n    return ;\n}\n\ndef pixelToCCFTransform = Warpy.getRealTransform(fTransform).inverse(); // Needs the inverse transform\n\ngetDetectionObjects().forEach(detection -&gt; {\n    RealPoint ccfCoordinates = new RealPoint(3);\n    MeasurementList ml = detection.getMeasurementList();\n    ccfCoordinates.setPosition([detection.getROI().getCentroidX(),detection.getROI().getCentroidY(),0] as double[]);\n    pixelToCCFTransform.apply(ccfCoordinates, ccfCoordinates);\n    ml.addMeasurement(\"Allen CCFv3 X mm\", ccfCoordinates.getDoublePosition(0) )\n    ml.addMeasurement(\"Allen CCFv3 Y mm\", ccfCoordinates.getDoublePosition(1) )\n    ml.addMeasurement(\"Allen CCFv3 Z mm\", ccfCoordinates.getDoublePosition(2) )\n})\n\n// change cell name to replace name with unique ID number\ncounter = 1\n\nselectDetections()\ndetections = getSelectedObjects()\n\nfor (detection in detections) {\n    if (detection.class.equals(PathCellObject.class)) {\n        detection.setName(counter.toString());\n        ++counter\n    } else {\n        detection.setName('');\n    }\n}\n\n\n// save annotations\nFile directory = new File(buildFilePath(PROJECT_BASE_DIR,'export2'));\ndirectory.mkdirs();\nimageName = ServerTools.getDisplayableImageName(imageData.getServer())\nsaveAnnotationMeasurements(buildFilePath(directory.toString(),imageName+'__annotations.tsv'));\nsaveDetectionMeasurements(buildFilePath(directory.toString(),imageName+'__detections.tsv'));\n</code></pre>", "<p>This repository is unfortunately not up to date anymore, please use this remote repository:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/NicoKiaru/ABBA-QuPath-utility-scripts\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/NicoKiaru/ABBA-QuPath-utility-scripts\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94cadeefb7f47d6e04d3ce970eb0c53a49283e32.png 2x\" data-dominant-color=\"F0EDEA\"></div>\n\n<h3><a href=\"https://github.com/NicoKiaru/ABBA-QuPath-utility-scripts\" target=\"_blank\" rel=\"noopener\">GitHub - NicoKiaru/ABBA-QuPath-utility-scripts: A collection of scripts for...</a></h3>\n\n  <p>A collection of scripts for ABBA, Fiji and QuPath. - GitHub - NicoKiaru/ABBA-QuPath-utility-scripts: A collection of scripts for ABBA, Fiji and QuPath.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>There\u2019s a PR opened here: <a href=\"https://github.com/nickdelgrosso/ABBA-QuPath-utility-scripts/pull/1\" class=\"inline-onebox\">Update ABBA scripts by NicoKiaru \u00b7 Pull Request #1 \u00b7 nickdelgrosso/ABBA-QuPath-utility-scripts \u00b7 GitHub</a></p>\n<p>I did not test it extensively, so I hope it works, but can\u2019t be sure.</p>\n<p>Regarding the error, can you show which extensions are installed in your QuPath ? It looks like you may be missing<a href=\"https://github.com/BIOP/qupath-extension-warpy\"> Warpy</a>, which is a requirement for the ABBA qupath plugin.</p>", "<p>Thanks Nicolas, using your repo worked!</p>"], "57821": ["<p>Hello,<br>\nI have noticed the paper called \u201cGPU-accelerated CellProfiler\u201d from Imen Chakroun, Nick Michiels and Roel Wuyts, 2018. The authors made the \u201cMeasureTexture\u201d module and \u201cMeasureObjectSizeShape\u201d module run on GPU and thus improved the performance 7.5 times.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://ieeexplore.ieee.org/document/8621271\">\n  <header class=\"source\">\n\n      <a href=\"https://ieeexplore.ieee.org/document/8621271\" target=\"_blank\" rel=\"noopener nofollow ugc\">ieeexplore.ieee.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/7/97b982ab95d326d8b25ac873d7262ba98123166d.png\" class=\"thumbnail onebox-avatar\" width=\"200\" height=\"200\">\n\n<h3><a href=\"https://ieeexplore.ieee.org/document/8621271\" target=\"_blank\" rel=\"noopener nofollow ugc\">GPU-accelerated CellProfiler</a></h3>\n\n  <p>CellProfiler excels at bridging the gap between advanced image analysis algorithms and scientists who lack computational expertise. It lacks however high performance capabilities needed for High Throughput Imaging experiments where workloads reach...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I have seen the threads from 2017 saying that GPUs are not employed. Is it still the case ?<br>\nAre any solutions of GPU-computing out there? And how hard would it be to implement it?</p>\n<p>Thanks a lot,<br>\nViktor</p>", "<p>I can not believe that a paper was published on this but there was no effort to push the changes upstream into the official CP code?</p>", "<p>The primary problem here is that, while every user will have a CPU, not every user will have a GPU. Even then only specific GPU types are supported for some workflows. Python dependencies specific to the hardware setup often need to be installed. This makes it difficult to produce a CellProfiler build which will work across a broad variety of system configurations</p>\n<p>Furthermore, CellProfiler\u2019s current approach to multiprocessing involves running multiple instances of the program concurrently, which doesn\u2019t always work when sharing a single GPU between them.</p>\n<p>To my knowledge we\u2019re yet to recieve a PR which would integrate GPU processing, but if the above challenges can be handled it\u2019d be more than welcome. As it stands I included some support for GPU processing in the new RunCellpose and RunStarDist plugins, since these run on CellProfiler built from source (enabling the user to configure GPU-specific dependencies for their machine).</p>\n<p>With regards to MeasureTexture and MeasureObjectSizeShape, substantial revisions were made to these modules in CellProfiler 4 in order to resolve performance issues. We managed to make some of the texture measurements 100-fold faster by optimising the existing code (instead of running the older, inefficient code on a GPU). Hopefully this relieves some of the issues for the time being.</p>", "<p>I see, thank you for the info. I am glad to hear that the pipelines I am using now are already optimized even without the use of GPU.</p>", "<p>How can I download this particular version of cellprofiler</p>"], "78304": ["<p>Analyzing a video that just worked 30 minutes ago on DLC 2.3.0.</p>\n<p>Now encountering the following error on analyzing that results in the GUI and DLC crashing:</p>\n<p>2023-03-08 13:59:44.949135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13404 MB memory:  \u2192 device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9<br>\n2023-03-08 13:59:44.971871: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled<br>\n0%|                                                                                         | 0/4501 [00:00&lt;?, ?it/s]2023-03-08 13:59:46.652324: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401<br>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4501/4501 [01:08&lt;00:00, 66.05it/s]<br>\n*** Received signal 11 ***<br>\n*** BEGIN STACK TRACE POINTERS ***<br>\n0x00007ffaec2fd08f<br>\n0x00007ffaec2fd277<br>\n0x00007ffbdd684090<br>\n0x00007ff6a8c327ec<br>\n0x00007ffbd031e390<br>\n0x00007ffbdfe13eaf<br>\n0x00007ffbdfd8eae6<br>\n0x00007ffbdfe12e9e<br>\n0x00007ffb390c66c8<br>\n0x00007ffb2b7110ee<br>\n0x00007ffb2b71008f<br>\n0x00007ffb2bd03e98<br>\n0x00007ffb390831b5<br>\n0x00007ffb390859ee<br>\n0x00007ffb389f0b3f<br>\n0x00007ffb391dcf40<br>\n0x00007ffb389f0b19<br>\n0x00007ffb3908888f<br>\n0x00007ffb39080fcd<br>\n0x00007ffb2bd00e15<br>\n0x00007ffb76986d82<br>\n0x00007ffb76a203f6<br>\n0x00007ffb76a28d88<br>\n0x00007ffb76a25169<br>\n0x00007ffb76940695<br>\n0x00007ffb769409d8<br>\n0x00007ffb76a203f6<br>\n0x00007ffb76a28d88<br>\n0x00007ffb76a22df8<br>\n0x00007ffb76a2798a<br>\n0x00007ffb76a217f4<br>\n0x00007ffb76a1ca4f<br>\n0x00007ffb76986bf5<br>\n0x00007ffb76a203f6<br>\n0x00007ffb76a28d88<br>\n0x00007ffb76a22df8<br>\n0x00007ffb76a2798a<br>\n0x00007ffb76940aaf<br>\n0x00007ffb76a203f6<br>\n0x00007ffb76a28d88<br>\n0x00007ffb76a22df8<br>\n0x00007ffb76a2798a<br>\n0x00007ffb76940aaf<br>\n0x00007ffb769403a6<br>\n0x00007ffb768a9d3c<br>\n0x00007ffb768aaeaa<br>\n0x00007ffb768abd23<br>\n0x00007ffb768abd96<br>\n0x00007ff6a8c314f4<br>\n0x00007ffbdedf26bd<br>\n0x00007ffbdfdcdfb8<br>\n*** END STACK TRACE POINTERS ***</p>\n<p>0x00007FFAEC383505      tensorflow::CurrentStackTrace<br>\n0x00007FFAEC2FD281      tensorflow::testing::InstallStacktraceHandler<br>\n0x00007FFBDD684090      log2f<br>\n0x00007FF6A8C327EC      OPENSSL_Applink<br>\n0x00007FFBD031E390      _C_specific_handler<br>\n0x00007FFBDFE13EAF      _chkstk<br>\n0x00007FFBDFD8EAE6      RtlFindCharInUnicodeString<br>\n0x00007FFBDFE12E9E      KiUserExceptionDispatcher<br>\n0x00007FFB390C66C8      QObject::event<br>\n0x00007FFB2B7110EE      QApplicationPrivate::notify_helper<br>\n0x00007FFB2B71008F      QApplication::notify<br>\n0x00007FFB2BD03E98      (unknown)<br>\n0x00007FFB390831B5      QCoreApplication::notifyInternal2<br>\n0x00007FFB390859EE      QCoreApplicationPrivate::sendPostedEvents<br>\n0x00007FFB389F0B3F      QWindowsGuiEventDispatcher::sendPostedEvents<br>\n0x00007FFB391DCF40      QEventDispatcherWin32::processEvents<br>\n0x00007FFB389F0B19      QWindowsGuiEventDispatcher::processEvents<br>\n0x00007FFB3908888F      QEventLoop::exec<br>\n0x00007FFB39080FCD      QCoreApplication::exec<br>\n0x00007FFB2BD00E15      (unknown)<br>\n0x00007FFB76986D82      PyCFunction_DebugMallocStats<br>\n0x00007FFB76A203F6      PyOS_URandomNonblock<br>\n0x00007FFB76A28D88      PyEval_GetFuncDesc<br>\n0x00007FFB76A25169      PyEval_EvalFrameDefault<br>\n0x00007FFB76940695      PyObject_Call<br>\n0x00007FFB769409D8      PyFunction_Vectorcall<br>\n0x00007FFB76A203F6      PyOS_URandomNonblock<br>\n0x00007FFB76A28D88      PyEval_GetFuncDesc<br>\n0x00007FFB76A22DF8      PyEval_EvalFrameDefault<br>\n0x00007FFB76A2798A      PyEval_EvalCodeWithName<br>\n0x00007FFB76A217F4      PyEval_EvalCode<br>\n0x00007FFB76A1CA4F      PyAST_Optimize<br>\n0x00007FFB76986BF5      PyCFunction_DebugMallocStats<br>\n0x00007FFB76A203F6      PyOS_URandomNonblock<br>\n0x00007FFB76A28D88      PyEval_GetFuncDesc<br>\n0x00007FFB76A22DF8      PyEval_EvalFrameDefault<br>\n0x00007FFB76A2798A      PyEval_EvalCodeWithName<br>\n0x00007FFB76940AAF      PyFunction_Vectorcall<br>\n0x00007FFB76A203F6      PyOS_URandomNonblock<br>\n0x00007FFB76A28D88      PyEval_GetFuncDesc<br>\n0x00007FFB76A22DF8      PyEval_EvalFrameDefault<br>\n0x00007FFB76A2798A      PyEval_EvalCodeWithName<br>\n0x00007FFB76940AAF      PyFunction_Vectorcall<br>\n0x00007FFB769403A6      PyVectorcall_Call<br>\n0x00007FFB768A9D3C      Py_hashtable_copy<br>\n0x00007FFB768AAEAA      Py_hashtable_copy<br>\n0x00007FFB768ABD23      Py_RunMain<br>\n0x00007FFB768ABD96      Py_Main<br>\n0x00007FF6A8C314F4      OPENSSL_Applink<br>\n0x00007FFBDEDF26BD      BaseThreadInitThunk<br>\n0x00007FFBDFDCDFB8      RtlUserThreadStart</p>\n<p>In addition, prior to my testing on a shortened video I tried creating a video from a DLC 2.3.0 created .h5 file that showed flawless tracking and it fails in v2.3.1 to output anything.</p>\n<p>I very much appreciate all you guys do for us and I\u2019m trying to help fix this but I\u2019m a little disappointed in this release so far.</p>"], "74209": ["<p>Is there any source code for converting .dcm file to any image format?</p>", "<p><a class=\"mention\" href=\"/u/chitralekha\">@Chitralekha</a> You can use the Bio-Formats command line tool <code>bfconvert</code>; see:</p>\n<p><a href=\"https://docs.openmicroscopy.org/bio-formats/6.11.0/users/comlinetools/conversion.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://docs.openmicroscopy.org/bio-formats/6.11.0/users/comlinetools/conversion.html</a></p>\n<p>Source code for the tool can be found here:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/ome/bioformats/blob/9eb4f2ef4ec708cc5c52025d22defff0b9cb4c0d/components/bio-formats-tools/src/loci/formats/tools/ImageConverter.java#L94-L97\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/ome/bioformats/blob/9eb4f2ef4ec708cc5c52025d22defff0b9cb4c0d/components/bio-formats-tools/src/loci/formats/tools/ImageConverter.java#L94-L97\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/ome/bioformats/blob/9eb4f2ef4ec708cc5c52025d22defff0b9cb4c0d/components/bio-formats-tools/src/loci/formats/tools/ImageConverter.java#L94-L97\" target=\"_blank\" rel=\"noopener\">ome/bioformats/blob/9eb4f2ef4ec708cc5c52025d22defff0b9cb4c0d/components/bio-formats-tools/src/loci/formats/tools/ImageConverter.java#L94-L97</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-java\">\n      <ol class=\"start lines\" start=\"94\" style=\"counter-reset: li-counter 93 ;\">\n          <li>/**</li>\n          <li> * ImageConverter is a utility class for converting a file between formats.</li>\n          <li> */</li>\n          <li>public final class ImageConverter {</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>And here is a tutorial example that converts from FV1000 to OME-TIFF:</p>\n<p><a href=\"https://docs.openmicroscopy.org/bio-formats/6.11.0/developers/conversion.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://docs.openmicroscopy.org/bio-formats/6.11.0/developers/conversion.html</a></p>\n<p>It should be easy to adapt it to DICOM instead.</p>\n<p>Note that Bio-Formats only supports writing to <em>open file formats</em>, not proprietary ones. You are unlikely to find tools that can write out proprietary formats other than the software provided by the vendor in question\u2014e.g. Zeiss Zen writes CZI, but nothing else is likely to do so.</p>", "<p>Hi,<br>\nJameswilliamson here<br>\nI built a <a href=\"https://convertzen.io/\" rel=\"noopener nofollow ugc\">website</a> that convert any image into JPJ to PNG or anyting that you want.<br>\nThanks.</p>"], "78306": ["<p>Hello,</p>\n<p>I am trying to script the production of cell detection density maps using QuPath\u2019s built in DensityMaps ( <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/analysis/heatmaps/DensityMaps.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DensityMaps (QuPath 0.4.0)</a> ), followed by exportation to an image file, which I think will have to go through imageJ</p>\n<p>However, I am unable to get anything working. I would really appreciate a short example of how to use DensityMaps with exportation.</p>\n<p>Here is all I have so far, I am not even sure if the map created by this contains anything.</p>\n<pre><code class=\"lang-auto\">import qupath.lib.analysis.heatmaps.DensityMaps\n\nselectAnnotations();\n\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImageBrightfield\": \"Optical density sum\",  \"requestedPixelSizeMicrons\": 0.0,  \"backgroundRadiusMicrons\": 0.0,  \"medianRadiusMicrons\": 0.0,  \"sigmaMicrons\": 1.5,  \"minAreaMicrons\": 10.0,  \"maxAreaMicrons\": 100.0,  \"threshold\": 0.23,  \"maxBackground\": 2.0,  \"watershedPostProcess\": true,  \"excludeDAB\": false,  \"cellExpansionMicrons\": 5.0,  \"includeNuclei\": true,  \"smoothBoundaries\": true,  \"makeMeasurements\": true}');\n\ndef params = new DensityMaps.DensityMapParameters()\ndef map = new DensityMaps.DensityMapBuilder(params)\n</code></pre>\n<p>Thank you!</p>", "<p>I was just looking this up again for someone who messaged me through my site, might be related, but it seems like it might be rather straightforward. I have not tested this, but the javadocs suggest that it only takes two strings.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#writeDensityMapImage(java.lang.String,java.lang.String)\">\n  <header class=\"source\">\n\n      <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#writeDensityMapImage(java.lang.String,java.lang.String)\" target=\"_blank\" rel=\"noopener\">qupath.github.io</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/scripting/QP.html#writeDensityMapImage(java.lang.String,java.lang.String)\" target=\"_blank\" rel=\"noopener\">QP (QuPath 0.4.0)</a></h3>\n\n  <p>declaration: package: qupath.lib.scripting, class: QP</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nGive that a shot, it should only take the name you used to save the density map, and the file location.<br>\n<a href=\"https://gist.github.com/petebankhead/2e7325d8c560677bba9b867f68070300\">This script from Pete</a> shows another example of using the saved density map name.</p>\n<p>That might look as simple as</p>\n<pre><code class=\"lang-auto\">def server = getCurrentServer()\n\n// Define output path (relative to project)\ndef name = GeneralTools.getNameWithoutExtension(server.getMetadata().getName())\npathOutput = buildFilePath(PROJECT_BASE_DIR, 'densityMaps', name)\nmkdirs(pathOutput)\nfileName = buildFilePath(pathOutput, 'Density map ' + name + '.tif')\nwriteDensityMapImage(\"MyFirstDensityMap\", fileName)\n</code></pre>\n<p>I have not tested it, however.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> , we are indeed trying to solve the same issue!</p>\n<p>I think our main issue is that we want to generate the density map programmatically through the script, and cannot figure out what the command that creates the map is. From there, we should be able to navigate through to saving from your post(s) and the code from Pete.</p>\n<p>You are correct, If I manually create the density map with the GUI then everything works as advertised, but for the life of me I cannot figure out how to generate the map with only scripting.</p>", "<p>Hmm, I also do not see any way to create the denisty map itself from a script. It seems possible to write out an existing map, or create hotspots or annotations from scratch, but not create the map itself - unless I am missing something <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>?</p>", "<p>Not sure if I\u2019ve ever scripted it myself, but there should be three main steps:</p>\n<ol>\n<li>Create a <code>DensityMapBuilder</code>\n<ul>\n<li>Use <a href=\"https://github.com/qupath/qupath/blob/25a6fde481830f149c4e65fa27a6deff0909c103/qupath-core-processing/src/main/java/qupath/lib/analysis/heatmaps/DensityMaps.java#L151\" class=\"inline-onebox\">qupath/DensityMaps.java at 25a6fde481830f149c4e65fa27a6deff0909c103 \u00b7 qupath/qupath \u00b7 GitHub</a>\n</li>\n</ul>\n</li>\n<li>Customize the builder if you need to\n<ul>\n<li>See <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/analysis/heatmaps/DensityMaps.DensityMapBuilder.html\">javadocs here</a>\n</li>\n</ul>\n</li>\n<li>Write the density map, using the current <code>ImageData</code>, <code>DensityMapBuilder</code> and export file path\n<ul>\n<li>See <a href=\"https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-core-processing/src/main/java/qupath/lib/scripting/QP.java#L3331\" class=\"inline-onebox\">qupath/QP.java at 1368912885c1a191beaea32c28d85a3707f657f8 \u00b7 qupath/qupath \u00b7 GitHub</a>\n</li>\n</ul>\n</li>\n</ol>", "<p>Thanks <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>, that certainly helped - I had not realized that the computation occurs when <code>writeDensityMapImage</code> is called. I wrote the following small script to test this but ran into an issue with <code>writeDensityMapImage</code>. Is it possible that I need to set additional parameters here? For reference, we are doing some simple cell identifications (2nd line of script) on nissl data and then looking to map the density (total number of cell identification centroids in each pixel) to an image for further processing.</p>\n<p>I left all parameters at their defaults except the radius, which I set to <code>100</code>. Doing the same thing in the gui (cell identification, then create density map and adjusting only the radius) works well. I am assuming here that the GUI is using the buildClassifier method to determine an optimal pixel size, so I do the same thing in the script below.</p>\n<p>Thanks ahead of time for any pointers!</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImageBrightfield\": \"Optical density sum\",  \"requestedPixelSizeMicrons\": 0.0,  \"backgroundRadiusMicrons\": 0.0,  \"medianRadiusMicrons\": 0.0,  \"sigmaMicrons\": 1.5,  \"minAreaMicrons\": 10.0,  \"maxAreaMicrons\": 100.0,  \"threshold\": 0.23,  \"maxBackground\": 2.0,  \"watershedPostProcess\": true,  \"excludeDAB\": false,  \"cellExpansionMicrons\": 5.0,  \"includeNuclei\": true,  \"smoothBoundaries\": true,  \"makeMeasurements\": true}');\n\ndef params = new DensityMaps.DensityMapParameters()\ndef builder = new DensityMaps.DensityMapBuilder(params)\nbuilder.radius(100) //this sets the value (essentially a setRadius)\nprintln builder.buildParameters().getRadius() // you can build the params and then see what the radius is, this is the only way to view (copy)\n\nprintln builder.buildParameters().getPixelSize()\nbuilder.buildClassifier(imageData) // to allow pixel size to be set according to input data\nprintln builder.buildParameters().getPixelSize() //check if this changes the pixelSize? (no)\n\nfileName = buildFilePath('./dMap.tif')\nwriteDensityMapImage(imageData, builder, fileName)\n</code></pre>\n<p>output / error:</p>\n<pre><code class=\"lang-auto\">INFO: 389 nuclei detected (processing time: 1.19 seconds)\nINFO: 1596 nuclei detected (processing time: 3.03 seconds)\nINFO: 2539 nuclei detected (processing time: 3.06 seconds)\nINFO: 2097 nuclei detected (processing time: 3.24 seconds)\nINFO: 3636 nuclei detected (processing time: 3.44 seconds)\nINFO: 2544 nuclei detected (processing time: 3.57 seconds)\nINFO: 3046 nuclei detected (processing time: 3.76 seconds)\nINFO: 2822 nuclei detected (processing time: 3.78 seconds)\nINFO: 2782 nuclei detected (processing time: 4.03 seconds)\nINFO: 4749 nuclei detected (processing time: 4.70 seconds)\nINFO: 5154 nuclei detected (processing time: 4.92 seconds)\nINFO: 5057 nuclei detected (processing time: 5.24 seconds)\nINFO: Processing complete in 5.32 seconds\nINFO: Tasks completed!\nINFO: 100.0\nINFO: null\nINFO: null\nWARN: Unable to read tile: java.lang.NullPointerException\nERROR: null in QuPathScript at line number 14\n\nERROR: java.base/java.util.Objects.requireNonNull(Unknown Source)\n    java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Unknown Source)\n    java.base/java.util.stream.ReduceOps$3ReducingSink.accept(Unknown Source)\n    java.base/java.util.stream.ReferencePipeline$2$1.accept(Unknown Source)\n    java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source)\n    java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)\n    java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)\n    java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(Unknown Source)\n    java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(Unknown Source)\n    java.base/java.util.stream.AbstractTask.compute(Unknown Source)\n    java.base/java.util.concurrent.CountedCompleter.exec(Unknown Source)\n    java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\n    java.base/java.util.concurrent.ForkJoinTask.invoke(Unknown Source)\n    java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(Unknown Source)\n    java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)\n    java.base/java.util.stream.ReferencePipeline.collect(Unknown Source)\n    qupath.lib.classifiers.pixel.PixelClassificationImageServer.readAllTiles(PixelClassificationImageServer.java:192)\n    qupath.opencv.ml.pixel.PixelClassifierTools.createPixelClassificationServer(PixelClassifierTools.java:494)\n    qupath.lib.analysis.heatmaps.DensityMaps$DensityMapBuilder.buildServer(DensityMaps.java:395)\n    qupath.lib.scripting.QP.writeDensityMapImage(QP.java:3341)\n    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n    QuPathScript.run(QuPathScript:14)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)\n    qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)\n    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)\n    qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)\n    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    java.base/java.lang.Thread.run(Unknown Source)\n\n</code></pre>", "<p>In the script provided it looks like you only set the radius but not the pixel size or anything else, so might be the expected result?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7b669db015b139315c2dd6d98f1f25c21a54542.png\" data-download-href=\"/uploads/short-url/uMhtsUCq2Nhdh784HG4ZoNPQCfo.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7b669db015b139315c2dd6d98f1f25c21a54542.png\" alt=\"image\" data-base62-sha1=\"uMhtsUCq2Nhdh784HG4ZoNPQCfo\" width=\"608\" height=\"500\" data-dominant-color=\"3C3F41\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">865\u00d7711 21.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<aside class=\"quote no-group\" data-username=\"steelec\" data-post=\"6\" data-topic=\"78306\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/steelec/40/68762_2.png\" class=\"avatar\"> Christopher J. Steele:</div>\n<blockquote>\n<p>Is it possible that I need to set additional parameters here?</p>\n</blockquote>\n</aside>\n<p>This</p>", "<p>Use of pixelcalibration here <a href=\"https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579\" class=\"inline-onebox\">Script for generating double threshold classifier</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/steelec\">@steelec</a>  you should create the density map builder with</p>\n<pre><code class=\"lang-groovy\">def builder = DensityMaps.builder(predicate)\n</code></pre>\n<p>In Java, it wouldn\u2019t be possible to do it with <code>new DensityMaps.DensityMapParameters()</code> because the construction is private \u2013 unfortunately Groovy is very lax about this kind of thing, so permits it and the exception only occurs later.</p>\n<p>The awkward thing is then to determine the predicate, which determines which objects will contribute to the map. It\u2019s cumbersome because it needs to be JSON-serializable.</p>\n<p>From your description, I think you want all detections - so this may work:</p>\n<pre><code class=\"lang-groovy\">def predicate = PathObjectPredicates.filter(PathObjectFilter.DETECTIONS_ALL)\nDensityMaps.builder(predicate)\n</code></pre>", "<p>Perfect, that solved it <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> !</p>\n<p>I put my work-in-progress code here for others\u2019 reference. I also tried my hand at setting up an alternative approach with <code>pixelCalibration</code> but wasn\u2019t able to get any output yet. I will update if/when I get there.</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy\" target=\"_blank\" rel=\"noopener nofollow ugc\">neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy</a></h4>\n\n\n      <pre><code class=\"lang-groovy\">// based on discussion here: https://forum.image.sc/t/scripted-densitymaps-and-exporting-to-image-file-with-imagej/78306/6\nimport qupath.lib.images.servers.PixelCalibration\n\n// SET OUTPUT\ndef out_dir = buildFilePath(PROJECT_BASE_DIR, 'results_cell_counts')\nmkdirs(out_dir)\n// ************************************************//\n\ndef imageData = getCurrentImageData()\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImageBrightfield\": \"Optical density sum\",  \"requestedPixelSizeMicrons\": 0.0,  \"backgroundRadiusMicrons\": 0.0,  \"medianRadiusMicrons\": 0.0,  \"sigmaMicrons\": 1.5,  \"minAreaMicrons\": 10.0,  \"maxAreaMicrons\": 100.0,  \"threshold\": 0.23,  \"maxBackground\": 2.0,  \"watershedPostProcess\": true,  \"excludeDAB\": false,  \"cellExpansionMicrons\": 5.0,  \"includeNuclei\": true,  \"smoothBoundaries\": true,  \"makeMeasurements\": true}');\n\n// // PIXEL SIZE CALCULATIONS FOR DOWNSAMPLING\n// // this appears to slow processing down extremely (killed the process after &gt;20 mins when running on small ROI)\n// // define target resolution for ouptut pixels, in calibrated units (um if possible!)\n// // note that using the builder.buildClassifier(imageData) approach does not allow you to set your pixel sizes directly, so comparability outside of QuPath can be an issue across multiple images  \n// double requestedPixelSize = 10\n// // determine the downsampling factor, once we know what the pixel sizes are for our x and y in the original image\n// double pixelSize = imageData.getServer().getPixelCalibration().getAveragedPixelSize()\n// double scale_factor = requestedPixelSize / pixelSize\n// // ***************************************************//\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/neuralabc/microscopy_scripts/blob/5292aabb2aa31cead2e0fd27cd9debe9e2b7e65b/QuPath_densityMapping.groovy\" target=\"_blank\" rel=\"noopener nofollow ugc\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "49633": ["<p>Hello all,</p>\n<p>Is it normal that running a script for a project is substantially slower than running it individually for each image?</p>\n<p>I have a pretty basic script that I\u2019ve compiled from forum posts and the workflow panel that, very briefly:</p>\n<blockquote>\n<p>sets image type<br>\nsets pixel size<br>\nsets de-convolution vectors<br>\nselects the entire image<br>\ncounts cells<br>\nsmooths<br>\napplies an existing classifier<br>\nexports the rendered image<br>\nexports annotation data<br>\nexports detection data</p>\n</blockquote>\n<p>When I run it on a newly imported image, it takes ~80 sec to run (images are between 600-800MB on average). When I run it for my project (9 images at the moment) it takes close to 10 minutes/image.</p>\n<p>Is there something I can do to allow the script to run for project more efficiently? Either on the QuPath/scripting side or the actual computer hardware side.</p>\n<p>Thanks in advance!</p>", "<p>Posting my computer specs in case they are relevant:</p>\n<p>iMac Pro (2017)<br>\nprocessor: 2.5 GHz 14-Core Intel Xeon W<br>\nmemory: 128 GB 2666 MHz DDR4<br>\ngraphics: Radeon Pro Vega 56 8 GB</p>", "<p>and here is the actual script:</p>\n<pre><code class=\"lang-auto\">setImageType('BRIGHTFIELD_H_E');\nsetPixelSizeMicrons(0.511904,0.511904);\nsetColorDeconvolutionStains('{\"Name\" : \"H&amp;E modified\", \"Stain 1\" : \"Hematoxylin\", \"Values 1\" : \"0.84285 0.49591 0.20896 \", \"Stain 2\" : \"Eosin\", \"Values 2\" : \"0.51405 0.67906 0.52405 \", \"Background\" : \" 255 255 255 \"}');\ncreateSelectAllObject(true);\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImageBrightfield\": \"Optical density sum\",  \"requestedPixelSizeMicrons\": 0.0,  \"backgroundRadiusMicrons\": 8.0,  \"medianRadiusMicrons\": 0.0,  \"sigmaMicrons\": 1.2,  \"minAreaMicrons\": 5.0,  \"maxAreaMicrons\": 200.0,  \"threshold\": 0.1,  \"maxBackground\": 2.0,  \"watershedPostProcess\": true,  \"cellExpansionMicrons\": 5.0,  \"includeNuclei\": true,  \"smoothBoundaries\": true,  \"makeMeasurements\": true}');\nrunPlugin('qupath.lib.plugins.objects.SmoothFeaturesPlugin', '{\"fwhmMicrons\": 50.0,  \"smoothWithinClasses\": false}');\nrunObjectClassifier(\"h1n1 c;lassifer (tri) 2\");\n\nimport qupath.imagej.tools.IJTools\nimport qupath.lib.gui.images.servers.RenderedImageServer\nimport qupath.lib.gui.viewer.overlays.HierarchyOverlay\nimport qupath.lib.regions.RegionRequest\n\nimport static qupath.lib.gui.scripting.QPEx.*\n\n\ndouble downsample = 2\n\nString path = buildFilePath(PROJECT_BASE_DIR, 'rendered', getProjectEntry().getImageName() + '.png')\n\ndef viewer = getCurrentViewer()\ndef imageData = getCurrentImageData()\n\ndef server = new RenderedImageServer.Builder(imageData)\n    .downsamples(downsample)\n    .layers(new HierarchyOverlay(viewer.getImageRegionStore(), viewer.getOverlayOptions(), imageData))\n    .build()\n\nif (path != null) {\n    mkdirs(new File(path).getParent())\n    writeImage(server, path)\n} else\n    IJTools.convertToImagePlus(server, RegionRequest.createInstance(server)).getImage().show()\n    print 'Image exported to ' + path\n    \n    def name2 = getProjectEntry().getImageName() + '.txt'\n    def path2 = buildFilePath(PROJECT_BASE_DIR, 'annotation measurements')\n    mkdirs(path2)\n    path = buildFilePath(path2, name2)\n    saveAnnotationMeasurements(path2)\n    print 'Results exported to ' + path2\n\n    def name1 = getProjectEntry().getImageName() + '.txt'\n    def path1 = buildFilePath(PROJECT_BASE_DIR, 'detection measurements')\n    mkdirs(path1)\n    path = buildFilePath(path1, name1)\n    saveDetectionMeasurements(path1)\n    print 'Results exported to ' + path1\n</code></pre>", "<p>Edited the script formatting to make it code.</p>\n<p>Any chance you are writing out large images over a network?</p>", "<p>Thanks, I didn\u2019t know how to do that <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>If I understand your question correctly, no, the images is being written to a folder on the computer. Does that answer it?</p>", "<p>Yep, that\u2019s about all I have other than the usual \u201ccheck the memory monitor and see if there are problems there.\u201d If you are capping out on memory, maybe increase the max memory. Or there are a few places around talking about releasing memory during the script - would have to search for them though.</p>\n<p>Not sure why anything else in your script would cause problems, though.<br>\n<em>View-Show memory monitor</em></p>", "<aside class=\"quote no-group\" data-username=\"davidnascari\" data-post=\"3\" data-topic=\"49633\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/davidnascari/40/39883_2.png\" class=\"avatar\"> davidnascari:</div>\n<blockquote>\n<p><code>saveDetectionMeasurements(path1)</code></p>\n</blockquote>\n</aside>\n<p>Come to think of it, this process can continue after the script finishes, IIRC. But I might be misremembering.<br>\nMaybe Pete has better ideas though.</p>", "<p>I\u2019d suggest trying it with shorter versions of the script / lines commented out to try to identify where the bottleneck is.</p>\n<p>If your detection measurement files are huge, writing those could be slow.</p>\n<p>The image files may be 600-800 MB, but I\u2019m guessing they are compressed and the actual images are much larger?</p>", "<p>As I am running a batch right now, I can share a quick screen grab:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c7fb87335366e642d7094d79e92fb7081d71b4a9.png\" data-download-href=\"/uploads/short-url/sx7UC8eXJfVg8L1MAIrNQ3swUwF.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c7fb87335366e642d7094d79e92fb7081d71b4a9_2_690x401.png\" alt=\"image\" data-base62-sha1=\"sx7UC8eXJfVg8L1MAIrNQ3swUwF\" width=\"690\" height=\"401\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c7fb87335366e642d7094d79e92fb7081d71b4a9_2_690x401.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c7fb87335366e642d7094d79e92fb7081d71b4a9_2_1035x601.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c7fb87335366e642d7094d79e92fb7081d71b4a9_2_1380x802.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c7fb87335366e642d7094d79e92fb7081d71b4a9_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1448\u00d7842 124 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I have it set to allocate 40% memory to tile caching, and it\u2019s set to the default 50% RAM allocation since I\u2019m on Mac, so I guess I\u2019m a little confused why the total memory says its around 24.</p>\n<p>Can I afford to increase either of these memory allocations? Do you think they may help?<br>\nThis computer was purchased for the lab specifically to run histopathology analysis through QuPath, so I don\u2019t think it\u2019s the end of the world to allocate it more RAM, but, I defer to your expertise as I am not very computer savvy\u2026</p>", "<p>Hi Pete, thanks for chiming in.</p>\n<p>The detection measurement files are between 200-300MB, and I honestly don\u2019t think I need them for what I\u2019m doing currently - the command made it over through copy/pasting an old script. I\u2019m currently interested in just the annotation measurements, so I can take it out next time around. That being said, I\u2019m still confused why the bottleneck when scaling up the batch?</p>\n<p>Also, the images are exported from the microscope as uncompressed TIFs, so I don\u2019t think they are any larger than stated unless there\u2019s some sort of compression going on I haven\u2019t accounted for or noticed.</p>", "<aside class=\"quote no-group\" data-username=\"davidnascari\" data-post=\"9\" data-topic=\"49633\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/davidnascari/40/39883_2.png\" class=\"avatar\"> davidnascari:</div>\n<blockquote>\n<p>I guess I\u2019m a little confused why the total memory says its around 24.</p>\n</blockquote>\n</aside>\n<p>I think that is allocated for use - note that the scale goes all the way up to 64, which would be 50% of your 128.<br>\nThat means you aren\u2019t pushing anything in terms of memory, and it is not likely the problem. Memory should only be allocated as needed - QuPath/Java does not hold the whole 64GB hostage.</p>", "<p>Right, so is something going on such that it only uses 24gb out of the available 64gb?</p>", "<p>I\u2019m confused as well, but I\u2019d definitely skip writing the detection measurements as a start. I\u2019d also skip writing the rendered image if you don\u2019t definitely need it; this is also likely to be slow if it is very large.</p>\n<aside class=\"quote no-group\" data-username=\"davidnascari\" data-post=\"12\" data-topic=\"49633\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/davidnascari/40/39883_2.png\" class=\"avatar\"> davidnascari:</div>\n<blockquote>\n<p>Right, so is something going on such that it only uses 24gb out of the available 64gb?</p>\n</blockquote>\n</aside>\n<p>Not necessarily, the maximum is 64 GB but it won\u2019t all be allocated at the very beginning if it isn\u2019t needed.</p>\n<p>(Java uses two flags for this at startup, <code>-Xmx</code> and <code>-Xms</code>\u2026 the first is for the maximum memory, the second for the initial memory. QuPath only sets the flag for the maximum memory, on the assumption that you won\u2019t always necessarily want to use the full amount immediately when you start the application. It will grow as more memory is required.)</p>", "<p>Thanks Pete! Sounds like it isn\u2019t necessary to change the overall allocated RAM then\u2026 How about the % allocated for tile caching? Would it be worth bumping up from 40%?</p>\n<p>Unfortunately, the rendered image is something I need for the time being\u2026</p>\n<p>My goal with this script was to create a one-click analysis that other people in my lab can use for their own projects (switching out for their own detection parameters and classifier, of course), without the need to invest a ton of time into learning QuPath (which I\u2019ve been more than happy to do <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> )\u2026 I guess it\u2019s not actually that much more effort to just separate and run as several different scripts in series, but it would\u2019ve been nice not to have to.</p>\n<p>Happy to hear from you or anyone else if there\u2019s any other thoughts on the matter!</p>", "<p>Depending on what is necessary for a given project, it might help to place different parts within if statements, and then put well described booleans at the start of the script.</p>\n<p>If you do not want to split the script, that gives the current user control without much code editing.</p>\n<pre><code class=\"lang-groovy\">exportRenderedPicture = true\n//......\n//.........\nif(exportRenderedPicture){\n //export code\n}\n</code></pre>\n<p>And maybe increase the downsample on the rendered image for the moment if speed is an issue.</p>", "<p>That\u2019s a good idea, thank you!</p>", "<p>And, in case anyone is curious, this is what the memory wrt time was when the script finally finished:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/72aa3f06613f525612de4e4db98e60f94a4ecb9a.png\" data-download-href=\"/uploads/short-url/gmnbjcdPiXSMFD8KTsSHlon8ge6.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72aa3f06613f525612de4e4db98e60f94a4ecb9a_2_690x402.png\" alt=\"image\" data-base62-sha1=\"gmnbjcdPiXSMFD8KTsSHlon8ge6\" width=\"690\" height=\"402\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72aa3f06613f525612de4e4db98e60f94a4ecb9a_2_690x402.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72aa3f06613f525612de4e4db98e60f94a4ecb9a_2_1035x603.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72aa3f06613f525612de4e4db98e60f94a4ecb9a_2_1380x804.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72aa3f06613f525612de4e4db98e60f94a4ecb9a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1446\u00d7844 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nKeep in mind, i started tracking memory about halfway through the batch so this was about 51 minutes to run 4 or 5 images.</p>", "<aside class=\"quote no-group\" data-username=\"davidnascari\" data-post=\"10\" data-topic=\"49633\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/davidnascari/40/39883_2.png\" class=\"avatar\"> davidnascari:</div>\n<blockquote>\n<p>Also, the images are exported from the microscope as uncompressed TIFs, so I don\u2019t think they are any larger than stated unless there\u2019s some sort of compression going on I haven\u2019t accounted for or noticed.</p>\n</blockquote>\n</aside>\n<p>When your image is open, QuPath should tell you the uncompressed size under the \u2018Image\u2019 tab. I\u2019d be interested to know the width/height; if these are large, PNG may not be the best export format (at least with downsample 2; you may benefit from decreasing the downsample to export a lower image).</p>\n<p>Things also tend to be a lot slower if the image isn\u2019t <a href=\"https://qupath.readthedocs.io/en/latest/docs/intro/formats.html\">\u2018pyramidal\u2019</a>. If your image isn\u2019t a pyramid from the start, QuPath lets you choose on import.</p>\n<p>Also, just to clarify, I presume you\u2019re running the script from the script editor with either <em>Run</em> or <em>Run for project</em> in the menu. The second option will also save the data file, which will take some time (but shouldn\u2019t be ~10 minutes\u2026)</p>\n<p>My main advice is to</p>\n<ul>\n<li>try repeating the script with parts removed (to find the bottleneck)\n<ul>\n<li>alternatively, use <a href=\"https://visualvm.github.io\">https://visualvm.github.io</a> (more technically involved, but the CPU sampling option helps identify bottlenecks)</li>\n</ul>\n</li>\n<li>confirm that the same image is much faster to process using <em>Run</em> than it is to process using <em>Run for project</em>.</li>\n</ul>\n<p>To check this, you can apply <em>Run for project</em> to a single image that isn\u2019t currently open. For a fair comparison, you should restart QuPath in between and run your script as the first thing you do (so that one method isn\u2019t advantaged by having cached part of the image already).</p>\n<p>I\u2019m not really able to investigate more without the results of the two tests suggested above, since any problem may well depend upon the specifics of your image and I don\u2019t know enough about them to replicate the issue myself.</p>", "<p>Thanks Pete, I\u2019m going to try this today to see if I can find the bottleneck. If you\u2019re interested, I\u2019m going to attach one of the images. I checked the uncompressed size in QuPath (830.9MB), which is weirdly smaller than the image size when I \u201cGet Info\u201d on the original image in its folder (872.4MB). Not sure what to make of that. The dimensions are quite large as its a mosaic (~6.7mm x 11.4mm).</p>\n<p>Link to image:<br>\n<a href=\"https://pitt.box.com/s/5e4rab5swy656nun7rzbu8v45imajpxc\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://pitt.box.com/s/5e4rab5swy656nun7rzbu8v45imajpxc</a></p>", "<p>Thanks <a class=\"mention\" href=\"/u/davidnascari\">@davidnascari</a>, the image was very helpful \u2013 I can confirm there\u2019s something wrong on the QuPath side\u2026 although I haven\u2019t been able to figure out exactly what.</p>\n<p>As far as I can tell, it traces back to the fact that the image is not stored in a tiled, pyramidal way. If you open the image in QuPath and choose <em>File \u2192 Export images\u2026 \u2192 OME TIFF</em> to create a new pyramidal TIFF image (with/without compression) you should find performance for everything is better if you use the new image.</p>\n<p>But after spending a couple of hours investigating, I\u2019ve only succeeded in proving most of my hypotheses regarding the cause to be wrong. We\u2019ll try to figure it out before the next release.</p>"], "78308": ["<p>So I have a Fluorescent protein molecules spread over ROI separate from each molecule (please see movie frames attached) which I take 50 ms frame rate 40,000 frames. I run thunderstorm and get the table which has all the data for each frame. In the intensity column (integrated photons number under the peak as a definition). But I really not able to understand this intensity meaning, what does it mean? Is it that if I have spot of molecule and the each frame suppose shows 4000 intensity (for one frame and one spot) that means it\u2019s 4000 total number of photons in that spot for that frame alone, so if i have to calculate total photon from that spot I will need to add all the frames till it\u2019s blinking? Please if you can explain.</p>\n<p>Table output file screenshot and image sequence 500 frames (For example) attached.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/a/2a91475c5e80fbe9aa563c034ca5fd8f8e60a275.png\" data-download-href=\"/uploads/short-url/64zjlwKa7gfLksBwNokhyWSlnYp.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/a/2a91475c5e80fbe9aa563c034ca5fd8f8e60a275.png\" alt=\"image\" data-base62-sha1=\"64zjlwKa7gfLksBwNokhyWSlnYp\" width=\"690\" height=\"257\" data-dominant-color=\"E7E7E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1038\u00d7388 20 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/axpXuCxawFIrpK5bs2jaSRCeKaW.tif\">mvi-1.tif</a> (14.4 MB)</p>"], "78310": ["<h6>\n<a name=\"by-matt-mccormick-orciduploadrobavbggcq0ltwsmonefcecmprygifhttpsorcidorg0000-0001-9475-3756-mary-elise-dedicke-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0001-8848-3235-jean-christophe-fillion-robin-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0002-9688-8950-will-schroeder-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0003-3815-9386-1\" class=\"anchor\" href=\"#by-matt-mccormick-orciduploadrobavbggcq0ltwsmonefcecmprygifhttpsorcidorg0000-0001-9475-3756-mary-elise-dedicke-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0001-8848-3235-jean-christophe-fillion-robin-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0002-9688-8950-will-schroeder-orcidhttpsi0wpcominfoorcidorgwp-contentuploads202012orcid_16x16gifresize162c16ssl1httpsorcidorg0000-0003-3815-9386-1\"></a>By: Matt McCormick <a href=\"https://orcid.org/0000-0001-9475-3756\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bff6a0d75375482cfaf128e71b58c428272d88a0.gif\" alt=\"ORCID\" data-base62-sha1=\"robAVbgGcQ0LTwsMOnEfcEcmpry\" width=\"16\" height=\"16\"></a>, Mary Elise Dedicke <a href=\"https://orcid.org/0000-0001-8848-3235\"><span alt=\"ORCID\" class=\"broken-image\" title=\"This image is broken\"><svg class=\"fa d-icon d-icon-unlink svg-icon\" aria-hidden=\"true\"><use href=\"#unlink\"></use></svg></span></a>, Jean-Christophe Fillion-Robin <a href=\"https://orcid.org/0000-0002-9688-8950\"><span alt=\"ORCID\" class=\"broken-image\" title=\"This image is broken\"><svg class=\"fa d-icon d-icon-unlink svg-icon\" aria-hidden=\"true\"><use href=\"#unlink\"></use></svg></span></a>, Will Schroeder <a href=\"https://orcid.org/0000-0003-3815-9386\"><span alt=\"ORCID\" class=\"broken-image\" title=\"This image is broken\"><svg class=\"fa d-icon d-icon-unlink svg-icon\" aria-hidden=\"true\"><use href=\"#unlink\"></use></svg></span></a>\n</h6>\n<p>Did you know that your web browser comes bundled with extremely powerful development tools?</p>\n<p>Modern web browsers are the foundation of the <a href=\"https://en.wikipedia.org/wiki/Web_platform\">The Web Platform</a>, a full-featured computing environment. However, <em>full-featured platforms are not useful in and of themselves</em>. A large community of software developers is required to program the applications on a platform that people love. And, <a href=\"https://www.kitware.com/how-to-debug-wasi-pipelines-with-itk-wasm/\">effective debugging results in effective programming</a>.</p>\n<p>In this tutorial, we will learn how to debug <a href=\"https://wasm.itk.org\">itk-wasm</a> C++ data processing pipelines built to <a href=\"https://webassembly.org\">WebAssembly (wasm)</a> with the full-featured graphical debugger built into Chromium-based browsers.</p>\n<p>In the following sections, we will first explain how to obtain a useful JavaScript backtrace in Node.js, then debug C++ code built to WebAssembly in a Chromium-based web browser. Let\u2019s get started! <img src=\"https://emoji.discourse-cdn.com/twitter/rocket.png?v=12\" title=\":rocket:\" class=\"emoji\" alt=\":rocket:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<h2>\n<a name=\"h-0-preliminaries-2\" class=\"anchor\" href=\"#h-0-preliminaries-2\"></a>0. Preliminaries</h2>\n<p>Before starting this tutorial, check out our <a href=\"https://www.kitware.com/how-to-debug-wasi-pipelines-with-itk-wasm/\">WASI WebAssembly command line debugging tutorial</a>.</p>\n<p>As with the previous tutorial, we will be debugging the following C++ code:</p>\n<pre><code class=\"lang-cpp\">#include &lt;iostream&gt;\n\nint main() {\n  std::cout &lt;&lt; \"Hello debugger world!\" &lt;&lt; std::endl;\n\n  const char * wasmDetails = \"are no longer hidden\";\n\n  const int a = 1;\n  const int b = 2;\n  const auto c = a + b;\n\n  // Simulate a crash.\n  abort();\n  return 0;\n}\n</code></pre>\n<p><a href=\"https://github.com/InsightSoftwareConsortium/itk-wasm/tree/main/examples/debugging\">The tutorial code</a> provides npm scripts as a convenient way to execute debugging commands, which you may also invoke directly in a command line shell.</p>\n<h2>\n<a name=\"h-1-nodejs-backtrace-3\" class=\"anchor\" href=\"#h-1-nodejs-backtrace-3\"></a>1. Node.js Backtrace</h2>\n<p>When debugging WebAssembly built with the itk-wasm Emscripten toolchain, set the <code>CMAKE_BUILD_TYPE</code> to <code>Debug</code> just like with native binary debug builds.</p>\n<p>As with native builds, this build configuration adds debugging symbols, the human-readable names of functions, variables, etc., into the binary.  This also adds support for C++ exceptions and retrieving the string name associated with exceptions. Without this itk-wasm instrumentation, a C++ exception will throw an error with an opaque integer value. And, Emscripten JavaScript WebAssembly bindings will not be minified, which facilitates debugging.</p>\n<p>When built with the default <code>Release</code> build type:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651.png\" data-download-href=\"/uploads/short-url/mJeSVckwyLmNlqL32UPhPqSjEop.png?dl=1\" title=\"Emscripten build Release\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651_2_690x155.png\" alt=\"Emscripten build Release\" data-base62-sha1=\"mJeSVckwyLmNlqL32UPhPqSjEop\" width=\"690\" height=\"155\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651_2_690x155.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651_2_1035x232.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f4cff8f0af9d6abc59f327cfae1c02601c5d651.png 2x\" data-dominant-color=\"0A0A0A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Emscripten build Release</span><span class=\"informations\">1066\u00d7241 23.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>the JavaScript support code is minified, and difficult to debug:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c.png\" data-download-href=\"/uploads/short-url/14OQ2Bch3htCzijyrT0JQshKd5i.png?dl=1\" title=\"Run Node Release\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c_2_690x224.png\" alt=\"Run Node Release\" data-base62-sha1=\"14OQ2Bch3htCzijyrT0JQshKd5i\" width=\"690\" height=\"224\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c_2_690x224.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/078da7ff4bd6d6e7f704acf9dcc0894f1ec8771c.png 2x\" data-dominant-color=\"151714\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Run Node Release</span><span class=\"informations\">991\u00d7322 77.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>However, when built with the <code>Debug</code> build type:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png\" data-download-href=\"/uploads/short-url/bgYuIjkw1VnTOuRQzsXtkehkf0v.png?dl=1\" title=\"Emscripten build Debug\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png\" alt=\"Emscripten build Debug\" data-base62-sha1=\"bgYuIjkw1VnTOuRQzsXtkehkf0v\" width=\"690\" height=\"192\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_1035x288.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png 2x\" data-dominant-color=\"090909\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Emscripten build Debug</span><span class=\"informations\">1138\u00d7318 34.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>you can obtain a useful backtrace:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f.png\" data-download-href=\"/uploads/short-url/nJkIl5oxkJ3W7UuYBEmJDcHxQ1F.png?dl=1\" title=\"Run Node Debug\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f_2_690x273.png\" alt=\"Run Node Debug\" data-base62-sha1=\"nJkIl5oxkJ3W7UuYBEmJDcHxQ1F\" width=\"690\" height=\"273\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f_2_690x273.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f_2_1035x409.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a651e49f7ace3fd1b1e7553f626e2327e430959f.png 2x\" data-dominant-color=\"061106\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Run Node Debug</span><span class=\"informations\">1118\u00d7443 58.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>This is helpful for debugging issues that occur in the Emscripten JavaScript interface. The next section describes how to debug issues inside the Emscripten-generated WebAssembly.</p>\n<h2>\n<a name=\"h-2-debugging-in-chromium-based-browsers-4\" class=\"anchor\" href=\"#h-2-debugging-in-chromium-based-browsers-4\"></a>2. Debugging in Chromium-based Browsers</h2>\n<p>Recent Chromium-based browsers have support for debugging C++ -based WebAssembly in the browser. With a few extra steps described in this section, it is possible to interactively step through and inspect WebAssembly that is compiled with C++ running in the browser.</p>\n<p>WebAssembly debugging in DevTools requires a few extra setup steps compared to a default browser installation.</p>\n<p>First, <a href=\"https://goo.gle/wasm-debugging-extension\">install the Chrome WebAssembly Debugging extension</a>.</p>\n<p>Next, enable it in DevTools:</p>\n<ol>\n<li>In DevTools, click the <em>gear (<img src=\"https://emoji.discourse-cdn.com/twitter/gear.png?v=12\" title=\":gear:\" class=\"emoji\" alt=\":gear:\" loading=\"lazy\" width=\"20\" height=\"20\">)</em> icon in the top-right corner.</li>\n<li>Go to the <em>Experiments</em> panel.</li>\n<li>Select <em>WebAssembly Debugging: Enable DWARF support</em>.</li>\n</ol>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8.png\" data-download-href=\"/uploads/short-url/nFt0HIS0tSO300OHyK27k2pqCy4.png?dl=1\" title=\"Enable Wasm Debugging\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8_2_582x500.png\" alt=\"Enable Wasm Debugging\" data-base62-sha1=\"nFt0HIS0tSO300OHyK27k2pqCy4\" width=\"582\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8_2_582x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8_2_873x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5e2031acdb914bb97f7c9296b0aa5c5e70ff8d8.png 2x\" data-dominant-color=\"F7F6F7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Enable Wasm Debugging</span><span class=\"informations\">901\u00d7774 75.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Exit Settings, then reload DevTools as prompted.</p>\n<p>Next, open the options for the Chrome WebAssembly Debugging extension:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b703b7b9cc9803e8cf705673444e2d6060908293.png\" alt=\"Wasm Debugging Options\" data-base62-sha1=\"q71ldlCRIO5znMxwCwBuKvkCleH\" width=\"527\" height=\"383\"></p>\n<p>Since itk-wasm builds in a clean Docker environment, the debugging source paths in the Docker environment are different from the paths on the host system. The debugging extension has a path substitution system that can account for these differences. In the Docker image, the directory where <code>itk-wasm</code> is invoked is mounted as <code>/work</code>. Substitute <code>/work</code> with the directory where the <code>itk-wasm</code> CLI is invoked. For example, if <code>itk-wasm</code> was invoked at <code>/home/matt/src/itk-wasm/examples/Debugging</code>, then set the path substitution as shown below:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15.png\" data-download-href=\"/uploads/short-url/pkEGg1S9dGlGZWH2cjwWUl68di5.png?dl=1\" title=\"Path substitution\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15_2_690x415.png\" alt=\"Path substitution\" data-base62-sha1=\"pkEGg1S9dGlGZWH2cjwWUl68di5\" width=\"690\" height=\"415\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15_2_690x415.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15_2_1035x622.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b18c1a5f08208cf951921b631f8bc53f9149ac15.png 2x\" data-dominant-color=\"F0F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Path substitution</span><span class=\"informations\">1278\u00d7770 95.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Build the project with itk-wasm and the <code>Debug</code> <code>CMAKE_BUILD_TYPE</code> to include DWARF debugging information:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png\" data-download-href=\"/uploads/short-url/bgYuIjkw1VnTOuRQzsXtkehkf0v.png?dl=1\" title=\"Emscripten build Debug\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png\" alt=\"Emscripten build Debug\" data-base62-sha1=\"bgYuIjkw1VnTOuRQzsXtkehkf0v\" width=\"690\" height=\"192\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_690x192.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b_2_1035x288.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f03298d9c0e447837b7497d86eefab8e8ca118b.png 2x\" data-dominant-color=\"090909\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Emscripten build Debug</span><span class=\"informations\">1138\u00d7318 34.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here we load and run the WebAssembly with a simple HTML file and server:</p>\n<pre><code class=\"lang-html\">&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/itk-wasm@1.0.0-a.11/dist/umd/itk-wasm.js\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p&gt;This is an example to demonstrate browser-based debugging of\n    C++-generated WebAssembly. For more information, please see the\n    &lt;a target=\"_blank\" href=\"https://wasm.itk.org/examples/debugging.html\"&gt;associated\n      documentation&lt;/a&gt;.&lt;/p&gt;\n\n    &lt;script&gt;\n      window.addEventListener('load', (event) =&gt; {\n        const pipeline = new URL('emscripten-build-debug/DebugMe', document.location)\n        itk.runPipeline(null, pipeline)\n      });\n    &lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b.png\" data-download-href=\"/uploads/short-url/cbGXSkN2P2HM3VK7aK3MAL7Eerp.png?dl=1\" title=\"HTTP Server\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b_2_690x472.png\" alt=\"HTTP Server\" data-base62-sha1=\"cbGXSkN2P2HM3VK7aK3MAL7Eerp\" width=\"690\" height=\"472\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b_2_690x472.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/556c7682b43fa7249522b07983b0bc1f825a062b.png 2x\" data-dominant-color=\"0A0B09\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HTTP Server</span><span class=\"informations\">1010\u00d7692 85.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And we can debug the C++ code in Chrome\u2019s DevTools debugger alongside the executing JavaScript!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67.png\" data-download-href=\"/uploads/short-url/zFfTJxejJDIk3sRviOi3DMG4vOf.png?dl=1\" title=\"Debug C++ DevTools\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_690x400.png\" alt=\"Debug C++ DevTools\" data-base62-sha1=\"zFfTJxejJDIk3sRviOi3DMG4vOf\" width=\"690\" height=\"400\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_690x400.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_1035x600.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9f5fac52f1cb1cbbd1bee6be3b7e0acaff7ec67_2_1380x800.png 2x\" data-dominant-color=\"F3EFF0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Debug C++ DevTools</span><span class=\"informations\">1571\u00d7911 202 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"whats-next-5\" class=\"anchor\" href=\"#whats-next-5\"></a>What\u2019s Next</h2>\n<p>In our next post, we will provide a tutorial on how to generate JavaScript and TypeScript bindings for Node.js and the browser.</p>\n<p><strong>Enjoy ITK!</strong></p>"], "70119": ["<p>Hi Image.sc community,</p>\n<p>I am trying to create a \u2018ground-truth\u2019 data set for validating an Object classification workflow.</p>\n<p>I have an ROI with cell detections from Stardist and have created Points annotations to label the cells with different classes. I am wondering if there is a way to automatically transfer the class of the Points annotation to the Detection object underneath. Here is a simple example of what I want to do:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/1515dc5378fd44259863593f8cc0da6bdfa266e7.png\" alt=\"before\" data-base62-sha1=\"30wRvLFxtOwbGDwrf8DjDyQvyFF\" width=\"360\" height=\"313\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a3d15c475cc3687fbd80d09c93eb4c1aa30a737e.png\" alt=\"after\" data-base62-sha1=\"nncnAhYpwe3Z6pv60X3v3Xt3XwW\" width=\"320\" height=\"321\"></p>\n<p>Please let me know if there is additional information that would be helpful to tackle this.</p>\n<p>Thanks,<br>\nAustin</p>", "<p>Just to clarify, you are using groups of Points objects and not individual points?</p>", "<p>Hi <a class=\"mention\" href=\"/u/austin_edwards\">@Austin_Edwards</a>,<br>\nI have been sitting on some scripts for quite a while now that were intended for classifier validation. Perhaps they would be of some use to you. I have shared them privately a few times, but I am not sure if they were ever <em>really</em> used.</p>\n<p>Below I have included the standard blurb when I pass out the scripts and a link to a project containing the scripts and a small demo. It has been quite a while, so there may need to be some updates to the scripts. I believe <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> mentioned something specifically relating to Points objects.</p>\n<p>=========<br>\nHoping to get some feedback and or testing on the script. I acknowledge that this is somewhat of a rough start (and Trevor has already run through the roughest of it), and would appreciate ideas on streamlining it. I already plan to adjust the output so that there are \u201cfor project\u201d and \u201cper image\u201d output <em>options</em> rather than two scripts\u2026 once I get around to it, but any suggestions like that would be great!</p>\n<p>How to use:<br>\n<strong>Before Running Any Scripts</strong><br>\nCreate some \u201cRegion*\u201d annotations (the class can be changed in the scripts).<br>\nPlace Points objects with the correct classifications to label the \u201cground truth\u201d for each/most cells within a given annotation.<br>\nDo this throughout the project.</p>\n<p>Run the \u201cStore ground truth labels\u201d script \u201cfor project\u201d on <strong>each image that had test/validation data</strong> .<br>\nThis will remove all of the Annotation regions and Points objects so they do not get in the way or bias any other classifier training. The objects can be restored and edited with the \u201cRestore ground truth objects\u201d script. I recommend creating validation annotations on UNCLASSIFIED cells, in order to reduce bias.</p>\n<p>Once you have classified cells, run the \u201cCheck ground truth\u201d script whenever you want. It will both generate a rather terrible confusion matrix in the text output window, and also save a CSV file with the confusion matrix - probably the easier way to see the results. I have not needed to yet, but the script was written such that it should be relatively easy to stratify the results if needed (multiple classes of annotation for different regions, different ground truth creators, etc). There is currently a second \u201cCheck ground truth per image\u201d script in the project that gives one CSV file per image rather than one total CSV file across the project.</p>\n<p>Please ask if you have any questions or problems, or want clarification on part of the code. Pete was not so keen on my making this a public method for validation (he was worried about people misusing it and getting a false sense of confidence), so I never fully commented it.</p>\n<p>If you have thoughts on the code, I lean more heavily to the biology side than the CS side. I suspect none of this is written in the most efficient way possible.</p>\n<p>Currently I am only sharing it through Google Drive as part of a project using duplicates of the LuCa image - expect a ~100MB download due to the image. If you have download cap concerns, I can send the scripts separately. The project already has had the scripts run , so you can look at the output CSV files, use the Restore script to look at the annotations I made, or delete the contents of the testSetPoints folder and create your own training.<br>\nThe \u201cnull\u201d rows in the CSV files are due to there being extra copies of images that were unclassified within the project, so all of those cells were null.</p>\n<p><a href=\"https://drive.google.com/file/d/1cUmoZ2Tx5At1dsfqlASAU3O3ryPYmhuK/view?usp=sharing\">drive.google.com</a></p>\n<h3>\n<a name=\"classifier-validationziphttpsdrivegooglecomfiled1cumoz2tx5at1dsfqlasau3o3rypymhukviewuspsharing-1\" class=\"anchor\" href=\"#classifier-validationziphttpsdrivegooglecomfiled1cumoz2tx5at1dsfqlasau3o3rypymhukviewuspsharing-1\"></a><a href=\"https://drive.google.com/file/d/1cUmoZ2Tx5At1dsfqlASAU3O3ryPYmhuK/view?usp=sharing\">Classifier validation.zip</a>\n</h3>\n<p>Google Drive file.</p>\n<p>Hopefully this is all still accurate still, from the message I copied this from. Let me know if you run into any issues!</p>\n<p>================<br>\nCheers,<br>\nMike</p>", "<p>Yes, that\u2019s correct. I have sets of points, which I generated using the points annotation/counting tool.</p>", "<p>Wow, this is awesome! Thanks for sharing these!</p>", "<p>Yeah, points are a little bit annoying that way. I don\u2019t know of a quick and clean way to label the cells based off of the points without splitting the Points annotation into individual points and then checking all of the cells against those individual points.</p>\n<p>If you had an individual point object for each, it would be fairly easy to use getObjectsForROI with just a few lines of code. Maybe <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> will have a clever way to do it. Otherwise there is the above, or converting your annotations into detections and then removing the detections (can use getObjectsForROI with the detections). The conversion has come up before due to needing to split points <a href=\"https://forum.image.sc/t/assign-point-objects-to-different-rois-by-overlap/26905/6\" class=\"inline-onebox\">Assign point objects to different ROIs by overlap - #6 by Research_Associate</a></p>", "<p>With some inspiration from the link <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> posted, the script below should work to transfer the Class of the points onto the cell detections.</p>\n<pre><code class=\"lang-auto\">def hierarchy = getCurrentHierarchy()\ndef points = getAnnotationObjects().findAll{it.getROI().isPoint()}\n\nfor (def pointGroup in points) {\n    def pathClass = pointGroup.getPathClass()\n    \n    for (def point in pointGroup.getROI().getAllPoints()) {\n        double x = point.getX()\n        double y = point.getY()\n        int z = 0 // assuming not Z stack\n        int t = 0 // assuming not time series\n\n        def cell = PathObjectTools.getObjectsForLocation(hierarchy, x, y, z, t, 0).findAll{it.isDetection()}\n        cell.each{it.setPathClass(pathClass)}\n    }\n}\nfireHierarchyUpdate()\n</code></pre>\n<p>This method checks for objects overlapping with each point, so you don\u2019t have to check all the objects if they contain the point or not.</p>", "<p>Ahhh I think the <code>getObjectsForLocation</code> function is exactly what I was looking for. Thank you all for the quick help!</p>", "<p>I know this was posted a while ago, but I am trying to implement the code posted by <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> and I am getting errors. When I run the entire script, I just get an empty confusion matrix (full of 0\u2019s) but no error. When I go to run selected portions to debug, it seems that there is an error with the line:</p>\n<pre><code class=\"lang-auto\">Set projectClassSet= gatherClassListFromProject(project1).sort()\n</code></pre>\n<p>As when I just run that line, I get the following error:</p>\n<pre><code class=\"lang-auto\">No such property: s for class: Check_ground_truth_per_image in Check ground truth per image.groovy at line number 1\n</code></pre>\n<p>I am not sure if anyone else has used this code and gotten it to work, but to be honest, I am quite a beginner and I am not sure how to fix this. Any help is appreciated.</p>", "<p>Hi <a class=\"mention\" href=\"/u/youngolaf\">@Youngolaf</a>,</p>\n<p>Sorry, it has been a while and no one really expressed interest in the code, so I have not done much with it. When testing the code on Validation1 it seemed to work for me once I changed the \u201cproject\u201d to \u201cdef project\u201d.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb2b95f9c8c6327ddee7032861b59068e282a82c.png\" data-download-href=\"/uploads/short-url/zPXe5OHCYJMsw61v5nov4s6CaQY.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb2b95f9c8c6327ddee7032861b59068e282a82c_2_669x500.png\" alt=\"image\" data-base62-sha1=\"zPXe5OHCYJMsw61v5nov4s6CaQY\" width=\"669\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb2b95f9c8c6327ddee7032861b59068e282a82c_2_669x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb2b95f9c8c6327ddee7032861b59068e282a82c_2_1003x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb2b95f9c8c6327ddee7032861b59068e282a82c_2_1338x1000.png 2x\" data-dominant-color=\"3F4244\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1849\u00d71381 276 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Validation1 is the image where the ground truth labels have already been stored, and so there are no additional Region* annotations. For Validation2, you would need to run the storage script first, and then generate the ground truth confusion matrix.</p>\n<p>The last two images have not been classified and are for the user to play with if they want to start over. I would expect an empty confusion matrix there since there are no classes.</p>\n<p>Otherwise, it is not clear which script you are testing on what image, and what steps you took before, making it difficult to troubleshoot.</p>\n<p>Cheers,<br>\nMike</p>", "<p>I will try to explain what I did to try to make it easier to follow.</p>\n<ol>\n<li>I went in and using the Counting button, put in the ground truth points</li>\n<li>I then saved the ground truth, and the dots went away</li>\n<li>I double checked that it worked by using the restore script, and they came back</li>\n<li>I then loaded a pre-trained classifier</li>\n<li>I ran the \u201cCheck ground truth per image.groovy\u201d script, but this was after the points had been saved (they are not showing)</li>\n</ol>\n<p>I was doing this all on the same image, and not having them broken into two, is that required?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d22dead44e446960a50bb09fb1c359c846c6dfd5.jpeg\" data-download-href=\"/uploads/short-url/tZkE3Q40wB3F7vzu1CJUsCpztvD.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d22dead44e446960a50bb09fb1c359c846c6dfd5_2_690x287.jpeg\" alt=\"image\" data-base62-sha1=\"tZkE3Q40wB3F7vzu1CJUsCpztvD\" width=\"690\" height=\"287\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d22dead44e446960a50bb09fb1c359c846c6dfd5_2_690x287.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d22dead44e446960a50bb09fb1c359c846c6dfd5_2_1035x430.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d22dead44e446960a50bb09fb1c359c846c6dfd5_2_1380x574.jpeg 2x\" data-dominant-color=\"2D2F3B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7800 214 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Do you know if there is a way that I can run a select few lines without it throwing an error? I\u2019ve used MATLAB before and that feature was very helpful in debugging. However, whenever I try to do that here, I keep getting errors.</p>", "<p>Can you provide a screenshot of the points and the hierarchy? It looks like you have 4 classes of cells, but haven\u2019t saved any classified points, though it is hard to be sure.</p>\n<p>If you only want to run certain lines, you can use Run selected code from the Run menu, but it usually doesnt accomplish much since it will not reference any objects before or after those lines. I do not believe there is anything like \u201cstepping through the code one line at a time\u201d if that is what you are asking about.  MATLAB stores variables persistently, and is a bit different than this kind of software.</p>\n<aside class=\"quote no-group\" data-username=\"Youngolaf\" data-post=\"9\" data-topic=\"70119\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png\" class=\"avatar\"> Youngolaf:</div>\n<blockquote>\n<p>When I go to run selected portions to debug, it seems that there is an error with the line:</p>\n<pre><code class=\"lang-auto\">Set projectClassSet= gatherClassListFromProject(project1).sort()\n</code></pre>\n<p>As when I just run that line, I get the following error:</p>\n</blockquote>\n</aside>\n<p>If you\u2019ve only run selected line that function will not be defined, as it is defined later in the script.</p>\n<p>Script does seem to work in the sample project provided, though there is an error with the \u201cproject\u201d line.</p>", "<aside class=\"quote no-group\" data-username=\"Youngolaf\" data-post=\"11\" data-topic=\"70119\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png\" class=\"avatar\"> Youngolaf:</div>\n<blockquote>\n<p>was doing this all on the same image, and not having them broken into two, is that required?</p>\n</blockquote>\n</aside>\n<p>That is only required if you want to go back and potentially edit the classifier based on the results of the confusion matrix. Generally speaking, it would be best to train a classifier on a variety of images, then test on a subset of different images from the same project. Training and testing on the same image will lead to less robust classifiers.</p>\n<p>The classifications for the Point objects and the cells do need to have the exact same class names, character for character. Occasionally situations have come up where users will label the points objects, but not classify them.</p>", "<p>Here is the hierarchy when I use the \u201cRestore ground truth\u201d so that the points appear. If I don\u2019t do this, there are no points and only the \u201cTest\u201d region.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/6/16cde59efb70a07ba39a4967ec79849807a2537e.png\" data-download-href=\"/uploads/short-url/3fJDsSKbgTLtKCW7yvHhtTQURSS.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16cde59efb70a07ba39a4967ec79849807a2537e_2_556x500.png\" alt=\"image\" data-base62-sha1=\"3fJDsSKbgTLtKCW7yvHhtTQURSS\" width=\"556\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16cde59efb70a07ba39a4967ec79849807a2537e_2_556x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16cde59efb70a07ba39a4967ec79849807a2537e_2_834x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16cde59efb70a07ba39a4967ec79849807a2537e_2_1112x1000.png 2x\" data-dominant-color=\"232226\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1287\u00d71156 259 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I can\u2019t seem to figure it out since it seems like the cells are being class is being set by the classifier, but it is not comparing anything as everything is 0, including the Cells that don\u2019t have ground truth assigned.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3cbab9cafda565be75de7b2eb79ab2ad3647b27.png\" data-download-href=\"/uploads/short-url/rW5t2NE2mJRwKItPFv2tjKhhJe7.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3cbab9cafda565be75de7b2eb79ab2ad3647b27.png\" alt=\"image\" data-base62-sha1=\"rW5t2NE2mJRwKItPFv2tjKhhJe7\" width=\"325\" height=\"500\" data-dominant-color=\"3C3F41\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">382\u00d7587 15.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Thank you, that does look correct. The classes for the points are in the parentheses and the names will not be used.</p>\n<p>I still do not see what the issue is in your case - could you turn on (uncomment) all of the print statements in the script and provide the full output as text?</p>\n<pre><code class=\"lang-auto\">    trainingAreas = hierarchy.getAnnotationObjects().findAll{it.getPathClass() == getPathClass(testClass)}\n    //print trainingAreas\n    //print \"outside function object count\"+ hierarchy.getDetectionObjects().findAll{it.isDetection() &amp;&amp; it.getMeasurementList().containsNamedMeasurement(\"TestObject\")}.size()\n    //print \"trainingArea count\" + trainingAreas.size()\n</code></pre>\n<p>It would help to see if the trainingAreas are being picked up among other useful information for debugging.<br>\nFor example, on the test project Validation1, the output would be:</p>\n<pre><code class=\"lang-auto\">WARN: PathObject using 'object_type' property - this should be updated to 'objectType'\nWARN: addPathObjects(Collection) is deprecated - use addObjects(Collection) instead\nINFO: ClassList size 4\nINFO: []\nINFO: outside function object count0\nINFO: trainingArea count0\nINFO: Training set has 0 cells in testing areas\nINFO: Writing object hierarchy with 5633 object(s)...\nINFO: Image data written in 0.07 seconds\nINFO: [Annotation (Region*)]\nINFO: outside function object count63\nINFO: trainingArea count1\nINFO: 102\nINFO: Validation1 has 102 cells in testing areas\nINFO: Immune cells\nINFO: value2 1\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 1\nINFO: Immune cells\nINFO: value2 2\nINFO: Tumor\nINFO: value2 1\nINFO: Tumor\nINFO: value2 2\nINFO: Immune cells\nINFO: Tumor\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 3\nINFO: Immune cells\nINFO: value2 2\nINFO: Immune cells\nINFO: Immune cells\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 3\nINFO: Immune cells\nINFO: Immune cells\nINFO: Immune cells\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 1\nINFO: Stroma\nINFO: value2 1\nINFO: Immune cells\nINFO: value2 4\nINFO: Immune cells\nINFO: value2 3\nINFO: Immune cells\nINFO: value2 4\nINFO: Immune cells\nINFO: value2 5\nINFO: Tumor\nINFO: value2 4\nINFO: Immune cells\nINFO: value2 5\nINFO: Immune cells\nINFO: value2 6\nINFO: Tumor\nINFO: Immune cells\nINFO: Tumor\nINFO: Immune cells\nINFO: value2 7\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 5\nINFO: Immune cells\nINFO: value2 6\nINFO: Immune cells\nINFO: value2 8\nINFO: Immune cells\nINFO: value2 7\nINFO: Immune cells\nINFO: value2 8\nINFO: Immune cells\nINFO: value2 9\nINFO: Immune cells\nINFO: value2 10\nINFO: Immune cells\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 11\nINFO: Tumor\nINFO: value2 6\nINFO: Immune cells\nINFO: value2 12\nINFO: Tumor\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 13\nINFO: Tumor\nINFO: value2 7\nINFO: Immune cells\nINFO: value2 9\nINFO: Tumor\nINFO: Tumor\nINFO: Tumor\nINFO: Stroma\nINFO: Immune cells\nINFO: value2 10\nINFO: Immune cells\nINFO: value2 14\nINFO: Tumor\nINFO: value2 8\nINFO: Immune cells\nINFO: Tumor\nINFO: Tumor\nINFO: value2 9\nINFO: Tumor\nINFO: value2 10\nINFO: Tumor\nINFO: Tumor\nINFO: value2 11\nINFO: Immune cells\nINFO: value2 11\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 12\nINFO: Tumor\nINFO: Immune cells\nINFO: value2 15\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 12\nINFO: Immune cells\nINFO: value2 13\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 13\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 14\nINFO: Immune cells\nINFO: value2 14\nINFO: Immune cells\nINFO: value2 16\nINFO: Immune cells\nINFO: value2 17\nINFO: Tumor\nINFO: value2 15\nINFO: Immune cells\nINFO: value2 18\nINFO: Stroma\nINFO: value2 1\nINFO: Immune cells\nINFO: Tumor\nINFO: Tumor\nINFO: value2 16\nINFO: Tumor\nINFO: value2 17\nINFO: Immune cells\nINFO: value2 19\nINFO: Immune cells\nINFO: value2 20\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 15\nINFO: Immune cells\nINFO: value2 21\nINFO: Immune cells\nINFO: value2 22\nINFO: Immune cells\nINFO: Tumor\nINFO: Tumor\nINFO: value2 18\nINFO: Tumor\nINFO: value2 19\nINFO: Immune cells\nINFO: value2 23\nINFO: Immune cells\nINFO: value2 24\nINFO: Immune cells\nINFO: value2 16\nINFO: Immune cells\nINFO: Immune cells\nINFO: Immune cells\nINFO: value2 25\nINFO: Tumor\nINFO: Writing object hierarchy with 5624 object(s)...\nINFO: Image data written in 0.04 seconds\nINFO: [Annotation (Region*)]\nINFO: outside function object count73\nINFO: trainingArea count1\nINFO: 86\nINFO: Validation2 has 86 cells in testing areas\nINFO: Immune cells\nINFO: value2 26\nINFO: Immune cells\nINFO: value2 27\nINFO: Immune cells\nINFO: value2 28\nINFO: Tumor\nINFO: Immune cells\nINFO: value2 29\nINFO: Stroma\nINFO: value2 2\nINFO: Stroma\nINFO: value2 3\nINFO: Stroma\nINFO: value2 2\nINFO: Stroma\nINFO: value2 4\nINFO: Tumor\nINFO: value2 20\nINFO: Stroma\nINFO: value2 5\nINFO: Stroma\nINFO: value2 6\nINFO: Stroma\nINFO: value2 3\nINFO: Stroma\nINFO: value2 7\nINFO: Tumor\nINFO: value2 2\nINFO: Immune cells\nINFO: value2 30\nINFO: Stroma\nINFO: Tumor\nINFO: value2 21\nINFO: Immune cells\nINFO: value2 31\nINFO: Tumor\nINFO: value2 1\nINFO: Stroma\nINFO: value2 4\nINFO: Tumor\nINFO: Immune cells\nINFO: value2 32\nINFO: Stroma\nINFO: value2 8\nINFO: Stroma\nINFO: value2 9\nINFO: Tumor\nINFO: value2 22\nINFO: Stroma\nINFO: value2 10\nINFO: Stroma\nINFO: value2 11\nINFO: Stroma\nINFO: value2 12\nINFO: Tumor\nINFO: value2 23\nINFO: Tumor\nINFO: value2 24\nINFO: Stroma\nINFO: value2 13\nINFO: Stroma\nINFO: value2 14\nINFO: Stroma\nINFO: value2 5\nINFO: Tumor\nINFO: Stroma\nINFO: value2 15\nINFO: Immune cells\nINFO: value2 33\nINFO: Stroma\nINFO: value2 16\nINFO: Stroma\nINFO: value2 17\nINFO: Tumor\nINFO: value2 25\nINFO: Stroma\nINFO: value2 1\nINFO: Stroma\nINFO: value2 18\nINFO: Stroma\nINFO: Stroma\nINFO: value2 19\nINFO: Immune cells\nINFO: value2 34\nINFO: Stroma\nINFO: value2 2\nINFO: Immune cells\nINFO: value2 17\nINFO: Stroma\nINFO: value2 20\nINFO: Tumor\nINFO: value2 26\nINFO: Stroma\nINFO: value2 21\nINFO: Tumor\nINFO: value2 27\nINFO: Stroma\nINFO: value2 22\nINFO: Tumor\nINFO: value2 2\nINFO: Stroma\nINFO: value2 6\nINFO: Immune cells\nINFO: value2 35\nINFO: Tumor\nINFO: value2 28\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 29\nINFO: Immune cells\nINFO: value2 36\nINFO: Stroma\nINFO: value2 23\nINFO: Stroma\nINFO: value2 24\nINFO: Stroma\nINFO: value2 25\nINFO: Tumor\nINFO: value2 30\nINFO: Tumor\nINFO: value2 31\nINFO: Tumor\nINFO: value2 32\nINFO: Stroma\nINFO: value2 26\nINFO: Immune cells\nINFO: value2 37\nINFO: Stroma\nINFO: value2 27\nINFO: Immune cells\nINFO: value2 38\nINFO: Tumor\nINFO: Stroma\nINFO: Immune cells\nINFO: value2 39\nINFO: Stroma\nINFO: value2 28\nINFO: Immune cells\nINFO: Tumor\nINFO: value2 3\nINFO: Stroma\nINFO: value2 29\nINFO: Stroma\nINFO: Tumor\nINFO: value2 33\nINFO: Stroma\nINFO: value2 30\nINFO: Immune cells\nINFO: value2 40\nINFO: Stroma\nINFO: Tumor\nINFO: Stroma\nINFO: Immune cells\nINFO: value2 41\nINFO: Tumor\nINFO: value2 3\nINFO: Stroma\nINFO: Writing object hierarchy with 5624 object(s)...\nINFO: Image data written in 0.04 seconds\nINFO: []\nINFO: outside function object count0\nINFO: trainingArea count0\nINFO: LuCa-7color_[13860,52919]_1x1component_data.tif - resolution #1 - FoxP3 (Opal 570) has 0 cells in testing areas\nINFO: Writing object hierarchy with 5624 object(s)...\nINFO: Image data written in 0.04 seconds\nINFO: []\nINFO: outside function object count0\nINFO: trainingArea count0\nINFO: LuCa-7color_[13860,52919]_1x1component_data.tif - resolution #1 - CD68 (Opal 620) has 0 cells in testing areas\nINFO: Writing object hierarchy with 5624 object(s)...\nINFO: Image data written in 0.03 seconds\nINFO: [null, Immune cells, Stroma, Tumor]\nINFO: Confusion Matrix \n\nINFO: [0, 0, 0, 0]\nINFO: [0, 41, 6, 3]\nINFO: [0, 17, 30, 3]\nINFO: [0, 0, 2, 33]\nINFO: Cells in testing areas that do not have ground truths assigned: 53\nINFO:  \nINFO: Test counts - how well balanced is your testing data?\nINFO: null   0\nINFO: Immune cells   50\nINFO: Stroma   50\nINFO: Tumor   35\nINFO:  \nINFO: Output saved in project folder as Project_output.csv\n\n</code></pre>", "<p>It seems like most of it is being counted as outside the function. This was the output:</p>\n<pre><code class=\"lang-auto\">INFO: ClassList size 4\nINFO: []\nINFO: outside function object count900\nINFO: trainingArea count0\nINFO: Writing object hierarchy with 902 object(s)...\nINFO: Image data written in 0.01 seconds\nINFO: [Endocrine, Exocrine, Not Stained, Vasculature]\nINFO: Confusion Matrix \n\nINFO: [0, 0, 0, 0]\nINFO: [0, 0, 0, 0]\nINFO: [0, 0, 0, 0]\nINFO: [0, 0, 0, 0]\nINFO: Cells in testing areas that do not have ground truths assigned: 0\nINFO:  \nINFO: Test counts - how well balanced is your testing data?\nINFO: Endocrine   0\nINFO: Exocrine   0\nINFO: Not Stained   0\nINFO: Vasculature   0\nINFO:  \nINFO: Output saved in project folder as ROI5_Test_Islet.tif_output.csv\nINFO: Result: [ROI5_Test_Islet.tif, ROI5_Test_Islet.tif (1)]\n</code></pre>", "<p>It would help to have all of the print statements, but that is a start. *Oops, the 900 outside function object count being 900 is correct, however you have no training areas, so the confusion matrix being all 0s makes sense. Why you have no training areas is probably the main question. Training areas are loaded from the stored training annotations/ROIs, and should have the same class as the variable used to check for them in the first few lines of code, <code>testClass</code></p>\n<aside class=\"quote no-group\" data-username=\"Youngolaf\" data-post=\"16\" data-topic=\"70119\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png\" class=\"avatar\"> Youngolaf:</div>\n<blockquote>\n<p><code>INFO: trainingArea count0</code></p>\n</blockquote>\n</aside>\n<p>In retrospect, and greater awareness of other QuPath functions, the comparison could have been <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/objects/PathObjectTools.html#getObjectsForLocation(qupath.lib.objects.hierarchy.PathObjectHierarchy,double,double,int,int,double)\">done more easily using X,Y, coordinates</a> without creating temporary objects, but at the time it was easier to think in and use objects.</p>\n<p>Updated for relearning my own code.</p>", "<p>Looking at the provided images again, it looks like the classes for the annotations might be backwards. Sorry if that was not clear in the text. By default, the areas that are manually labeled should have the class Region*. If you store the  objects using \u201cTest\u201d as the class, then you would need to change Region* in the <code>testClass</code> variable to <code>Test</code></p>", "<p>I am still confused. I tried changing the variable in the code but it still isnt working. I also uncommented all the print statements and I\u2019m getting the same output (weird).</p>\n<p>You said that the 900 outside is correct, and that the training area should be &gt; 0, but in the Validation1 output, it has a 0 outside and a 0 training area:</p>\n<pre><code class=\"lang-auto\">outside function object count0\nINFO: trainingArea count0\n</code></pre>\n<p>I really appreciate your help!</p>", "<aside class=\"quote no-group\" data-username=\"Youngolaf\" data-post=\"19\" data-topic=\"70119\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png\" class=\"avatar\"> Youngolaf:</div>\n<blockquote>\n<p>I am still confused. I tried changing the variable in the code but it still isnt working. I also uncommented all the print statements and I\u2019m getting the same output (weird).</p>\n</blockquote>\n</aside>\n<p>Haha, I am confused too then. It sounds like you are not running the script. Even adding a <code>print \"Hi\"</code> at the beginning of the script should change the output to reflect the added print statement.</p>\n<aside class=\"quote no-group\" data-username=\"Youngolaf\" data-post=\"19\" data-topic=\"70119\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/a8b319/40.png\" class=\"avatar\"> Youngolaf:</div>\n<blockquote>\n<p>You said that the 900 outside is correct, and that the training area should be &gt; 0,</p>\n</blockquote>\n</aside>\n<p>Yes, the script cycles through all images in the project. The first image, \u201cTraining set\u201d has no objects in testing areas, it was where the classifier was trained. The second image, \u201cValidation 1\u201d has 102<br>\ncells in testing areas. The training area count is 1, as listed in the output above.</p>\n<p>Have you been able to get the scripts to work for the test project?</p>"], "78311": ["<p>Just downloaded imageJ and I am getting the following error while trying to run template match -cvMatch. Kindly help.</p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 38MB of 12097MB (&lt;1%)<br>\njava.lang.UnsatisfiedLinkError: no jniopencv_core in java.library.path<br>\nat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1875)<br>\nat java.lang.Runtime.loadLibrary0(Runtime.java:872)<br>\nat java.lang.System.loadLibrary(System.java:1124)<br>\nat org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:1543)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1192)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1042)<br>\nat org.bytedeco.javacpp.opencv_core.(opencv_core.java:10)<br>\nat java.lang.Class.forName0(Native Method)<br>\nat java.lang.Class.forName(Class.java:348)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1109)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1058)<br>\nat TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:232)<br>\nat TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:219)<br>\nat TemplateMatching.cvMatch_Template.run(cvMatch_Template.java:91)<br>\nat ij.IJ.runUserPlugIn(IJ.java:237)<br>\nat ij.IJ.runPlugIn(IJ.java:203)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.lang.Thread.run(Thread.java:750)<br>\nCaused by: java.lang.UnsatisfiedLinkError: no opencv_core2410 in java.library.path<br>\nat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1875)<br>\nat java.lang.Runtime.loadLibrary0(Runtime.java:872)<br>\nat java.lang.System.loadLibrary(System.java:1124)<br>\nat org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:1543)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1143)<br>\n\u2026 14 more</p>", "<p>Current Error below</p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 76MB of 12097MB (&lt;1%)</p>\n<p>java.lang.NoClassDefFoundError: Could not initialize class org.bytedeco.javacpp.opencv_core<br>\nat java.lang.Class.forName0(Native Method)<br>\nat java.lang.Class.forName(Class.java:348)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1109)<br>\nat org.bytedeco.javacpp.Loader.load(Loader.java:1058)<br>\nat TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:232)<br>\nat TemplateMatching.cvMatch_Template.doMatch(cvMatch_Template.java:219)<br>\nat TemplateMatching.cvMatch_Template.run(cvMatch_Template.java:91)<br>\nat ij.IJ.runUserPlugIn(IJ.java:237)<br>\nat ij.IJ.runPlugIn(IJ.java:203)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.lang.Thread.run(Thread.java:750)</p>"], "29164": ["<p>Dear All,</p>\n<p>I know that it is possible to use manually Trackmate as it is explained here:<a href=\"https://imagej.net/Scripting_TrackMate#Export_spot.2C_edge_and_track_numerical_features_after_tracking\" rel=\"nofollow noopener\">https://imagej.net/Scripting_TrackMate#Export_spot.2C_edge_and_track_numerical_features_after_tracking</a></p>\n<p>but in my case I have already detected all the spots in a 2d+T stack and i would like to track them. Do i have to just skip the DetectorFactory in settings by inserting my own detected spot as written in the scripting Trackmate page and configure only the trackerFactory ?<br>\nCould you send me an example how to proceed if it is different of what I am suggesting\u2026<br>\nThanks by advance</p>\n<p>regards<br>\nPhilippe</p>", "<p>Hi <a class=\"mention\" href=\"/u/philgi\">@philgi</a></p>\n<p>I am not sure how you can do that from within the GUI.<br>\nWhere do the spots come from?<br>\nIf they come from a CSV file, you can use the TrackMate CSV importer.</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"10075\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tinevez/40/21_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/a-new-plugin-to-import-csv-files-into-trackmate/10075\">A new plugin to import CSV files into TrackMate</a> <a class=\"badge-wrapper  bullet\" href=\"/c/announcements/10\"><span class=\"badge-category-bg\" style=\"background-color: #AB9364;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for announcements, such as new software releases and upcoming community events.\">Announcements</span></a>\n  </div>\n  <blockquote>\n    I just released and push a new plugin that can import CSV files into TrackMate. \nCheck this page for information: \n \n \nHere is a primer of its content: \nCSV to TrackMate importer.\nThis plugin allows for importing detections and tracks contained in CSV files into TrackMate, or to export them as TrackMate XML file in headless mode. \nInstallation.\nThis plugin lives on a separate update site]].  you want to use it, you first need to subscribe to the update site named TrackMateCSVImporter, as explain\u2026\n  </blockquote>\n</aside>\n\n<p>Otherwise, I would use a Jython script.<br>\nAdd the spots to the <code>SpotCollection</code> and track them with TrackMate. Just adapt the Jython script <a href=\"https://imagej.net/Scripting_TrackMate#A_full_example\" class=\"inline-onebox\">Scripting TrackMate</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a></p>\n<p>sorry maybe my explanations were not clear\u2026<br>\nI would like to track the mouvement of cells  but I cannot use the spot Detector of Trackmate because only the cell membrane is labeled. However I have managed to analyse them and to segment each cell separately.<br>\nNow I can measure the center of mass of each segmented region (a segmented region corresponds to one cell) in order to create a SpotCollection of each of these center of mass and to track them with Trackmate.<br>\nI am using the Particle Analyzer to get the center of mass (array of Xm and Ym) of the corresponding frame (array of Tm) and I create the SpotCollection:</p>\n<pre><code>spots = SpotCollection()\nfor j in range(len(Tm)):\n   spots.add(Spot(Xm[j], Ym[j], 0, 1, -1), Tm[j])\n</code></pre>\n<p>After in the ExampleScript_3.py that you have provided on GitHub, I just have to skip the 2 lines</p>\n<pre><code>settings.detectorFactory = ...\nsettings.detectorSettings = ...\n</code></pre>\n<p>and to replace them by</p>\n<pre><code>model.setSpots(spots,False)\n</code></pre>\n<p>That\u2019s right ???</p>\n<p>regards<br>\nPhilippe</p>", "<p>This is a great idea!</p>\n<p>I have never done it before, and I would be glad to work on a script that does exactly this. Would you mind posting the data somewhere, as so that I can run what you did on my side?</p>\n<p>I would create a Jython plugin that does the tracking part and add it to the TrackMate wiki pages, if that is ok.</p>", "<p>I received the stack. Thanks! It looks great!</p>\n<p>Could you post here the draft of your script? I will start from that,</p>", "<p>Hi <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a><br>\nwith the data I sent you, only the tracking part is missing, I have try the following code but it does not work</p>\n<pre><code class=\"lang-python\">    def process_im(imp_):\n\trt = ResultsTable()\n\trt.reset\n\tp = PA(PA.SHOW_RESULTS, PA.STACK_POSITION+PA.CENTER_OF_MASS, rt, 10, MAXSIZE)\n\tfor i in range(imp_.getStackSize()):\n\t\timp_.setSliceWithoutUpdate(i+1)\n\t\tp.analyze(imp_)\n\tTm = rt.getColumnAsDoubles(rt.getColumnIndex(\"Slice\"))\n\tXm = rt.getColumnAsDoubles(rt.getColumnIndex(\"XM\"))\n\tYm = rt.getColumnAsDoubles(rt.getColumnIndex(\"YM\"))\n\tspots_ = SpotCollection()\n\tfor j in range(len(Tm)):\n\t\tspots_.add(Spot(Xm[j], Ym[j], 0, 1, -1), Double.intValue(Tm[j]))\n\treturn spots_\n    #------------------------------\n    imp = WindowManager.getCurrentImage()\n    spots = SpotCollection()\n    spots = process_im(imp)\n    #  TrackMate\n    model = Model()\n    model.setLogger(Logger.IJ_LOGGER)\n    settings = Settings()\n    settings.setFrom(imp)\n     \n    # Configure SpotCollection\n    model.setSpots(spots,False)\n    # Configure tracker\n    settings.trackerFactory = SparseLAPTrackerFactory()\n    settings.trackerSettings = LAPUtils.getDefaultLAPSettingsMap()\n    settings.trackerSettings['LINKING_MAX_DISTANCE'] = 10.0\n    settings.trackerSettings['GAP_CLOSING_MAX_DISTANCE']=10.0\n    settings.trackerSettings['MAX_FRAME_GAP']= 3\n\n    settings.addTrackAnalyzer(TrackSpeedStatisticsAnalyzer())\n  \n    settings.initialSpotFilterValue = 1\n  \n    print(str(settings))\n    trackmate = TrackMate(model, settings)\n    ok = trackmate.checkInput()\n    if not ok :\n\tsys.exit(str(trackmate.getErrorMessage()))\n    \n    ok = trackmate.process()\n    if not ok:\n\tsys.exit(str(trackmate.getErrorMessage()))\n    \n    model.getLogger().log('Found ' + str(model.getTrackModel().nTracks(True)) + ' tracks.')\n   \n    fm = model.getFeatureModel()\n\n\n    rt_exist = WindowManager.getWindow(\"TrackMate Results\")\n    if rt_exist==None or not isinstance(rt_exist, TextWindow):\n         table= ResultsTable()\n    else:\n         table = rt_exist.getTextPanel().getOrCreateResultsTable()\n\n\n    table.reset\n\n    for id in model.getTrackModel().trackIDs(True):\n\tv = fm.getTrackFeature(id, 'TRACK_MEAN_SPEED')\n\tmodel.getLogger().log('')\n\tmodel.getLogger().log('Track ' + str(id) + ': mean velocity = ' + str(v) + ' ' + model.getSpaceUnits() + '/' + model.getTimeUnits())\n\ttrack = model.getTrackModel().trackSpots(id)\n\t\n\tfor spot in track:\n\t\tsid = spot.ID()\n\t\t# Fetch spot features directly from spot. \n\t\tx=spot.getFeature('POSITION_X')\n\t\ty=spot.getFeature('POSITION_Y')\n\t\tt=spot.getFeature('FRAME')\n\t\tq=spot.getFeature('QUALITY')\n\t\tsnr=spot.getFeature('SNR')\n\t\tmean=spot.getFeature('MEAN_INTENSITY')\n\t\tmodel.getLogger().log('\\tspot ID = ' + str(sid) + ': x='+str(x)+', y='+str(y)+', t='+str(t)+', q='+str(q) + ', snr='+str(snr) + ', mean = ' + str(mean))\n\t\ttable.incrementCounter()\n\t\ttable.addValue(\"TRACK ID\", id)\n\t\ttable.addValue(\"SPOT ID\", sid)\n\t\ttable.addValue(\"POSITION_X\", x)\n\t\ttable.addValue(\"POSITION_Y\", y)\n\t\ttable.addValue(\"FRAME\", t)\n\t\ttable.addValue(\"QUALITY\", q)\n\t\ttable.addValue(\"SNR\", snr)\n\t\ttable.addValue(\"MEAN_INTENSITY\", mean)\n\n    table.show(\"TrackMate Results\")     \n</code></pre>\n<p>For the segmented analysis it is a little complicated because the code is not robust and have to manually work on it<br>\nthank you for your help</p>\n<p>regards<br>\nPhilippe</p>", "<p>Hi <a class=\"mention\" href=\"/u/philgi\">@philgi</a></p>\n<p>OK I got it!</p>\n<p>I am sorry, I had to start again from scratch. But here is what the script allows to do.</p>\n<p>You have to start from a 2D+T image (nothing else) and a results table that contains at least the center of mass <code>XM</code>, <code>YM</code>, the <code>slice</code> and the <code>Area</code> for the cells in the movie. The results table is typically generated from the ROI manager, that would contain the results of the particle analyzer.</p>\n<p>So an ideal starting situation would like this:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/cad44bb2f6a2fa89f1fc2500039dc9d1fd250cc3.png\" data-download-href=\"/uploads/short-url/sWji0yWHkdpxdnqYmFpoDNuE99N.png?dl=1\" title=\"TrackMateScriptBeforeCapture\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cad44bb2f6a2fa89f1fc2500039dc9d1fd250cc3_2_690x378.png\" alt=\"TrackMateScriptBeforeCapture\" data-base62-sha1=\"sWji0yWHkdpxdnqYmFpoDNuE99N\" width=\"690\" height=\"378\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cad44bb2f6a2fa89f1fc2500039dc9d1fd250cc3_2_690x378.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cad44bb2f6a2fa89f1fc2500039dc9d1fd250cc3_2_1035x567.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/cad44bb2f6a2fa89f1fc2500039dc9d1fd250cc3.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/cad44bb2f6a2fa89f1fc2500039dc9d1fd250cc3_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">TrackMateScriptBeforeCapture</span><span class=\"informations\">1272\u00d7697 96.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>this script will generate the following tracks:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3cee76938213f8d3e43ae1ca4cfbe3e4453546e.gif\" data-download-href=\"/uploads/short-url/rWcozMheWWlYmCg6f5JuJCPLJLw.gif?dl=1\" title=\"TrackMate%20capture%20of%20SegmentedStack-small-tracks\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c3cee76938213f8d3e43ae1ca4cfbe3e4453546e_2_690x180.gif\" alt=\"TrackMate%20capture%20of%20SegmentedStack-small-tracks\" data-base62-sha1=\"rWcozMheWWlYmCg6f5JuJCPLJLw\" width=\"690\" height=\"180\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c3cee76938213f8d3e43ae1ca4cfbe3e4453546e_2_690x180.gif, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3cee76938213f8d3e43ae1ca4cfbe3e4453546e.gif 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3cee76938213f8d3e43ae1ca4cfbe3e4453546e.gif 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c3cee76938213f8d3e43ae1ca4cfbe3e4453546e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">TrackMate%20capture%20of%20SegmentedStack-small-tracks</span><span class=\"informations\">694\u00d7182 2.23 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Cool no? The output can be controlled via a TrackMate GUI that will be shown upon running the script. Showing the GUI might not be desirable in batch mode, but from the GUI you can save your data, export to IJ tables and save to CSV, export a movie etc.</p>\n<p>The script also offers to color the ROIs by track ID, if you have the ROI manager that was used to create the results table. It looks like this:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/237d4f70b01b9d4fa596f89adea5c10fc481fbc0.gif\" data-download-href=\"/uploads/short-url/53XbD22MeJmeR4GgJUw0gOCt99S.gif?dl=1\" title=\"TrackMate%20capture%20of%20SegmentedStack-small\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/237d4f70b01b9d4fa596f89adea5c10fc481fbc0.gif\" alt=\"TrackMate%20capture%20of%20SegmentedStack-small\" data-base62-sha1=\"53XbD22MeJmeR4GgJUw0gOCt99S\" width=\"690\" height=\"180\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/3/237d4f70b01b9d4fa596f89adea5c10fc481fbc0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">TrackMate%20capture%20of%20SegmentedStack-small</span><span class=\"informations\">694\u00d7182 1.05 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nAlso cool no?</p>\n<p>So, here is the script. It is about 270 lines long.<br>\n<a class=\"mention\" href=\"/u/philgi\">@philgi</a> please tell me whether it works!</p>\n<pre><code class=\"lang-python\">import sys\nfrom math import pi\nfrom math import sqrt\nfrom random import shuffle\n\nfrom java.awt import Color\n\nfrom ij import WindowManager\nfrom ij.measure import ResultsTable\nfrom ij.plugin.frame import RoiManager\n\nfrom fiji.plugin.trackmate import Logger\nfrom fiji.plugin.trackmate import Model\nfrom fiji.plugin.trackmate import SelectionModel\nfrom fiji.plugin.trackmate import Settings\nfrom fiji.plugin.trackmate import Spot\nfrom fiji.plugin.trackmate import SpotCollection\nfrom fiji.plugin.trackmate import TrackMate\nfrom fiji.plugin.trackmate.detection import ManualDetectorFactory\nfrom fiji.plugin.trackmate.tracking import LAPUtils\nfrom fiji.plugin.trackmate.providers import SpotAnalyzerProvider\nfrom fiji.plugin.trackmate.providers import EdgeAnalyzerProvider\nfrom fiji.plugin.trackmate.providers import TrackAnalyzerProvider\nfrom fiji.plugin.trackmate.tracking.sparselap import SparseLAPTrackerFactory\nfrom fiji.plugin.trackmate.visualization.hyperstack import HyperStackDisplayer\nfrom fiji.plugin.trackmate.gui import TrackMateGUIController\nfrom org.jfree.chart.renderer.InterpolatePaintScale import Jet\n\n\n\n\n\n\ndef spots_from_results_table( results_table, frame_interval ):\n\t\"\"\" \n\tCreates a spot collection from a results table in ImageJ.\n\tRequires the current results table, in which the results from \n\tparticle analysis should be. We need at least the center\n\tof mass, the area and the slice to be specified there.\n\tWe also query the frame interval to properly generate the \n\tPOSITION_T spot feature.\n\t\"\"\"\n\t\n\tframes = results_table.getColumnAsDoubles( results_table.getColumnIndex( 'Slice' ) )\n\txs = results_table.getColumnAsDoubles( results_table.getColumnIndex( 'XM' ) )\n\tys = results_table.getColumnAsDoubles( results_table.getColumnIndex( 'YM' ) )\n\tz = 0.\n\t# Get radiuses from area.\n\tareas = results_table.getColumnAsDoubles( results_table.getColumnIndex( 'Area' ) )\n\tspots = SpotCollection()\n\n\tfor i in range( len( xs ) ):\n\t\tx = xs[ i ]\n\t\ty = ys[ i ]\n\t\tframe = frames[ i ]\n\t\tarea = areas[ i ]\n\t\tt = ( frame - 1 ) * frame_interval\n\t\tradius = sqrt( area / pi )\n\t\tquality = i # Store the line index, to later retrieve the ROI.\n\t\tspot = Spot( x, y, z, radius, quality )\n\t\tspot.putFeature( 'POSITION_T', t )\n\t\tspots.add( spot, int( frame ) )\n\t\t\n\treturn spots\n\n\ndef create_trackmate( imp, results_table ):\n\t\"\"\"\n\tCreates a TrackMate instance configured to operated on the specified\n\tImagePlus imp with cell analysis stored in the specified ResultsTable\n\tresults_table.\n\t\"\"\"\n\t\n\tcal = imp.getCalibration()\n\t\n\t# TrackMate.\n\t\n\t# Model.\n\tmodel = Model()\n\tmodel.setLogger( Logger.IJ_LOGGER )\n\tmodel.setPhysicalUnits( cal.getUnit(), cal.getTimeUnit() )\n\t\n\t# Settings.\n\tsettings = Settings()\n\tsettings.setFrom( imp )\n\t\n\t# Create the TrackMate instance.\n\ttrackmate = TrackMate( model, settings )\n\t\n\t# Add ALL the feature analyzers known to TrackMate, via\n\t# providers. \n\t# They offer automatic analyzer detection, so all the \n\t# available feature analyzers will be added. \n\t# Some won't make sense on the binary image (e.g. contrast)\n\t# but nevermind.\n\t\n\tspotAnalyzerProvider = SpotAnalyzerProvider()\n\tfor key in spotAnalyzerProvider.getKeys():\n\t\tprint( key )\n\t\tsettings.addSpotAnalyzerFactory( spotAnalyzerProvider.getFactory( key ) )\n\t\n\tedgeAnalyzerProvider = EdgeAnalyzerProvider()\n\tfor  key in edgeAnalyzerProvider.getKeys():\n\t\tprint( key )\n\t\tsettings.addEdgeAnalyzer( edgeAnalyzerProvider.getFactory( key ) )\n\t\n\ttrackAnalyzerProvider = TrackAnalyzerProvider()\n\tfor key in trackAnalyzerProvider.getKeys():\n\t\tprint( key )\n\t\tsettings.addTrackAnalyzer( trackAnalyzerProvider.getFactory( key ) )\n\t\n\ttrackmate.getModel().getLogger().log( settings.toStringFeatureAnalyzersInfo() )\n\ttrackmate.computeSpotFeatures( True )\n\ttrackmate.computeEdgeFeatures( True )\n\ttrackmate.computeTrackFeatures( True )\n\t\n\t# Skip detection and get spots from results table.\n\tspots = spots_from_results_table( results_table, cal.frameInterval )\n\tmodel.setSpots( spots, False )\n\t\n\t# Configure detector. We put nothing here, since we already have the spots \n\t# from previous step.\n\tsettings.detectorFactory = ManualDetectorFactory()\n\tsettings.detectorSettings = {}\n\tsettings.detectorSettings[ 'RADIUS' ] = 1.\n\t\n\t# Configure tracker\n\tsettings.trackerFactory = SparseLAPTrackerFactory()\n\tsettings.trackerSettings = LAPUtils.getDefaultLAPSettingsMap()\n\tsettings.trackerSettings[ 'LINKING_MAX_DISTANCE' ] \t\t= 10.0\n\tsettings.trackerSettings[ 'GAP_CLOSING_MAX_DISTANCE' ]\t= 10.0\n\tsettings.trackerSettings[ 'MAX_FRAME_GAP' ]\t\t\t\t= 3\n\t\n\tsettings.initialSpotFilterValue = -1.\n\n\treturn trackmate\n\n\n\ndef process( trackmate ):\n\t\"\"\"\n\tExecute the full process BUT for the detection step.\n\t\"\"\"\n\t# Check settings.\n\tok = trackmate.checkInput()\n\t# Initial filtering\n\tprint( 'Spot initial filtering' )\n\tok = ok and trackmate.execInitialSpotFiltering()\n\t# Compute spot features.\n\tprint( 'Computing spot features' )\n\tok = ok and trackmate.computeSpotFeatures( True ) \n\t# Filter spots.\n\tprint( 'Filtering spots' )\n\tok = ok and trackmate.execSpotFiltering( True )\n\t# Track spots.\n\tprint( 'Tracking' )\n\tok = ok and trackmate.execTracking()\n\t# Compute track features.\n\tprint( 'Computing track features' )\n\tok = ok and trackmate.computeTrackFeatures( True )\n\t# Filter tracks.\n\tprint( 'Filtering tracks' )\n\tok = ok and trackmate.execTrackFiltering( True )\n\t# Compute edge features.\n\tprint( 'Computing link features' )\n\tok = ok and trackmate.computeEdgeFeatures( True )\n\n\treturn ok\n\n\ndef display_results_in_GUI( trackmate ):\n\t\"\"\"\n\tCreates and show a TrackMate GUI to configure the display \n\tof the results. \n\n\tThis might not always be desriable in e.g. batch mode, but \n\tthis allows to save the data, export statistics in IJ tables then\n\tsave them to CSV, export results to AVI etc...\n\t\"\"\"\n\t\n\tgui = TrackMateGUIController( trackmate )\n\n\t# Link displayer and GUI.\n\t\n\tmodel = trackmate.getModel()\n\tselectionModel = SelectionModel( model)\n\tdisplayer = HyperStackDisplayer( model, selectionModel, imp )\n\tgui.getGuimodel().addView( displayer )\n\tdisplaySettings = gui.getGuimodel().getDisplaySettings()\n\tfor key in displaySettings.keySet():\n\t\tdisplayer.setDisplaySettings( key, displaySettings.get( key ) )\n\tdisplayer.render()\n\tdisplayer.refresh()\n\t\n\tgui.setGUIStateString( 'ConfigureViews' )\n\n\n\ndef color_rois_by_track( trackmate, rm ):\n\t\"\"\"\n\tColors the ROIs stored in the specified ROIManager rm using a color\n\tdetermined by the track ID they have.\n\t\n\tWe retrieve the IJ ROI that matches the TrackMate Spot because in the\n\tlatter we stored the index of the spot in the quality feature. This\n\tis a hack of course. On top of that, it supposes that the index of the \n\tROI in the ROIManager corresponds to the line in the ResultsTable it \n\tgenerated. So any changes to the ROIManager or the ResultsTable is \n\tlikely to break things.\n\t\"\"\"\n\tmodel = trackmate.getModel()\n\ttrack_colors = {}\n\ttrack_indices = [] \n\tfor i in model.getTrackModel().trackIDs( True ):\n\t\ttrack_indices.append( i )\n\tshuffle( track_indices )\n\t\n\tindex = 0\n\tfor track_id in track_indices:\n\t\tcolor = Jet.getPaint( float(index) / ( len( track_indices) - 1 ) )\n\t\ttrack_colors[ track_id ] = color\n\t\tindex = index + 1\n\t\n\tspots = model.getSpots()\n\tfor spot in spots.iterable( True ):\n\t\tq = spot.getFeature( 'QUALITY' ) # Stored the ROI id.\n\t\troi_id = int( q )\n\t\troi = rm.getRoi( roi_id )\n\t\n\t\t# Get track id.\n\t\ttrack_id = model.getTrackModel().trackIDOf( spot )\n\t\tif track_id is None:\n\t\t\tcolor = Color.GRAY\n\t\telse:\n\t\t\tcolor = track_colors[ track_id ] \n\t\t\t\n\t\troi.setFillColor( color )\n\n\n\n#------------------------------\n# \t\t\tMAIN \n#------------------------------\n\n# Get current image.\nimp = WindowManager.getCurrentImage()\n\n# Remove overlay if any.\nimp.setOverlay( None )\n\n# Get results table.\nresults_table = ResultsTable.getResultsTable()\n\n# Create TrackMate instance.\ntrackmate = create_trackmate( imp, results_table )\n\n#-----------------------\n# Process.\n#-----------------------\n\nok = process( trackmate )\nif not ok:\n\tsys.exit(str(trackmate.getErrorMessage()))\n\n#-----------------------\n# Display results.\n#-----------------------\n\n# Create the GUI and let it control display of results.\ndisplay_results_in_GUI( trackmate )\n\n# Color ROI by track ID!\nrm = RoiManager.getInstance()\ncolor_rois_by_track( trackmate, rm )\n\n\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a><br>\nSorry for the delay,<br>\nIt is perfect and exactly what I was looking for\u2026 I have already tried on my mac and it is working\u2026 I am still adjusting some parameter.<br>\nFor the GUI i have to think about that because the plan is to do the analysis in a batch of data.<br>\nFor the moment I am playing with your code\u2026 When everything will run perfectly I will post my code\u2026<br>\nThank you again for your help.</p>\n<p>regards<br>\nPhilippe</p>", "<p>This is one of the most useful features of TrackMate that I have seen. Would it be possible to implement it directly as a plug-in or as feature of TrackMate???</p>", "<p>Hello everybody,</p>\n<p>I\u2019ve modified <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a> script a bit, to make extraction of tracks and finding track\u2019s ROIs easier:</p>\n<ul>\n<li>added track ID/# to the Results table after tracking;</li>\n<li>ROIs in RoiManager are renamed in a following way: \u201ctr3_f_2_0302-3232\u201d, where tr3 = track ID <span class=\"hashtag\">#3</span><br>\nand f_2 = frame 2.</li>\n</ul>\n<p>Here is link to <a href=\"https://gist.github.com/ekatrukha/ac32d624665da81ad521ab4b040a4bad\" rel=\"nofollow noopener\">updated script</a>.</p>\n<p>Maybe it would be useful to somebody.</p>\n<p>P.S. In general, I think it would be maybe nice to ask for linking parameters (LINKING_MAX_DISTANCE, etc) in the beginning of the script, so it is a bit more flexible.</p>\n<p>Cheers,<br>\nEugene</p>", "<p>Cool!<br>\nCan I put it on the wiki instead of my version?</p>", "<p>Yes, of course!</p>\n<p>////</p>", "<p>Some more related discussions and new scripts also can be found in <a href=\"https://forum.image.sc/t/rois-to-trackmate/48918\">this thread</a></p>", "<p>Dear <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a></p>\n<p>I am using a Jython script to track cells. So, I have already done segmentation for the cells by a Python code and prepared a csv file of all spots for that and also would like to import the csv file to TrackMate. I know it is possible to use \u201cTrackMate CSV importer\u201d manually, but I\u2019m interested to import my csv file to the Jython script and then track the cells automatically, since I have many image files for tracking cells, so automation is key.</p>\n<p>I have also used \u201cimport csv\u201d and could open my csv file in the Jython script and read the segmentation data (spots) there, but I could not find any good way to build the \u201cSpotCollection()\u201d.<br>\nI wonder if there is any command line for \u201c TrackMate CSV importer\u201d to be used inside a Jython script?</p>\n<p>Thanks a lot for any help/suggestions!</p>", "<p>Hello <a class=\"mention\" href=\"/u/setareh\">@Setareh</a></p>\n<p>Let\u2019s make a new topic out of your question and discuss it.</p>", "<p>Hi,<br>\nsure, would be great. Actually using \u201cTrackMate CSV importer\u201d manually , I could do cell tracking perfectly. So now I\u2019m involving with doing all processes automatically within a Jython script.</p>", "<p>Dear friends,Dear <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a> ,</p>\n<p>I\u2019m tracking cells using Trackmate by this script,</p>\n<p>from ij import IJ, ImagePlus, ImageStack<br>\nimport os, glob, sys<br>\nimport csv<br>\nimport fiji.plugin.trackmate.Settings as Settings<br>\nimport fiji.plugin.trackmate.Model as Model<br>\nimport fiji.plugin.trackmate.SelectionModel as SelectionModel<br>\nimport fiji.plugin.trackmate.TrackMate as TrackMate<br>\nimport fiji.plugin.trackmate.Logger as Logger<br>\nimport fiji.plugin.trackmate.detection.DetectorKeys as DetectorKeys<br>\nimport fiji.plugin.trackmate.detection.DogDetectorFactory as DogDetectorFactory<br>\nimport fiji.plugin.trackmate.tracking.sparselap.SparseLAPTrackerFactory as SparseLAPTrackerFactory<br>\nimport fiji.plugin.trackmate.tracking.LAPUtils as LAPUtils<br>\nimport fiji.plugin.trackmate.visualization.hyperstack.HyperStackDisplayer as HyperStackDisplayer<br>\nimport fiji.plugin.trackmate.features.FeatureFilter as FeatureFilter<br>\nimport fiji.plugin.trackmate.features.FeatureAnalyzer as FeatureAnalyzer<br>\nimport fiji.plugin.trackmate.features.spot.SpotContrastAndSNRAnalyzerFactory as SpotContrastAndSNRAnalyzerFactory<br>\nimport fiji.plugin.trackmate.action.ExportStatsToIJAction as ExportStatsToIJAction<br>\nimport fiji.plugin.trackmate.io.TmXmlReader as TmXmlReader<br>\nimport fiji.plugin.trackmate.action.ExportTracksToXML as ExportTracksToXML<br>\nimport fiji.plugin.trackmate.io.TmXmlWriter as TmXmlWriter<br>\nimport fiji.plugin.trackmate.features.ModelFeatureUpdater as ModelFeatureUpdater<br>\nimport fiji.plugin.trackmate.features.SpotFeatureCalculator as SpotFeatureCalculator<br>\nimport fiji.plugin.trackmate.features.spot.SpotContrastAndSNRAnalyzer as SpotContrastAndSNRAnalyzer<br>\nimport fiji.plugin.trackmate.features.spot.SpotIntensityAnalyzerFactory as SpotIntensityAnalyzerFactory<br>\nimport fiji.plugin.trackmate.features.track.TrackSpeedStatisticsAnalyzer as TrackSpeedStatisticsAnalyzer<br>\nimport fiji.plugin.trackmate.util.TMUtils as TMUtils<br>\nfrom fiji.plugin.trackmate.detection import DownsampleLogDetectorFactory<br>\nfrom fiji.plugin.trackmate.tracking.oldlap import LAPTrackerFactory<br>\nfrom fiji.plugin.trackmate.detection import DetectorKeys<br>\nfrom fiji.plugin.trackmate.tracking import LAPUtils<br>\nfrom fiji.plugin.trackmate.action import ExportStatsToIJAction<br>\nfrom fiji.plugin.trackmate.action import TrackBranchAnalysis<br>\nfrom fiji.plugin.trackmate.graph import GraphUtils</p>\n<p>from fiji.plugin.trackmate.providers import SpotAnalyzerProvider<br>\nfrom fiji.plugin.trackmate.providers import EdgeAnalyzerProvider<br>\nfrom fiji.plugin.trackmate.providers import TrackAnalyzerProvider</p>\n<p>import fiji.plugin.trackmate.features.track.TrackSpotQualityFeatureAnalyzer as TrackSpotQualityFeatureAnalyzer<br>\nimport fiji.plugin.trackmate.features.track.TrackDurationAnalyzer as TrackDurationAnalyzer<br>\nimport fiji.plugin.trackmate.features.track.TrackIndexAnalyzer as TrackIndexAnalyzer</p>\n<p>#****************************************</p>\n<h1>\n<a name=\"open-image-1\" class=\"anchor\" href=\"#open-image-1\"></a>Open image</h1>\n<p>#****************************************</p>\n<p>imp = IJ.openImage(\u2018<a href=\"https://fiji.sc/samples/FakeTracks.tif\" rel=\"noopener nofollow ugc\">https://fiji.sc/samples/FakeTracks.tif</a>\u2019)</p>\n<p><span class=\"hashtag\">#-------------------------</span></p>\n<h1>\n<a name=\"instantiate-model-object-2\" class=\"anchor\" href=\"#instantiate-model-object-2\"></a>Instantiate model object</h1>\n<p><span class=\"hashtag\">#-------------------------</span></p>\n<p>model = Model()</p>\n<h1>\n<a name=\"set-logger-3\" class=\"anchor\" href=\"#set-logger-3\"></a>Set logger</h1>\n<p>model.setLogger(Logger.IJ_LOGGER)</p>\n<p><span class=\"hashtag\">#------------------------</span></p>\n<h1>\n<a name=\"prepare-settings-object-4\" class=\"anchor\" href=\"#prepare-settings-object-4\"></a>Prepare settings object</h1>\n<p><span class=\"hashtag\">#------------------------</span></p>\n<p>settings = Settings()<br>\nsettings.setFrom(imp)</p>\n<h1>\n<a name=\"configure-detector-5\" class=\"anchor\" href=\"#configure-detector-5\"></a>Configure detector</h1>\n<p>settings.detectorFactory = DogDetectorFactory()<br>\nsettings.detectorSettings = {<br>\nDetectorKeys.KEY_DO_SUBPIXEL_LOCALIZATION : True,<br>\nDetectorKeys.KEY_RADIUS : 5.,<br>\nDetectorKeys.KEY_TARGET_CHANNEL : 1,<br>\nDetectorKeys.KEY_THRESHOLD : 5.,<br>\nDetectorKeys.KEY_DO_MEDIAN_FILTERING : False,<br>\n}</p>\n<h1>\n<a name=\"configure-tracker-6\" class=\"anchor\" href=\"#configure-tracker-6\"></a>Configure tracker</h1>\n<p>settings.trackerFactory = LAPTrackerFactory()<br>\nsettings.trackerSettings = LAPUtils.getDefaultLAPSettingsMap()</p>\n<p><a class=\"hashtag\" href=\"/tag/print\">#<span>print</span></a>(LAPUtils.getDefaultLAPSettingsMap())<br>\nsettings.trackerSettings[\u2018LINKING_MAX_DISTANCE\u2019] = 15.0<br>\nsettings.trackerSettings[\u2018LINKING_FEATURE_PENALTIES\u2019] = {}<br>\n<span class=\"hashtag\">#gap</span> closing<br>\nsettings.trackerSettings[\u2018ALLOW_GAP_CLOSING\u2019] = True<br>\nsettings.trackerSettings[\u2018GAP_CLOSING_MAX_DISTANCE\u2019] = 15.0<br>\nsettings.trackerSettings[\u2018MAX_FRAME_GAP\u2019] = 5<br>\nsettings.trackerSettings[\u2018GAP_CLOSING_FEATURE_PENALTIES\u2019] = {}<br>\n<a class=\"hashtag\" href=\"/tag/splitting\">#<span>splitting</span></a><br>\nsettings.trackerSettings[\u2018ALLOW_TRACK_SPLITTING\u2019] = True<br>\nsettings.trackerSettings[\u2018SPLITTING_MAX_DISTANCE\u2019] = 15.0<br>\nsettings.trackerSettings[\u2018SPLITTING_FEATURE_PENALTIES\u2019] = {}</p>\n<p>settings.addSpotAnalyzerFactory(SpotIntensityAnalyzerFactory())<br>\nsettings.addSpotAnalyzerFactory(SpotContrastAndSNRAnalyzerFactory())</p>\n<p>settings.addTrackAnalyzer(TrackIndexAnalyzer())<br>\nsettings.addTrackAnalyzer(TrackSpeedStatisticsAnalyzer())<br>\nsettings.addTrackAnalyzer(TrackDurationAnalyzer())</p>\n<p><a class=\"hashtag\" href=\"/tag/filtering\">#<span>filtering</span></a></p>\n<p>settings.initialSpotFilterValue = 1<br>\n<span class=\"hashtag\">#filter1</span> = FeatureFilter(\u2018NUMBER_OF_SPLIT_EVENTS\u2019, 1 , True)<br>\n<span class=\"hashtag\">#settings</span>.addTrackFilter(filter1)</p>\n<p><span class=\"hashtag\">#----------------------</span></p>\n<h1>\n<a name=\"instantiate-trackmate-7\" class=\"anchor\" href=\"#instantiate-trackmate-7\"></a>Instantiate trackmate</h1>\n<p><span class=\"hashtag\">#----------------------</span></p>\n<p>trackmate = TrackMate(model, settings)</p>\n<p><span class=\"hashtag\">#------------</span></p>\n<h1>\n<a name=\"execute-all-8\" class=\"anchor\" href=\"#execute-all-8\"></a>Execute all</h1>\n<p><span class=\"hashtag\">#------------</span></p>\n<p>ok = trackmate.checkInput()<br>\nif not ok:<br>\nsys.exit(str(trackmate.getErrorMessage()))</p>\n<p>ok = trackmate.process()<br>\nif not ok:<br>\nsys.exit(str(trackmate.getErrorMessage()))</p>\n<p><span class=\"hashtag\">#----------------</span></p>\n<h1>\n<a name=\"display-results-9\" class=\"anchor\" href=\"#display-results-9\"></a>Display results</h1>\n<p><span class=\"hashtag\">#----------------</span></p>\n<p>model.getLogger().log(\u2018Found \u2019 + str(model.getTrackModel().nTracks(True)) + \u2019 tracks.\u2019)</p>\n<p>selectionModel = SelectionModel(model)<br>\ndisplayer =  HyperStackDisplayer(model, selectionModel, imp)<br>\ndisplayer.render()<br>\ndisplayer.refresh()</p>\n<h1>\n<a name=\"the-feature-model-that-stores-track-features-10\" class=\"anchor\" href=\"#the-feature-model-that-stores-track-features-10\"></a>The feature model, that stores track features.</h1>\n<p>fm = model.getFeatureModel()</p>\n<p>for id in model.getTrackModel().trackIDs(True):</p>\n<pre><code># Fetch the track feature from the feature model.\nv = fm.getTrackFeature(id, 'TRACK_MEAN_SPEED')\nmodel.getLogger().log('')\nmodel.getLogger().log('Track ' + str(id) + ': mean velocity = ' + str(v) + ' ' + model.getSpaceUnits() + '/' + model.getTimeUnits())\n\ntrack = model.getTrackModel().trackSpots(id)\nfor spot in track:\n    sid = spot.ID()\n    # Fetch spot features directly from spot.\n    x=spot.getFeature('POSITION_X')\n    y=spot.getFeature('POSITION_Y')\n    t=spot.getFeature('FRAME')\n    q=spot.getFeature('QUALITY')\n    snr=spot.getFeature('SNR')\n    mean=spot.getFeature('MEAN_INTENSITY')\n    model.getLogger().log('\\tspot ID = ' + str(sid) + ': x='+str(x)+', y='+str(y)+', t='+str(t)+', q='+str(q) + ', snr='+str(snr) + ',ID='+str(id)+', mean = ' + str(mean))\n</code></pre>\n<p>ExportStatsToIJAction(selectionModel).execute(trackmate)<br>\nIJ.selectWindow(\u201cSpots in tracks statistics\u201d);<br>\nIJ.saveAs(\u201cResults\u201d,\u201cSpots in tracks statistics.csv\u201d);</p>\n<p>I\u2019m confused with an issue! by this code and also using Trackmate manually, I could reach the same result table for  \u201c\u2018<a href=\"https://fiji.sc/samples/FakeTracks.tif\" rel=\"noopener nofollow ugc\">https://fiji.sc/samples/FakeTracks.tif</a>\u2019\u201d.</p>\n<p>but when I do it for another images files, the results are so different.<br>\nI can not find the problem with the code! I\u2019ll appreciate your help on this.</p>\n<p>Thank you so much in advance.</p>", "<p>Hi,<br>\nI am also stuck on this problem. I want to use the detection results by \u201cTrackMate CSV importer\u201d and then link particles by TrackMate, but I don\u2019t know how to write this script. Can you share your script? Thank you so much!</p>"], "78316": ["<p>I\u2019m hoping to find others doing .NET development with Bioformats &amp; ImageJ searching Github, I\u2019ve only found CellTool which seems to have been abandoned. Also could we create a tag for .NET development or just C#.</p>"], "78320": ["<p>#@ File (label = \u201cInput directory\u201d, style = \u201cdirectory\u201d) InputDir<br>\n#@ File (label = \u201cOutput directory\u201d, style = \u201cdirectory\u201d) OutputDir<br>\nFileList = getFileList(InputDir);<br>\nfor (f = 0; f &lt; lengthOf(FileList); f++) {<br>\nActiveImage = InputDir + File.separator + FileList[f];<br>\nif (!File.isDirectory(ActiveImage));<br>\nopen(ActiveImage);<br>\nImageName = getTitle();</p>\n<pre><code>run(\"Z Project...\", \"projection=[Max Intensity]\");\n//run(\"Channels Tool...\");\nStack.setActiveChannels(\"111\");\nStack.setActiveChannels(\"100\");\nrun(\"RGB Color\");\nsaveAs(\"Tiff\", OutputDir + File.separator + ImageName + \"_dapi\" + \".TIF\");\nclose();\nStack.setActiveChannels(\"000\");\nStack.setActiveChannels(\"010\");\nrun(\"RGB Color\");\nsaveAs(\"Tiff\", OutputDir + File.separator + ImageName + \"_tra\" + \".TIF\");\nclose();\nStack.setActiveChannels(\"000\");\nStack.setActiveChannels(\"001\");\nrun(\"RGB Color\");\nsaveAs(\"Tiff\", OutputDir + File.separator + ImageName + \"_ints2\" + \".TIF\");\nclose();\nclose();\nselectWindow(ImageName);\nclose();\n</code></pre>\n<p>}</p>", "<p>Dear <a class=\"mention\" href=\"/u/uberchimpy\">@uberchimpy</a> ,<br>\nyou need to loop in the serie of images</p>\n<pre><code class=\"lang-auto\">input = getDirectory(\"Choose folder with lif files \");\nlist = getFileList(input);\nrun(\"Bio-Formats Macro Extensions\");\nlist = getFileList(input);   \nfor (i=0; i&lt;list.length; i++) {\n\tif (endsWith(list[i],\".lif\")){\n\t\tinputPath= input +  File.separator + list[i];\n\t\t//how many series in this lif file?\n\t\tExt.setId(inputPath);\n\t\tExt.getSeriesCount(seriesCount); //-- Gets the number of image series in the active dataset.\n\t\tfor (j=1; j&lt;=seriesCount; j++) {\n            // open an image with Bio-Format\n\t\t\trun(\"Bio-Formats\", \"open=inputPath autoscale color_mode=Default view=Hyperstack stack_order=XYCZT series_\"+j);\n\t\t\t\n// Continue your code here \n\t\t}\n\t}\n}\n\n</code></pre>"], "51699": ["<p><a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a> <a class=\"mention\" href=\"/u/oburri\">@oburri</a> <a class=\"mention\" href=\"/u/simonfn\">@simonfn</a> <a class=\"mention\" href=\"/u/aarpon\">@aarpon</a></p>\n<p>Within Imaris we have two Surfaces (A and B) which partially overlap. Each Surface contains several objects (connected components). We would now like to measure for each object in Surface A with which objects it overlaps in Surface B (we would need the object label indices as a list). And we would like to solve this within ImarisXT.</p>\n<p>Could you give me a hint how to achieve this? Related, is the ImarisXT API somewhere online?</p>\n<hr>\n<p>There is a MATLAB XT that does create a new colocalization Surface object, but it does that only on the binary masks.</p>\n<pre><code class=\"lang-matlab\">%Generate surface mask for each surface over time - currently using\n%GetDataVolumeBytes, as other methods suchas 1DArrayBytes were not working\nfor vTimeIndex= 0:aSizeT-1\n    vSurfaces1Mask = vSurfaces1.GetMask(aExtendMinX,aExtendMinY,aExtendMinZ,aExtendMaxX,aExtendMaxY,aExtendMaxZ,aSizeX, aSizeY,aSizeZ,vTimeIndex);\n    vSurfaces2Mask = vSurfaces2.GetMask(aExtendMinX,aExtendMinY,aExtendMinZ,aExtendMaxX,aExtendMaxY,aExtendMaxZ,aSizeX, aSizeY,aSizeZ,vTimeIndex);\n    \n    ch1 = vSurfaces1Mask.GetDataVolumeAs1DArrayBytes(0,vTimeIndex);\n    ch2 = vSurfaces2Mask.GetDataVolumeAs1DArrayBytes(0,vTimeIndex);\n    \n    %Determine the Voxels that are colocalized\n    Coloc=ch1+ch2;\n    Coloc(Coloc&lt;2)=0;\n    Coloc(Coloc&gt;1)=1;\n    \n    vDataSet.SetDataVolumeAs1DArrayBytes(Coloc, vLastChannel, vTimeIndex);\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"1\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian_Tischer:</div>\n<blockquote>\n<p><code>vSurfaces1.GetMask</code></p>\n</blockquote>\n</aside>\n<p>A specific question I would have is whether there also is something like <code>vSurfaces1.GetLabelMask</code>? Or some other way to get the surface object indices?</p>", "<p>I finally managed:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/e4lqy14iWGsEXYPAvbjbMQGZ3iT.m\">XTSurfaceSurfaceColoc2.m</a> (8.2 KB)</p>\n<p>This XTension adds overlap information to the second to the two Surface objects. One can see how much volume overlap there was for each object with any other object, e.g. here with object ID 1:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/764a630b6c037078a004496fceb1b5a143990915.png\" data-download-href=\"/uploads/short-url/gSrIji8zRCz21DNMXlLNxWXcNz7.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/764a630b6c037078a004496fceb1b5a143990915_2_273x250.png\" alt=\"image\" data-base62-sha1=\"gSrIji8zRCz21DNMXlLNxWXcNz7\" width=\"273\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/764a630b6c037078a004496fceb1b5a143990915_2_273x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/764a630b6c037078a004496fceb1b5a143990915_2_409x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/764a630b6c037078a004496fceb1b5a143990915_2_546x500.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/764a630b6c037078a004496fceb1b5a143990915_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">752\u00d7688 81.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks to <a class=\"mention\" href=\"/u/oburri\">@oburri</a> for pointing me towards how to handle string arrays in MATLAB <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"></p>\n<p>The code that I added is:</p>\n<pre><code class=\"lang-matlab\">// Measure the volume overlap of Surface objects with each another\nfor vTimeIndex= 0:aSizeT-1\n    n1 = vSurfaces1.GetNumberOfSurfaces();\n    n2 = vSurfaces2.GetNumberOfSurfaces();\n    ids1 = vSurfaces1.GetIds();\n    ids2 = vSurfaces2.GetIds();\n    \n    for i1 = 1 : n1\n        vOverlaps = 1:n2;\n        vNames = strings(1,n2);\n        vFactorNames = \"Category\";\n        vFactors = strings(1,n2);\n        vOverlapUnits = strings(1,n2);\n        vIds = 1:n2;\n        vStatName = strcat(\"Overlap with \", string(vSurfaces1.GetName), \" ID \", string(ids1(i1)));\n        \n        for i2 = 1 : n2\n            vSingleMask1 = vSurfaces1.GetSingleMask(i1-1,aExtendMinX,aExtendMinY,aExtendMinZ,aExtendMaxX,aExtendMaxY,aExtendMaxZ,aSizeX,aSizeY,aSizeZ);\n            vSingleMask2 = vSurfaces2.GetSingleMask(i2-1,aExtendMinX,aExtendMinY,aExtendMinZ,aExtendMaxX,aExtendMaxY,aExtendMaxZ,aSizeX,aSizeY,aSizeZ);\n            ch1 = vSingleMask1.GetDataVolumeAs1DArrayBytes(0,vTimeIndex);\n            ch2 = vSingleMask2.GetDataVolumeAs1DArrayBytes(0,vTimeIndex);\n            coloc = ch1 .* ch2 ;\n            \n            vOverlaps(i2) = sum(coloc); \n            vIds(i2) = ids2(i2);\n            vOverlapUnits(i2) = \"voxel\";\n            vNames(i2) = vStatName;\n            vFactors(i2) = \"Custom\";\n        end\n        \n        vSurfaces2.RemoveStatistics( vStatName ); \n        vSurfaces2.AddStatistics(vNames, vOverlaps, vOverlapUnits, vFactors, vFactorNames, vIds);        \n    \n    end\nend    \n</code></pre>\n<hr>\n<p>TODO:</p>\n<ul>\n<li>Make this more efficient by only cropping out the volumes where the surface objects are present. Right now, e.g. this code always fetches the whole image: <code>vSurfaces1.GetSingleMask(i1-1,aExtendMinX,aExtendMinY,aExtendMinZ,aExtendMaxX,aExtendMaxY,aExtendMaxZ,aSizeX,aSizeY,aSizeZ);</code>\n</li>\n</ul>", "<p>Hope that in future imaris will provide GetLabelMask which will accelerate many sub surface based processing. Do u have any idea if we can infer the index of subsurfaces from the mask of whole XYZT volume\uff1f Like from top to bottom, from left to tight\uff1f</p>", "<p><a class=\"mention\" href=\"/u/goehlmann_imaris\">@goehlmann_imaris</a></p>\n<p>Are there any news on <code>GetLabelMask()</code>?</p>\n<p>For me iterating with <code>GetSingleMask()</code> to create a label image is too slow and and too memory intensive for bigger volumes.</p>\n<p>As a workaround, I tried to leverage <code>GetSurfaceDataLayout()</code> but the retrieved bounding box extents do not fit the output of <code>GetSingleMask()</code>. When I used <code>GetSurfaceDataLayout()</code> with <code>GetSurfaceData()</code> to create a label image/ mask, the result did not match what I get from <code>GetMask()</code> (shifts in Z).</p>\n<p>Cheers,<br>\nChris</p>", "<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"3\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian Tischer:</div>\n<blockquote>\n<p>GetSingleMask(i1-1,aExtendMinX,aExtendMinY,aExtendMinZ,aExtendMaxX,aExtendMaxY,aExtendMaxZ,</p>\n</blockquote>\n</aside>\n<p>You can change the x,y,z limit to only return a volume mask for each subsurface in GetSingleMask.<br>\naSizeX,aSizeY,aSizeZ need also to be trimed accordingly with same resolution.</p>\n<p>Another solution is to get a full mask and cast it to 3D binary volume, then input to matlab to get bwlabeln, which will isolate each disconnected subvolume. However by this way the index of each label was not matched to the id of each subsurface, unless you could find the rules how the subsurface was indexed. It looks that each subsurface was indexed by its relative position but the exact rule is not easy to guess. By feeding the bwlabeln output into regionprops/regionprops3, multiple metadata for each labeled volume could be obtained which could be used to match the imaris subsurface metadata and helps to align and match the index. The numeric metadata between imaris and matlab is not an exact match but should be close enough to make an almost perfect match. Another risk is that if there are subsurfaces very close to each other, e.g., hand-cut, they will be seperated in imaris but might be connected in binary volume, thus they could not be matched one-by-one.</p>", "<blockquote>\n<p>You can change the x,y,z limit to only return a volume mask for each subsurface in GetSingleMask.<br>\naSizeX,aSizeY,aSizeZ need also to be trimed accordingly with same resolution.</p>\n</blockquote>\n<p>Yes, I thought of that, too. However, how can I get the bounding box limits for the single masks? The output of <code>GetSurfaceDataLayout()</code> does not seem to fit the masks.</p>\n<blockquote>\n<p>Another solution is to get a full mask and cast it to 3D binary volume [\u2026] risk is that if there are subsurfaces very close to each other, e.g., hand-cut, they will be seperated in imaris but might be connected in binary volume, thus they could not be matched one-by-one.</p>\n</blockquote>\n<p>That\u2019s my current workaround and I am running in that exact problem: Imaris surfaces end up merged without a way to recover from that.</p>", "<p>bounding box limits could be obtained from the surface mesh vertices of each subsurface:<br>\naVertices = aSurface.GetVertices(aSurfaceIndex)<br>\naVertices =<br>\n0         0    0.5053<br>\n0    0.5103         0<br>\n0.5103         0         0<br>\n1.1867         0    0.2575<br>\n1.1867    0.2601         0<br>\n\u2026</p>\n<p>Then get minXYZ and maxXYZ from each dimension.</p>\n<p>minX then need to be trimed to the floor direction to the nearest number from a vector of:<br>\n[aExtendedMinX+k*(aExtendedMaxX-aExtendedMinX)/aSizeX] (k=0,1,2,3,\u2026aSizeX). Similar for minY,Z.</p>\n<p>maxX then need to be trimed to the ceiling direction to the nearest number from a vector of:<br>\n[aExtendedMinX+k*(aExtendedMaxX-aExtendedMinX)/aSizeX] (k=0,1,2,3,\u2026aSizeX). Similar for maxY,Z.</p>\n<p>Use the trimed minXYZ and maxXYZ as argument for GetSingleMask:</p>\n<p>\u2026 = GetSingleMask(i,MinX,MinY,MinZ,MaxX,MaxY,MaxZ,(maxX-minX)/((aExtendedMaxX-aExtendedMinX)/aSizeX),\u2026Y\u2026Z\u2026</p>\n<p>By this way, you will get the EXACT binary volume the same as imaris\u2019s mask operation.<br>\nThen calculate the position inside original 3D volume from the previous ceiling /flooring step.</p>\n<p>Then, a labelled volume (positionXrange,Yrange,Zrange) += (i+1)*(SingleMask&gt;0), i=0,1,2,\u2026</p>\n<p>This sholuld be the final way to get the same labeled mask.<br>\nThe processing time should be tolerable by this method.</p>\n<p>To speed up, the resolution could be reduced by reducing the argument: maxX-minX)/((aExtendedMaxX-aExtendedMinX)/aSizeX),\u2026Y\u2026Z\u2026(This is really an awesome magic of imaris)</p>", "<aside class=\"quote no-group\" data-username=\"mendel\" data-post=\"8\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mendel/40/17740_2.png\" class=\"avatar\"> mendel :</div>\n<blockquote>\n<p>aSurface.GetVertices(aSurfaceIndex)</p>\n</blockquote>\n</aside>\n<p>Thanks for the detailed help! Unfortunately, <code>GetVertices()</code> is not available in <strong>Imaris 9.9.1</strong> anymore.</p>", "<p>Then try export scene as scene file, the scene file may have triangle-mesh information?<br>\nThen open image file in imaris7/8, File-Load scene, then save as ims file.<br>\nThen continue with the SingleMask.</p>\n<p>I do not have Imairs9 (the removal of triangle-mesh surface is also the reason why I do not use Imaris 9), so not sure if this works.</p>", "<p>What\u2019s the bounding box argout? Are they in floating coordinates or voxel intergers?<br>\nNote that there are two independent coordinates in imaris:<br>\nThe universal coordinates, which exist as if in real world.<br>\nThe dataset volume take up a defined space in the 3D universal coordinates,and could shift, squeeze, compress,flatten,swell\u2026<br>\nIf the only difference is shift in Z, then this may be not a problem. There should be a way to correct this shift.</p>\n<p>Can also try to export surface as wrl format, then load into other 3D software/package to see if there is a solution. The scene or wrl format may have information of vertices for all surfaces, then aVertices<br>\ncould be used to get the bounding box position for each subsurface. A xml reader may be used to read the scene file or other exported formats.</p>", "<aside class=\"quote no-group\" data-username=\"mendel\" data-post=\"12\" data-topic=\"51699\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mendel/40/17740_2.png\" class=\"avatar\"> mendel :</div>\n<blockquote>\n<p>What\u2019s the bounding box argout? Are they in floating coordinates or voxel intergers?</p>\n</blockquote>\n</aside>\n<p>Do you mean the output of <code>GetSurfaceDataLayout()</code>. It is in floats and physical unit.</p>\n<aside class=\"quote no-group\" data-username=\"mendel\" data-post=\"12\" data-topic=\"51699\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mendel/40/17740_2.png\" class=\"avatar\"> mendel :</div>\n<blockquote>\n<p>Note that there are two independent coordinates in imaris:<br>\nThe universal coordinates, which exist as if in real world.<br>\nThe dataset volume take up a defined space in the 3D universal coordinates,and could shift, squeeze, compress,flatten,swell\u2026<br>\nIf the only difference is shift in Z, then this may be not a problem. There should be a way to correct this shift.</p>\n</blockquote>\n</aside>\n<p>Yes, When I overlay the output of <code>GetMask()</code> with my own attempt by leveraging <code>GetSurfaceData()</code> and <code>GetSurfaceDataLayout()</code> I see differences in Z. There are slight differences in XY also, but it seems only to affect \u00b11px, probably due to quantiztion/rounding.</p>\n<p>The weird thing is that <code>GetSurfaceData()</code> returns some surface channels on a different resolution level. It seems to me that this depends on the surface grain size set in the creation of that surface.</p>\n<aside class=\"quote no-group\" data-username=\"mendel\" data-post=\"12\" data-topic=\"51699\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mendel/40/17740_2.png\" class=\"avatar\"> mendel :</div>\n<blockquote>\n<p>Can also try to export surface as wrl format, then load into other 3D software/package to see if there is a solution. The scene or wrl format may have information of vertices for all surfaces, then aVertices<br>\ncould be used to get the bounding box position for each subsurface. A xml reader may be used to read the scene file or other exported formats.</p>\n</blockquote>\n</aside>\n<p>Indeed. Currently, I am still looking for a solution entirely from inside a Imaris extension.</p>\n<p>Thanks again for you time!<br>\nCheers, Chris</p>", "<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"3\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian Tischer:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">            ch1 = vSingleMask1.GetDataVolumeAs1DArrayBytes(0,vTimeIndex);\n</code></pre>\n</blockquote>\n</aside>\n<p>There is an error:<br>\nch1 = vSingleMask1.GetDataVolumeAs1DArrayBytes(0,vTimeIndex);<br>\nshould be changed to this anywhere and forever:<br>\nch1 = vSingleMask1.GetDataVolumeAs1DArrayBytes(0,0);</p>", "<p>I just figre out what GetSurfaceDataLayout returns as this is a new command in Imaris9.</p>\n<p>sl = s.GetSurfaceDataLayout(i)<br>\nsl.mExtendMaxX/Y/Z<br>\nsl.mExtendMinX/Y/Z<br>\nsl.mSizeX/Y/Z</p>\n<p>Then things are easy:</p>\n<p>sl.mExtendMin//Max/X//Y//Z are the 6 physical position of the surface\u2019s bounding box extendted to the closest voxel wall outside.</p>\n<blockquote>\n<p>entire3dVol = zeros(aDataSet.GetSizeX,aDataSet.GetSizeY,aDataSet.GetSizeZ);</p>\n<pre><code>aVoxelLenX = (aDataSet.GetExtendedMaxX - aDataSet.GetExtendedMinX)/aDataSet.GetSizeX;\naVoxelLenY = (aDataSet.GetExtendedMaxY - aDataSet.GetExtendedMinY)/aDataSet.GetSizeY;\naVoxelLenZ = (aDataSet.GetExtendedMaxZ - aDataSet.GetExtendedMinZ)/aDataSet.GetSizeZ;\n</code></pre>\n<p>for i = 1 : nSurface</p>\n<pre><code>sl = vSurfaces1.GetSurfaceDataLayout(i);\n% This will return a binary 3D block that just enough to contain the subsurface.\n\nvSingleMask1 = vSurfaces1.GetSingleMask(i-1,sl.mExtendMinX,sl.mExtendMinY,sl.mExtendMinZ,sl.mExtendMax,sl.mExtendMaxY,sl.mExtendMaxZ,sl.mSizeX,sl.mSizeY,sl.mSizeZ);\n\nch1 = vSingleMask1.GetDataVolumeAs1DArrayBytes(0,0);\nch1_3D = reshape(ch1,sl.mSizeX,sl.mSizeY,sl.mSizeZ);\n\naBlockStartX = (sl.mExtendMinX - aDataSet.GetExtendedMinX)/aVoxelLenX +1;\naBlockEndX = (sl.mExtendMaxX - aDataSet.GetExtendedMinX)/aVoxelLenX;\n\naBlockStartY = (sl.mExtendMinY - aDataSet.GetExtendedMinY)/aVoxelLenY +1;\naBlockEndY = (sl.mExtendMaxY - aDataSet.GetExtendedMinY)/aVoxelLenY;\n\naBlockStartZ = (sl.mExtendMinZ - aDataSet.GetExtendedMinZ)/aVoxelLenZ +1;\naBlockEndX = (sl.mExtendMaxZ - aDataSet.GetExtendedMinZ)/aVoxelLenZ;\n       \n% To place this block into the original entire 3D volume:\nentire3dVol(aBlockStartX:aBlockEndX,aBlockStartY:aBlockEndY,aBlockStartZ:aBlockEndX) = entire3dVol(aBlockStartX:aBlockEndX,aBlockStartY:aBlockEndY,aBlockStartZ:aBlockEndX) + ch1_3D;\n</code></pre>\n<p>end</p>\n</blockquote>\n<p>I have no idea what the output of GetSurfaceData is as I have no Imaris9 at hand. Dose it need argument? It returns a 3D binary volume?<br>\nBut from what I find, GetSurfaceDataLayout  is enought to solve the problem.</p>\n<p>then i found some description of GetSurfaceData.</p>\n<h1>\n<a name=\"getsurfacedata-1\" class=\"anchor\" href=\"#getsurfacedata-1\"></a>GetSurfaceData()</h1>\n<p>Returns a dataset of type UInt16 containing signed int16 values. Values below zero are outside the surface, above zero is inside. A sub-voxel precise surface can be reconstructed by linearly interpolating the values around zero.</p>\n<p>GetSurfaceData should be a +/- floating volume with + and - indicate inside and outside,respectively. The numbers helps extrapolate the finest details of surface in different grain size.<br>\nSince there is ambiguity in the voxel that surface cut through, you should avoid use GetSurfaceData  for this purpose.</p>\n<p>I am planning to write an ImarisXT Surface2LabelledChannels that create labelled mask channel from specified surfaces, compatible with Imaris 7/8/9+, and could deal with imaris file with very large size unlimited (&gt;100GB,e.g.) and could run on old office-level (e.g., no GPU, i5 CPU) computers like ten years ago.</p>", "<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"3\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian Tischer:</div>\n<blockquote>\n<p>dds overlap information to the second to the two Surface objects. One can see how much volume ov</p>\n</blockquote>\n</aside>\n<p>There is no need to do SingleMask to get overlap informaton.<br>\nThe oriignal SurfaceColoc is much faster to get the overlap channel.<br>\nTo get how many overlap voxels are located in each subsurface, you can go to Statistics tab of any of the two Surface(or even other surface), and check the Intensity Sum  Ch=[overlap channel id], thus there is also no need to add statistics to each subsurface. Intensity Sum means the sum of voxels intensity (0-1 for coloc channel) that located inside the surface, and no subvoxel fraction was involved in the voxel that was cut through.</p>\n<p>Both GetSingleMask and AddStatistics are very time-consuming with large amounts of subsrfaces.</p>", "<p>Dear <a class=\"mention\" href=\"/u/mendel\">@mendel</a>,</p>\n<p>thanks agian for the input. I had tried <code>sl = GetSurfaceDataLayout()</code> to get single masks with <code>GetSingleMask</code>. However, it could happen (for some surface channels) that the sizes were not fitting.</p>\n<pre><code class=\"lang-auto\">sl.mSizeX != (aBlockEndX - aBlockStartX)\n</code></pre>\n<p>For some channels they fit, for others they were off by factor of 2. It\u2019s unclear to me why and how to know it\u2026</p>\n<p>Anyway, I wrote a label image function that works:</p>\n<pre><code class=\"lang-python\">def getSurfaceLabelImage(surface, ds):\n    nSurfaces = len(surface.GetIds())\n\n    label_img = np.zeros((ds.GetSizeX(), ds.GetSizeY(), ds.GetSizeZ()), np.uint16)\n\n    voxel_len_x = (ds.GetExtendMaxX() - ds.GetExtendMinX()) / ds.GetSizeX()\n    voxel_len_y = (ds.GetExtendMaxY() - ds.GetExtendMinY()) / ds.GetSizeY()\n    voxel_len_z = (ds.GetExtendMaxZ() - ds.GetExtendMinZ()) / ds.GetSizeZ()\n\n    for i in trange(nSurfaces):\n        sl = surface.GetSurfaceDataLayout(i)\n\n        block_start_x = int((sl.mExtendMinX - ds.GetExtendMinX()) / voxel_len_x)\n        block_end_x = int((sl.mExtendMaxX - ds.GetExtendMinX()) / voxel_len_x + 1)\n\n        block_start_y = int((sl.mExtendMinY - ds.GetExtendMinY()) / voxel_len_y)\n        block_end_y = int((sl.mExtendMaxY - ds.GetExtendMinY()) / voxel_len_y + 1)\n\n        block_start_z = int((sl.mExtendMinZ - ds.GetExtendMinZ()) / voxel_len_z)\n        block_end_z = int((sl.mExtendMaxZ - ds.GetExtendMinZ()) / voxel_len_z + 1)\n\n        simgle_mask = surface.GetSingleMask(\n            i,\n            sl.mExtendMinX,\n            sl.mExtendMinY,\n            sl.mExtendMinZ,\n            sl.mExtendMaxX,\n            sl.mExtendMaxY,\n            sl.mExtendMaxZ,\n            block_end_x - block_start_x,\n            block_end_y - block_start_y,\n            block_end_z - block_start_z,\n        )\n        arr_single_mask = np.array(simgle_mask.GetDataShorts(), dtype=bool)[0, 0]\n\n        block = label_img[\n            block_start_x:block_end_x,\n            block_start_y:block_end_y,\n            block_start_z:block_end_z,\n        ]\n\n        # binary indexing to set label id\n        block[arr_single_mask] = i + 1\n\n    return label_img\n</code></pre>\n<p>When comparing this label image to the output of <code>GetMask</code></p>\n<pre><code class=\"lang-python\">def getSurfaceBinaryMask(surface, ds):\n    extent = [\n        ds.GetExtendMinX(),\n        ds.GetExtendMinY(),\n        ds.GetExtendMinZ(),\n        ds.GetExtendMaxX(),\n        ds.GetExtendMaxY(),\n        ds.GetExtendMaxZ(),\n    ]\n    m = surface.GetMask(*extent, ds.GetSizeX(), ds.GetSizeY(), ds.GetSizeZ(), 0)\n    mask = np.array(m.GetDataShorts())[0, 0]\n\n    return mask\n</code></pre>\n<p>there are slight differences at the border of the surfaces, probably due to rounding, but nothing serious.</p>\n<p>In addition, on my datasets <code>getSurfaceLabelImage()</code> is way faster (&gt;100x) than <code>getSurfaceBinaryMask()</code> <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I bundled the function to export Imaris surfaces to ImageJ compatible .tif files to a Imaris Python 3.7 extension.</p>\n<p>It requires Imaris 9.9.1. Perhaps other versions from 9.x.x also work*.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/8SM6qI65h0rbm8xvOc4hXbmiwvW.zip\">export_surface_label_image.zip</a> (1.6 KB)</p>\n<p>The Python (3.7) envirnoment for the extensions needs:</p>\n<ul>\n<li>numpy</li>\n<li>tiffile</li>\n<li>tqdm</li>\n</ul>\n<p>*(dunno when <code>GetSurfaceDataLayout()</code> was introduced)</p>", "<p>The offset may be caused by indexing difference of argumentas between python and matlab. The GetSingleMask last 3 arguments should minus 1 because ending position in python is stopped before but covered in matlab.</p>\n<p>The 100x time is depends on sparsity and size of objects. For many small objects occupy a tiny space, SingleMask is faster.</p>\n<p>Christoph Sommer via Image.sc Forum &lt;<a href=\"mailto:notifications@imagej.discoursemail.com\">notifications@imagej.discoursemail.com</a>&gt; \u4e8e 2023\u5e741\u670817\u65e5\u5468\u4e8c 18:00\u5199\u9053\uff1a</p>", "<aside class=\"quote no-group\" data-username=\"mendel\" data-post=\"19\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mendel/40/17740_2.png\" class=\"avatar\"> mendel :</div>\n<blockquote>\n<p>The offset may be caused by indexing difference of argumentas between python and matlab. The GetSingleMask last 3 arguments should minus 1 because ending position in python is stopped before but covered in matlab.</p>\n</blockquote>\n</aside>\n<p>I did a quick test comparing the re-binarized label image (red) against Imaris\u2019 <code>GetMask</code> (green) as the reference.</p>\n<p>Screenshot of a single slice:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/e/7e2313c8dcaa67bf79e495983bc12aee4209f73a.png\" data-download-href=\"/uploads/short-url/hZRjt3qywjrrzRvWbrXQraQdBSW.png?dl=1\" title=\"lab_msk_0_rgb\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e2313c8dcaa67bf79e495983bc12aee4209f73a_2_575x500.png\" alt=\"lab_msk_0_rgb\" data-base62-sha1=\"hZRjt3qywjrrzRvWbrXQraQdBSW\" width=\"575\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e2313c8dcaa67bf79e495983bc12aee4209f73a_2_575x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e2313c8dcaa67bf79e495983bc12aee4209f73a_2_862x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/e/7e2313c8dcaa67bf79e495983bc12aee4209f73a_2_1150x1000.png 2x\" data-dominant-color=\"100F00\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">lab_msk_0_rgb</span><span class=\"informations\">1992\u00d71731 16.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Most of the non-yellow pixels are not at the bounding-box edge and I can\u2019t see an obvious pattern\u2026 My guess is there\u2019s a slight variation of how <code>GetSingleMask</code> and <code>GetMask</code> draw the border internally. Anyway, I think that\u2019s good enough for my purposes.</p>\n<aside class=\"quote no-group\" data-username=\"mendel\" data-post=\"19\" data-topic=\"51699\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mendel/40/17740_2.png\" class=\"avatar\"> mendel :</div>\n<blockquote>\n<p>The 100x time is depends on sparsity and size of objects. For many small objects occupy a tiny space, SingleMask is faster</p>\n</blockquote>\n</aside>\n<p>Yes, that might be the case.</p>\n<p>Thanks again for all your help and input Mendel! Highly appreciated!</p>\n<p>Cheers, Chris</p>", "<p>Fixed a bug when Surfaces exceeded image borders. Updated extension can be found at:</p>\n<p><a href=\"https://github.com/sommerc/PyImarisExportSWCWithSurfaceIntersection/blob/bb654a35dba95262695142248056c8326843c98d/xt_swc/export_surface_label_image.py\" rel=\"noopener nofollow ugc\">export_surface_label_image.py</a></p>\n<p>C</p>"], "78324": ["<p>Hello.</p>\n<p>I might be formatting my message wrong but I am really stuck on something and I am in desperate need for some help.</p>\n<p>So I want to call the plugin Linear Stack Alignment with SIFT in python and I do not know what\u2019s wrong\u2026 I\u2019ve uploaded my script in order for you to get a better idea.</p>\n<p>So basically my arguments are the following, input file is the full path to my 3D tif file and the output file is the path where I want to save the registered 3D tif file (name of the output file included as well). So I have a doubt  that I should include my image like this in the line output = ij.py.run_plugin(plugin, args, input_img) by previously loading it. I\u2019ve read somewhere that I should only input the path but it does not seem to work or there is an error somewhere else. I\u2019ve checked the path and my 3D tif file is loaded correctly (I\u2019m saying this because multiple times I got this error There are no images open                                                                                                                                                                                                                                                                                                     [java.lang.Enum.toString] There are no images open[java.lang.Enum.toString]). SO I\u2019ve checked by printing its dimensions and it\u2019s working. I\u2019ve also checked by printing the version of Fiji is my path for finding it was good and it does print the version.</p>\n<p>Currently I am using this version of my code that I\u2019ve uploaded and I still get the error : There are no images open                                                                                                                                                                                                                                                                                                     [java.lang.Enum.toString] There are no images open[java.lang.Enum.toString]<br>\n<a class=\"attachment\" href=\"/uploads/short-url/nXcyyfZWvaj8DQXmghC81wBuGJc.txt\">script.txt</a> (301 Bytes)</p>\n<p>I would also be grateful if there is any documentation of what arguments are accepted by this plugin and if you could provide a link because I could not find any. I am asking this because I want to exactly reproduce the results that I get using the interface on Windows.</p>\n<p>Also I am not sure how to save my output.</p>\n<p>Thank you in advance.</p>"], "78327": ["<p>Hello,</p>\n<p>My first post, hope I can explain what I mean sufficiently well. I have a dataset acquired using focused ion beam slice and view. This means that my dataset contains a stack of 2D images. I need to align these after aquisition and it is driving me nuts. I believe that it should be a relatively simple task.</p>\n<p>To the problem: The ion beam makes a trench in my sample at normal incidence angle. I image the 2D image on the surface created by the ion beam but at a incidence angle of 54 degrees (tilt correction is made \u201con the fly\u201d so no problem here). Next the ion beam shaves off a layer and I acquire the next image and so on. This means that the location of the image will shift in the z-direction as this process progresses. Aligning the images is easy and works well. But the coordinate system will not be as I like it to be. Due to the geometry of the setup the x and y cordinate system is in the plane of the image but the z-axis will not be aligned with the aligned slides. Instead the image stack will be progressing in a direction inclined to the z-axis direction. The deviation is either 54 degrees or 90-54 degrees (have to think more to be certain). Does anyone know how to correct for this?</p>\n<p>I believe that what I wanna do is illustrated correctly in the attached image. Change from coordinate system (x,y,z\u2019)-&gt;(x,y,z).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2.jpeg\" data-download-href=\"/uploads/short-url/9o3YKihZ29E9lycDEkCUU4Hc4Ei.jpeg?dl=1\" title=\"FIBSEM tilt\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2_2_690x306.jpeg\" alt=\"FIBSEM tilt\" data-base62-sha1=\"9o3YKihZ29E9lycDEkCUU4Hc4Ei\" width=\"690\" height=\"306\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2_2_690x306.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41cbf135c9f8e8ed344735812868cc820ad3fba2.jpeg 2x\" data-dominant-color=\"C9D4E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">FIBSEM tilt</span><span class=\"informations\">1031\u00d7458 23.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks,<br>\nDaniel</p>", "<p>Personally I find all the angles involved in FIB-SEM the trickiest thing (or well, second trickiest- finding the stupid target is worse), so maybe I\u2019m not fulling understanding the issue, but here\u2019s my take: With respect to the SEM gun, the images should all be aligned in z. The 54 degree angle is the angle between the SEM and FIB guns, but that won\u2019t change the angle at which the SEM is scanning each image. So the z axis would be defined by what the SEM is seeing, not at what you are milling.</p>\n<p>If that\u2019s not exactly what you\u2019re describing, could you maybe link to a small reduced dataset (scaled down/cropped so we don\u2019t kill the internet)?</p>", "<p>The SEM imaging is very surface sensitive, so you\u2019ll see more or less the free surface created by FIB cut under the angle of 36 \u00b0 (90 \u00b0-54 \u00b0).<br>\nThe information depth depends on electron beam energy and detector used.<br>\nFocusing problems (depth of field) can be negligible by setting the imaging parameters.<br>\nTilt correction is often applied during SEM image acquisition.</p>\n<ul>\n<li>The sample itself may shift during the long-time milling process due to stage drift and thermal drift of the sample.</li>\n<li>During FIB cut, the position of objects in the cut surface and the working distance will also change for geometrical reasons (see image).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3.png\" data-download-href=\"/uploads/short-url/29jh7917k2XCJCiIhmsFvnbO00P.png?dl=1\" title=\"FIB_Cut_Shift\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_666x500.png\" alt=\"FIB_Cut_Shift\" data-base62-sha1=\"29jh7917k2XCJCiIhmsFvnbO00P\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_666x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_999x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0f11ca21cd3e535bab97b75654477762aa16cdb3_2_1332x1000.png 2x\" data-dominant-color=\"FBF7F7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">FIB_Cut_Shift</span><span class=\"informations\">3000\u00d72250 184 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div>\n</li>\n</ul>\n<p>The change in working distance may be (partly) compensated by applying a function like \u201ctrack working distance\u201d during milling.</p>\n<p>The above-mentioned effects will depend on the object size and required resolution.<br>\nDrift of objects if the acquired series of SEM images may be compensated by tracking objects in different images or bey tracking fiducial markers. Depending on your software such corrections may be applied in the process of image acquisition.</p>\n<p>There are a low of publications (and PhD thesis) about FIB tomography.<br>\nMaybe you\u2019ll find something for your system.</p>\n<p>The post-processing of the images (image stack) is often done using expensive commercial software (e.g. AVIZO).<br>\nIf you don\u2019t have access: a lot of work was done suing free software (e.g. ImageJ/Fiji, IMOD) or Matlab add-ons.</p>\n<p>Some references:<br>\nImage registration (correction of shift): <a href=\"http://bigwww.epfl.ch/thevenaz/stackreg/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">StackReg</a></p>\n<p>Phyton based:<br>\n<a href=\"https://fibtracking.readthedocs.io/en/latest/intro.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://fibtracking.readthedocs.io/en/latest/intro.html</a></p>"], "78332": ["<p>Hello. I am using cellfinder and meet the following issue, does that mean it can\u2019t work because my work station is 128 threading, more than 63??   Does anyone know how to process it? Thanks a lot!</p>\n<p>Exception in thread Thread-1:<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\threading.py\u201d, line 932, in _bootstrap_inner<br>\nself.run()<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\threading.py\u201d, line 870, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\multiprocessing\\pool.py\u201d, line 519, in _handle_workers<br>\ncls._wait_for_updates(current_sentinels, change_notifier)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\multiprocessing\\pool.py\u201d, line 499, in _wait_for_updates<br>\nwait(sentinels, timeout=timeout)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\multiprocessing\\connection.py\u201d, line 879, in wait<br>\nready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\multiprocessing\\connection.py\u201d, line 811, in _exhaustive_wait<br>\nres = _winapi.WaitForMultipleObjects(L, False, timeout)<br>\nValueError: need at most 63 handles, got a sequence of length 128</p>", "<p>I\u2019m not sure about the exact issue, but I don\u2019t think cellfinder will be able to efficiently use 128 cores. Try setting <code>--n-free-cpus 100</code>.</p>", "<p>thanks a lot! This issus is solved by your suggestion, however, it just come with the following problem, but my command line is: cellfinder -s G:/cz/signaltif -b G:/cz/bgtif -o G:/cz/output -v 5 5 5 --orientation sal --atlas allen_mouse_50um --no-classification --n-free-cpus 100 --ball-xy-size 15 --ball-z-size 15, which suggest I don\u2019t want cell classification.</p>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\runpy.py\u201d, line 194, in _run_module_as_main<br>\nreturn _run_code(code, main_globals, None,<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\runpy.py\u201d, line 87, in <em>run_code<br>\nexec(code, run_globals)<br>\nFile \"C:\\Users\\ps.conda\\envs\\cellfinder\\Scripts\\cellfinder.exe_<em>main</em></em>.py\", line 7, in <br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\cellfinder\\main.py\u201d, line 96, in main<br>\nrun_all(args, what_to_run, atlas)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\cellfinder\\main.py\u201d, line 210, in run_all<br>\npoints = get_cells(args.paths.classified_points, cells_only=True)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\imlib\\IO\\cells.py\u201d, line 23, in get_cells<br>\nreturn get_cells_xml(cells_file_path, cells_only=cells_only)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\imlib\\IO\\cells.py\u201d, line 55, in get_cells_xml<br>\nwith open(xml_file_path, \u201cr\u201d) as xml_file:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018G:/cz/output\\points\\cell_classification.xml\u2019</p>", "<p>Strange. Could you try running again but setting a new output directory?</p>", "<p>I retried it but just got the same result, then I cancelled the \u201cno-classification\u201d, but met the following issue:<br>\nTraceback (most recent call last):<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\runpy.py\u201d, line 194, in _run_module_as_main<br>\nreturn _run_code(code, main_globals, None,<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\runpy.py\u201d, line 87, in <em>run_code<br>\nexec(code, run_globals)<br>\nFile \"C:\\Users\\ps.conda\\envs\\cellfinder\\Scripts\\cellfinder.exe_<em>main</em></em>.py\", line 7, in <br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\cellfinder\\main.py\u201d, line 96, in main<br>\nrun_all(args, what_to_run, atlas)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\cellfinder\\main.py\u201d, line 173, in run_all<br>\npoints = classify.main(<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\cellfinder_core\\classify\\classify.py\u201d, line 66, in main<br>\nmodel = get_model(<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\cellfinder_core\\classify\\tools.py\u201d, line 46, in get_model<br>\nmodel.load_weights(model_weights)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u201d, line 70, in error_handler<br>\nraise e.with_traceback(filtered_tb) from None<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\h5py_hl\\files.py\u201d, line 567, in <strong>init</strong><br>\nfid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)<br>\nFile \u201cC:\\Users\\ps.conda\\envs\\cellfinder\\lib\\site-packages\\h5py_hl\\files.py\u201d, line 231, in make_fid<br>\nfid = h5f.open(name, flags, fapl=fapl)<br>\nFile \u201ch5py_objects.pyx\u201d, line 54, in h5py._objects.with_phil.wrapper<br>\nFile \u201ch5py_objects.pyx\u201d, line 55, in h5py._objects.with_phil.wrapper<br>\nFile \u201ch5py\\h5f.pyx\u201d, line 106, in h5py.h5f.open<br>\nOSError: Unable to open file (truncated file: eof = 152043520, sblock-&gt;base_addr = 0, stored_eof = 184831728)</p>", "<p>Could you attach the log file?</p>"], "78333": ["<p>Hi everyone, hi <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a>, hi <a class=\"mention\" href=\"/u/hinerm\">@hinerm</a>,</p>\n<p>Sorry to perform forum thread necromancy. After some testing, i had to correct 2 lines on GitHub (the classes \u201cScriptGenerator\u201d and \u201cScriptRunner\u201d where called with bad paths when calling from \u201crun-script\u201d and \"gen-script).</p>\n<p>The generated Python scripts launch well (i have a correct front-end window with fields to enter imageID and else), but when i click \u201cRun Script\u201d, i have this error:</p>\n<pre><code class=\"lang-auto\">OpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release\n[ERROR] java.lang.NullPointerException\n\tat net.imagej.omero.OMEROSession.&lt;init&gt;(OMEROSession.java:181)\n\tat net.imagej.omero.OMEROSession.&lt;init&gt;(OMEROSession.java:157)\n\tat net.imagej.omero.module.ModuleAdapter.session(ModuleAdapter.java:393)\n\tat net.imagej.omero.module.ModuleAdapter.launch(ModuleAdapter.java:212)\n\tat net.imagej.omero.module.ScriptRunner.invoke(ScriptRunner.java:87)\n\tat net.imagej.omero.module.ScriptRunner.invoke(ScriptRunner.java:70)\n\tat net.imagej.omero.module.ScriptRunner.main(ScriptRunner.java:112)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat net.imagej.launcher.ClassLauncher.launch(ClassLauncher.java:279)\n\tat net.imagej.launcher.ClassLauncher.run(ClassLauncher.java:186)\n\tat net.imagej.launcher.ClassLauncher.main(ClassLauncher.java:87)\n</code></pre>\n<p>It looks like the problem come from the connection with OMERO (OMERO session), so, is it something like a permissions problem? How can i fix it?</p>\n<p>Thanks by advance, Marc.</p>", "<p>Hello <a class=\"mention\" href=\"/u/mmongy\">@mmongy</a>,</p>\n<p>I took the liberty to split your message from the rest of the topic.</p>\n<p>The null pointer exception refers to this line:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181\" target=\"_blank\" rel=\"noopener\">imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROSession.java#L181</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-java\">\n      <ol class=\"start lines\" start=\"171\" style=\"counter-reset: li-counter 170 ;\">\n          <li>\t\tfinal OMEROCredentials credentials) throws OMEROException</li>\n          <li>\t{</li>\n          <li>\t\tthis(omeroService, server, credentials, //</li>\n          <li>\t\t\tnew omero.client(server.host, server.port));</li>\n          <li>\t}</li>\n          <li>\n          </li>\n<li>\tprivate OMEROSession(final OMEROService omeroService,</li>\n          <li>\t\tfinal OMEROServer omeroServer, final OMEROCredentials omeroCredentials,</li>\n          <li>\t\tfinal omero.client omeroClient) throws OMEROException</li>\n          <li>\t{</li>\n          <li class=\"selected\">\t\tomeroCredentials.validate();</li>\n          <li>\n          </li>\n<li>\t\tthis.omeroService = omeroService;</li>\n          <li>\t\tthis.server = omeroServer;</li>\n          <li>\n          </li>\n<li>\t\tinitializeSession(omeroCredentials, omeroClient);</li>\n          <li>\t}</li>\n          <li>\n          </li>\n<li>\t// -- Data transfer --</li>\n          <li>\n          </li>\n<li>\t/**</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>So <code>omeroCredentials</code> may be null.</p>\n<p>I\u2019m not really familiar with the server side of OMERO and running script on it, but the error seems to indicate the absence of credentials. How do you collect the credentials ?</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>,</p>\n<p>I haven\u2019t written that piece of code. I just tried to follow the track given by the error message to see where it fails, and it lead to this. I didn\u2019t really thought about this.</p>\n<p>For me, the credentials are the username and password you type at the web frontend, or on the frontend of an external client like OMERO.insight, or in the command line for more \u201ccrude\u201d clients like OMERO.downloader, to connect to OMERO.</p>\n<p>When using ImageJ-OMERO, i figured this problem would probably have already been addressed by the developers. For this reason, my first thought was the fact that ImageJ-OMERO couldn\u2019t access to the credentials already entered on the web frontend, for some reason (likely misconfigured permissions, i encountered this problem too often in cases where program can\u2019t be launched or data can\u2019t be accessed).</p>\n<p>In fact, i\u2019m probably less familiar than you with the server side of OMERO, in terms of programming. The best people that could answer this are either <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> or <a class=\"mention\" href=\"/u/hinerm\">@hinerm</a>.</p>\n<p>Sorry for that, i\u2019m learning as i go along.</p>\n<p>Thanks anyway, Marc.</p>", "<p><a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>, however, you\u2019re probably right: when trying to get to the source of the error, in OMEROCredentials, here is the problematic function:</p>\n<pre><code class=\"lang-auto\">public void validate() {\n\t\tif (user == null || password == null) {\n\t\t\tthrow new IllegalArgumentException(\"Invalid credentials: \" +\n\t\t\t\t\"must specify either session ID OR username+password\");\n\t\t}\n\t}\n</code></pre>\n<p>and by looking inside the file, i don\u2019t know how the \u201cuser\u201d and \u201cpassword\u201d are supposed to be obtained. Normally, you should import omero.gateway to get them, isn\u2019t it?</p>\n<p>Marc.</p>", "<aside class=\"quote no-group\" data-username=\"mmongy\" data-post=\"4\" data-topic=\"78333\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/51bf81/40.png\" class=\"avatar\"> Marc Mongy:</div>\n<blockquote>\n<p>i don\u2019t know how the \u201cuser\u201d and \u201cpassword\u201d are supposed to be obtained. Normally, you should import omero.gateway to get them, isn\u2019t it?</p>\n</blockquote>\n</aside>\n<p>Note that these are the ImageJ-OMERO <a href=\"https://github.com/imagej/imagej-omero/blob/7d3312a57c058d7f0cd3f9781060eabbf9ac7b49/src/main/java/net/imagej/omero/OMEROCredentials.java\">credentials wrapper</a>.</p>\n<p>I believe the way credentials work was changed in the ImageJ-OMERO refactoring, so I am guessing/hoping there is just a missing connection (passing the credentials around) that wasn\u2019t hooked up properly.</p>\n<p>I have to say that I only have experience running ImageJ-OMERO client-side. You\u2019re using the server-side paradigm, correct?</p>", "<p>It is not a web app installed on OMERO.web, so yes, i guess.</p>"], "78335": ["<p>Has anyone managed to run the Columbus software in a CentOS 6 Docker container? I have got it to install in the container, but the database service does not seem to start up properly when I try to start the Columbus service.</p>\n<p>Thanks</p>\n<p>Sam Braithwaite</p>"], "78337": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da.jpeg\" data-download-href=\"/uploads/short-url/8MR7tJUprZj4nOVgXhFNJOFMksO.jpeg?dl=1\" title=\"HWP1_5-0009\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_662x500.jpeg\" alt=\"HWP1_5-0009\" data-base62-sha1=\"8MR7tJUprZj4nOVgXhFNJOFMksO\" width=\"662\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_1324x1000.jpeg 2x\" data-dominant-color=\"838B7C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HWP1_5-0009</span><span class=\"informations\">1920\u00d71448 164 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62.jpeg\" data-download-href=\"/uploads/short-url/hyfOgesYpPuUiKp8fhgl7fQc9oK.jpeg?dl=1\" title=\"HWP1_5-0002\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_662x500.jpeg\" alt=\"HWP1_5-0002\" data-base62-sha1=\"hyfOgesYpPuUiKp8fhgl7fQc9oK\" width=\"662\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b043d28cbedaad68aeacfcab7fd4e022f810f62_2_1324x1000.jpeg 2x\" data-dominant-color=\"7D8578\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HWP1_5-0002</span><span class=\"informations\">1920\u00d71448 194 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628.jpeg\" data-download-href=\"/uploads/short-url/sqXR4JzDtxHOUr5xAK0dwb9Fl1K.jpeg?dl=1\" title=\"HWP9_HW65_5-0018\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_662x500.jpeg\" alt=\"HWP9_HW65_5-0018\" data-base62-sha1=\"sqXR4JzDtxHOUr5xAK0dwb9Fl1K\" width=\"662\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749344bd5a840ae4257ee1133c8b91d36b3b628_2_1324x1000.jpeg 2x\" data-dominant-color=\"697962\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HWP9_HW65_5-0018</span><span class=\"informations\">1920\u00d71448 249 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hi,</p>\n<p>Background:<br>\nI\u2019m working with additive manufacturing, and need to calculate cross section area of stringer beads in polished specimens. Please see the example photos showing typical images, if the upload didn\u2019t work, the most relevant example can be found here:</p>\n<p><a href=\"https://sintef-my.sharepoint.com/:i:/g/personal/trond_arne_hassel_sintef_no/EQDGCuom0MxLm_vObIlLbl8BeLiLGts9zo-IoHXW-sH25w?e=8HZ5Za\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://sintef-my.sharepoint.com/:i:/g/personal/trond_arne_hassel_sintef_no/EQDGCuom0MxLm_vObIlLbl8BeLiLGts9zo-IoHXW-sH25w?e=8HZ5Za</a></p>\n<p>Pitted sample: <a href=\"https://sintef-my.sharepoint.com/:i:/g/personal/trond_arne_hassel_sintef_no/EQDGCuom0MxLm_vObIlLbl8BeLiLGts9zo-IoHXW-sH25w?e=4pHjKK\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Sign in to your account</a></p>\n<p>Analysis goal:<br>\nI have quite a few of these, so I would prefer to batch process them using a macro in ImageJ/Fiji. I\u2019ve figured out how to set scale, convert to 8 bit greyscale and do threshold adjustment, but I\u2019ve yet to find an automated way of measuring only the area of the bead (the rounded deposit on top of the plate), not the substrate (the bright rectangular section in the lower part of the image).</p>\n<p>The position of the substrate surface is not in the exact same height in each photo, and neither is it perfectly level. An horizontal assumption could still be acceptable, since manual cropping, which is my current alternative, also requires a horizontal line for crop area selection.</p>\n<p>Macro for setting scale, converting to grayscale and thresholding:</p>\n<p>run(\u201cSet Scale\u2026\u201d, \u201cdistance=715.0313 known=0.5 unit=mm global\u201d);<br>\nrun(\u201c8-bit\u201d);<br>\nsetAutoThreshold(\u201cDefault\u201d);<br>\n//run(\u201cThreshold\u2026\u201d);<br>\n//setThreshold(0, 45);<br>\nrun(\u201cConvert to Mask\u201d);<br>\nrun(\u201cClose\u201d);<br>\nrun(\u201cMeasure\u201d);</p>\n<p>I believe the this topic is close to what I\u2019m trying to achieve: <a href=\"https://forum.image.sc/t/crop-image-between-two-lines/4405/4\">Crop image between two lines - Image Analysis - Image.sc Forum</a>, but I\u2019ve not been successful in modifying the macro presented there to provide a useable result in my case.</p>\n<p>Modified from \u201cCrop image between two lines\u201d, that doesn\u2019t work the way it should (either stops the rectangle above the bead, or way into the substrate, instead of the desired top surface of the substrate):</p>\n<p>run( \u201cSelect All\u201d );<br>\nsetKeyDown( \u201calt\u201d );<br>\nprojection = getProfile();<br>\nmin = Array.findMinima( projection, 7.3 );<br>\nsetKeyDown( \u201cnone\u201d );<br>\nif ( min[0] &lt; min[1] ) { y = min[0]; } else { y = min[1]; }<br>\nmakeRectangle( 0, 0, getWidth(), abs(min[1] ) );<br>\nrun( \u201cCrop\u201d );<br>\nrun( \u201cSelect None\u201d );</p>\n<ol>\n<li>\n<p>Do any know of an automated way to crop the image or make a selection so that only the actual stringer area is measured?</p>\n</li>\n<li>\n<p>In addition, I\u2019m also looking for an automated way to include dark spots (pitting) from preparation in the assessed cross section area, shown in the photo of the pitted sample. Any suggestions on a non-manual way to do this?</p>\n</li>\n</ol>", "<p>To be a bit more clear, I want to only measure the hatched area as shown in this image:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec.jpeg\" data-download-href=\"/uploads/short-url/J9au7hAkTteOBfqxUeX3E2VLn6.jpeg?dl=1\" title=\"HWP1__5-0000_markup\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_662x500.jpeg\" alt=\"HWP1__5-0000_markup\" data-base62-sha1=\"J9au7hAkTteOBfqxUeX3E2VLn6\" width=\"662\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/5/051a80b0f3882e0e853af5fd17199416046d60ec_2_1324x1000.jpeg 2x\" data-dominant-color=\"707367\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HWP1__5-0000_markup</span><span class=\"informations\">1920\u00d71448 189 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Kind regards,<br>\nTrond</p>", "<p>A proposal (valid for this image);<br>\nAppreciate any feedback. Thanks in advance.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da.jpeg\" data-download-href=\"/uploads/short-url/8MR7tJUprZj4nOVgXhFNJOFMksO.jpeg?dl=1\" title=\"HWP1_5-0009\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_331x250.jpeg\" alt=\"HWP1_5-0009\" data-base62-sha1=\"8MR7tJUprZj4nOVgXhFNJOFMksO\" width=\"331\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_331x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_496x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3d973939fe5cfc1e8a4de47dab9d4010b96093da_2_662x500.jpeg 2x\" data-dominant-color=\"838B7C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HWP1_5-0009</span><span class=\"informations\">1920\u00d71448 164 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-auto\">macro \"measuring the area of the bead\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\nimg=getImageID();\n// Start batch mode\nsetBatchMode(true);\nselectImage(img);\nrun(\"Duplicate...\", \"title=1\");\nclose(\"\\\\Others\");\n//---------------------------\n// Image processing\nrun(\"8-bit\");\nsetOption(\"BlackBackground\", true);\nrun(\"Convert to Mask\");\n//setTool(\"wand\");\ndoWand(33, 33, 58.0, \"Legacy\");\nsetBackgroundColor(255,255, 255);\nrun(\"Clear Outside\");\nsetBackgroundColor(0,0, 0);\nrun(\"Select None\")\n//------------------------------\n// start of scan\ntotal_number=0;\nh=getHeight();\nw=getWidth();\n//------------------------------\n//vertical scan\nfor(i=0;i&lt;h;i++){\n            number=0;\n\t//horizontal scan\t\n\tfor(j=0;j&lt;w;j++){\n//makeLine(0,i,w,i);\nif(getPixel(j,i)&gt;0)\nnumber+=1;\n}\n//print(\"for the row \"+i+\" the area is: \"+  number);\nif(number&lt;w)\ntotal_number=total_number+=number;\n}\nprint(\"The area of the bead =\", total_number+ \" px\");\n// end of scan\n//------------------------------\n// End of processing\n// End of batch mode\nsetBatchMode(false);\nclose();\nexit();\n}\ntype or paste code here\n</code></pre>", "<p>I have a sort of odd and a bit cumbersome solution. I\u2019ve only tested it on the first image, but it works for me\u2026 (it should work on all images)<br>\nThe idea is to reslice the image and use a 2D fill to fill the bead, then subtract the images to get only the bead. (For some reason the reslicing doesn\u2019t work on just one image, hence the odd workaround to make a small stack first.)</p>\n<pre><code class=\"lang-auto\">titel_orig = getTitle();\nrun(\"Duplicate...\", \" \"); //keep the orginal\nrename(\"img1\");\nrun(\"8-bit\");\nsetAutoThreshold(\"Default dark\");\nsetOption(\"BlackBackground\", true);\nrun(\"Convert to Mask\");\nrun(\"Fill Holes\"); //fill small holes created by thresholding\n// make a stack of four images, the first and last black\nrun(\"Duplicate...\", \" \");\nrename(\"img2\");\nrun(\"Images to Stack\", \"  title=img \");\nsetSlice(2);\nrun(\"Add Slice\"); //adds slice after the active one\nrun(\"Reverse\"); //black slice at the beginning\nsetSlice(3);\nrun(\"Add Slice\"); //add black slice at end\nrename(\"img3\");\nstack1 = getTitle();\n\n//reslice the stack and fill holes - this fills the bead, but not the substrate, because that goes to the image border\nrun(\"Invert\", \"stack\");\nrun(\"Reslice [/]...\", \"output=1.000 start=Top avoid\");\nrename(\"img4\");\nrun(\"Fill Holes\", \"stack\");\nrun(\"Reslice [/]...\", \"output=1.000 start=Top avoid\");\nrename(\"img5\")\nstack2 = getTitle();\n\n//subtract the two images to leave only the bead\nimageCalculator(\"Subtract create stack\", stack2,stack1);\n//delete the extra images\nsetSlice(1);\nrun(\"Delete Slice\"); \nsetSlice(2);\nrun(\"Delete Slice\"); \nsetSlice(2);\nrun(\"Delete Slice\"); \nrename(titel_orig + \"_area\");\n\n//measure area\nrun(\"Set Measurements...\", \"area display redirect=None decimal=0\");\nrun(\"Measure\");\n\n//tidy-up\nclose(\"img*\");\n</code></pre>", "<p>Thanks for the proposal.</p>\n<p>I\u2019ve done a bit further testing with some other images and found a span from very good performance, to not so good (worst in a normal image was 58 % deviation and it did not handle images without a substrate at all). I\u2019ve also gotten another proposal that seemed to perform better in general, so I probably won\u2019t proceed with this one.</p>\n<p>I\u2019m rather inexperienced when it comes to image analysis, so do you mind explaining briefly how this macro avoids adding the substrate to the area count?</p>\n<p>Kind regards,<br>\nTrond</p>", "<p>Hi Julia,</p>\n<p>Thanks for helping.</p>\n<p>I\u2019ve also compared your macro along with the one proposed by Mathew, and found that it worked ok after adding a threshold at the end to avoid measuring the whole image. Your macro also had some deviations, up to 36 % compared to a manual polygon approximation of the shape and deviations in the range 10-20 % were common, so I\u2019ll be proceeding with another proposal.</p>\n<p>It seemed like it had some challenges removing all of the substrate, so I would typically see lines of the substrate remaining in the measured image.</p>\n<p>Kind regards,<br>\nTrond</p>", "<p>Hi Trond,</p>\n<p>Thanks for testing it out! It was just an intresting problem to try and solve, hah.I knew it wouldn\u2019t be perfect, because it would only work it the substrate touched both sides of the image in each horizontal line.</p>\n<p>Glad you found an approach that works for you. Do you mind sharing how you solved the problem?</p>", "<p>Hi Julia,</p>\n<p>I received a macro from Herbie, where he used getProfile to acquire and subsequently evaluate the derivative of the grayscale image intensity, and then crop at the position of the maximum derivative, since the intensity change is strongest at the substrate/bead transition. The rest was handled by thresholding and measuring the area.</p>\n<p>Kind regards,<br>\nTrond</p>", "<p>Hi Trond,</p>\n<p>Thanks for the explanation! There are often multiple ways to solve the same problem, glad that one is working well for you.</p>"], "78341": ["<p>Hej <a class=\"mention\" href=\"/u/constantinpape\">@constantinpape</a> and other mobie-utils-python fans,</p>\n<p>I have been trying to understand how affine transformations and views work during project generation and I have prepared the following minimal example: <a href=\"https://github.com/CamachoDejay/mobie-python-examples/blob/0860d919f29ff6eb4853790eea01eb3f79c65a22/mobie-project-views_tmatrix.ipynb\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">mobie-python-examples/mobie-project-views_tmatrix.ipynb at 0860d919f29ff6eb4853790eea01eb3f79c65a22 \u00b7 CamachoDejay/mobie-python-examples \u00b7 GitHub</a></p>\n<p>At the beginning I was surprised that my view <code>\"Overlay_A\"</code> did not work as I expected:</p>\n<pre><code class=\"lang-auto\">source_list = [[\"original\"], [\"rot_transformed\"]]\nsettings = [ \n    {\"color\": \"green\", \"contrastLimits\": [0., 255.], \"blendingMode\": \"sum\"},\n    {\"color\": \"magenta\", \"contrastLimits\": [0., 255.], \"blendingMode\": \"sum\"},\n]\n\n\nmobie.create_view(dataset_folder, \"Overlay_A\",\n                  sources=source_list, \n                  display_settings=settings,\n                  overwrite=True)\n</code></pre>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/a/2af0b8d98c11a1fe0a81c298c0ff1ce476bd31c7.jpeg\" alt=\"Overlay_A\" data-base62-sha1=\"67RNv6sj7V9YcGwybk9KPBBg8UD\" width=\"216\" height=\"210\"></p>\n<p>This is because I used <code>source_list = [[\"original\"], [\"rot_transformed\"]]</code>, where <code>\"rot_transformed\"</code> included a transformation matrix during <code>mobie.add_image</code>.</p>\n<pre><code class=\"lang-auto\">transformation = [0.8034685581387304,0.46418531456408085,0.0,17949.15847223811,\n                 -0.46418531456408085,0.8034685581387304,0.0,206061.42501785012,\n                  0.0,0.0,0.9279168767591658,0.0]\n\nview = mobie.metadata.get_default_view(\"image\", raw_name, color=\"white\", source_transform={\"parameters\": transformation})\n\nmobie.add_image(...\n</code></pre>\n<p>I guess that this came from the misunderstanding that this affine matrix was <strong>stored</strong> as a transformation of the data which is always associated to it as default. I guess that the correct interpretation is that upon <code>mobie.add_image</code> a <strong>new view</strong> is created with the name <strong>rot_transformed</strong> and stored in the <strong>lm</strong> menu, but that when I create other views with that source this information is not kept.</p>\n<p>With that in mind I then created a <code>\"Overlab_B\"</code> view via:</p>\n<pre><code class=\"lang-auto\">source_list = [[\"original\"], [\"rot_transformed\"]]\n\ntIdentity =  [1., 0., 0., 0.,\n              0., 1., 0., 0.,\n              0., 0., 1., 0.]\n\ntransformation = [0.8034685581387304,0.46418531456408085,0.0,17949.15847223811,\n                 -0.46418531456408085,0.8034685581387304,0.0,206061.42501785012,\n                  0.0,0.0,0.9279168767591658,0.0]\n\ntransformations = [{'affine':tIdentity}, {'affine':transformation}]\ntransformations = [{'affine': {'parameters':tIdentity, 'sources': source_list[0]}}, {'affine': {'parameters':transformation, 'sources': source_list[1]}}]\nsettings = [ \n    {\"color\": \"green\", \"contrastLimits\": [0., 255.], \"blendingMode\": \"sum\"},\n    {\"color\": \"magenta\", \"contrastLimits\": [0., 255.], \"blendingMode\": \"sum\"},\n]\n\n\nmobie.create_view(dataset_folder, \"Overlay_B\",\n                  sources=source_list, \n                  display_settings=settings,\n                  source_transforms=transformations,\n                  overwrite=True)\n</code></pre>\n<p>This way I got the desired result:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/4/643af584eea3c63f855ca5fe413fb6cfe1e233af.jpeg\" alt=\"Overlay_B\" data-base62-sha1=\"eiG46EEHzk0ThvWu6PqROyhdWKX\" width=\"210\" height=\"210\"></p>\n<p>Is this correct <a class=\"mention\" href=\"/u/constantinpape\">@constantinpape</a>?</p>\n<p>Best regards,<br>\nRafa</p>", "<p>Hey <a class=\"mention\" href=\"/u/camachodejay\">@CamachoDejay</a>,<br>\nyour understanding of what happens is pretty much correct, but let me add a bit more context.<br>\nWhen you add a new image source to MoBIE, you have two options for how you can specify a transformation that should be applied to it:</p>\n<ol>\n<li>by adding it to the <code>view</code> for the source (what you do in the example notebook you have linked). In this case, the transformation is written to the view that is being created for the source, but (as you correctly realized), it will not be automatically used if you create other subsequent views that include the source you have added.</li>\n<li>by passing the transformation as value for the <code>transformation</code> argument (see <a href=\"https://github.com/mobie/mobie-utils-python/blob/977120d6b90154211e6c1c121a77e1d9e13e4186/mobie/image_data.py#L165\" class=\"inline-onebox\">mobie-utils-python/image_data.py at 977120d6b90154211e6c1c121a77e1d9e13e4186 \u00b7 mobie/mobie-utils-python \u00b7 GitHub</a>). In this case the transformation will be written to the underlying image metadata (i.e. to the bdv xml file or the ome.zarr metadata). In this case the transformation will always be applied when the source is loaded, and any transformations specified in a <code>view</code> are applied on top of the transformation in the image metadata.</li>\n</ol>\n<p>So in practice this means: if you have a transformation that should always be applied (i.e. also in subsequent views you would want to create), use option 2. If you potentially want to create views without the transformation then use option 1.</p>\n<p>Note that this is how things should work in theory, if you try it and run into any unexpected behavior please let me know; since there might be some cases I haven\u2019t run into and that don\u2019t fully work yet.<br>\nAlso, all of this is clearly not documented well enough. Any contribution to document it better (for example through example notebooks that show different ways how to add transformations and views) would be highly welcome ;).</p>", "<p>One more note: option 2 actually does not work for ome.zarr yet, only for bdv fileformats.<br>\nI have started a PR to work on this: <a href=\"https://github.com/mobie/mobie-utils-python/pull/94\" class=\"inline-onebox\">Start implementing transformation support for ome.zarr by constantinpape \u00b7 Pull Request #94 \u00b7 mobie/mobie-utils-python \u00b7 GitHub</a></p>", "<p>Hej <a class=\"mention\" href=\"/u/constantinpape\">@constantinpape</a>,</p>\n<p>I think I am running into some problems again <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Please have a look at the updated version of the notebook: <a href=\"https://github.com/CamachoDejay/mobie-python-examples/blob/388fcab147549c75e9a8b82633344cf389c03a11/mobie-project-views_tmatrix.ipynb\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">mobie-python-examples/mobie-project-views_tmatrix.ipynb at 388fcab147549c75e9a8b82633344cf389c03a11 \u00b7 CamachoDejay/mobie-python-examples \u00b7 GitHub</a></p>\n<p>As you will see in import option 3 I now add a transformation \u201con-the-fly\u201d via:</p>\n<pre><code class=\"lang-auto\">input_file = \"./data/blobs_crop_rot.ome.tif\"\nraw_name = \"rot_816nmPix_03\"\nunit = \"nanometer\"\nresolution = (1., 816, 816)\n\ntransformation = [0.8034685581387304,0.46418531456408085,0.0,17949.15847223811,\n                 -0.46418531456408085,0.8034685581387304,0.0,206061.42501785012,\n                  0.0,0.0,0.9279168767591658,0.0]\n\ntransformations = {'parameters': transformation}\n\nmobie.add_image(\n    input_path=input_file, \n    input_key='',  # the input is a single tif image, so we leave input_key blank\n    root=mobie_project_folder,\n    dataset_name=dataset_name,\n    image_name=raw_name,\n    menu_name=menu_name,\n    resolution=resolution,\n    chunks=chunks,\n    scale_factors=scale_factors,\n    transformation=transformations,\n    is_default_dataset=True,  # mark this dataset as the default dataset that will be loaded by mobie\n    target=target,\n    max_jobs=max_jobs,\n    unit=unit,\n    file_format=\"bdv.n5\"#\"ome.zarr\"\n)\n</code></pre>\n<p>However, when I create a view using this <code>image_source</code> via:</p>\n<pre><code class=\"lang-auto\">view_title = \"Overlay_C1\"\nsource_list = [[\"blobs_454nmPix\"], [\"rot_816nmPix_03\"]]\nsettings = [ \n    {\"color\": \"green\", \"contrastLimits\": [0., 255.], \"blendingMode\": \"sum\"},\n    {\"color\": \"magenta\", \"contrastLimits\": [0., 255.], \"blendingMode\": \"sum\"},\n]\n\n\nmobie.create_view(dataset_folder, view_name=view_title,\n                  sources=source_list,\n                  display_group_names=[source_list[0][0], source_list[1][0]], \n                  display_settings=settings,\n                  overwrite=True)\n</code></pre>\n<p>I run into problems.</p>\n<p>It seems that, while the transformation information was stored, I lost the \u201cscaling\u201d information. Note how once I go into this view in MoBIE I the overlay is not workin, here if I focus on the non-transformed image source:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9.jpeg\" data-download-href=\"/uploads/short-url/t31ukttM69mmJ6RfMp6pGd5I1Pb.jpeg?dl=1\" title=\"Focus_on_454nm\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_200x250.jpeg\" alt=\"Focus_on_454nm\" data-base62-sha1=\"t31ukttM69mmJ6RfMp6pGd5I1Pb\" width=\"200\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_200x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_300x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/b/cb968e176f08770fc4a88d038aa76f247c0cb2b9_2_400x500.jpeg 2x\" data-dominant-color=\"163816\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Focus_on_454nm</span><span class=\"informations\">524\u00d7649 29.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I can not see the second source because the pixel size after transformation is tiny. Here if I focus on that \u201ctransformed\u201d source, note that the pixel size should be <code>unit = \"nanometer\"; resolution = (1., 816, 816)</code>:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f.jpeg\" data-download-href=\"/uploads/short-url/bD6Aws3eFA8pDovIztho9DwLdCv.jpeg?dl=1\" title=\"Focus_on_816nm\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_200x250.jpeg\" alt=\"Focus_on_816nm\" data-base62-sha1=\"bD6Aws3eFA8pDovIztho9DwLdCv\" width=\"200\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_200x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_300x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51839517d706356fc8ca78f27af491e0e236fa6f_2_400x500.jpeg 2x\" data-dominant-color=\"14DE14\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Focus_on_816nm</span><span class=\"informations\">526\u00d7652 18.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is there something off or am I misunderstanding when the transformation takes place? Because the rotation angle and the translation seem to be ok.</p>\n<p>Saludos,<br>\nRafa</p>", "<p>Hey Rafa,<br>\nI think that when you pass a transform the scale information will be over-written. It would be good to handle this smarter in the MoBIE python library. I probably won\u2019t have time to look into this until Friday.<br>\nIn the meantime, could you try scaling the transformation matrix accordingly and see if this fixes the issue?</p>", "<p>Just a short update on this: I looked into adding support for the transformations with ome.zarr file format, but I decided to wait till ome.zarr v0.5 will be released (which will include significant more transformation functionality).</p>\n<p>But I extended the current code, in order to throw an error message when trying to add a transformation for ome.zarr, see <a href=\"https://github.com/mobie/mobie-utils-python/pull/94\" class=\"inline-onebox\">Start implementing transformation support for ome.zarr by constantinpape \u00b7 Pull Request #94 \u00b7 mobie/mobie-utils-python \u00b7 GitHub</a>.</p>"], "68103": ["<p>I am currently working with a large 3D dataset and when trying to use the 3D view there is a lot of stuttering when moving the image.<br>\n</p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0614790779ee6413aa01097930e43bf27b1b005.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0614790779ee6413aa01097930e43bf27b1b005.mp4\" rel=\"noopener nofollow ugc\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0614790779ee6413aa01097930e43bf27b1b005.mp4</a>\n    </source></video>\n  </div><p></p>\n<p>Currently running on windows 10 and the system should have enough power to render the image and while I do see power draw on the A6000 spike when moving the image, it seems that total GPU load remains low. I am currently using the latest version of <a href=\"https://github.com/haesleinhuepf/devbio-napari\" rel=\"noopener nofollow ugc\">Devbio</a> which is running 0.4.15 Napari but I have the same issue on multiple versions.</p>\n<p>Also, are there any tools to view slices on the inside of the cell? Haven\u2019t been able to play around with it much due to the stuttering but it is difficult to get a look of cells deeper inside for segmentation.</p>", "<p>Could you please provide more information about your dataset? size in x/y/z, and data type?</p>\n<p>I agree with you that napari is probably doing something egregious on the CPU that is preventing the GPU from doing its thing <img src=\"https://emoji.discourse-cdn.com/twitter/joy.png?v=12\" title=\":joy:\" class=\"emoji\" alt=\":joy:\" loading=\"lazy\" width=\"20\" height=\"20\">. I see that you have a Labels layer selected. My guess is that the labels layer is still doing a <code>get_value</code> even though the layer is invisible, which means a march through a 1D ray of pixels in 3D on every mouse move. A quick way to test this is: if you delete all Labels layers, do you see a speedup?</p>\n<p>Also, could you update to the latest napari from the main branch? A <a href=\"https://github.com/napari/napari/pull/4488\">recent change</a> might have actually fixed this.</p>\n<p>Regarding your inside slices, you can actually use <a href=\"https://github.com/napari/napari/blob/f810a6e24992af5c623b7bb79c3964dbb7670d2f/examples/clipping_planes_interactive_.py\">clipping planes</a> or <a href=\"https://github.com/napari/napari/blob/f810a6e24992af5c623b7bb79c3964dbb7670d2f/examples/volume_plane_rendering.py\">a slicing plane depiction</a> to look inside the volume. The latter is no longer experimental and is accessible from the UI since 0.4.16 thanks to <a class=\"mention\" href=\"/u/alisterburt\">@AlisterBurt</a>!</p>", "<p>What\u2019s very suspicious is that the stuttering seems to only show up when the volume is rotated 90 degrees with respect to the initial position, so there must be some anisotropic slow-down O.o I don\u2019t see the same issue with a simple 400x400x400 volume. A minimal working example would be great here to help us debug, <a class=\"mention\" href=\"/u/myrk\">@Myrk</a> !</p>\n<p>Maybe the get_value is indeed the culprit, but I\u2019m still surprised it\u2019s not rotation-independent.</p>", "<p>It\u2019s not surprising at all <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a>: <code>get_value</code> marches through the volume between the front face and the back face at a rate of 1 pixel per step. That means that it\u2019ll take 10x longer if viewing through a 1000-pixel-deep side of a rectangle than through a 100-pixel deep one.</p>\n<p>I don\u2019t know much about the performance characteristics of the GL side but I expect similar issues to come to play (though based on <a class=\"mention\" href=\"/u/myrk\">@Myrk</a>\u2019s description this is not a GL bottleneck).</p>", "<p>Sorry about the initial post, I was misunderstanding one of the percentage values in in nvidia-smi, so in reality GPU utilization for 3D is at 100%. So it might just be a too large an image, but I find it weird that it only stutters along certain axes.</p>\n<p>The image size is <code>345, 1600, 1700</code> and I still get the stuttering issue with just the image loaded. Minimal working size with minimal stuttering is <code>345, 900, 900</code>. I still get some stuttering around the longer axes though.</p>\n<p>This is a <code>345, 1200, 1200</code> crop.<br>\n</p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5df8d8894201c449747c81b3b6215871d1659d65.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5df8d8894201c449747c81b3b6215871d1659d65.mp4\" rel=\"noopener nofollow ugc\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5df8d8894201c449747c81b3b6215871d1659d65.mp4</a>\n    </source></video>\n  </div><p></p>\n<p>And <a class=\"mention\" href=\"/u/jni\">@jni</a> thank you for the slice viewing recommendations, I\u2019ll try those out.</p>", "<p>Yeah, so then I think it\u2019s a limitation with the Volume shader in VisPy, which doesn\u2019t really have many tricks for accelerating performance in large volumes. The anisotropic behaviour makes less sense in GL though because in all cases you\u2019re traversing every pixel \u2014 just in a different order and with different \u201cray\u201d size. Perhaps sampling a short ray is nonlinearly faster than sampling a long ray? Or maybe long rays are also cast along the \u201ccorners\u201d, so that you end up sampling more pixels?</p>\n<p>Anyway, for anyone that wants to play with this, you can use the <a href=\"https://github.com/napari/napari/blob/c92beba61781af09dc87add863214ae814dbe8a6/examples/viewer_fps_label.py\">viewer fps label example</a>, modified to have this large dataset, to verify that it is slower along the long axis of the volume:</p>\n<pre><code class=\"lang-python\">\"\"\"\nViewer FPS label\n================\n\nDisplay a 3D volume and the fps label.\n\"\"\"\nimport numpy as np\nimport napari\n\nimage = np.random.default_rng().random((345, 900, 1600), dtype=np.float32)  # about 2GB\nviewer = napari.Viewer(ndisplay=3)\n\ndef update_fps(fps):\n    \"\"\"Update fps.\"\"\"\n    viewer.text_overlay.text = f\"{fps:1.1f} FPS\"\n\nviewer.add_image(image)\nviewer.text_overlay.visible = True\nviewer.window.qt_viewer.canvas.measure_fps(callback=update_fps)\n\nif __name__ == '__main__':\n    napari.run()\n</code></pre>\n<p>For me (M1max MBP) it goes from ~30-40fps to 10-15fps, depending on the angle.</p>\n<p>One option, which I hope <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a> will be motivated to tackle <img src=\"https://emoji.discourse-cdn.com/twitter/stuck_out_tongue_winking_eye.png?v=12\" title=\":stuck_out_tongue_winking_eye:\" class=\"emoji\" alt=\":stuck_out_tongue_winking_eye:\" loading=\"lazy\" width=\"20\" height=\"20\">, is to add adaptive/nonlinear step size to the VisPy Volume shader. Two tricks that <a class=\"mention\" href=\"/u/royerloic\">@royerloic</a> told me about:</p>\n<ol>\n<li>the step size is 1 pixel at the start of the volume, but gradually increases as you march through it \u2014 eg every pixel for first 100 pix, every 2 pixels for next 200, every 4 pixels for next 400, etc. Dunno what the best function is here, but the key is that the \u201cback\u201d of the volume is rendered in increasingly less detail, and you never do all that much work even as the volume grows.</li>\n<li>the relative step size is adjusted dynamically based on the fps, so that as the fps starts to drop, you increase the step size, trading off quality for smooth movement. When the cursor stops moving, you jack up the quality again.</li>\n</ol>\n<p><a class=\"mention\" href=\"/u/brisvag\">@brisvag</a> if you want to pair up on these things in the near future let me know! Would be great to have more performant rendering in napari, and I think those two tricks are pretty low-hanging fruit.</p>", "<p>Sorry for the long delay, my notifications are not working as I thought\u2026</p>\n<p>I\u2019m not sure on point 1: that would mean that our maximum intensity projection is actually sampling differently different parts of the volume\u2026 Seems wrong.</p>\n<p>Point 2 is definitely reasonable; <a class=\"mention\" href=\"/u/kevinyamauchi\">@kevinyamauchi</a> already opened a WIP PR for it! <a href=\"https://github.com/napari/napari/pull/4764\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[WIP] adaptively set volume rendering ray step size by kevinyamauchi \u00b7 Pull Request #4764 \u00b7 napari/napari \u00b7 GitHub</a></p>", "<aside class=\"quote no-group\" data-username=\"brisvag\" data-post=\"7\" data-topic=\"68103\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/brisvag/40/39603_2.png\" class=\"avatar\"> Lorenzo Gaifas:</div>\n<blockquote>\n<p>I\u2019m not sure on point 1: that would mean that our maximum intensity projection is actually sampling differently different parts of the volume\u2026 Seems wrong.</p>\n</blockquote>\n</aside>\n<p>It\u2019s an approximation, and results in the foreground being more detailed than the background, which is \u201cnatural\u201d, but yes, not strictly correct. Ultimately with multiscale octrees we\u2019ll do this a lot, so you\u2019d better get used to the wrongness! <img src=\"https://emoji.discourse-cdn.com/twitter/rofl.png?v=12\" title=\":rofl:\" class=\"emoji\" alt=\":rofl:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I just had a thought for this after my brief introduction to the problem in the napari community meeting today.</p>\n<p>I think it was <a class=\"mention\" href=\"/u/myrk\">@Myrk</a> who mentioned attenuated mip is even worse performance, however it can be even more amenable to such optimization. We should be able to break the raycasting loop once the attenuation is effectively zero.</p>\n<p>For anyone familiar with the shader code, it could look something like this:</p>\n<pre><code class=\"lang-diff\">        sumval = sumval + clamp((val - clim.x) / (clim.y - clim.x), 0.0, 1.0);\n+        scale = exp(-u_attenuation * (sumval - 1) / u_relative_step_size);\n+        if( scale &lt; 1e-6 * clim.y ) {\n+            iter = nsteps;  // this breaks the loop on the next iteration\n+        } else if( val * scale &gt; maxval ) {\n-        scaled = val * exp(-u_attenuation * (sumval - 1) / u_relative_step_size);\n-        if( scaled &gt; maxval ) {\n            maxval = val * scale;\n            maxi = iter;\n            max_loc_tex = loc;\n         }\n</code></pre>\n<p>I just tested this with the provided example and see pretty significant performance improvement (up to 60 fps), but it\u2019s hard to tell how it might affect image quality depending on specifics of a dataset. There\u2019s probably a smarter way to check for \u201ccomplete attenuation\u201d to break the loop.</p>", "<p>Good idea; actually, can\u2019t we be <em>sure</em> when the attenuation is \u201ceffectively zero\u201d because we know the contrast limits (and thus the maximum possible value)?</p>\n<p>EDIT: by that I mean, there is an exact value, rather than a heuristic like <code>1e6</code>.</p>", "<p>Yes! I think this is what I meant by a smarter way to check for \u201ccomplete attenuation\u201d but your comment helped me think through it.</p>\n<p>I think this would mean breaking if <code>maxval &gt; clim.y * scale</code>? This gives me up to 80fps with the big cube of random values from above.</p>", "<p>I\u2019m a little confused\u2026 In my mind the contrast limits should apply to the value <em>after</em> attenuation? This might explain why attenuation and the clims and the attenuation constant have such annoying interplay, where as you increase the attenuation you have to decrease the contrast limits?</p>", "<p>(Either way, loving the discussion here!)</p>"], "78344": ["<p>Hi,<br>\nI generated a classifier model using CPA 3.0.4 based on features from the actin cytoskeleton of cells. When I plug this model into the ClassifyObjects module in CP 4.2.4 and run the pipeline on new images, I get an error message indicating that some measurements don\u2019t exist, but the appropriate modules are present and running. I would appreciate any help or insight I can get for this issue. I am attaching here all the required information for others to look into (error message, CP pipeline, CPA model, sample images).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png\" data-download-href=\"/uploads/short-url/sAwr8M64Nvc1FyOISchV1Hie36Y.png?dl=1\" title=\"errorCellProfiler\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png\" alt=\"errorCellProfiler\" data-base62-sha1=\"sAwr8M64Nvc1FyOISchV1Hie36Y\" width=\"421\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 2x\" data-dominant-color=\"CBD2DA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">errorCellProfiler</span><span class=\"informations\">430\u00d7510 73.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/5v48dYmZdYzze8TvbWMxMGMPWX.zip\">CPAmodel_CPpipeline.zip</a> (9.8 MB)</p>\n<p>Thank you!</p>\n<p>Victoria</p>"], "78346": ["<p>Hello everyone,</p>\n<p>It is my first time using Deeplabcut and any machine learning program. I have an issue with the training. First of all my system is: RTX3080 10GB memmory, Windows 10, CUDA version 11.6 (I tried different CUDA versions, didnt work either). My image size is 2048X2048 and I do not want to crop them, I have 900 labeled frames in total.</p>\n<p>Important config parameters: Bach size: 1, Max iterations: 500000, Max_inputsize: 6000, allow_growth=True. I ran <code>conda install cudnn -c conda-forge</code> after installing DeepLabCut and it installed everything normally.</p>\n<p>The training starts normally and I get the \u201cStarting training\u2026\u201d message. However, no new iterations can be seen four hours even when I set display_iterations to 10. No activity on GPU CUDA can be seen in the task menager. I am sure Deeplabcut recognizes my GPU because it occupies the memmory, but I see no activity in CUDA:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/8/b8e88e7e5f459aca16b3c6668a5a6c257f88bb43.png\" data-download-href=\"/uploads/short-url/qnM6uU7v1kxXEASWXLxiGYQ58xd.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/8/b8e88e7e5f459aca16b3c6668a5a6c257f88bb43.png\" alt=\"image\" data-base62-sha1=\"qnM6uU7v1kxXEASWXLxiGYQ58xd\" width=\"690\" height=\"494\" data-dominant-color=\"F5F8F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">972\u00d7697 23.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a38dcf7941c6770e14e97a4e4fbef04bf07b209d.png\" data-download-href=\"/uploads/short-url/nkRECqxBHNwIdfW5vtIhigwAnGR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a38dcf7941c6770e14e97a4e4fbef04bf07b209d.png\" alt=\"image\" data-base62-sha1=\"nkRECqxBHNwIdfW5vtIhigwAnGR\" width=\"690\" height=\"272\" data-dominant-color=\"1D1D1B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">707\u00d7279 8.62 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>In the Prompt, I get some non-fatal error messages including 2 root errors:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bf1bf68450a5a5dde292b5423e0a1ef001e0557a.png\" data-download-href=\"/uploads/short-url/rgD6E81BascFyPu6rpbkQnslkwO.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/f/bf1bf68450a5a5dde292b5423e0a1ef001e0557a.png\" alt=\"image\" data-base62-sha1=\"rgD6E81BascFyPu6rpbkQnslkwO\" width=\"690\" height=\"59\" data-dominant-color=\"171717\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1601\u00d7139 7.85 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And \u201cAllocator ran out of memmory\u201d messages:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c68224777bd796a48d22a0914bb3dc37d610a259.png\" data-download-href=\"/uploads/short-url/sk5mOOzrHQ4kP7RgThcRAIjIBNn.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c68224777bd796a48d22a0914bb3dc37d610a259.png\" alt=\"image\" data-base62-sha1=\"sk5mOOzrHQ4kP7RgThcRAIjIBNn\" width=\"690\" height=\"58\" data-dominant-color=\"1C1D1C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1901\u00d7161 15 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I am not sure if the memmory of my GPU is not enough or there is a mismatch between tensorflow, CUDA and Cudnn but I tried almost all 11.X CUDA versions? Any help you could provide is in great value for me. Thank you very much in advance.</p>", "<p>Hi Baris,<br>\nI\u2019m running a similar setup like you. If you use DLC 2.3, I encountered some issues with napari-gui so could not label my data.<br>\nWhat I did is I created a specific environment with DLC 2.2.3:</p>\n<pre><code class=\"lang-auto\">conda create --name DLC python=3.8\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\nconda install -c nvidia cuda-nvcc\npip install --upgrade pip\npip install \"tensorflow&lt;2.11\"\npip install --upgrade matplotlib==3.5.2\npip install deeplabcut[gui]==2.2.3\npip install torch\npip install -U wxPython\n</code></pre>\n<p>Check if you have cuda-nvcc in your environment.<br>\nTry these smoke tests for tensorflow:</p>\n<pre><code class=\"lang-auto\">python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre>\n<p>should return a tensor;</p>\n<pre><code class=\"lang-auto\">python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre>\n<p>should recognize your GPU;</p>\n<p>I ran into some errors when trying to use EfficiNet b6 or ResNet 152 where they ran out of memory. Went down to ResNet 50 and it\u2019s working.<br>\nHope this helps!</p>", "<p>Hi Gauss,</p>\n<p>I am using ResNet50. The smoke test you provided recognizes my GPU however, it shows 7427 MB memmory eventhough I have 10000 MB. Now, I decreased global_scale from 0.8 to 0.6 and it is working now. But the problem is I do not know what global_scale do exactly. Every spot in my images are important for me, do you know what does global_scale do and decreasing it is harmful for my data?</p>", "<p>I guess 2048x2048 might be a bit too much and downsampling would be a good idea.<br>\nHere\u2019s a thread on it. From my understanding it\u2019s a training parameter for the network to downscale the size while it\u2019s training. I guess Konrad could give you a good idea of what it is and does.</p>\n<aside class=\"quote\" data-post=\"1\" data-topic=\"66290\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/aendrs/40/56358_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/regarding-the-global-scale-parameter-and-its-use-during-inference/66290\">Regarding the global_scale parameter and its use during inference</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    I have a trained model with a global_scale=0.25 parameter in the pose_cfg.yaml files. \nMy question is, when I use the function deeplabcut.analyze_videos to process new videos, should I resize them to match the global_scale parameter? \nThe analyze_videos function gets as an argument the config.yaml file, where no global_scale parameter is present. \nThanks\n  </blockquote>\n</aside>\n", "<aside class=\"quote no-group\" data-username=\"GaussGap\" data-post=\"4\" data-topic=\"78346\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/gaussgap/40/68130_2.png\" class=\"avatar\"> GaussGap:</div>\n<blockquote>\n<p>downsampling w</p>\n</blockquote>\n</aside>\n<p>Thank you very much for your contribution. global_scale seems hamless unless it is very small. I will train with a decreased global_scale, if it doesn\u2019t work well then I will go for downsampling.</p>"], "78349": ["<p>Dear all,<br>\nI have this scenario:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcada0259649ee306271a191b6d7e703c5853d15.png\" alt=\"image\" data-base62-sha1=\"qV7KQdB20CNd5hlPZSsdTHU3vlX\" width=\"447\" height=\"409\"></p>\n<p>what I would like to do is to insert programmatically E-cad-Cleared as a child of CleanTumor</p>\n<p>I am trying to use this</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://qupath.github.io/javadoc/docs/qupath/lib/objects/hierarchy/PathObjectHierarchy.html\">\n  <header class=\"source\">\n\n      <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/objects/hierarchy/PathObjectHierarchy.html\" target=\"_blank\" rel=\"noopener\">qupath.github.io</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/objects/hierarchy/PathObjectHierarchy.html\" target=\"_blank\" rel=\"noopener\">PathObjectHierarchy (QuPath 0.4.0)</a></h3>\n\n  <p>declaration: package: qupath.lib.objects.hierarchy, class: PathObjectHierarchy</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nbut I think I am failing generally to get the PathObjects</p>\n<p>so this is my little snippet</p>\n<pre><code class=\"lang-groovy\">parentAnnotation = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Tissue_D\")}\nprint(parentAnnotation)\nchildDetection = getDetectionObjects().findAll{it.getPathClass() == getPathClass(\"E-cad-Cleared\")}\nprint(childDetection)\n\n\nPathObjectHierarchy().addPathObjectBelowParent(parentAnnotation, childDetection, true)\n//fireHierarchyUpdate()\n</code></pre>\n<p>this is the error I get</p>\n<pre><code class=\"lang-auto\">INFO: [CleanTumor (Geometry) (Tissue_D) (3 objects)]\nINFO: [Detection (Geometry) (E-cad-Cleared)]\nERROR: MissingMethodException at line 7: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.PathObjectHierarchy() is applicable for argument types: () values: []\n\nERROR: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.callGlobal(GroovyScriptEngineImpl.java:404)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.access$100(GroovyScriptEngineImpl.java:90)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl$3.invokeMethod(GroovyScriptEngineImpl.java:303)\n    org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:73)\n    org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:176)\n    Script36.run(Script36.groovy:8)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)\n    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:982)\n    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:914)\n    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:829)\n    qupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1345)\n    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    java.base/java.lang.Thread.run(Unknown Source)\n</code></pre>\n<p>I\u2019ve tried also with addObject instead of addPathObject</p>\n<p>what am I doing wrong?</p>\n<p>thanks<br>\nEmanuele</p>", "<aside class=\"quote no-group\" data-username=\"emartini\" data-post=\"1\" data-topic=\"78349\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/emartini/40/292_2.png\" class=\"avatar\"> Emanuele Martini:</div>\n<blockquote>\n<p><code>PathObjectHierarchy()</code></p>\n</blockquote>\n</aside>\n<p>That should be <code>getCurrentHierarchy()</code></p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"2\" data-topic=\"78349\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> Pete:</div>\n<blockquote>\n<p>getCurrentHierarchy</p>\n</blockquote>\n</aside>\n<p>that has worked out, thanks a lot<br>\njust with a little mod, add [0] to take just the single object</p>\n<pre><code class=\"lang-groovy\">parentAnnotation = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Tissue_D\")}[0]\nprint(parentAnnotation)\nchildDetection = getDetectionObjects().findAll{it.getPathClass() == getPathClass(\"E-cad-Cleared\")}[0]\nprint(childDetection)\n\ngetCurrentHierarchy().addPathObjectBelowParent(parentAnnotation, childDetection, true)\n</code></pre>\n<p>thanks a lot again</p>", "<p>If you know there will only be one object and want to take the first, you can also use \u201cfind\u201d rather than \u201cfindAll\u201d and the [0]</p>"], "78350": ["<p>Dear folks,</p>\n<p>I am trying to follow along these instructions (goes wrong for me at item 8 in the list under <code>Usage: Object and Semantic Segmentation</code>):</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pypi.org/project/napari-accelerated-pixel-and-object-classification/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/8/08f9ca89433aef1933c7c6d5755a75dbf2fafaaa.png\" class=\"site-icon\" width=\"32\" height=\"30\">\n\n      <a href=\"https://pypi.org/project/napari-accelerated-pixel-and-object-classification/\" target=\"_blank\" rel=\"noopener nofollow ugc\">PyPI</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f97026709e67b2111b465be6427519ead928642.webp\" class=\"thumbnail onebox-avatar\" width=\"300\" height=\"300\">\n\n<h3><a href=\"https://pypi.org/project/napari-accelerated-pixel-and-object-classification/\" target=\"_blank\" rel=\"noopener nofollow ugc\">napari-accelerated-pixel-and-object-classification</a></h3>\n\n  <p>Pixel and label classification using OpenCL-based Random Forest Classifiers</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>It is launched in a jupyter-notebook (venv). I successfully open an image in napari via:<br>\n<code>viewer = napari.view_image(\"path/blobs.jpg\")</code></p>\n<p>After running the above command I do the following in napari that just opened:</p>\n<ol>\n<li>Add a labels layer</li>\n<li>Draw annotations for 2 layers</li>\n<li><code>Tools &gt; Segmentation / labeling &gt; Object Segmentation (APOC)</code></li>\n<li>Select options as stated in the instructions (link above).</li>\n</ol>\n<br>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3.png\" data-download-href=\"/uploads/short-url/hsBWjmTXBHum8xB2W80DuqTsT6z.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3_2_690x447.png\" alt=\"image\" data-base62-sha1=\"hsBWjmTXBHum8xB2W80DuqTsT6z\" width=\"690\" height=\"447\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3_2_690x447.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3_2_1035x670.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a60f02da3c82d72d934d41569511f4d982d3ac3.png 2x\" data-dominant-color=\"404348\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1088\u00d7706 108 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<br>\n<p>Now when I press <code>Train</code> I receive the following error. It seems that the label layer does not have the same dimensions as the blob image. <code>Selected images and annotation must have the same dimensionality and size!</code><br>\nI cannot work out how to solve this and ensure the annotation has the same size as the image (to which I added a Labels layer). I am not sure how to check or adjust the dimensions.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f422c750459b5d949abf44af78a8cfdd98b67df.png\" data-download-href=\"/uploads/short-url/6K4duahCJ9MGzhzkwC9tw7gOx5l.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f422c750459b5d949abf44af78a8cfdd98b67df.png\" alt=\"image\" data-base62-sha1=\"6K4duahCJ9MGzhzkwC9tw7gOx5l\" width=\"690\" height=\"114\" data-dominant-color=\"F0E4E5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1009\u00d7168 17.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Many thanks for your help <img src=\"https://emoji.discourse-cdn.com/twitter/crossed_fingers/5.png?v=12\" title=\":crossed_fingers:t5:\" class=\"emoji\" alt=\":crossed_fingers:t5:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/danielle_z\">@Danielle_Z</a> ,</p>\n<p>I presume APOC is confused about the image format. It doesn\u2019t support RGB images. Could you try with a greyscale image such as the one provided with APOC?</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif\" target=\"_blank\" rel=\"noopener\">haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif</a></h4>\n\n\n  This file is binary. <a href=\"https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/blobs.tif\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Alternatively, you could split the RGB image into three channels (right-click on the image layer in the layer list) and then train on one channel or all three if they are not identical.</p>\n<p>Let us know if this helps!</p>\n<p>Best,<br>\nRobert</p>", "<p>Just to followup on <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a> you can check the sizes:</p>\n<pre><code class=\"lang-auto\">viewer.layers[\"Labels\"].data.shape   # this will return a 2D tuple, like (256, 256)\nviewer.layers[\"myimg\"].data.shape    # this will return a 3D tuple, ending in 3, like (256, 256, 3)\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a>,<br>\nthis is indeed it! The data that I intend to work with is RGB, so I will explore how to work with an image where I have split the channels.</p>\n<p>Hi <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a>,<br>\nthanks for these instructions on how to obtain the sizes of the different layers. And also, I now know that I can run commands in the notebook while napari is running.</p>\n<p>I am quite new to napari (started yesterday <img src=\"https://emoji.discourse-cdn.com/twitter/see_no_evil.png?v=12\" title=\":see_no_evil:\" class=\"emoji\" alt=\":see_no_evil:\" loading=\"lazy\" width=\"20\" height=\"20\">), there seem to be so many possibilities. I hope to master more soon and get my head around the different options and comands I can use. Thanks for getting me unstuck here.</p>"], "78355": ["<p>I\u2019m trying to get cell counts in different brain regions. qupath provides the function of mapping brain slice images to atlas and identifying cell borders, which I highly appreciate. However, there are a few issues or lack of function so I\u2019m not able to accomplish what I want.</p>\n<ol>\n<li>The automatic cell detection is error-prone. This is possibly due to the fact that neuron shapes are usually not that regular and there are neurofilaments crossing cell bodies. I would like a function to manually inspact and curate the auto-detection result.</li>\n<li>The cell count function only exists for brightfield image and I haven\u2019t found any function to count cells per brain region.</li>\n</ol>\n<p>These functionalities will be greatly helpful to me and, I believe, also the community. I\u2019m not sure if these are something the developers have in mind or something exist but I am not aware of. I\u2019m thinking about adding these functions myselves but would like suggestions about alternatives or better approaches before I start working on it.</p>", "<aside class=\"quote no-group\" data-username=\"congconghu\" data-post=\"1\" data-topic=\"78355\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/congconghu/40/68831_2.png\" class=\"avatar\"> Congcong Hu:</div>\n<blockquote>\n<p>The automatic cell detection is error-prone. This is possibly due to the fact that neuron shapes are usually not that regular and there are neurofilaments crossing cell bodies. I would like a function to manually inspact and curate the auto-detection result.</p>\n</blockquote>\n</aside>\n<p>If you classify the cells as good or bad, you can remove the bad ones, but there\u2019s no way to \u201ctrain\u201d the cell detection method. Unless you mean StarDist or CellPose, in which case retrain away, there is documentation for each on the forum.<br>\nUsing standard cell detection and classification, you can go through and validate your classifications using  <a href=\"https://forum.image.sc/t/rarecellfetcher-a-tool-for-annotating-rare-cells-in-qupath/33654\" class=\"inline-onebox\">RareCellFetcher- a tool for annotating rare cells in QuPath</a></p>\n<aside class=\"quote no-group\" data-username=\"congconghu\" data-post=\"1\" data-topic=\"78355\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/congconghu/40/68831_2.png\" class=\"avatar\"> Congcong Hu:</div>\n<blockquote>\n<p>The cell count function only exists for brightfield image and I haven\u2019t found any function to count cells per brain region.</p>\n</blockquote>\n</aside>\n<p>Cell detection works for brightfield, IF, etc, anything where there are roughly circular objects to detect as nuclei, either lighter or darker than the background.</p>\n<p>If you have the brain regions imported from ABBA, then cell detection of various kinds can be run within those regions.</p>", "<p>Thank you so much for the reply!</p>\n<p>I looked into RareCellFetcher, which seems to be a function to label detected cells. It will be useful if I want to delete false positive cells, but it doesn\u2019t seem to tackle the problem of false negative or offer ways to split when multiple cells are detected as one. So far, I am not interested in training a cell detection model. That might be something to consider in the future after I get a semi-automation pipeline work to feel confident about the cell labeling and have accumulated enough labeled images.</p>\n<p>I will explore the cell count function more. What I have hoped for is after mapping images to the atlas and have the images segregated into regions, I will be able to get cell counts in dozens of different regions automatically. I noticed there is a function to count cells in a user-defined ROI, but it doesn\u2019t seem to streamline the process of counting cells in multiple regions.</p>\n<p>Please let me know if there are already solutions to these problems. I\u2019m new to this software and any help is appreciated!</p>", "<p>Yeah, rareCellFetcher if just if you want to fine tune the results of the cell detection, otherwise, you have to chose improved settings or use a different type of cell detection. It is intended for nuclear staining. If you don\u2019t have that, you don\u2019t generally use cell detection.</p>\n<p>All annotations have cell counts automatically. How you generate the regions is up to you, automatic through ABBA or something similar, or manual.</p>\n<p>Example <a href=\"https://forum.image.sc/t/cell-detections-in-overlapping-annotations-produces-multiple-cell-counts-per-actual-cell/61836/5\" class=\"inline-onebox\">Cell Detections in Overlapping Annotations Produces Multiple Cell Counts per Actual cell - #5 by oburri</a><br>\n<a href=\"https://forum.image.sc/t/abba-qupath-script-to-measure-fluorescence-in-atlas-regions/76320\" class=\"inline-onebox\">ABBA -&gt; Qupath: script to Measure Fluorescence in Atlas Regions</a><br>\nEtc.</p>"], "78356": ["<p>Hi!<br>\nI just installed cellfinder and on our first run we encountered this error:</p>\n<p><code>File failed to load with imio. Ensure all image files contain the same number of pixels. Full traceback above.</code></p>\n<p>Here is the output log if it helps:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/lbIhexDuOVIrkKxb9SVBbbF3agq.txt\">cellfinder_log_2023-03-09_14-19-33.txt</a> (3.5 KB)<br>\nAnd here is our setup steps:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/xZwAtPHT2ufZpwgrmCUKOyQRZ7H.txt\">Brainglobe setup notes.txt</a> (287 Bytes)</p>", "<p>Hey <a class=\"mention\" href=\"/u/kleski\">@kleski</a> the first thing to check is the error posted. Are all your images exactly the same size and shape? This would be both:</p>\n<ul>\n<li>Your signal and autofluroesence images are the same (only the voxel intensity varies)</li>\n<li>If your data is a directory of 2D tiffs, then each one is exactly the same size and shape.</li>\n</ul>\n<p>If either of these assumptions are not true, then cellfinder won\u2019t be able to run.</p>"], "78361": ["<p>I am having issues running AxonTracer - does anyone have a tutorial or a stepwise process (other than the direct literature paper from the author) that can explain the steps ? Starting with image preparation (I am starting with confocal images with 2 channels)</p>\n<p>I have never used Fiji/ImageJ before and I want to see if this macro can work for our research.</p>\n<p>Thanks!</p>"], "78362": ["<p>I am trying to create a rudimentary z-stack sequence as below:</p>\n<p>sequence = MDASequence(<br>\ntime_plan={\u201cinterval\u201d: 2, \u201cloops\u201d:1},<br>\nz_plan={\u201cabove\u201d:4 , \u201cbelow\u201d:4, \u201cstep\u201d:0.25},<br>\n)</p>\n<p>It runs successfully but not as intended. Instead of acquiring 4 steps above and below the current position, it moves all the way back to zero and acquires from -4 to 4. Is there a way to pass the starting z position as reference?</p>"], "78363": ["<p>Hi</p>\n<p>I am using Deeplabcut with the GUI to analyze videos by adding markers at 500 frames of a video in which a mouse moves in its cage. Currently the neural network has already been trained and evaluated. I proceed to the analyze videos tab and yet it shows me an error line which says:<br>\nAttributeError: \u2018AnalyzeVideos\u2019 object has no attribute \u2018logger\u2019</p>\n<p>Does anyone know what causes this problem and how it can be solved?</p>", "<p>Which version of DLC are you on?</p>", "<p>Hello, I have the same question on version 2.3.0<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f44e1382e160f37a745d9a64fe0374bc8b06ecee.png\" data-download-href=\"/uploads/short-url/yRdMnvQ90rIvmA1hNzzVNpc0CwS.png?dl=1\" title=\"1679141790682\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f44e1382e160f37a745d9a64fe0374bc8b06ecee.png\" alt=\"1679141790682\" data-base62-sha1=\"yRdMnvQ90rIvmA1hNzzVNpc0CwS\" width=\"690\" height=\"359\" data-dominant-color=\"2C2C2C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">1679141790682</span><span class=\"informations\">1488\u00d7776 70.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78364": ["<p>Hello napari community, I\u2019m coming to Europe 3/27-4/5 and I want to meet you!</p>\n<p>I\u2019m an Application Scientist at Chan Zuckerberg Initiative, and I\u2019ll be traveling with my colleague Lucy Obus <a class=\"mention\" href=\"/u/lco\">@lco</a>, a user researcher at CZI, to Europe for conferences. We\u2019ll be in Munich, Germany March 27/28, VizBi in Heidelberg, Germany March 28-31, and Focus on Microscopy in Porto, Portugal April 2-5. We\u2019d love to connect with fellow napari users at the conference or come visit your lab or institution in the Munich, Heidelberg, or Porto area. Reply here or reach out at <a href=\"mailto:dmccarthy@chanzuckerberg.com\">dmccarthy@chanzuckerberg.com</a> - we\u2019d love to meet new folks and visit familiar faces. <img src=\"https://emoji.discourse-cdn.com/twitter/grinning_face_with_smiling_eyes.png?v=12\" title=\":grinning_face_with_smiling_eyes:\" class=\"emoji\" alt=\":grinning_face_with_smiling_eyes:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78365": ["<p>Hi,</p>\n<p>I am trying to create macro for a circular gradient that goes from 1 in the centre to 0, like below.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578.png\" data-download-href=\"/uploads/short-url/3eUTpU5NKzMsq2DkKinOmU8Gq5G.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_247x250.png\" alt=\"image\" data-base62-sha1=\"3eUTpU5NKzMsq2DkKinOmU8Gq5G\" width=\"247\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_247x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_370x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/6/16b6368525106889a73b956c1ecf20316ef64578_2_494x500.png 2x\" data-dominant-color=\"2D2B2D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">509\u00d7514 49.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The inner circle with a radius A (filled with 1) and the larger circle with a radius B (with the gradient); 32-bit image.<br>\n(red circles are only for demo)</p>\n<p>I can only think of excessively convoluted ways of doing this; If anyone has a suggestion I\u2019d be happy to have it!</p>\n<p>Thanks a lot!</p>", "<p>Hi <a class=\"mention\" href=\"/u/swa\">@Swa</a> ,</p>\n<p>Interestingly, I needed something like this for <a href=\"https://github.com/saalfeldlab/bigwarp/blob/b396576bf1ec66b5069818b54b9d6fd746168d6b/src/main/java/bigwarp/source/PlateauSphericalMaskRealRandomAccessible.java\">bigwarp recently</a>, so I adapted that code to this ImageJ2 script:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy\" target=\"_blank\" rel=\"noopener\">bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy</a></h4>\n\n\n      <pre><code class=\"lang-groovy\">#@ Integer sizeX\n#@ Integer sizeY\n#@ Double centerX\n#@ Double centerY\n#@ Double innerRadius\n#@ Double outerRadius\n#@ UIService ui\n\n/**\n * Creates a circular gradient\n * \n * see:\n * https://forum.image.sc/t/circular-gradient/78365\n * \n * John Bogovic\n */\n\n// the size of the image\nitvl = new FinalInterval( [sizeX, sizeY ] as long[] );\n\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/bogovicj/fiji-scripts/blob/99ae5741bb28dd7257946d5563f3546a7bb4dd2b/circleGradient.groovy\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Here are some of the results I get (with the parameters shown):<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0a2e95283d73ef7d6b4b6c2f7c4f696dbaed2c17.png\" data-download-href=\"/uploads/short-url/1s4zO97uZoxcRkurob1M520ryCP.png?dl=1\" title=\"Screenshot from 2023-03-10 10-56-42\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0a2e95283d73ef7d6b4b6c2f7c4f696dbaed2c17.png\" alt=\"Screenshot from 2023-03-10 10-56-42\" data-base62-sha1=\"1s4zO97uZoxcRkurob1M520ryCP\" width=\"517\" height=\"315\" data-dominant-color=\"868084\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-10 10-56-42</span><span class=\"informations\">746\u00d7456 27.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It uses a cosine-shaped falloff between the center and the outside, but feel free to take my code for a <a href=\"https://github.com/saalfeldlab/bigwarp/blob/b396576bf1ec66b5069818b54b9d6fd746168d6b/src/main/java/bigwarp/source/PlateauSphericalMaskRealRandomAccessible.java#L352-L380\">Gaussian</a> or <a href=\"https://github.com/saalfeldlab/bigwarp/blob/b396576bf1ec66b5069818b54b9d6fd746168d6b/src/main/java/bigwarp/source/PlateauSphericalMaskRealRandomAccessible.java#L412-L438\">linear</a> shape falloff if you\u2019d like. Adapting this code will mean learning a little ImageJ2 / imglib2, but please post back with questions, I and others will be happy to help.</p>\n<p>John</p>", "<p>By the way, another option is the windowing plugin from H. Gl\u00fcnder here:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.gluender.de/Miscellanea/MiscTexts/UtilitiesText.html#Gl-2019-4\">\n  <header class=\"source\">\n\n      <a href=\"https://www.gluender.de/Miscellanea/MiscTexts/UtilitiesText.html#Gl-2019-4\" target=\"_blank\" rel=\"noopener\">gluender.de</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://www.gluender.de/Miscellanea/MiscTexts/UtilitiesText.html#Gl-2019-4\" target=\"_blank\" rel=\"noopener\">ImageJ-PlugIns and -Macros as well as Classic MacOS\u2122 Utilities</a></h3>\n\n  <p>H. Gluender's  List of Selected Macintosh\u2122 Utilities and Java ImageJ-PlugIns with Links to the ReadMe Documents and Downloads.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "76319": ["<p>Hi there.<br>\nWondering if there is a way to get the cellpose seg.npy (or potentially the text file that is supposed to be for imagej) into napari so that I can use napari-skimage-regionprops or clesperanto to analyze 3d stack. I have tried napari-cellpose wrapper, but this has issues with 3D. I have additionally played around with napari-serialcellpose, but this doesn\u2019t seem to support 3D. In essence I just want to take the 3d masks I have generated with my custom cellpose model to napari, so if there is an alternative way to do this I am all ears.</p>\n<p>if there is an imagej way to do this that would be helpful as well!</p>\n<p>Thanks in advance!</p>", "<p>Hmm, numpy arrays saved to a file (<code>.npy</code>) should be handled by the <code>builtins</code> reader in napari.</p><aside class=\"onebox githubpullrequest\" data-onebox-src=\"https://github.com/napari/napari/pull/3271\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/napari/napari/pull/3271\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/napari/napari</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n\n\n\n    <div class=\"github-icon-container\" title=\"Pull Request\">\n      <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 12 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z\"></path></svg>\n    </div>\n\n  <div class=\"github-info-container\">\n\n\n\n      <h4>\n        <a href=\"https://github.com/napari/napari/pull/3271\" target=\"_blank\" rel=\"noopener nofollow ugc\">add .npy reader to builtin reader</a>\n      </h4>\n\n    <div class=\"branches\">\n      <code>napari:master</code> \u2190 <code>thanushipeiris:single-npy-loading</code>\n    </div>\n\n      <div class=\"github-info\">\n        <div class=\"date\">\n          opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-08-29\" data-time=\"03:08:27\" data-timezone=\"UTC\">03:08AM - 29 Aug 21 UTC</span>\n        </div>\n\n        <div class=\"user\">\n          <a href=\"https://github.com/thanushipeiris\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n            <img alt=\"thanushipeiris\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/6/263beb52088d44cabfc2eae5931dfab9f9619c03.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n            thanushipeiris\n          </a>\n        </div>\n\n        <div class=\"lines\" title=\"4 commits changed 2 files with 28 additions and 0 deletions\">\n          <a href=\"https://github.com/napari/napari/pull/3271/files\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n            <span class=\"added\">+28</span>\n            <span class=\"removed\">-0</span>\n          </a>\n        </div>\n      </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Description\nThis PR addresses #2976</p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>For me at least writing a numpy array to a <code>.npy</code> works with drag-n-drop and <code>add_image</code> or <code>imshow</code><br>\nMake a random data file:</p>\n<pre><code class=\"lang-auto\">import numpy as np\nrand_data = np.random.rand(10,10, 10)\nnp.save(\"rand_data.npy\", rand_data)\n</code></pre>\n<p>Read it:</p>\n<pre><code class=\"lang-auto\">import numpy as np\nimport napari\nout = np.load(\"rand_data.npy\")\nviewer, rand_data = napari.imshow(out)\n</code></pre>\n<p>Can you paste any errors or clarify the issue?</p>", "<p>I think I may have found the issue then. Upon startup in the conda env, i get this message before napari starts: napari.manifest \u2192 \u2018napari\u2019 could not be imported: Cannot find module \u2018napari_builtins\u2019 declared in entrypoint: \u2018napari_builtins:builtins.yaml\u2019</p>\n<p>Not sure why the builtins arent being recognized?</p>\n<p>Additionally, this results in this error when I attempt to drag and drop a npy file onto napari window:</p>\n<p>File ~\\Anaconda3\\envs\\napari-env\\lib\\site-packages\\numpy\\lib\\format.py:776, in read_array(fp=&lt;_io.BufferedReader name=\u2018C:/Users/jelrod14/Desk\u2026F/wt/z5-z14_NW-RW10-2.3-Map128_MergedMC_seg.npy\u2019&gt;, allow_pickle=False, pickle_kwargs={\u2018encoding\u2019: \u2018ASCII\u2019, \u2018fix_imports\u2019: True}, max_header_size=10000)<br>\n773 if dtype.hasobject:<br>\n774     # The array contained Python objects. We need to unpickle the data.<br>\n775     if not allow_pickle:<br>\n \u2192 776         raise ValueError(\"Object arrays cannot be loaded when \"<br>\n777                          \u201callow_pickle=False\u201d)<br>\n778     if pickle_kwargs is None:<br>\n779         pickle_kwargs = {}</p>\n<p>ValueError: Object arrays cannot be loaded when allow_pickle=False</p>", "<p>Hmm, can you provide the output of <code>napari --info</code>?<br>\nAlso you can put code and errors in between three ` (backticks) to make it more readable.</p>\n<p>Also, can you try:<br>\n<code>ls ~\\Anaconda3\\envs\\napari-env\\lib\\site-packages\\napari_builtins</code></p>\n<p>You should have that package, if not it means your install is somehow borked.</p>\n<pre><code class=\"lang-auto\">napari_builtins:\n__init__.py       _skimage_data.py  builtins.yaml\n__pycache__/      _tests/           io/\n</code></pre>", "<p>Guessing my download is messed up as ls couldn\u2019t find the folder. Going to redownload and remake environment in conda as well.<br>\n<code>napari --info</code>:</p>\n<pre><code class=\"lang-auto\">napari.manifest -&gt; 'napari' could not be imported: Cannot find module 'napari_builtins' declared in entrypoint: 'napari_builtins:builtins.yaml'\nnapari: 0.4.15\nPlatform: Windows-10-10.0.19044-SP0\nPython: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]\nQt: 5.15.2\nPyQt5: 5.15.7\nNumPy: 1.23.5\nSciPy: 1.9.3\nDask: 2022.12.0\nVisPy: 0.11.0\n\nOpenGL:\n  - GL version:  4.6.0 - Build 30.0.101.1692\n  - MAX_TEXTURE_SIZE: 16384\n\nScreens:\n  - screen 1: resolution 1920x1080, scale 1.0\n  - screen 2: resolution 1920x1080, scale 1.0\n  - screen 3: resolution 1600x900, scale 1.0\n\nPlugins:\n  - bbii-decon: 0.0.1\n  - bfio: 2.3.0\n  - clEsperanto: 0.21.0\n  - console: 0.0.6\n  - napari-allencell-segmenter: 2.1.3\n  - napari-assistant: 0.4.3\n  - napari-napari_allencell_annotator: 1.0.7\n  - napari-time-slicer: 0.4.9\n  - napari-tools-menu: 0.1.17\n  - napari_skimage_regionprops1: 0.7.0\n  - napari_skimage_regionprops2: 0.7.0\n  - ome-types: 0.3.2\n  - scikit-image: 0.4.17\n  - svg: 0.1.6 ```</code></pre>", "<aside class=\"quote no-group\" data-username=\"psobolewskiPhD\" data-post=\"2\" data-topic=\"76319\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\"> Peter Sobolewski:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">import numpy as np\nimport napari\nout = np.load(\"rand_data.npy\")\nviewer, rand_data = napari.imshow(out)\n</code></pre>\n</blockquote>\n</aside>\n<p>Okay, I have freshly downloaded napari, and can run the code above. Output is an image with random rectangles varying in intensity. However I am still running into the issue when I try to import my .npy files from cellpose. I drag and drop npy into the viewer and get this error:</p>\n<pre><code class=\"lang-auto\">ReaderPluginError: Tried to read C:\\Users\\jelrod14\\Desktop\\nearestneighboranalysis\\16bit012118NAIVES1BF\\het\\z8-z17_NH-RW9-1.4-Map128_MergedMC_seg.npy with plugin napari, because it was associated with that file extension/because it is the only plugin capable of reading that path, but it gave an error. Try associating a different plugin or installing a different plugin for this kind of file.\n</code></pre>\n<p>When I attempt to alt+drag and drop and select builtins, it throws the same <code>\"Object arrays cannot be loaded load when allow_pickle = False\"</code> error</p>", "<p>Hmm, I suspect then that it\u2019s the pickle flag. Looking a bit more closely at the .npy file IO I see that:</p>\n<p>napari reader uses <code>np.load(filename)</code>, which by default has <code>allow_pickle=False</code> (<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.load.html#numpy.load\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">numpy.load \u2014 NumPy v1.24 Manual</a>)</p>\n<p>While cellpose uses <code>np.save()</code> with defaults, which include <code>allow_pickle=True</code><br>\n<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.save.html#numpy.save\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://numpy.org/doc/stable/reference/generated/numpy.save.html#numpy.save</a></p>\n<p>The two default arguments <img src=\"https://emoji.discourse-cdn.com/twitter/exploding_head.png?v=12\" title=\":exploding_head:\" class=\"emoji\" alt=\":exploding_head:\" loading=\"lazy\" width=\"20\" height=\"20\">  being opposite result in the error.</p>\n<p>Now I\u2019m not sure what is the best solution going forward.</p>\n<ol>\n<li>cellpose switches to pickle False, but all of cellpose np.load use <code>allow_pickle=True</code>)</li>\n<li>napari switches to pickle True (or provides some way to change it)</li>\n</ol>\n<p>Temporarily I guess you can:</p>\n<ol>\n<li>read the file programatically using <code>data = np.load('file.npy', allow_pickle=True)</code>\n</li>\n<li>re-save with pickle False (<code>np.save('file-napari.npy', allow_pickle=False)</code>\n</li>\n</ol>\n<p>Edit2: I can\u2019t say I fully understand the whole <code>pickle</code> aspect, but cellpose docs show <code>allow_pickle=True</code> for loading:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/docs/outputs.rst?plain=1#L32\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/docs/outputs.rst?plain=1#L32\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/docs/outputs.rst?plain=1#L32\" target=\"_blank\" rel=\"noopener nofollow ugc\">MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/docs/outputs.rst?plain=1#L32</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-rst?plain=1\">\n      <ol class=\"start lines\" start=\"22\" style=\"counter-reset: li-counter 21 ;\">\n          <li>            flows[3] is [dY, dX, cellprob] (or [dZ, dY, dX, cellprob] for 3D), flows[4] is pixel destinations (for internal use)</li>\n          <li>- *est_diam* : estimated diameter (if run on command line)</li>\n          <li>- *zdraw* : for each mask, which planes were manually labelled (planes in between manually drawn have interpolated ROIs)</li>\n          <li>\n          </li>\n<li>Here is an example of loading in a ``*_seg.npy`` file and plotting masks and outlines</li>\n          <li>\n          </li>\n<li>::</li>\n          <li>\n          </li>\n<li>    import numpy as np</li>\n          <li>    from cellpose import plot, utils</li>\n          <li class=\"selected\">    dat = np.load('_seg.npy', allow_pickle=True).item()</li>\n          <li>\n          </li>\n<li>    # plot image with masks overlaid</li>\n          <li>    mask_RGB = plot.mask_overlay(dat['img'], dat['masks'],</li>\n          <li>                            colors=np.array(dat['colors']))</li>\n          <li>\n          </li>\n<li>    # plot image with outlines overlaid in red</li>\n          <li>    outlines = utils.outlines_list(dat['masks'])</li>\n          <li>    plt.imshow(dat['img'])</li>\n          <li>    for o in outlines:</li>\n          <li>        plt.plot(o[:,0], o[:,1], color='r')</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Looking more closely at the cellpose save code:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/cellpose/io.py#L352-L360\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/cellpose/io.py#L352-L360\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/cellpose/io.py#L352-L360\" target=\"_blank\" rel=\"noopener nofollow ugc\">MouseLand/cellpose/blob/03e02bca6abf15c4532088cec62a927923a8bf03/cellpose/io.py#L352-L360</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"352\" style=\"counter-reset: li-counter 351 ;\">\n          <li>np.save(base+ '_seg.npy',</li>\n          <li>            {'outlines': outlines.astype(np.uint16) if outlines.max()&lt;2**16-1 else outlines.astype(np.uint32),</li>\n          <li>                'masks': masks.astype(np.uint16) if outlines.max()&lt;2**16-1 else masks.astype(np.uint32),</li>\n          <li>                'chan_choose': channels,</li>\n          <li>                'img': images,</li>\n          <li>                'ismanual': np.zeros(masks.max(), bool),</li>\n          <li>                'filename': file_names,</li>\n          <li>                'flows': flowi,</li>\n          <li>                'est_diam': diams})</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>The <code>.npy</code> file contains actually a dictionary with more than just the labels, which is what you\u2019d visualize with napari and further process with region props, etc. There\u2019s no way I don\u2019t think for napari builtin reader to a priori know that it needs to do:</p>\n<pre><code class=\"lang-auto\">data_to_load = np.load('_seg.npy', allow_pickle=True).item()\nlabels = data_to_load['masks']\n</code></pre>\n<p>Seems like this requires a cellpose-specific npy reader plugin.</p>\n<p>Now cellpose offers a png output, which would work with napari builtins, but probably doesn\u2019t support 3D? Odd they don\u2019t save a TIFF.</p>", "<p>Alternatively, how about saving the masks directly to TIFF. Wouldn\u2019t that work?<br>\nPersonal <a href=\"https://github.com/leogolds/MicroscopyPipeline/blob/3346facb359cfb448e01514a45aaa80fa58381f1/utils.py#L73\" rel=\"noopener nofollow ugc\">example</a> for saving in H5 and optionally to TIFF.</p>"], "78368": ["<p>Here is an excellent recent article on AI in Pathology. The issues raised are equally applicable to other medical disciplines.<br>\nThe LinkedIn link has a link to a pre-print in case you can\u2019t access the journal online but you will need a LinkedIn longin:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.sciencedirect.com/science/article/abs/pii/S0740257023000175\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/000a88b211fc700d3929bb1e37ab763b322cfe10.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0740257023000175\" target=\"_blank\" rel=\"noopener nofollow ugc\">sciencedirect.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:113/150;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57abd04ecab695c1253298623c39678584e4e958.gif\" class=\"thumbnail\" width=\"113\" height=\"150\"></div>\n\n<h3><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0740257023000175\" target=\"_blank\" rel=\"noopener nofollow ugc\">AI in Pathology: What could possibly go wrong?</a></h3>\n\n  <p>The field of medicine is undergoing rapid digital transformation. Pathologists are now striving to digitize their data, workflows, and interpretations\u2026</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.linkedin.com/posts/richardlevenson_ai-in-pathology2023-in-presspdf-activity-7039629015137062912-co5s?utm_source=share&amp;utm_medium=member_desktop\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/5/f525ddc94e173c36457abea5945e26fa22323293.png\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https://www.linkedin.com/posts/richardlevenson_ai-in-pathology2023-in-presspdf-activity-7039629015137062912-co5s?utm_source=share&amp;utm_medium=member_desktop\" target=\"_blank\" rel=\"noopener nofollow ugc\">linkedin.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:375/500;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_375x500.jpeg\" class=\"thumbnail\" width=\"375\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a6fe6e877c742b8f413c240aeaaa7e74b42aaa6_2_750x1000.jpeg 2x\" data-dominant-color=\"EAEAEA\"></div>\n\n<h3><a href=\"https://www.linkedin.com/posts/richardlevenson_ai-in-pathology2023-in-presspdf-activity-7039629015137062912-co5s?utm_source=share&amp;utm_medium=member_desktop\" target=\"_blank\" rel=\"noopener nofollow ugc\">Richard Levenson, MD, FCAP on LinkedIn: AI in Pathology_2023 in press.pdf</a></h3>\n\n  <p>Trying again to post our new article on AI in Pathology: What could possibly go wrong. This time with images and poem! Hope this works. Thanks.\u2026</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "78374": ["<p>Hello ! I\u2019m trying to learn Napari and wanted to try multithreading with Generator functions as presented in the napari documentation at <a href=\"https://napari.org/stable/guides/threading.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Multithreading in napari \u2014 napari</a></p>\n<p>Unfortunately after having written something to understand it, I don\u2019t see why this simple example that I wrote is not working :</p>\n<pre><code class=\"lang-python\">import napari\nimport numpy as np\nfrom napari.qt.threading import thread_worker\nfrom magicgui import magicgui\n\nlist_dimensions = [1,2,4,8,16]\n\n@thread_worker\ndef image_generator(list_dimensions):\n    \n    for dim in list_dimensions:\n        a = np.random.rand(dim, dim)\n        b = np.random.rand(dim, dim)\n        \n        b = b &gt; 0.5\n        \n        yield (a, b)\n        \nviewer = napari.Viewer()\nworker = image_generator(list_dimensions)\n        \n\n        \ndef on_yielded(value):\n    print(\"Yielded smtg\")\n    worker.pause()\n    print(f\"Worker paused : {worker.is_paused}\")\n    viewer.add_image(value[0])\n    viewer.add_labels(value[1])\n    \n    \ndef fetch_new_image():\n    worker.resume()\n    print(worker.is_running)\n    \n@magicgui()\ndef next_image():\n    fetch_new_image()\n\n\nworker.yielded.connect(on_yielded)\nviewer.window.add_dock_widget(next_image)\n\n\nworker.start()\nnapari.run()\n</code></pre>\n<p>When I run this code, all images are added in the beginning and it seem that I cannot pause the worker in the on_yielded function</p>\n<p>If anyone has an idea as to why it is the case ?</p>\n<p>Thank you very much for the help !</p>", "<p>Hey <a class=\"mention\" href=\"/u/leroyadrien\">@LeroyAdrien</a>,<br>\nThe problem here is the order of execution. When the generator yields the value, the <code>on_yielded</code> function is triggered. However, the execution of the worker does not stop, but rather moves on to the next loop, while the <code>on_yielded</code> function <strong>runs in parallel</strong>. It is similar with <code>worker.pause</code>. Here we ask the worker to stop, but we don\u2019t wait for it to happen, but continue with adding the layers to the viewer. Pausing will happen only between yields. The loop runs so fast, that when the worker should pause, all the images were already generated, so the worker quits.</p>\n<p>In the official napari tutorial it is handled by the <code>worker.send</code> method:</p>\n<pre><code class=\"lang-auto\">def send_next_value():\n    worker.send(float(line_edit.text())) # &lt;--\n    worker.resume()\n</code></pre>\n<p>Here we send some data for the worker object. This data is processed on yielding:</p>\n<pre><code class=\"lang-auto\">@thread_worker\ndef multiplier():\n    total = 1\n    while True:\n        time.sleep(0.1)\n        new = yield total # &lt;-- here we get the data from send()\n        total *= new if new is not None else 1\n        if total == 0:\n            return \"Game Over!\"\n</code></pre>\n<p>Also here it is true, that <code>yield</code> does not wait for receiving data. When there\u2019s no data to process, <code>yield</code> returns with <code>None</code>. In the next line the value of <code>new</code> is checked whether it contains a value other than <code>None</code>.</p>\n<p>In your case, it can be fixed with a similar trick:</p>\n<pre><code class=\"lang-auto\">import napari\nimport numpy as np\nfrom napari.qt.threading import thread_worker\nfrom magicgui import magicgui\nimport time\n\nlist_dimensions = [1,2,4,8,16]\n@thread_worker\ndef image_generator(list_dimensions):\n    for dim in list_dimensions:\n        a = np.random.rand(dim, dim)\n        b = np.random.rand(dim, dim)\n        \n        b = b &gt; 0.5\n        yielded = yield (a, b)  #|\n        while not yielded:      #|=&gt; waiting for data from 'worker.send', meaning 'on_yielded' finished\n            yielded = yield     #|\n\n        \nviewer = napari.Viewer()\nworker = image_generator(list_dimensions)\n        \n        \ndef on_yielded(value):\n    if value is None:\n        return\n    print(\"Yielded smtg\")\n    worker.pause()\n    print(f\"Worker paused : {worker.is_paused}\")\n    im_layer = viewer.add_image(value[0])\n    l_layer = viewer.add_labels(value[1])\n    worker.send(True)\n\n    \ndef fetch_new_image():\n    worker.resume()\n    print(worker.is_running)\n    \n@magicgui()\ndef next_image():\n    fetch_new_image()\n\n\nworker.yielded.connect(on_yielded)\nviewer.window.add_dock_widget(next_image)\n\n\nworker.start()\nnapari.run()\n</code></pre>\n<p>I\u2019m not sure if I\u2019m correct about all parts, maybe someone from the napari team can correct me if not. But the code seems to work, try it!</p>", "<p>Great explanation, Thanks a lot for the help !</p>\n<p>I was definitely not familiar enough with Generators and the yield keyword it seems.</p>\n<p>Cheers</p>"], "78376": ["<p>Hello Everyone,</p>\n<p>I am wondering what the best way is to implement MATLAB\u2019s roifilt2 function using scikit-image?</p>\n<p>From their docs, \" <code>roifilt2(h</code>,<a href=\"https://www.mathworks.com/help/images/ref/roifilt2.html#d124e292852\" rel=\"noopener nofollow ugc\"><code>I</code></a>,<a href=\"https://www.mathworks.com/help/images/ref/roifilt2.html#d124e292909\" rel=\"noopener nofollow ugc\"><code>BW</code></a>) filters regions of interest (ROIs) in the 2-D image <code>I</code> using the 2-D linear filter <code>h</code>. <code>BW</code> is a binary mask, the same size as <code>I</code>, that defines the ROIs in <code>I</code>. <code>roifilt2</code> returns an image that consists of filtered values for pixels in locations where <code>BW</code> contains <code>1</code>s, and unfiltered values for pixels in locations where <code>BW</code> contains <code>0</code>s. If you specify a filter, <a href=\"https://www.mathworks.com/help/images/ref/roifilt2.html#d124e292828\" rel=\"noopener nofollow ugc\"><code>h</code></a>, then <code>roifilt2</code> calls <a href=\"https://www.mathworks.com/help/images/ref/imfilter.html\" rel=\"noopener nofollow ugc\"><code>imfilter</code></a> to implement the filter.\"</p>\n<p>Thanks!</p>\n<p>Adam</p>", "<p>Hi <a class=\"mention\" href=\"/u/adamerickson\">@adamerickson</a>! It looks like there\u2019s no obvious support right now for this. If performance is not a huge issue, you can do it with:</p>\n<pre><code class=\"lang-auto\">import numpy as np\nfrom scipy import ndimage as ndi\n\n# I -&gt; image, h -&gt; footprint\nfiltered = ndi.correlate(image, footprint)\n\n# BW -&gt; mask\nroi_filtered = np.where(mask, filtered, image)\n</code></pre>\n<p>This means that you are computing the filtered image everywhere but only keeping it where the mask is True, so much of the computation is wasted if the rois themselves are small. But, it\u2019ll work!</p>", "<p>Hi <a class=\"mention\" href=\"/u/jni\">@jni</a> ! Thank you for the numpy equivalent. That should suffice for now.</p>\n<p>It would be great if <code>scikit-image</code> could serve as a drop-in replacement for MATLAB\u2019s Image Processing Library.</p>", "<p>We try! <img src=\"https://emoji.discourse-cdn.com/twitter/joy.png?v=12\" title=\":joy:\" class=\"emoji\" alt=\":joy:\" loading=\"lazy\" width=\"20\" height=\"20\"> But scikit-image is designed to <em>supplement</em> NumPy and SciPy for image processing. We typically don\u2019t reimplement/rename things in those libraries. Come to think of it, one thing we would definitely welcome is a documentation webpage, \u201cscikit-image for Matlab Image Processing Toolbox users\u201d, where we would document the different ways to implement common toolbox functionality using scientific Python tools.</p>", "<p>Around 2011/2012, scikit-image was going in this direction. We even had a compatibility table, and was working to cover the same API. Except, we soon realized that this took the joy out of it: we were stuck with APIs that made no sense in Python, and were chasing someone else\u2019s roadmap. Letting that go allowed scikit-image to evolve in a more natural way, while greatly improving developer morale.</p>\n<p>That said, there is no reason not to have a document that explains a mapping from MATLAB features to skimage; we just don\u2019t want to use it as a feature selection strategy.</p>"], "78377": ["<p>I have thousands of ROIs with overlays and would like to use the edit &gt; clear outside command in batch on all of these to make the area outside the overlay white. I\u2019m new to scripting in imageJ/FIJI and was wondering if anybody had any insight into how I may do this?</p>"], "78382": ["<p>Hi I am trying to use this BeanShell script for the trainable weka segmentation.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/8kgZRopGZg1V9UWfGm1QRXT7JsA.txt\">skriptAuto.txt</a> (2.1 KB)</p>\n<p>I would like to apply a created classifier to a new image. Unfortunately i get this error message and the statement could not apply classifier.</p>\n<p>WARNING: core mtj jar files are not available as resources to this classloader (sun.misc.Launcher$AppClassLoader@764c12b6)<br>\njava.lang.NullPointerException<br>\njava.util.Arrays.fill(Arrays.java:3021)<br>\ntrainableSegmentation.WekaSegmentation.setFeaturesDirty(WekaSegmentation.java:7435)<br>\ntrainableSegmentation.WekaSegmentation.adjustSegmentationStateToData(WekaSegmentation.java:4983)<br>\ntrainableSegmentation.WekaSegmentation.loadClassifier(WekaSegmentation.java:814)<br>\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>\njava.lang.reflect.Method.invoke(Method.java:498)<br>\nbsh.Reflect.invokeMethod(Reflect.java:160)<br>\nbsh.Reflect.invokeObjectMethod(Reflect.java:93)<br>\nbsh.Name.invokeMethod(Name.java:854)<br>\nbsh.BSHMethodInvocation.eval(BSHMethodInvocation.java:67)<br>\nbsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:94)<br>\nbsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:39)<br>\nbsh.Interpreter.eval(Interpreter.java:673)<br>\nbsh.Interpreter.eval(Interpreter.java:764)<br>\nbsh.Interpreter.eval(Interpreter.java:753)<br>\nbsh.BshScriptEngine.evalSource(BshScriptEngine.java:89)<br>\nbsh.BshScriptEngine.eval(BshScriptEngine.java:61)<br>\njavax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)<br>\norg.scijava.script.ScriptModule.run(ScriptModule.java:164)<br>\norg.scijava.module.ModuleRunner.run(ModuleRunner.java:163)<br>\norg.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>\norg.scijava.module.ModuleRunner.call(ModuleRunner.java:63)<br>\norg.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)<br>\njava.util.concurrent.FutureTask.run(FutureTask.java:266)<br>\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>\njava.lang.Thread.run(Thread.java:750)<br>\nat java.util.Arrays.fill(Arrays.java:3021)<br>\nat trainableSegmentation.WekaSegmentation.setFeaturesDirty(WekaSegmentation.java:7435)<br>\nat trainableSegmentation.WekaSegmentation.adjustSegmentationStateToData(WekaSegmentation.java:4983)<br>\nat trainableSegmentation.WekaSegmentation.loadClassifier(WekaSegmentation.java:814)<br>\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>\nat java.lang.reflect.Method.invoke(Method.java:498)<br>\nat bsh.Reflect.invokeMethod(Reflect.java:160)<br>\nat bsh.Reflect.invokeObjectMethod(Reflect.java:93)<br>\nat bsh.Name.invokeMethod(Name.java:854)<br>\nat bsh.BSHMethodInvocation.eval(BSHMethodInvocation.java:67)<br>\nat bsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:94)<br>\nat bsh.BSHPrimaryExpression.eval(BSHPrimaryExpression.java:39)<br>\nat bsh.Interpreter.eval(Interpreter.java:673)<br>\nat bsh.Interpreter.eval(Interpreter.java:764)<br>\nat bsh.Interpreter.eval(Interpreter.java:753)<br>\nat bsh.BshScriptEngine.evalSource(BshScriptEngine.java:89)<br>\nat bsh.BshScriptEngine.eval(BshScriptEngine.java:61)<br>\nat javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)<br>\nat org.scijava.script.ScriptModule.run(ScriptModule.java:164)<br>\nat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)<br>\nat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)<br>\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>\nat java.lang.Thread.run(Thread.java:750)</p>\n<p>Can someone help me ? I am at a loss.<br>\nThank you!</p>"], "78383": ["<p>Hi,</p>\n<p>I try to launch Fiji with Java 8 on ubuntu (Ubuntu 22.04.1) but it does not work when using:</p>\n<pre><code class=\"lang-auto\">$HOME/Programs/Fiji/Fiji.app/ImageJ-linux64 --java-home \\  \n/usr/lib/jvm/java-8-openjdk-amd64\n</code></pre>\n<p>I receive the following error message: <code>Could not launch system-wide Java (No such file or directory)</code><br>\nHowever, when I launch<code> $HOME/Programs/Fiji/Fiji.app/ImageJ-linux64</code> , Fiji opens correctly.</p>\n<p>Any ideas on how I could fix my problem and launch Fiji  with a different version of Java?</p>\n<p>Cheers</p>"], "78384": ["<p>Does anyone know how QuPath determines which resolutions to show to the user? I have noticed many whole slide images contain resolutions which include the entire slide including the areas that should not be shown to the user like the label area on a slide. I\u2019m creating a whole slide image viewer control for .NET and I want it to be like QuPath.</p>", "<p>Not a dev, but as a user and from other posts, it\u2019s usually one image per image in the original file. Macro images and slide labels are shown/selectable in the Image tab. I <code>think</code> most of that is organized by BioFormats or Openslide which is what QuPath uses to open the images, but I also recall there being posts about recognizing the \u201cmacro\u201d label etc. when deciding which image/images to show.</p>\n<p>Post bout the metadata needed to get those labels read correctly: <a href=\"https://forum.image.sc/t/create-associatedimagemap-in-svs-file/77387/5\" class=\"inline-onebox\">Create AssociatedImageMap in SVS file - #5 by major1819</a><br>\nYou can find others if you search for macro, label image etc.</p>", "<p>Is there a way to determine this using Bioformats? Otherwise I will need to read the tiff tags and determine it that way.</p>", "<aside class=\"quote no-group\" data-username=\"BiologyTools\" data-post=\"3\" data-topic=\"78384\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/biologytools/40/66366_2.png\" class=\"avatar\"> Erik Repo:</div>\n<blockquote>\n<p>Is there a way to determine this using Bioformats?</p>\n</blockquote>\n</aside>\n<p>Yes, see <a href=\"https://bio-formats.readthedocs.io/en/stable/developers/wsi.html\" class=\"inline-onebox\">Working with whole slide images \u2014 Bio-Formats 6.12.0 documentation</a></p>\n<p>QuPath uses <code>setFlattenedResolutions(false)</code> when Bio-Formats is the reader \u2013 but the code used is quite involved, as it has evolved a lot over the years:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java\" target=\"_blank\" rel=\"noopener\">qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java</a></h4>\n\n\n      <pre><code class=\"lang-java\">/*-\n * #%L\n * This file is part of QuPath.\n * %%\n * Copyright (C) 2014 - 2016 The Queen's University of Belfast, Northern Ireland\n * Contact: IP Management (ipmanagement@qub.ac.uk)\n * Copyright (C) 2018 - 2020 QuPath developers, The University of Edinburgh\n * %%\n * QuPath is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as\n * published by the Free Software Foundation, either version 3 of the\n * License, or (at your option) any later version.\n * \n * QuPath is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n * \n * You should have received a copy of the GNU General Public License \n * along with QuPath.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-extension-bioformats/src/main/java/qupath/lib/images/servers/bioformats/BioFormatsImageServer.java\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "66098": ["<p>I came across the need for partial projections several times now, so I thought I\u2019d share a little script I was using to create a \u201csliding-window projection\u201d over stacks:</p>\n<aside class=\"onebox githubgist\" data-onebox-src=\"https://gist.github.com/imagejan/5b34af505de190d957c385587bd47a1f\">\n  <header class=\"source\">\n\n      <a href=\"https://gist.github.com/imagejan/5b34af505de190d957c385587bd47a1f\" target=\"_blank\" rel=\"noopener\">gist.github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://gist.github.com/imagejan/5b34af505de190d957c385587bd47a1f\" target=\"_blank\" rel=\"noopener\">https://gist.github.com/imagejan/5b34af505de190d957c385587bd47a1f</a></h4>\n\n  <h5>Sliding_Window_Projection.groovy</h5>\n  <pre><code class=\"Groovy\">#@ Dataset input\n#@ OpService ops\n#@ Integer (label=\"Width of sliding window (frames)\") window\n#@output projected\n\n// Add a new dimension of window size\nadded = ops.transform().addDimensionView(input, 0, window-1)\nextended = ops.transform().extendBorderView(added)\n\n// Shear along time dimension,</code></pre>\n    This file has been truncated. <a href=\"https://gist.github.com/imagejan/5b34af505de190d957c385587bd47a1f\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n<p>\n</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/ef41c9ec36a582f0c58cd615807e68ee0f94f62f.gif\" alt=\"FakeTracks\" data-base62-sha1=\"y8z4algma1eNxwAMLqY7pjEiA5p\" width=\"128\" height=\"128\" class=\"animated\"> <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/9/19b8e8c6a0cd430ca7a3a305f84671ddbb927181.gif\" alt=\"FakeTracks_avg5frames\" data-base62-sha1=\"3Fy6oc3K3MMARMJdhCf2JgDWmCl\" width=\"128\" height=\"128\" class=\"animated\"> <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/695d1610185a042d13cb6d54de8616b37f92ab69.gif\" alt=\"FakeTracks_10frame-avg\" data-base62-sha1=\"f25zntPlc7ObCHAt9UsHGtqyfpL\" width=\"128\" height=\"128\" class=\"animated\"></p>\n<p>The example images show the <code>FakeTracks</code> example image from <a class=\"hashtag\" href=\"/tag/trackmate\">#<span>trackmate</span></a>, in its original version, and averaged with a 5- or 10-frame sliding window.</p>\n<p>I found partial projections equally useful for time stacks as for z stack volumes, to include some local context. They\u2019re also helpful for denoising without losing too much of the temporal or spatial information of the projection axis.</p>\n<p>Hope it\u2019ll be helpful for someone else as well. Let me know your experience with this kind of projections.</p>", "<p>For those stumbling upon this topic and wondering if <a class=\"hashtag\" href=\"/tag/napari\">#<span>napari</span></a> has something similar to offer: the pull request linked below seems to offer what I was looking to achieve here (and beyond that):</p>\n<aside class=\"onebox githubpullrequest\" data-onebox-src=\"https://github.com/napari/napari/pull/5522\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/napari/napari/pull/5522\" target=\"_blank\" rel=\"noopener\">github.com/napari/napari</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n\n\n\n    <div class=\"github-icon-container\" title=\"Pull Request\">\n      <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 12 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z\"></path></svg>\n    </div>\n\n  <div class=\"github-info-container\">\n\n\n\n      <h4>\n        <a href=\"https://github.com/napari/napari/pull/5522\" target=\"_blank\" rel=\"noopener\">Thick Slices (Dims  as nD-box instead of nD-point)</a>\n      </h4>\n\n    <div class=\"branches\">\n      <code>napari:main</code> \u2190 <code>brisvag:feature/dims-span</code>\n    </div>\n\n      <div class=\"github-info\">\n        <div class=\"date\">\n          opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2023-02-01\" data-time=\"14:26:45\" data-timezone=\"UTC\">02:26PM - 01 Feb 23 UTC</span>\n        </div>\n\n        <div class=\"user\">\n          <a href=\"https://github.com/brisvag\" target=\"_blank\" rel=\"noopener\">\n            <img alt=\"brisvag\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e07ffe67a07bb19c3fcff9219a44ea3c49f4acec.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n            brisvag\n          </a>\n        </div>\n\n        <div class=\"lines\" title=\"41 commits changed 20 files with 690 additions and 319 deletions\">\n          <a href=\"https://github.com/napari/napari/pull/5522/files\" target=\"_blank\" rel=\"noopener\">\n            <span class=\"added\">+690</span>\n            <span class=\"removed\">-319</span>\n          </a>\n        </div>\n      </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Description\nFinally reviving this old code to upgrade our `Dims` model from a<span class=\"show-more-container\"><a href=\"https://github.com/napari/napari/pull/5522\" target=\"_blank\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\"> single point to an n-dimensional bounding box.\n\n*NOTE: currently includes changes from #5528, so refer to that one for the changes to `EventedModel` itself!*\n\nThe basic change is that instead of defining the current `Dims` \"position\" by an nD point (n-tuple), it is instead defined by a `span`, which is an n-tuple of 2-tuples, which describes the range of world coordinates that are considered \"visible\". Everything is defined in world coordinates.\n\nThis results in a visible space with higher dimensionality than `ndisplay`, and thus needs to be projected onto the actual canvas space as preferred.\n\nExample functionality once layer are hooked up (from https://github.com/napari/napari/pull/4012#issuecomment-1049850169):\n\nhttps://user-images.githubusercontent.com/23482191/155531043-644cfa4c-e784-4d01-9d2c-89f7c3235b72.mp4\n\n---\n\nWith this PR, the main \"primary\" fields for `Dims` look like this:\n\nSame as before:\n- `ndim`\n- `ndisplay`\n- `order`\n- `axis_labels`\n\nChanged:\n- `range`: the last component (the `step`) was split out into its own field\n- `step`: used to be in `range` (indicates how much to move in world space with 1 tick of the slider/one arrow click)\n\nRemoved:\n- `current_step`: indicated the current position in \"slider coordinates\" (index of the step)\n\nAdded:\n- `span`: indicates the current bounding box as a `Tuple[Tuple[float, float], ...]` in *world space*\n\nThere are also several additions to properties and methods that help with dealing with the setters:\n\nChanged properties:\n- `nsteps`: now settable (will change `step` to match the requested number of steps)\n- `current_step`: is now deprecated in favor of `point_step` (see below)\n- `displayed`: can now be set (will affect `self.order`)\n\nAdded properties:\n- `thickness`: returns the thickness of the bounding box in each dimension. Can be set, which will set new values for `span` by extending the limits symmetrically.\n- `point`: returns the middle point of `span` for each dimension. If set, will move both limits in `step` to match the new center.\n\nFor most of the above, there is also a `_step` version (e.g: `point_step`, `_thickness_step`, which returns/sets the same values as the normal counterpart, but in \"slider space\" rather than world space. So the equivalent of the old `current_step` is now `point_step`. Please feel free to suggest another naming scheme if you have better ideas!\n\nFinally, I added `set_X` utility methods just like we had for the existing fields. They basically allow to set a single component of dims without having to do this all the time:\n```\nnew_range = list(dims.range)\nnew_range[2] = new_value\ndims.range = new_range\n```\n\n---\n\nOther relevant notes:\n- now everything is in world space, and anything which is not gets computed as a property (differently from main, where `current_step` is in \"slider space\" and is the primary source of the dims state).\n- one critique I received the first time around whas that this was increasing the surface of the public API too much, so I kept most of the new stuff private, when possible. Does the current state look reasonable, or should I privatize more/less?\n- one thing that I changed but was not *strictly* necessary is that `range` is now just a 2-tuple, and the `step` is held separately. They are often needed together, but just as often separately, and to me they made no sense in the same object. This simplified a lot of the work in `Dims`, but is not backward compatible and required some changes elsewhere in the codebase.\n- there were many previous iterations and a lot of discussion around the implementation (particularly around using evented containers for a more ergonomic interface). Please refer to the Reference section below for a complete history, but **TL;DR: nested evented containers with non-buggy validation are impossible or unreasonably slow with our current `EventedModel`**.\n\n## Type of change\n\n- [ ] Bug-fix (non-breaking change which fixes an issue)\n- [x] New feature (non-breaking change which adds functionality)\n- [x] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n- [x] This change requires a documentation update\n\n# References\n- original implementation (which ultimately is very close to this :P): #4012\n- example of how layers could be hooked up to it: https://github.com/napari/napari/pull/4012#issuecomment-1049850169 and https://github.com/napari/napari/pull/4012#issuecomment-1057098537\n- broken version with evented containers: #4334 \n- some extra stuff that didn't work out to make working with evented objects and property better:\n\t- #4417 \n\t- #4457\n- ... and the \"Thick slices\" PR that came out of these: #4448, #4522\n- attempts at getting nested evented models to work:\n\t- #4474 \n\t- #4597 \n\t- #4609 \n\t- #4804 \n\n# How has this been tested?\n\n- [ ] example: the test suite for my feature covers cases x, y, and z\n- [ ] example: all tests pass with my change\n- [ ] example: I check if my changes works with both PySide and PyQt backends\n      as there are small differences between the two Qt bindings.  \n\n## Final checklist:\n- [ ] My PR is the minimum possible work for the desired functionality\n- [ ] I have commented my code, particularly in hard-to-understand areas\n- [ ] I have made corresponding changes to the documentation\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] If I included new strings, I have used `trans.` to make them localizable.\n      For more information see our [translations guide](https://napari.org/developers/translations.html).</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "78386": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b.jpeg\" data-download-href=\"/uploads/short-url/9gA5RL2ZKcE3iCzeM74PjMaZ5ub.jpeg?dl=1\" title=\"Contour\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_192x500.jpeg\" alt=\"Contour\" data-base62-sha1=\"9gA5RL2ZKcE3iCzeM74PjMaZ5ub\" width=\"192\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_192x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_288x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/0/40f36ce73d45288c5c7354d68c5f3703e968b35b_2_384x1000.jpeg 2x\" data-dominant-color=\"161616\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Contour</span><span class=\"informations\">1554\u00d74026 96.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236.jpeg\" data-download-href=\"/uploads/short-url/3El57GmjAJDFw7N9m0aFv5j7CBg.jpeg?dl=1\" title=\"Contour_distances\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_204x500.jpeg\" alt=\"Contour_distances\" data-base62-sha1=\"3El57GmjAJDFw7N9m0aFv5j7CBg\" width=\"204\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_204x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_306x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/9/1995e4ccef76a6b9fbab70ebe146e06fe7abe236_2_408x1000.jpeg 2x\" data-dominant-color=\"232323\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Contour_distances</span><span class=\"informations\">461\u00d71125 27.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hello everyone,</p>\n<p>I would have to measure the contour as automatically as possible (macro). Specifically, the following values would be important for me:</p>\n<ul>\n<li>mean value of the distance between contour left and right</li>\n<li>smallest distance between contour left and right (xmin)</li>\n<li>largest distance between contour left and right (xmax)</li>\n<li>distance between contour left and right measured exactly in the middle top to bottom (x at ymiddle)</li>\n</ul>\n<p>The same in y-direction (contour from top to bottom).</p>\n<p>Does anyone have a solution for me?</p>\n<p>Thank you a lot in advance!</p>", "<p>Hi <a class=\"mention\" href=\"/u/carsten2023\">@Carsten2023</a>. Welcome to the forum!</p>\n<p>There\u2019s a great search functionality here that will let you see similar questions that have been asked. Here is an example, searching for \u201cdistance between lines\u201d:<br>\n<a href=\"https://forum.image.sc/search?expanded=true&amp;q=distance%20between%20lines\">https://forum.image.sc/search?expanded=true&amp;q=distance%20between%20lines</a></p>\n<p>Perhaps some of these approaches would help?</p>", "<p>Hi <a class=\"mention\" href=\"/u/carsten2023\">@Carsten2023</a><br>\nPlease provide feedback.<br>\nThanks in advance.</p>\n<p>Here is the result of using the macro below.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed.png\" data-download-href=\"/uploads/short-url/yj2ShLVEtmbUrFX5M0CjL9vuSsJ.png?dl=1\" title=\"Image captur\u00e9e-10-03-2023 19-29-24\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_345x220.png\" alt=\"Image captur\u00e9e-10-03-2023 19-29-24\" data-base62-sha1=\"yj2ShLVEtmbUrFX5M0CjL9vuSsJ\" width=\"345\" height=\"220\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_345x220.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_517x330.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed_2_690x440.png 2x\" data-dominant-color=\"F3F3F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Image captur\u00e9e-10-03-2023 19-29-24</span><span class=\"informations\">971\u00d7620 56.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-auto\">macro \"distance between contours\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\nimg=getImageID();\n//---------------------------\n// Start batch mode\nsetBatchMode(true);\nselectImage(img);\nrun(\"Duplicate...\", \"title=1\");\nclose(\"\\\\Others\")\n//-------------------------------\n// Start image processing\nh=getHeight();\nw=getWidth();\n//-------------------------------\n// scan vertical\nfor(i=0;i&lt;=h-1;i++){\nw=getWidth();\n//-------------------------------\n// scan horizontal\nn=0;\nfor(j=0;j&lt;=w-1;j++){\nvalue=getValue(j,i);\nif(value==0)\nn++;\nsetResult(\"thickness\",i,n);}\nupdateResults();\n}\n//-------------------------------\n// Graphic representation of thickness\nPlot.create(\"Plot of Results\", \"x\", \"thickness\");\nPlot.add(\"Line\", Table.getColumn(\"thickness\", \"Results\"));\nPlot.setStyle(0, \"blue,#a0a0ff,1.0,Line\");\nprint(\"ended\");\n//-------------------------------\n// End of processing\n// End of batch mode\nsetBatchMode(false);\nclose();\nexit();\n}\n\n</code></pre>"], "4658": ["<p>Hello All, lurker for some time but first time poster. I am having an issue with the Trainable Weka Segmentation where I\u2019m wanting to apply known classifiers and loaded data to a directory of tiff images. Each image is somewhat similar but also somewhat different in terms of the micro structure; but when I apply the trainable segmentation with the following ijm script:</p>\n<pre><code class=\"lang-javascript\">print(\"Getting input directory.\");\n\ninputDir = getDirectory(\"C:\\Users\\clopez\\Pictures\\output tiffs\");\n\nprint(\"Getting output directory\");\n\noutputDir = getDirectory(\"C:\\Users\\clopez\\Pictures\\output tiffs segmented\");\n\nprint(\"Processing directory\");\n\nsetBatchMode(true);\nsegmentFolder(inputDir, outputDir);\nsetBatchMode(false);\n\n\nfunction segmentFolder(input,output)\n{\t\n\tlistOfFiles = getFileList(input);\t\n\t\n\tfor(i = 0; i &lt; listOfFiles.length-1; i++) \n\t{\n\t\tif(endsWith(listOfFiles[i], \".tiff\")) {\n\t\t\tprint(\"File found, processing ...\");\n\t\t\tsegmentFile(input, output, listOfFiles[i]);\n\t\t\tprint(\"File segmented.\");\t\n\t\t}\n\t }\n}\n\n\nfunction segmentFile(input, output, filename)\n{\n\topen(input+filename);\n\trun(\"Trainable Weka Segmentation\");\n\tselectWindow(\"Trainable Weka Segmentation v3.2.5\");\n\tcall(\"trainableSegmentation.Weka_Segmentation.loadClassifier\", \"C:\\\\Users\\\\clopez\\\\Pictures\\\\output tiffs\\\\myClassifier_3-23-17.model\");\n\tcall(\"trainableSegmentation.Weka_Segmentation.loadData\", \"C:\\\\Users\\\\clopez\\\\Pictures\\\\output tiffs\\\\myData_3-23-17.arff\");\n\tcall(\"trainableSegmentation.Weka_Segmentation.trainClassifier\");\n\tcall(\"trainableSegmentation.Weka_Segmentation.getResult\");\n\tselectWindow(\"Classified image\");\n\trun(\"8-bit\");\n\tsetOption(\"BlackBackground\", true);\n\trun(\"Make Binary\");\n\trun(\"Options...\", \"iterations=8 count=1 black do=Close\");\n\trun(\"Invert LUT\");\n\trun(\"Watershed\");\n\trun(\"Microstructure Analysis\", \"perimeter=[Crofton (4 dirs.)] add_porosity\");\n\trun(\"Analyze Particles...\", \"pixel show=Outlines display exclude clear include summarize\");\n\tselectWindow(\"Classified image\");\n\tsaveAs(\"Tiff\", output+filename);\n\tclose();\n}\n</code></pre>\n<p>the images all come out basically the same as if it\u2019s just iterating on the same image over and over again, and this is the confirmed by the measurement call. I was hoping to get some third party eyes on my script and see if it\u2019s something there that\u2019s forcing this. Any and all help is appreciated, thanks!</p>", "<p>Dear <a class=\"mention\" href=\"/u/clopez\">@clopez</a>,</p>\n<p>your issue might be the call to <code>trainableSegmentation.Weka_Segmentation.loadData()</code>, although I am not sure.</p>\n<p>You should only have to load a classifier and be able to apply it to a new image. As described on the <a href=\"http://imagej.net/Trainable_Weka_Segmentation#Macro_language_compatibility\">Wiki page</a>, you can use <code>applyClassifier()</code> for that purpose.</p>\n<p>If you need further guidance, just post again.</p>\n<p>Best,<br>\nStefan</p>", "<p>Hello <a class=\"mention\" href=\"/u/stelfrich\">@stelfrich</a> and <a class=\"mention\" href=\"/u/clopez\">@clopez</a>,</p>\n<p>Sorry to get so late into the conversation. The code looks OK, but since you are using macros, you might need to use some <code>wait</code> calls every now and then to avoid the commands being executed before they should. Have a look at the<a href=\"http://imagej.net/Trainable_Weka_Segmentation#Complete_macro_example:\"> wiki example</a> (line 8).</p>\n<p>That being said, I would recommend you to use <a href=\"http://imagej.net/Scripting_the_Trainable_Segmentation\">proper scripts</a> instead of macros to prevent these errors from happening. I can help you with that if you need it.</p>\n<p>ignacio</p>", "<p>Hi,</p>\n<p>I\u2019ve been trying (and failing) to do this same task in ImageJ. I found the example beanshell script and tried running that, but it popped up with \u201cError while adjusting data!\u201d and then everything went downhill. The console popped up with a lot of java errors and then another window popped up saying \u201cClassifier could not be applied\u201d. Am I missing a step? Or what have I done wrong?</p>\n<p>Thanks so much!<br>\n~Katie</p>"], "78389": ["<p>Hello guys,</p>\n<p>I have another question because I cannot simply find the answer.<br>\nI created one annotation called \u201ctumor\u201d and a rectangle which includes the tumor area. Now I want to inverse the tumor area within the rectangle to get a new area called stroma (but within in the stroma). If I only inverse the tumor area, QuPath creates an annotation which includes the whole slide which is not demandend.<br>\nHow can i do that?</p>\n<p>E.g.<br>\nI want to substract the violet area from the red to get a new area which includes everything else inside the rectangle</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bac8f176dac7474a809cb451216cffa68db215d3.jpeg\" data-download-href=\"/uploads/short-url/qEnkmeVk5zAXdkbJVKTesZAzq7N.jpeg?dl=1\" title=\"grafik\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bac8f176dac7474a809cb451216cffa68db215d3.jpeg\" alt=\"grafik\" data-base62-sha1=\"qEnkmeVk5zAXdkbJVKTesZAzq7N\" width=\"656\" height=\"500\" data-dominant-color=\"BB67A5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">grafik</span><span class=\"informations\">872\u00d7664 92.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/77a6b824a46bdcf1686a9214df208226e05f37da.gif\" alt=\"inverse anno\" data-base62-sha1=\"h4u0PIUfYdLDEubxMEHib9Ub7Jg\" width=\"690\" height=\"492\" class=\"animated\"><br>\nGUI first using two annotations that are not associated and inverting the selected annotation,<br>\nthen inserting into hierarchy and inverting.</p>\n<p>You can also subtract, but you will need to duplicate the interior annotation.</p>\n<p>All of this can be scripted if you look up examples of hierarchy posts and/or annotation subtraction.</p>"], "78390": ["<p>Hello i works with the plugin \" segmentation / \u201cclor clustering\u201d on ImageJ.</p>\n<p>The plugin works perfect to analyse the cover of crops on the field.</p>\n<p>But the record a macro does not work!</p>\n<p>Can anyone help me make it automatic?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81.jpeg\" data-download-href=\"/uploads/short-url/28eUTo1wgC9GQ7TQV8bTWShH225.jpeg?dl=1\" title=\"color4\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81_2_435x500.jpeg\" alt=\"color4\" data-base62-sha1=\"28eUTo1wgC9GQ7TQV8bTWShH225\" width=\"435\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81_2_435x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0ef2d127ae3929b34621e6f243a74e92dd699a81.jpeg 2x\" data-dominant-color=\"E1E5E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">color4</span><span class=\"informations\">606\u00d7695 87.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce.jpeg\" data-download-href=\"/uploads/short-url/f5BNZ4s16YPFKagXAYLqypYK0Sa.jpeg?dl=1\" title=\"color3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce_2_690x431.jpeg\" alt=\"color3\" data-base62-sha1=\"f5BNZ4s16YPFKagXAYLqypYK0Sa\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69c2f2683908e262457b36675a7f25c1ab5894ce.jpeg 2x\" data-dominant-color=\"9D9776\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">color3</span><span class=\"informations\">1211\u00d7758 290 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee.jpeg\" data-download-href=\"/uploads/short-url/9RYSO89QYwO2WvbkQqFmMg49aEu.jpeg?dl=1\" title=\"Color\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee_2_690x416.jpeg\" alt=\"Color\" data-base62-sha1=\"9RYSO89QYwO2WvbkQqFmMg49aEu\" width=\"690\" height=\"416\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee_2_690x416.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee_2_1035x624.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/452db66571673e0ff0b8fbde3bf2acd285070eee.jpeg 2x\" data-dominant-color=\"7D7D7D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Color</span><span class=\"informations\">1259\u00d7760 309 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks<br>\nMarcus<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4ce98020f59eaa28e420f3e35860b325f0838365.jpeg\" alt=\"color2\" data-base62-sha1=\"aYoyJH4RdXA1UtdYU72zKjXAVSZ\" width=\"506\" height=\"168\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/marcus_imagej\">@Marcus_Imagej</a>,<br>\nas far as I know (I could be wrong) you can not use it from a macro, but you could use the <a href=\"https://weka.sourceforge.io/doc.dev/weka/clusterers/SimpleKMeans.html\" rel=\"noopener nofollow ugc\">weka api</a> from a script (for example jython), if that\u2019s an option for you.</p>\n<p>For our <a href=\"https://montpellierressourcesimagerie.github.io/mri-workshop-machine-learning/\" rel=\"noopener nofollow ugc\">DL/ML workshop</a></p>\n<p>I wrote something similar, but based on the apache clustering library (which also comes with FIJI).</p>\n<p>If you want to try that:<br>\nPut the two files <a href=\"https://raw.githubusercontent.com/MontpellierRessourcesImagerie/mri-workshop-machine-learning/master/k-means_experiment/k-means_segmentation.py\" rel=\"noopener nofollow ugc\">k-means_segmentation.py</a> and  <a href=\"https://raw.githubusercontent.com/MontpellierRessourcesImagerie/mri-workshop-machine-learning/master/k-means_experiment/normalize_feature_stack.py\" rel=\"noopener nofollow ugc\">normalize_feature_stack.py</a> into <code>plugins</code> or a subfolder and restart FIJI.<br>\nYou can now run it from the ImageJ GUI and it is macro recordable. The input is a feature stack. So you need to call <code>Image&gt;Type&gt;??? Stack</code> on your RGB image, or use another way to create the components in the color space you want to use.</p>\n<p>Best,<br>\nVolker</p>", "<p>Hello Volker,</p>\n<p>thanks for the quick reply, i will test your suggestion.</p>\n<p>Marcus</p>"], "78395": ["<p>Hi folks,<br>\nI have a two channel color composite.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/tuqjbf2HtXD5J7QwN7CGNUIj4o3.tif\">composite.tif</a> (18.6 KB)</p>\n<p>Lets say I want the same Pixel value in Channel 2 to appear 10 times darker than in Channel 1, without altering the actual pixel values or changing the maximum Intensity value of Channel 1. Just altering the LUT for Channel 2.<br>\nIt is a 16-bit Image and while raising the maximum intensity in the Brightness&amp;Contrast tab lowers the brightness, it only has an effect till the maximum 16bit value of 65535. I can set the maximum intensity value to let\u2019s say 100k, but it has no further effect, unless I convert the image to 32bit. The maximum window width to have an effect on the LUT seems to be 65535. So my question is: Do you know any possibility to have a bigger effective window width than 65535 for visualization (some kind of stretching of the LUT?) without converting to 32bit which would double the needed disk space?</p>\n<p>Thank you for your input!</p>", "<p>Hi <a class=\"mention\" href=\"/u/librethinker\">@librethinker</a>,</p>\n<p>This bug is fixed in the ImageJ 1.54d5 daily build.</p>"], "78397": ["<p>Hi,</p>\n<p>I do not manage to import data in a fresh omero docker deployment.</p>\n<p>The server runs, login works fine, importing inplace inside the server works, but I have a IceConnectException when trying to \u201cclasically\u201d import data.</p>\n<p>Here is a minimal repository reproducing the issue:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/glyg/minimal_import_fail-\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/glyg/minimal_import_fail-\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/2/c2d15e7ce76bd684bfb14b2046aea61980b401d9.png 2x\" data-dominant-color=\"EBEEEA\"></div>\n\n<h3><a href=\"https://github.com/glyg/minimal_import_fail-\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - glyg/minimal_import_fail-: a minimal example of import fail in omero...</a></h3>\n\n  <p>a minimal example of import fail in omero CLI. Contribute to glyg/minimal_import_fail- development by creating an account on GitHub.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>The docker-compose file:</p>\n<pre><code class=\"lang-yaml\">version: \"3\"\n\nservices:\n  db_omero:\n    image: \"postgres:11\"\n    environment:\n      POSTGRES_USER: omero\n      POSTGRES_DB: omero\n      POSTGRES_PASSWORD: omero\n    networks:\n      - omero\n\n  omeroserver:\n    image: openmicroscopy/omero-server:latest\n    container_name: omeroserver\n    environment:\n      CONFIG_omero_db_host: db_omero\n      # This is the postgres service name in docker-compose\n      CONFIG_omero_db_user: omero\n      CONFIG_omero_db_pass: omero\n      CONFIG_omero_db_name: omero\n      # This is the omeroserver root password\n      ROOTPASS: omero\n      OMERO_ROOT_PASSWORD: omero\n    networks:\n      - omero\n    ports:\n      - \"4064:4064\"\n      - \"4065:4065\"\n      - \"4066:4066\"\n\nnetworks:\n  omero:\n</code></pre>\n<p>The script:</p>\n<pre><code class=\"lang-sh\">#!/usr/bin/bash\n\n# needs omero-py\nset -eu\ndocker-compose up -d\n\nOMERO_USER=root\nOMERO_PASS=omero\nOMERO=/opt/omero/server/OMERO.server/bin/omero\n\n# Wait up to 2 mins\ndocker-compose exec -T omeroserver $OMERO login -C -s localhost -u \"$OMERO_USER\" -q -w \"$OMERO_PASS\" --retry 120\necho \"OMERO.server connection established\"\nomero import -u root -w omero -s localhost -p 4064 img0.tif\n\n</code></pre>\n<p>The error:</p>\n<details>\n<pre><code class=\"lang-auto\">2023-03-10 13:54:45,084 105        [      main] INFO          ome.formats.importer.ImportConfig - OMERO.blitz Version: 5.6.0\n2023-03-10 13:54:45,093 114        [      main] INFO          ome.formats.importer.ImportConfig - Bioformats version: 6.11.1 revision: 383bac974cd52e83908b54e4769ebb1e0d0673ee date: 1 December 2022\n2023-03-10 13:54:45,133 154        [      main] INFO   formats.importer.cli.CommandLineImporter - Log levels -- Bio-Formats: ERROR OMERO.importer: INFO\n2023-03-10 13:54:45,300 321        [      main] INFO      ome.formats.importer.ImportCandidates - Depth: 4 Metadata Level: MINIMUM\n2023-03-10 13:54:45,390 411        [      main] INFO      ome.formats.importer.ImportCandidates - 1 file(s) parsed into 1 group(s) with 1 call(s) to setId in 88ms. (90ms total) [0 unknowns]\n2023-03-10 13:54:45,409 430        [      main] INFO       ome.formats.OMEROMetadataStoreClient - Attempting initial SSL connection to localhost:4064\n2023-03-10 13:54:45,595 616        [      main] INFO       ome.formats.OMEROMetadataStoreClient - Insecure connection requested, falling back\n2023-03-10 13:54:45,629 650        [      main] ERROR  formats.importer.cli.CommandLineImporter - Error during import process.\nIce.ConnectFailedException: java.net.ConnectException: Connexion refus\u00e9e\n\tat IceInternal.AsyncResultI.__wait(AsyncResultI.java:276)\n\tat Ice.ObjectPrxHelperBase.end_ice_isA(ObjectPrxHelperBase.java:310)\n\tat Ice.ObjectPrxHelperBase.ice_isA(ObjectPrxHelperBase.java:92)\n\tat Ice.ObjectPrxHelperBase.ice_isA(ObjectPrxHelperBase.java:69)\n\tat Ice.ObjectPrxHelperBase.checkedCastImpl(ObjectPrxHelperBase.java:2810)\n\tat Ice.ObjectPrxHelperBase.checkedCastImpl(ObjectPrxHelperBase.java:2770)\n\tat Glacier2.RouterPrxHelper.checkedCast(RouterPrxHelper.java:1787)\n\tat omero.client.getRouter(client.java:889)\n\tat omero.client.createSession(client.java:810)\n\tat omero.client.joinSession(client.java:745)\n\tat omero.client.createClient(client.java:544)\n\tat ome.formats.OMEROMetadataStoreClient.unsecure(OMEROMetadataStoreClient.java:810)\n\tat ome.formats.OMEROMetadataStoreClient.initialize(OMEROMetadataStoreClient.java:770)\n\tat ome.formats.importer.ImportConfig.createStore(ImportConfig.java:381)\n\tat ome.formats.importer.cli.CommandLineImporter.&lt;init&gt;(CommandLineImporter.java:158)\n\tat ome.formats.importer.cli.CommandLineImporter.main(CommandLineImporter.java:1021)\nCaused by: java.net.ConnectException: Connexion refus\u00e9e\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)\n\tat IceInternal.Network.doFinishConnect(Network.java:437)\n\tat IceInternal.StreamSocket.connect(StreamSocket.java:96)\n\tat IceInternal.TcpTransceiver.initialize(TcpTransceiver.java:24)\n\tat Ice.ConnectionI.initialize(ConnectionI.java:1921)\n\tat Ice.ConnectionI.message(ConnectionI.java:940)\n\tat IceInternal.ThreadPool.run(ThreadPool.java:395)\n\tat IceInternal.ThreadPool.access$300(ThreadPool.java:12)\n\tat IceInternal.ThreadPool$EventHandlerThread.run(ThreadPool.java:832)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n</code></pre>\n<details></details></details>", "<p>It might be because port 4063 is not also exposed. Does <code>omero import --encrypted=true</code> also fail?</p>", "<p>So the error was the wrong exposed ports in the <code>docker-compose.yml</code> file. Using <code>encrypted=true</code> bypasses the unencrypted 4063 port (IIUC).</p>\n<p>I don\u2019t know where I found the 4065 and 4066 ports in that docker-compose file I\u2019ve been copy/pasting, it\u2019s 63 and 64 everywhere else \u2026</p>", "<p>I think those are the ports if you are using websockets. ~J</p>", "<p>Oooh right \u2026 sorry for the noise (I <em>plan</em> on using websockets mind you, but not now)</p>"], "78400": ["<p>A new paper, <a href=\"https://t.co/kOJBPfw54o\">\u201cToward scalable reuse of vEM data: OMEZarr to the rescue\u201d</a> from <a class=\"mention\" href=\"/u/normanrz\">@normanrz</a> , <a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a> and me is out now!  Apologies for lack of open access. For the next 50 days (until 2023-04-28) you can access the chapter on using <span class=\"hashtag\">#OMEZarr</span> in <span class=\"hashtag\">#VolumeEM</span> <a href=\"https://t.co/4npp6iHztS\">here</a>.</p>\n<p>Special thanks to volume editors Lucy Collinson, Kedar Narayan, and Paul Verkade, and artist <a href=\"https://twitter.com/DrHenningFalk\">Henning Falk</a> for the excellent illustrations! <a href=\"https://github.com/zarr-developers/zarr-illustrations-falk-2022\">All art from the paper</a> can be <a href=\"https://creativecommons.org/licenses/by/4.0/\">shared and adapted with attribution</a>.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63.jpeg\" data-download-href=\"/uploads/short-url/aebHQvcMlzJASfTaIvPFEW7w8eL.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_420x375.jpeg\" alt=\"image\" data-base62-sha1=\"aebHQvcMlzJASfTaIvPFEW7w8eL\" width=\"420\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_420x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_630x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b0570d387d8e9b58f80544e810f63e6e9b2e63_2_840x750.jpeg 2x\" data-dominant-color=\"F1F0F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71711 151 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div>  <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735.jpeg\" data-download-href=\"/uploads/short-url/4iuXn56BWnikDw6rfCZmhBGR2LP.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_256x375.jpeg\" alt=\"image\" data-base62-sha1=\"4iuXn56BWnikDw6rfCZmhBGR2LP\" width=\"256\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_256x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_384x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1e2008d2121299c507aa5f92149f78df3b07d735_2_512x750.jpeg 2x\" data-dominant-color=\"E7E7E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d72806 403 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>Abstract</strong>: The growing size of EM volumes is a significant barrier to findable, accessible, interoperable, and reusable (FAIR) sharing. Storage, sharing, visualization and processing are challenging for large datasets. Here we discuss a recent development toward the standardized storage of volume electron microscopy (vEM) data which addresses many of the issues that researchers face. The OME-Zarr format splits data into more manageable, performant chunks enabling streaming-based access, and unifies important metadata such as multiresolution pyramid descriptions. The file format is designed for centralized and remote storage (e.g., cloud storage or file system) and is therefore ideal for sharing large data. By coalescing on a common, community-wide format, these benefits will expand as ever more data is made available to the scientific community.</p>"], "78406": ["<p>Hi all,</p>\n<p>I am studying cell alignment (using OrientationJ on fluorescence images) in a uniaxially fixed 3D scaffold. I would like to compare the angle of the cell signal to the angle of the scaffold. In most cases, my scaffold is fixed to 2 points which are facing each other (left drawing), so that the angle of my scaffold is 0 degrees (compared to a vertical line). However, it turned out that not all attachment points are facing each other perfectly, which causes that some scaffolds are fixed in the well under an angle (right drawing). To correct for this, I would like to subtract the angle of the scaffold from the angle of the cells that I get from OrientationJ, however I struggle to determine the angle of this scaffold in an unbiased way. Could you help me to find a way to (automate) the detection of the angle of the scaffold, to reduce the bias compared to doing this manually with the angle tool? The outline of the scaffold is visible in the fluorescence images that I use to run orientation J (second upload).</p>\n<p>Best,<br>\nJudith</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41.jpeg\" data-download-href=\"/uploads/short-url/kk8CnzHZDtMAjxr8Wl8jS4sLUHL.jpeg?dl=1\" title=\"Velcro angle\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_690x349.jpeg\" alt=\"Velcro angle\" data-base62-sha1=\"kk8CnzHZDtMAjxr8Wl8jS4sLUHL\" width=\"690\" height=\"349\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_690x349.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_1035x523.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e7244a0e6aee7d92598544d407c3547e271fa41_2_1380x698.jpeg 2x\" data-dominant-color=\"E9EAEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Velcro angle</span><span class=\"informations\">1862\u00d7943 58.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d.jpeg\" data-download-href=\"/uploads/short-url/mbMOQRN2yTL55iPvMc6vAi0WPoN.jpeg?dl=1\" title=\"Velcro angle_ex\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_132x499.jpeg\" alt=\"Velcro angle_ex\" data-base62-sha1=\"mbMOQRN2yTL55iPvMc6vAi0WPoN\" width=\"132\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_132x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_198x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/b/9b84ef6736645ddc2a46ad3378310d5585cc853d_2_264x998.jpeg 2x\" data-dominant-color=\"340305\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Velcro angle_ex</span><span class=\"informations\">307\u00d71160 35.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "41544": ["<p>I\u2019m trying to connect MicroManager to Axio Observer 7, and we know the serial port connection and all the hardware details. We tried all three options ( Zeiss CAN29 adapter, Zeiss AxioZoom adapter, Zeiss CAN adapter) but none of the offered configuration files and their listed hardware features worked. If anyone can help let me know which configuration file to use or if someone built a configuration file for the Zeiss Axio Observer 7 could send it to me?</p>", "<p>Never seen that model in the wild yet.  The Zeiss CAN29 adapter should be the one, but it is possible that Zeiss changed the interface (slightly) so that the MM code does not work.  Go to Help &gt; Report a Problem, run the Hardware Configuration Wizard again (using the Zeiss CAN29 adapter), and send the error report.  I\u2019ll have a look to see if there is a simple solution.</p>", "<p>Will do. Really appreciate the help!</p>", "<p>Hello everyone! I was wondering if this problem had ever been solved and if <a class=\"mention\" href=\"/u/ayang\">@ayang</a> was able to get Micromanager to work with the Axio Observer 7? My company is looking into potentially purchasing this microscope but we will not purchase it if we cannot easily get Micromanager to communicate with it. Thanks!</p>", "<p>Many have successfully connected MM.  Best thing to do would be to test yourself (or ask Zeiss to ensure it will work).  A laptop with USB to serial adapter should be enough to test.</p>", "<p>Since your microscope is Zeiss I would recommend using the Zeiss MTB API incase you can\u2019t get micromanager to work and know C#. I have a github project which uses the API which may be helpful. <a href=\"https://github.com/BiologyTools/BioImager\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - BiologyTools/BioImager: A .NET microscopy imaging application based on Bio library. Supports various microscopes by using imported libraries &amp; GUI automation. Supports XInput game controllers to move stage, take images, run ImageJ macros on images or Bio C# scripts.</a></p>"], "78408": ["<p>Hi all,</p>\n<p>I just tried to convert a CZI file using the OME-NGFF converter tool, but it fails immediately complaing that \u201cseries 96 is not present\u201d. But when opening in Fiji via BioFomats one can clearly see, that there are 96 series.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.dropbox.com/s/uuh8ktgk5lz7e7d/testwell96_test.czi?dl=0\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d0b5830ec317206a6bb1fa759b18c97832111733.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.dropbox.com/s/uuh8ktgk5lz7e7d/testwell96_test.czi?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">Dropbox</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png\" class=\"thumbnail onebox-avatar\" width=\"160\" height=\"160\">\n\n<h3><a href=\"https://www.dropbox.com/s/uuh8ktgk5lz7e7d/testwell96_test.czi?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">testwell96_test.czi</a></h3>\n\n  <p>Shared with Dropbox</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>The metadata of this CZI are fine and I see no issues when running itthrough our CZIChekcer.Any ideas?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/a/5a3605477d1c96c51ee6c367e16999f7b4986b78.png\" data-download-href=\"/uploads/short-url/cS2I52GTSyrHQN0bnqX88PkPc00.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/a/5a3605477d1c96c51ee6c367e16999f7b4986b78.png\" alt=\"image\" data-base62-sha1=\"cS2I52GTSyrHQN0bnqX88PkPc00\" width=\"690\" height=\"288\" data-dominant-color=\"232424\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1583\u00d7662 41.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Github Issue: <a href=\"https://github.com/glencoesoftware/NGFF-Converter/issues/42\" rel=\"noopener nofollow ugc\">NGFF-Converter 1.1.4 fails to convert CZI file which opens normally in Fiji using BioFormats \u00b7 Issue #42 \u00b7 glencoesoftware/NGFF-Converter (github.com)</a></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c.jpeg\" data-download-href=\"/uploads/short-url/1wmMKMr6g5FBk5Z0zD3zl7S4wLG.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_690x431.jpeg\" alt=\"image\" data-base62-sha1=\"1wmMKMr6g5FBk5Z0zD3zl7S4wLG\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aaad5aef800a87f8ba058cb252c316183fc5b2c_2_1380x862.jpeg 2x\" data-dominant-color=\"E4E5E4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71201 215 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94f4db372486c4909caba42d98b8e1d57c1f55a1.png\" alt=\"image\" data-base62-sha1=\"lfJgBCwlfgDWILiSn7NTS2WJrWN\" width=\"351\" height=\"497\"></p>\n<pre><code class=\"lang-plaintext\">17:11:36 DEBUG l.f.FormatHandler - loci.formats.in.ZeissCZIReader.initFile(image.czi)\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getParent()\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1460572002 OPEN\n17:11:36 TRACE l.f.FormatHandler - plane #0 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 3296, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=2645, size=421, startCoordinate=0.0, storedSize=421; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=0, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #1 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 858240, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=2267, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=1, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #2 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 1717248, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=1889, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=2, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #3 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 2576256, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=1511, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=3, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #4 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 3435264, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=1133, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=4, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #5 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 4294272, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=755, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=5, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #6 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 5153280, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=377, size=423, startCoordinate=0.0, storedSize=423; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=6, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.f.FormatHandler - plane #7 = coreIndex=0, planeIndex=0, resolutionIndex=0, x=0, y=0, row=0, col=0, metadata=&lt;METADATA /&gt;, attachmentSize=0, directoryEntry=schemaType = DV, pixelType = 0, filePosition = 6012288, filePart = 0, compression = 0, pyramidType = 0, dimensionCount = 3, dimensions = [dimension=X, start=0, size=422, startCoordinate=0.0, storedSize=422; dimension=Y, start=0, size=2030, startCoordinate=0.0, storedSize=2030; dimension=M, start=7, size=1, startCoordinate=0.0, storedSize=1]\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 858450849 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 858450849 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1303715745 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1303715745 CLOSE\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, C:\\Program Files\\NGFF-Converter\\image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - getParent()\n17:11:36 TRACE l.c.Location - Location(null, C:\\Program Files\\NGFF-Converter)\n17:11:36 TRACE l.c.Location - list(true)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(C:\\Program Files\\NGFF-Converter, app)\n17:11:36 TRACE l.c.Location - isHidden()\n17:11:36 TRACE l.c.Location - Location(C:\\Program Files\\NGFF-Converter, NGFF-Converter.exe)\n17:11:36 TRACE l.c.Location - isHidden()\n17:11:36 TRACE l.c.Location - Location(C:\\Program Files\\NGFF-Converter, NGFF-Converter.ico)\n17:11:36 TRACE l.c.Location - isHidden()\n17:11:36 TRACE l.c.Location - Location(C:\\Program Files\\NGFF-Converter, runtime)\n17:11:36 TRACE l.c.Location - isHidden()\n17:11:36 TRACE l.c.Location -   returning 4 files\n17:11:36 TRACE l.f.FormatHandler - rotations = 1\n17:11:36 TRACE l.f.FormatHandler - illuminations = 1\n17:11:36 TRACE l.f.FormatHandler - phases = 1\n17:11:36 TRACE l.f.FormatHandler -     assigned plane index = 0; series index = 0; coreIndex = 0\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1341850139 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1341850139 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1814946073 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1814946073 CLOSE\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, image.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 146696414 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 146696414 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1838364287 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1838364287 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2062486028 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2062486028 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2034833761 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 2034833761 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 38434318 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 38434318 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1990101238 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1990101238 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1956860739 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1956860739 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = image.czi, writable = false)\n17:11:36 TRACE l.c.Location - Location.getHandle: image.czi -&gt; loci.common.ByteArrayHandle@60b5a65a\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1158733479 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1158733479 CLOSE\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1460572002 CLOSE\n17:11:36 DEBUG l.c.Location - Location.mapFile: image.czi -&gt; null\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getName()\n\n...\n\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@143b3a0d\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@143b3a0d\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 375445828 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 375445828 CLOSE\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@78b7b980\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@78b7b980\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 991016407 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 991016407 CLOSE\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n\n...\n\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@76370bea\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@76370bea\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1712146065 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1712146065 CLOSE\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 598968425 OPEN\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 598968425 CLOSE\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n\n...\n\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getAbsolutePath()\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - getName()\n17:11:36 DEBUG l.c.Location - Location.mapFile: embedded-stream.raw -&gt; null\n17:11:36 DEBUG l.c.Location - Location.mapFile: embedded-stream.raw -&gt; null\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 43023822 CLOSE\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - lastModified()\n17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\\Testdata_Zeiss\\CZI_Testfiles\\.testwell96_test.czi.bfmemo (8817839 bytes)\n17:11:36 DEBUG l.f.Memoizer - start[1678464696498] time[34] tag[loci.formats.Memoizer.loadMemo]\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@5136de40\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@5136de40\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1634768657 OPEN\n17:11:36 DEBUG l.f.Memoizer - start[1678464696498] time[34] tag[loci.formats.Memoizer.setId]\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - lastModified()\n17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\\Testdata_Zeiss\\CZI_Testfiles\\.testwell96_test.czi.bfmemo (8817839 bytes)\n17:11:36 DEBUG l.f.Memoizer - start[1678464696534] time[33] tag[loci.formats.Memoizer.loadMemo]\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@7d75a378\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@7d75a378\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 773527809 OPEN\n17:11:36 DEBUG l.f.Memoizer - start[1678464696534] time[34] tag[loci.formats.Memoizer.setId]\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - lastModified()\n17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\\Testdata_Zeiss\\CZI_Testfiles\\.testwell96_test.czi.bfmemo (8817839 bytes)\n17:11:36 DEBUG l.f.Memoizer - start[1678464696568] time[32] tag[loci.formats.Memoizer.loadMemo]\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@6e0c015c\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@6e0c015c\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1499170649 OPEN\n17:11:36 DEBUG l.f.Memoizer - start[1678464696568] time[33] tag[loci.formats.Memoizer.setId]\n17:11:36 TRACE l.c.Location - Location(null, F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi)\n17:11:36 TRACE l.c.Location - lastModified()\n17:11:36 DEBUG l.f.Memoizer - loaded memo file: F:\\Testdata_Zeiss\\CZI_Testfiles\\.testwell96_test.czi.bfmemo (8817839 bytes)\n17:11:36 DEBUG l.f.Memoizer - start[1678464696602] time[30] tag[loci.formats.Memoizer.loadMemo]\n17:11:36 TRACE l.c.Location - getHandle(id = F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi, writable = false)\n17:11:36 TRACE l.c.Location - no handle was mapped for this ID\n17:11:36 TRACE l.c.Location - Created new handle F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@32aff37f\n17:11:36 TRACE l.c.Location - Location.getHandle: F:\\Testdata_Zeiss\\CZI_Testfiles\\testwell96_test.czi -&gt; loci.common.NIOFileHandle@32aff37f\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 280616683 OPEN\n17:11:36 DEBUG l.f.Memoizer - start[1678464696601] time[31] tag[loci.formats.Memoizer.setId]\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.DichroicRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.DichroicRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef\n17:11:36 DEBUG o.x.m.ManufacturerSpec - Unable to handle reference of type: class ome.xml.model.EmissionFilterRef\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 773527809 CLOSE\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1499170649 CLOSE\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 280616683 CLOSE\n17:11:36 TRACE l.c.RandomAccessInputStream - RandomAccessInputStream 1634768657 CLOSE\n17:11:36 ERROR c.g.c.ConverterTask - java.lang.IllegalArgumentException: Series 96 not present in metadata!\n17:11:36 INFO  c.g.c.ConverterTask - Failed with Exit Code 1 : testwell96_test.zarr\n\n17:11:36 INFO  c.g.c.ConverterTask - Completed conversion of 0 files.\n</code></pre>", "<p>I did some test with more CZI images and the one above is not the only one that fails. But all of them open just fine within Fiji using the latest BioFormats w/o any complaints about missing metadata. Any help is greatly appreciated.</p>", "<p>Hi <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a>, thanks for the report.</p>\n<p>As you will have seen, I have transferred the issue to the bioformats2raw GitHub repository as this is where the error came from and suggested a couple of options to move forward with the conversion. To avoid tracking too many places, I suggest we use GitHub until the issue feels resolved from your side. If some aspects of the discussion are deemed to be relevant to the wider bioimaging community, we can definitely update this post later on.</p>\n<p>Best,<br>\nSebastien</p>"], "78410": ["<p>Hi all,</p>\n<p>I\u2019m trying to use pylibczirw and some other related packages to read large .czi files. I want to read them as tiles with 1024*1024 shapes. However, I should colorize multi-dimensional images. But if I use the \u2018max\u2019 value of each channel, different tiles will have different colors.<br>\nAlso, I guess a max-min value of each channel and normalize them after I converted the float16 grayscale images into RGB24 images. Then I added the RGB channels and use the guessed max-min value to normalize the merged image. The contrast will become lower than the performance in Zen Lite.<br>\nIs there a method to generate tiled images the same as the performance in Zen Lite?</p>", "<p>Hi <a class=\"mention\" href=\"/u/suica46\">@Suica46</a></p>\n<p>I am not sure I understand the question correctly.</p>\n<ul>\n<li>you read a large CZI using pylibCZIrw</li>\n<li>tile can be read by defining an ROI and in addition tile objects can be created by using the <a href=\"https://pypi.org/project/cztile/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">cztile \u00b7 PyPI</a> library</li>\n<li>the display settings are store inside the metadata of the CZI</li>\n</ul>\n<p>AFAIK in ZEN the display settings are created by checking the whole image. If you \u201ccalculate\u201d your own min&amp;max etc. why should this look exactly as in ZEN lite then?</p>", "<p>Thank you for your replay.</p>\n<p>In my work, I want to show a .czi WSI in my web-based front-end, so I need to cut the WSI into tiles. I know that the Low&amp;High value is stored in the display setting and I also used them. I used opencv and numpy to process the image like the following pseudo code.</p>\n<pre><code class=\"lang-auto\">tiles = []\nfor ch in range(channels):\n    tile = crop_roi[ch]\n    tile = np.clip(tile,low[ch],high[ch])\n    tile = (tile-tile.min())/(tile.max()-tile.min())*255\n    tile = tile.astype(np.uint8)\n    tiles.append(tile)\napply_color_map(tiles)\ncombined_rgb = combine_tiles(tiles)\n</code></pre>\n<p>Using this method, the color of each combined tile will be different. However, I want a WSI with the same color. Maybe I should try to use cztile and matplotlib to process these images.</p>\n<p>Thank you very much.</p>"], "78411": ["<p>I\u2019m currently working on a large image set, where I have 24 counting annotations (24 different sets of points). I enter them all from \u201cCreate points from all classes\u201d and they\u2019re in order of how I created them (which is also how I will count them). As soon as I start counting and placing points, the list re-orders randomly.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e.png\" data-download-href=\"/uploads/short-url/hWGUuAR0IGrokd6DIykxoRTKo7Y.png?dl=1\" title=\"Screenshot 2023-03-10 at 11.29.13 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_239x500.png\" alt=\"Screenshot 2023-03-10 at 11.29.13 AM\" data-base62-sha1=\"hWGUuAR0IGrokd6DIykxoRTKo7Y\" width=\"239\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_239x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_358x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7dc7682c9c584bb14c7c7920eeb39188a038092e_2_478x1000.png 2x\" data-dominant-color=\"EAEAEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-10 at 11.29.13 AM</span><span class=\"informations\">770\u00d71606 136 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10.png\" data-download-href=\"/uploads/short-url/zuEKDOhgTQhfqy0VsyFMsQHvk6k.png?dl=1\" title=\"Screenshot 2023-03-10 at 11.29.40 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_241x499.png\" alt=\"Screenshot 2023-03-10 at 11.29.40 AM\" data-base62-sha1=\"zuEKDOhgTQhfqy0VsyFMsQHvk6k\" width=\"241\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_241x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_361x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8c3423399209fa42468d40109c2accba7659a10_2_482x998.png 2x\" data-dominant-color=\"ECEBEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-10 at 11.29.40 AM</span><span class=\"informations\">776\u00d71608 120 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It\u2019s just super time consuming to have to search through a list of 24 similar names to identify which annotation category I want to use next. Is there any way around this? I can\u2019t figure out a way to re-organize them while counting.</p>\n<p>Thanks,<br>\nClaudia</p>"], "78412": ["<p>I have tried viewing an image on Fiji and the channel tools function just ticks all 3 channels even though I have only cfos and DAPI images. When I set the image type to RGB, it manages to put colours but produces 3 sets of DAPI images with different colours.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c2b0658a0ae757e4783e6ef5194581c2b015c0d.jpeg\" data-download-href=\"/uploads/short-url/aROt0LvbdXmxbyq64S4RV26Grp3.jpeg?dl=1\" title=\"Screenshot 2023-03-10 at 4.32.14 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c2b0658a0ae757e4783e6ef5194581c2b015c0d.jpeg\" alt=\"Screenshot 2023-03-10 at 4.32.14 PM\" data-base62-sha1=\"aROt0LvbdXmxbyq64S4RV26Grp3\" width=\"690\" height=\"392\" data-dominant-color=\"AEAEAE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-10 at 4.32.14 PM</span><span class=\"informations\">1108\u00d7630 64.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d.jpeg\" data-download-href=\"/uploads/short-url/iXsMeKVyLugZAL4EB9puJAHwL93.jpeg?dl=1\" title=\"Screenshot 2023-03-10 at 4.32.43 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_642x500.jpeg\" alt=\"Screenshot 2023-03-10 at 4.32.43 PM\" data-base62-sha1=\"iXsMeKVyLugZAL4EB9puJAHwL93\" width=\"642\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_642x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_963x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/84dfec2ec2283dc902827c2406761128c58b405d_2_1284x1000.jpeg 2x\" data-dominant-color=\"AEAE15\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-10 at 4.32.43 PM</span><span class=\"informations\">1322\u00d71028 147 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/tia_sanders\">@Tia_Sanders</a>,</p>\n<p>Please post the original image so we can reproduce the problem.</p>"], "78409": ["<p>Hi all,</p>\n<p>I am currently using OrientationJ to determine the alignment of cells and matrix in my cell cultures. I use the analysis and distribution funtions in OrientationJ and this works generally quite okay. However, in some of my images I have aspecific spots, and/or larger areas (staining artifacts) which disturb the image analysis. Do you know of a way to remove these from my images, so that I can get more accurate determination of the angles an representation in the analysis HSB function?</p>\n<p>I attached HSB OrientationJ analysis images of the spots (indicated in blue circles). The red lines are the structures that I try to analyze.</p>\n<p>Kind regards,<br>\nJudith<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/922959794b0e5107a9912817f7026042087a1a4a.jpeg\" data-download-href=\"/uploads/short-url/kR0iHkJ3j6fBgliIicECKvSDahY.jpeg?dl=1\" title=\"OriJ_Large.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/922959794b0e5107a9912817f7026042087a1a4a.jpeg\" alt=\"OriJ_Large.PNG\" data-base62-sha1=\"kR0iHkJ3j6fBgliIicECKvSDahY\" width=\"492\" height=\"499\" data-dominant-color=\"332727\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">OriJ_Large.PNG</span><span class=\"informations\">611\u00d7620 53.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/8/68a67a8075882c64c4eac96024bb0db6dda0408d.jpeg\" data-download-href=\"/uploads/short-url/eVMkNIXQtRAZeUEcb51rbIyIbg1.jpeg?dl=1\" title=\"OriJ_Mixed.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/8/68a67a8075882c64c4eac96024bb0db6dda0408d.jpeg\" alt=\"OriJ_Mixed.PNG\" data-base62-sha1=\"eVMkNIXQtRAZeUEcb51rbIyIbg1\" width=\"466\" height=\"500\" data-dominant-color=\"221818\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">OriJ_Mixed.PNG</span><span class=\"informations\">590\u00d7633 42.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "45643": ["<p>Hello,</p>\n<p>I just discovered omero.py and it looks very nice! I wonder if it possible get tiles at other zoom level than the native resolution?</p>\n<p>I am following this guide: <a href=\"https://docs.openmicroscopy.org/omero/5.6.0/developers/Python.html\" rel=\"noopener nofollow ugc\">https://docs.openmicroscopy.org/omero/5.6.0/developers/Python.html</a></p>\n<p>For instance, this works fine:</p>\n<pre><code class=\"lang-auto\">from omero.gateway import BlitzGateway\n\nconn = BlitzGateway(\"username\", \"password\", port=4064, host=\"localhost\")\nconn.connect()\n\nIMAGE_ID = 1234\nimage = conn.getObject(\"Image\", IMAGE_ID)\n\npixels = image.getPrimaryPixels()\ntile = (0, 0, 256, 256)\nim = pixels.getTile(0, 0, 0, tile)\n</code></pre>\n<p>Now I would like to the the same at different zoom level of the pyramid. The target image is an ome.tiff file, tiled and pyramidal. Is it possible?</p>\n<p>Kind regards</p>", "<p>I\u2019m afraid that\u2019s not supported by the <code>PixelsWrapper.getTile()</code>.<br>\nYou\u2019ll need to do a bit more work to use the underlying <code>rawPixelsStore</code>.</p>\n<p>Hopefully this will work:</p>\n<pre><code class=\"lang-auto\">import numpy as np\nfrom omero.gateway import BlitzGateway\nfrom omero.model import enums as omero_enums\n\nconn = BlitzGateway('user', 'password', port=4064, host='localhost')\nconn.connect()\n\nPIXEL_TYPES = {\n    omero_enums.PixelsTypeint8: np.int8,\n    omero_enums.PixelsTypeuint8: np.uint8,\n    omero_enums.PixelsTypeint16: np.int16,\n    omero_enums.PixelsTypeuint16: np.uint16,\n    omero_enums.PixelsTypeint32: np.int32,\n    omero_enums.PixelsTypeuint32: np.uint32,\n    omero_enums.PixelsTypefloat: np.float32,\n    omero_enums.PixelsTypedouble: np.float64,\n}\n\nIMAGE_ID = 2566\nimage = conn.getObject(\"image\", IMAGE_ID)\nprint('image', image.name)\n\npixels = image.getPrimaryPixels()\ndtype = PIXEL_TYPES.get(pixels.getPixelsType().value, None)\n\npix = image._conn.c.sf.createRawPixelsStore()\npid = image.getPixelsId()\nlevel = 0\nz, c, t, x, y = (0, 0, 0, 0, 0)\n\ntry:\n    pix.setPixelsId(pid, False)\n\n    # Get info on number of levels and sizes\n    print([(r.sizeX, r.sizeY) for r in pix.getResolutionDescriptions()])\n\n    pix.setResolutionLevel(2)\n    print(pix.getTileSize())\n    w, h = pix.getTileSize()\n    tile = pix.getTile(z, c, t, x, y, w, h)\nfinally:\n    pix.close()\n\ntile = np.frombuffer(tile, dtype=dtype)\ntile = tile.reshape((h, w))\n\nprint(tile.shape)\n</code></pre>\n<p>Regards,<br>\nWill</p>", "<p>Excellent, thanks a lot!</p>", "<p>Hi <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>,<br>\nDo you have a java version of this code ? <img src=\"https://emoji.discourse-cdn.com/twitter/grimacing.png?v=9\" title=\":grimacing:\" class=\"emoji\" alt=\":grimacing:\"></p>", "<p>Hi Nicolas.<br>\nI tried to come up with an example using the Java gateway. But unfortunately it doesn\u2019t look like there would be a practical way to do this at the moment. I openend an issue: <a href=\"https://github.com/ome/omero-gateway-java/issues/48\">Allow to set resolution level for retrieving planes of pyramidal images</a> and will do some more investigation.<br>\nRetrieving pixel values is easy using the gateway, e. g. <a href=\"https://docs.openmicroscopy.org/omero/5.6.3/developers/Java.html#raw-data-access\">OMERO Java language bindings</a>, but you can\u2019t do that at the moment for a specific zoom level.<br>\nKind Regards,<br>\nDominik</p>", "<p>That\u2019s unfortunate <img src=\"https://emoji.discourse-cdn.com/twitter/cry.png?v=9\" title=\":cry:\" class=\"emoji\" alt=\":cry:\">! Thanks for your effort. I (and particularly <a class=\"mention\" href=\"/u/stoffelc\">@stoffelc</a>) are really really interested into this, we already managed to get the full resolution tiles (that\u2019s easy indeed), but what we need now are the pyramid levels.</p>", "<p>Sorry, I have to correct myself, it is actually possible pretty much the same way. I just got a bit lost in Insight / Gateway code earlier\u2026 Here\u2019s the nearly same in Java:</p>\n<pre><code class=\"lang-auto\">        LoginCredentials lc = new LoginCredentials(\"user\", \"password\", \"some_hostname\");\n        Gateway gw = new Gateway(new SimpleLogger());\n        ExperimenterData e = gw.connect(lc);\n        SecurityContext ctx = new SecurityContext(e.getGroupId());\n\n        long imageId = 12345;\n        ImageData img = gw.getFacility(BrowseFacility.class).getImage(ctx, imageId);\n        System.out.println(img.getDefaultPixels().getPixelType());\n\n        RawPixelsStorePrx pix = gw.getPixelsStore(ctx);\n        pix.setPixelsId(img.getDefaultPixels().getId(), False);\n        for (ResolutionDescription desc: pix.getResolutionDescriptions()) {\n            System.out.println(desc.sizeX);\n            System.out.println(desc.sizeY);\n        }\n\n        pix.setResolutionLevel(2);\n        System.out.println(pix.getTileSize());\n        byte[] tile = pix.getTile(0, 0, 0, 0, 0, 100, 100);\n</code></pre>\n<p>How to read the byte array is unfortunately I bit trickier than in Python. You can have a look at the <a href=\"https://github.com/ome/omero-gateway-java/search?q=BytesConverter\">BytesConverter</a> Classes in the Gateway.</p>\n<p>But I\u2019ll leave the github issue open. Would be much nicer to have that functionality available in the gateway instead of using the RawPixelstore directly.</p>\n<p>Regards,<br>\nDominik</p>", "<p>Thanks <a class=\"mention\" href=\"/u/dominikl\">@dominikl</a> !</p>\n<p>Just one more question : how do we get the size in Z of the different resolution levels ?<br>\nOr are the number of z plane identical for all resolution levels ?</p>\n<p>I have a problem because <code>ResolutionDescription desc</code> has no <code>sizeZ</code> field, and I can\u2019t find a <code>getSizeZ</code>method for the raw pixel store.</p>", "<p>Hi, the number of Z planes is the same for all resolution levels.<br>\nRegards,<br>\nWill</p>", "<aside class=\"quote no-group\" data-username=\"will-moore\" data-post=\"2\" data-topic=\"45643\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/will-moore/40/17250_2.png\" class=\"avatar\"> will-moore:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">import numpy as np\nfrom omero.gateway import BlitzGateway\nfrom omero.model import enums as omero_enums\n\nconn = BlitzGateway('user', 'password', port=4064, host='localhost')\nconn.connect()\n\nPIXEL_TYPES = {\n    omero_enums.PixelsTypeint8: np.int8,\n    omero_enums.PixelsTypeuint8: np.uint8,\n    omero_enums.PixelsTypeint16: np.int16,\n    omero_enums.PixelsTypeuint16: np.uint16,\n    omero_enums.PixelsTypeint32: np.int32,\n    omero_enums.PixelsTypeuint32: np.uint32,\n    omero_enums.PixelsTypefloat: np.float32,\n    omero_enums.PixelsTypedouble: np.float64,\n}\n\nIMAGE_ID = 2566\nimage = conn.getObject(\"image\", IMAGE_ID)\nprint('image', image.name)\n\npixels = image.getPrimaryPixels()\ndtype = PIXEL_TYPES.get(pixels.getPixelsType().value, None)\n\npix = image._conn.c.sf.createRawPixelsStore()\npid = image.getPixelsId()\nlevel = 0\nz, c, t, x, y = (0, 0, 0, 0, 0)\n\ntry:\n    pix.setPixelsId(pid, False)\n\n    # Get info on number of levels and sizes\n    print([(r.sizeX, r.sizeY) for r in pix.getResolutionDescriptions()])\n\n    pix.setResolutionLevel(2)\n    print(pix.getTileSize())\n    w, h = pix.getTileSize()\n    tile = pix.getTile(z, c, t, x, y, w, h)\nfinally:\n    pix.close()\n\ntile = np.frombuffer(tile, dtype=dtype)\ntile = tile.reshape((h, w))\n\nprint(tile.shape)\n</code></pre>\n</blockquote>\n</aside>\n<p>Thank you <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>, this is very helpful. One hiccup I had was that I\u2019m on a little-endian machine (Windows 10, thanks to miniconda!!), therefore I had to specify the <code>byteorder</code> when converting <code>tile</code> bytes to a <code>numpy</code> array</p>\n<pre><code class=\"lang-python\">tile = np.frombuffer(tile, dtype=np.dtype(dtype).newbyteorder('&gt;'))\n</code></pre>\n<p>Is there a programmatic way to know the endian-ness of an image on the omero server using <code>omero-py</code>? Or is it always big-endian after importing? It looks like being assumed as big-endian when decoding in <a href=\"https://github.com/ome/omero-py/blob/e99bcbbdfdbfffdff9beaa954b5bb2143f23effa/src/omero/gateway/__init__.py#L7536-L7539\" rel=\"noopener nofollow ugc\">the following lines in omero-py</a></p>\n<pre><code class=\"lang-python\">                convertType = '&gt;%d%s' % (\n                    (planeY*planeX), pixelTypes[pixelType][0])\n                if isinstance(rawPlane, bytes):\n                    convertedPlane = unpack(convertType, rawPlane)\n</code></pre>\n<p>Thanks!<br>\nYu-An</p>", "<p>Hi <a class=\"mention\" href=\"/u/yu-anchen\">@Yu-AnChen</a>,</p>\n<p>answering your last question, your interpretation of the OMERO.py code is correct. The endianness of the original imaging data is format-specific and preserved by the Bio-Formats API. The OMERO API includes an API, <a href=\"https://docs.openmicroscopy.org/omero-romio/5.6.2/javadoc/ome/io/bioformats/BfPixelsWrapper.html#swapIfRequired-byte:A-\">BfPixelsWrapper.swapIfRequired</a>, which conditionally swaps the bytes to produce an big-endian byte array and is consumed in various places e.g. <a href=\"https://github.com/ome/omero-romio/blob/8e53730d44244952d6e93ac9ab8222e85db6c189/src/main/java/ome/io/bioformats/BfPixelBuffer.java#L518-L519\" class=\"inline-onebox\">omero-romio/BfPixelBuffer.java at 8e53730d44244952d6e93ac9ab8222e85db6c189 \u00b7 ome/omero-romio \u00b7 GitHub</a>.</p>\n<p>This means the plane/tiles returned by OMERO should  be expected to be ordered in a big-endian order.</p>", "<p>Thank you <a class=\"mention\" href=\"/u/s.besson\">@s.besson</a> for the clarification, this is very helpful <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>,</p>\n<p>Thank you for your helpful comment in this thread.<br>\n(<a href=\"https://forum.image.sc/t/omero-py-how-to-get-tiles-at-different-zoom-level-pyramidal-image/45643/2\" class=\"inline-onebox\">OMERO.py: How to get tiles at different zoom level (pyramidal image)? - #2 by will-moore</a>)</p>\n<p>We have a question about the relation of the level in <code>get/setResolutionLevel()</code> compared to <code>getResolutionDescriptions()</code>.</p>\n<p>In our case we find images where these seem reversed: <code>getResolutionDescriptions</code> going from largest to smallest pyramid size (expected) and <code>get/setResolutionLevel</code> connecting to the inverse level, smallest size first. We\u2019ve validated this by checking the default <code>getResolutionLevel</code>, and <code>getTileSize</code> to detect the smallest pyramid size image.<br>\nIs this expected behaviour?</p>\n<p>Version info:<br>\nOmero python package: 5.13.1<br>\nOMERO Plus OMERO.server-5.6.5-86-gs-ice36</p>", "<p>Hi <a class=\"mention\" href=\"/u/folterj\">@folterj</a> - apologies for delayed response\u2026</p>\n<p>I\u2019m afraid that appears to be the way that the API works, even though this may not be the expected behaviour - and I see that the documentation at <a href=\"https://docs.openmicroscopy.org/omero-blitz/5.5.12/slice2html/omero/api/PyramidService.html#setResolutionLevel\" class=\"inline-onebox\">OMERO API Index - omero::api::PyramidService</a> doesn\u2019t give any clues about how this works.</p>\n<p>Another example of this usage, where I\u2019m handling that \u201cinverse\u201d relationship is at <a href=\"https://github.com/ome/omero-figure/blob/13ccea055a45452f1c02272682dd75b142b1d41e/omero_figure/views.py#L207\" class=\"inline-onebox\">omero-figure/views.py at 13ccea055a45452f1c02272682dd75b142b1d41e \u00b7 ome/omero-figure \u00b7 GitHub</a></p>", "<p>Thank you for the info <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a> ,</p>\n<p>Though sub-optimal, we also implemented a work-around, depending on the initial value returned by <code>getResolutionLevel()</code> which seems to correspong to the largest pyramid size.</p>", "<p>Thanks Will.</p>\n<p>Yeah, did some checking, it seems that one needs to reverse the list return from getResolutionDescriptions()</p>\n<pre><code class=\"lang-auto\">lst_resol = [(r.sizeX, r.sizeY) for r in pixels_store.getResolutionDescriptions()]\n</code></pre>\n<pre><code class=\"lang-python\">len(lst_resol)\n</code></pre>\n<pre><code>7\n</code></pre>\n<pre><code class=\"lang-python\">lst_resol.reverse()\nprint(lst_resol)\n</code></pre>\n<pre><code>[(538, 520), (1077, 1041), (2155, 2083), (4310, 4166), (8621, 8333), (17243, 16667), (34486, 33335)]\n</code></pre>\n<p>``python<br>\npixels_store.setResolutionLevel(3)<br>\nprint(\u201cTile size:\u201d, pixels_store.getTileSize())<br>\nresol_lvl = pixels_store.getResolutionLevel()<br>\nprint(\u201cresol_lvl:\u201d, resol_lvl, \u201cresolution:\u201d, lst_resol[resol_lvl])<br>\nplane = pixels_store.getPlane(0,0,0)<br>\nprint(\u201cplane size:\u201d, len(plane))<br>\nprint(\u201cplane sizeY:\u201d, len(plane)/lst_resol[resol_lvl][0])</p>\n<pre><code class=\"lang-auto\">    Tile size: [1024, 1024]\n    resol_lvl: 3 resolution: (4310, 4166)\n    plane size: 17955460\n    plane sizeY: 4166.0</code></pre>", "<p>Thanks <a class=\"mention\" href=\"/u/folterj\">@folterj</a> <a class=\"mention\" href=\"/u/ken.ho\">@ken.ho</a> , created an issue <a href=\"https://github.com/ome/omero-py/issues/366\" class=\"inline-onebox\">getResolutionDescriptions pyramids order \u00b7 Issue #366 \u00b7 ome/omero-py \u00b7 GitHub</a> for the problem</p>"], "78415": ["<p>Does anyone have any experience setting up Cellprofiler pipelines to use in Orbit? I am trying to analyze intensity of a slide with 3 fluorescence channels. I\u2019m not entirely sure what I am doing wrong, as I am not even getting errors. I have followed the documentation on Orbit\u2019s website. My measurements should output to a csv file, and it never appears. The tiles are created by Orbit and 6 folders are created with various sections of the ROI. Alternatively, does anyone have experience processing wholes slides using Cellprofiler? Thanks in advance!</p>", "<p>Dear <a class=\"mention\" href=\"/u/hroberts49\">@HRoberts49</a> ,<br>\nplease make sure that you selected the cp features according to the orbit documentation and use cp 2.x.x .<br>\nI recommend to start with the cp pipeline provided at the orbit doc.</p>\n<p>Regards,<br>\nManuel</p>"], "55888": ["<p>##<span class=\"hashtag\">#Hi</span> everyone,</p>\n<p>I\u2019m relatively new to using MuscleJ 1 0 2 plugin for fiber diameter and other morphological analysis.  When I try to run an image I get the error message \u201cno window with the title Results\u201d found (screenshot attached).  I have the image saved as a TIF, named properly, an input/output folder set up, the image has no artifact and was processed well, but I cannot get the image to analyze.  A co-worker who frequently uses MuscleJ sent me an image of his to analyze and it worked with no issues, so I\u2019m not sure why my image won\u2019t analyze.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50f0cd7e865f89303f7182f8307b9cd25d435c3e.png\" alt=\"Screen Shot 2021-07-30 at 10.00.41 AM\" data-base62-sha1=\"by278jCpBy4kOw0PsuXzEe9dRLw\" width=\"646\" height=\"268\"></p>\n<p>I\u2019m not familiar with coding so unfortunately I can\u2019t try to troubleshoot by trying a different code.  If someone could please tell me how to troubleshoot this issue, that would be awesome!  Thank you.</p>", "<p>Hey. Did you figure out a solution to this error? I\u2019m also getting it and can\u2019t figure out why (previously worked for me in the past)</p>", "<p>Hi! Did someone find out how it could be solved? I have the same problem.</p>"], "78418": ["<p>I wanted to merge two stacks, one is the red channel and the other is the result of the Z-stack depth color code plugin, meaning that it\u2019s color coded according to the depth. How can I do this without losing the colors? the idea is then visualize it in 3D viewer.</p>\n<p>I tried merge both stacks in Image&gt;Color&gt;Merge Channels (Which is not ideal because I have to chose a color for the color coded stack). First, the program shows this warning saying that both channels need to have the same bit-depth. After I subjected the red channel to the same process, Z-stack depth colo, it accepts the merging but when it does it merges only one z plane.</p>"], "78424": ["<p>Greetings,<br>\nI am trying to calculate mean, max, min and Sd numbers of distance between these two lines in an OCT image. Is there any method to help me calculate the thickness of this layer?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2.jpeg\" data-download-href=\"/uploads/short-url/w3KM4Nr4Otsi0XgBc0BhGhtBowq.jpeg?dl=1\" title=\"Retinaa\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_690x301.jpeg\" alt=\"Retinaa\" data-base62-sha1=\"w3KM4Nr4Otsi0XgBc0BhGhtBowq\" width=\"690\" height=\"301\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0b236d2b438af7292f9ae076c13269d6b3648c2.jpeg 2x\" data-dominant-color=\"4C4A4A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Retinaa</span><span class=\"informations\">1162\u00d7508 76.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>A new <a href=\"https://forum.image.sc/t/imagej-macro-to-measure-distance-between-two-lines-edges/42019\">macro</a> came out recently that should do as you ask. Scroll to near the bottom to download the latest version of the macro and the instructions.</p>", "<p>Hi <a class=\"mention\" href=\"/u/demehdix\">@Demehdix</a><br>\nHave you checked out this theme?</p><aside class=\"quote quote-modified\" data-post=\"3\" data-topic=\"78386\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/5daacb/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/measuring-the-distance-between-two-varying-contours/78386/3\">Measuring the distance between two varying contours</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hi <a class=\"mention\" href=\"/u/carsten2023\">@Carsten2023</a> \nPlease provide feedback. \nThanks in advance. \nHere is the result of using the macro below. \n <a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/0/f07115a9bc61a9d46ce52a10473cc8e3af5e17ed.png\" data-download-href=\"/uploads/short-url/yj2ShLVEtmbUrFX5M0CjL9vuSsJ.png?dl=1\" title=\"Image captur\u00e9e-10-03-2023 19-29-24\" rel=\"noopener nofollow ugc\">[Image captur\u00e9e-10-03-2023 19-29-24]</a> \nmacro \"distance between contours\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\nimg=getImageID();\n//---------------------------\n// Start batch mode\nsetBatchMode(true);\nselectImage(img);\nrun(\"Duplicate...\", \"title=1\");\nclose(\"\\\\Others\")\n//-------------------------------\n// Start image processing\nh=getHeight();\nw=ge\u2026\n  </blockquote>\n</aside>\n<p>\nSuch an approach is possible, if you define how to measure the thickness. That is to say: how should the segments be?<br>\nYou have a nice discussion here on the definition of thickness measurements.</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"9883\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/joost/40/57507_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/determine-average-width-of-irregular-line/9883\">Determine average width of irregular line</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hey there! \nI have two (newbie) questions about ImageJ and analysis/math in general (if this is the right place to ask). \nFor analysis of some of my data I need to obtain the average width of lines of my extruded material. Now I could just measure the width at several points and take the average, but I was wondering if it is possible to select the whole area and then calculate the average width of the entire line. I have tried changing the brightness and threshold to be able to automatically sel\u2026\n  </blockquote>\n</aside>\n", "<p>Beside the previous suggestions have also a look at this simple solution if it fits your purpose:</p>\n<aside class=\"quote\" data-post=\"1\" data-topic=\"26065\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/3ab097/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/distance-between-polygons/26065\">Distance between polygons</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hello, \nI would like to measure the distances between the two membranes of bacteria. \nI can do it manually but this is fastidious as it results in 400-500 measures per bacterium. \nI figured that I can easily delimits the membranes with the polygon function but I cannot find a way to measure distances between the two polygons at defined intervals. \nSorry if it is obvious, but I am only starting to learn how to use ImageJ \nThank you for your help, \nPierre\n  </blockquote>\n</aside>\n", "<p>\u2026if you like python:</p>\n<pre><code class=\"lang-python\">from vedo import *\nfrom vedo.applications import SplinePlotter\n\npic = Picture(\"your_image.jpeg\")\n\nplt1 = SplinePlotter(pic, size=(1000,500))\nplt1.lcolor = 'yellow'\nplt1.show(pic, \"draw one line\", zoom=2, mode='image')\nplt1.close()\nline1 = plt1.line\n\nplt2 = SplinePlotter(pic, size=(1000,500))\nplt2.lcolor = 'light blue'\nplt2.show(pic, \"now draw the other line\", zoom=2, mode='image')\nplt2.close()\nline2 = plt2.line\n\nd = line1.distance_to(line2)\nprint(\"min, mean, max =\", np.min(d), np.mean(d), np.max(d))\n\n# show the result\nplt = Plotter(size=(1000,500))\nshow(pic, line1, line2, axes=1, zoom=2)\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097.png\" data-download-href=\"/uploads/short-url/cZ5hJXfcwlFwmSeDH50LEIRTT4H.png?dl=1\" title=\"Screenshot from 2023-03-12 21-48-36\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097_2_690x310.png\" alt=\"Screenshot from 2023-03-12 21-48-36\" data-base62-sha1=\"cZ5hJXfcwlFwmSeDH50LEIRTT4H\" width=\"690\" height=\"310\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097_2_690x310.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b01ca649bf9215d8dba4576b9c49a6107c6d097.png 2x\" data-dominant-color=\"6D6E6D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-12 21-48-36</span><span class=\"informations\">807\u00d7363 164 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-plaintext\">min, mean, max = 133.05594 168.6623 203.39598\n</code></pre>", "<p><a class=\"mention\" href=\"/u/mmusy\">@mmusy</a> Hello,<br>\nThanks for your help. I tried to use this code in python, but my problem is that I can not draw a line manually. It perfectly shows me two pictures, but I can\u2019t draw lines.<br>\nIt would be great if you could help me.</p>\n<p>Thanks</p>", "<p><a class=\"mention\" href=\"/u/mathew\">@Mathew</a> <a class=\"mention\" href=\"/u/mmusy\">@mmusy</a> My main goal is to measure the retinal thickness manually, by drawing lines manually and then get the max, min, and mean. Pardon me if I seem to be very noobie since I am pretty new to image processing subject and using ImageJ or other programs.<br>\nSo retina has 11 layers and I need to measure each layer separately:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10.png\" data-download-href=\"/uploads/short-url/zTtiiG8AfvaBt8hTHYl1vU7iyTm.png?dl=1\" title=\"Segmented\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10_2_690x270.png\" alt=\"Segmented\" data-base62-sha1=\"zTtiiG8AfvaBt8hTHYl1vU7iyTm\" width=\"690\" height=\"270\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10_2_690x270.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb915e477827c28a9459e833c08c1deab0972b10.png 2x\" data-dominant-color=\"4F5255\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Segmented</span><span class=\"informations\">850\u00d7333 196 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nand this is an unannotated version of the image:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7AQ6ZLmR8ySskVI7uMBSaX3vUfq.tif\">Retina.tif</a> (976.0 KB)</p>\n<p>Thank you very much,</p>\n<p>Mehdi</p>", "<p>Hi, by \u201cdrawing\u201d I meant \u201cclicking points\u201d which are then connected by a spline (right-clicking removes last point, pressing <code>c</code> clears all points).<br>\nOnce you are done with one line you press <code>q</code> to go to the next\u2026<br>\nYou can also save your clicked points to file for later analysis with eg:</p>\n<pre><code class=\"lang-python\">np.save(\"verhoeff_membrane.npy\", line1.points())\n</code></pre>\n<p>let me know if it\u2019s not clear.</p>", "<p><a class=\"mention\" href=\"/u/mmusy\">@mmusy</a> Thank you for the quick reply, actually, by clicking on pictures, no points show up. I think I might run the code incorrectly. just to clarify what I mean, I recorded my screen:<br>\n          <iframe class=\"vimeo-onebox\" src=\"https://player.vimeo.com/video/807347893?h=75eecd8b7c&amp;app_id=122963\" data-original-href=\"https://vimeo.com/807347893\" frameborder=\"0\" allowfullscreen=\"\" seamless=\"seamless\" sandbox=\"allow-same-origin allow-scripts allow-forms allow-popups allow-popups-to-escape-sandbox allow-presentation\"></iframe>\n<br>\nThank you for your patience <img src=\"https://emoji.discourse-cdn.com/twitter/rose.png?v=12\" title=\":rose:\" class=\"emoji\" alt=\":rose:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>oh I see! No, you need to run the script from a terminal shell, not in a ipython/notebook environment (in that case you can only get a static snapshot image with no interactivity).<br>\nAn independent window must pop up.</p>\n<hr>\n<p>An other option is to add</p>\n<pre><code class=\"lang-auto\">settings.default_backend = \"vtk\"\n</code></pre>"], "70233": ["<p>Hi there,<br>\nI am running macros in fiji to batch process folders of images.<br>\nWhat is happening is that the processing is slowing down massively overtime. On further reflection, it seems to be oscillating between fast and slow.  ie there is a burst of activity and then it hangs for an extended period.<br>\nI am running ImageJ 1.53q on windows 11 (32 Core Processor;128GB ram). There is no buildup of RAM.<br>\nAs an example at the start it is producing 26 (7 image) stacks per min.<br>\nAfter producing ~1600 stacks, the speed has slowed  the last 3 stacks have taken 3min, 8 min and 25min. It is still chugging away on the last images.<br>\nCan anyone explain the behaviour?  Is there a solution.</p>\n<p>I am using a recursive macro, to process multiple folders, but even if I do it on a per folder basis I still observe this behavior.</p>\n<p>The macro in question is</p>\n<pre><code class=\"lang-auto\"> macro \"Recursive_ConvertPhenixData_to_stack\" {\n\trequires(\"1.53k\"); \n\t//Improvments to test Save as OME-TIFF to see if METAdata is preserved\n\tdir = getDirectory(\"Choose a Directory\");\n\tsetBatchMode(true);\n\tcount = 0;\n\tfindFiles(dir);\n\trun(\"Collect Garbage\");\n\tn = 0;\n\t// processFiles(dir);\n\tprint(count+\" files processed\");\n\t   \n\tfunction findFiles(dir) {\n\t\tlist = getFileList(dir);\n\t\tfor (j=0; j&lt;list.length; j++) {\n\t\t\tif(File.isDirectory(dir+list[j])){\n\t\t\t\tif (endsWith(list[j],\"Images\"+\"/\")){\n\t\t\t\t\timagelist=getFileList(dir+list[j]);\n\t\t\t\t\tdest=dir+File.separator+\"processed\";\n\t\t\t\t\tif(File.exists(dest)==0){\n\t\t\t\t\t\tprint(\"No dest\");\n\t\t\t\t\t\tFile.makeDirectory(dest);\n\t\t\t\t\t}\n\t\t\t\t\tprocessImages(dir+list[j]);\n\t\t\t\t\t//print(dir+list[j]);    \n\t\t\t\t\tcount++;  \t\t\n\t\t\t\t}      \t\t\n\t\t\t\telse{\n\t\t\t\t\tif (endsWith(list[j],\"/\")){\n\t\t\t\t\t\t//print (list[j]);\n\t\t\t\t\t\tprint (\"no images found moving to next layer\");\n\t\t\t\t\t\tfindFiles(\"\"+dir+list[j]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t}\n\t\t}\n\t}//end function findFiles\n\tfunction processImages(dir) {\n\t\tlist=getFileList(dir);\n\t\tmax_planes=0;\n\t\tmax_channels=0;\n\t\tmax_fields=0;\n\t\tmax_images=0;\n\t\trowlist=newArray();\n\t\tcollist=newArray();\n\n\n\n\t\tfor (i = 0; i &lt; list.length; i++) {\n\t\t\tif (endsWith(list[i], \".tiff\")) {\n\t\t\t\tname=list[i];\n\t\t\t\trow =parseInt(substring(name, indexOf(name, \"r\")+1, indexOf(name, \"c\")));\n\t\t\t\t//print(\"row = \"+ row);\n\t\t\t\tcol= parseInt(substring(name, indexOf(name, \"c\")+1, indexOf(name, \"f\")));\n\t\t\t\t//print(\"col = \"+ col);\n\t\t\t\tfield= parseInt(substring(name, indexOf(name, \"f\")+1, indexOf(name, \"p\")));\n\t\t\t\t//print(\"field = \"+ field);\n\t\t\t\tplane=parseInt(substring(name, indexOf(name, \"p\")+1, indexOf(name, \"-\")));\n\t\t\t\t//print(\"plane = \"+ plane);\n\t\t\t\tchan=parseInt(substring(name, indexOf(name, \"ch\")+2, indexOf(name, \"sk\")));\n\t\t\t\t//print(\"channel = \"+ chan);\n\t\t\t\tif(max_planes&lt;plane){\n\t\t\t\t\tmax_planes=plane;\n\t\t\t\t}\n\t\t\t\tif(max_channels&lt;chan){\n\t\t\t\t\tmax_channels=chan;\n\t\t\t\t}\n\t\t\t\tif(max_fields&lt;field){\n\t\t\t\t\tmax_fields=field;\n\t\t\t\t}\n\t\t\t\tmax_images++;\n\t\t\t\tif(rowlist.length&lt;1){\n\t\t\t\t\trowlist[0]=row;\n\t\t\t\t}else {\n\t\t\t\t\trowmatch=0;\n\t\t\t\t\tfor(r=0; r&lt;rowlist.length; r++){\n\t\t\t\t\t\tif(rowlist[r]==row){\n\t\t\t\t\t\t\trowmatch++;\n\t\t\t\t\t\t\t//print(\"rowmatch\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t}\n\t\t\t\t\tif(rowmatch==0){\n\t\t\t\t\t\trowlist[rowlist.length]=row;\n\t\t\t\t\t}\n\t\t\t\t}\t\n\t\t\t\t//create colist\n\t\t\t\tif(collist.length&lt;1){\n\t\t\t\t\tcollist[0]=row;\n\t\t\t\t}else {\n\t\t\t\t\tcolmatch=0;\n\t\t\t\t\tfor(c=0; c&lt;collist.length; c++){\n\t\t\t\t\t\tif(collist[c]==col){\n\t\t\t\t\t\t\tcolmatch++;\n\t\t\t\t\t\t\t//print(\"colmatch = \");\n\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t}\n\t\t\t\t\tif(colmatch==0){\n\t\t\t\t\t\tcollist[collist.length]=col;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t}\n\t\t//getMetaData from XML\n\t\trun(\"Bio-Formats Macro Extensions\");\n\t\t//dir=getDirectory(\"choose\");\n\t\tlist = getFileList(dir);\n\t\tfor (i = 0; i &lt; list.length; i++) {\n\t\t\tif(endsWith(list[i],\".xml\")){\n\t\t\t\tname=list[i];\n\t\t\t\t//print(name);\n\t\t\t\tExt.setId(dir+list[i]);\n\t\t\t\tExt.getSeriesCount(seriesNum);\n\t\t\t\t//print(\"Number of Series is \"+seriesNum);\n\t\t\t\tExt.getMetadataValue(\"Plate name\",ID);\n\t\t\t\t//print(\"Plate ID is \"+ID);\n\t\t\t\tExt.getPixelsPhysicalSizeX(sizeX);\n\t\t\t\t//\tprint(\"Size:\"+sizeX);\n\t\t\t}\n\t\t}\n\t\tprint(\"Plate ID is \"+ID);\n\t\tprint(\"Size (pixels/um):\"+sizeX);\n\t\tprint (max_channels+\" Channels detected\");\n\t\tprint (max_planes+\" planes detected\");\n\t\tprint (max_fields+\" fields detected\");\n\t\tprint(max_images+ \" total images\");\n\t\tImage_Sets=max_images/max_channels/max_planes;\n\t\tprint(max_images/max_channels/max_planes+\" Image sets\");\n\t\tprint(Image_Sets/max_fields+\" Wells detected\");\n\t\tprint(\"Rows detected = \");\n\t\tArray.print(rowlist);\n\t\tprint(\"Columns detected = \");\n\t\tArray.print(collist);\n\t\t//print(\"_____________________\");\n\t\t\n\t\t//for (i = 0; i &lt;= Image_Sets; i++) {//prob not needed\n\t\tcount=0;\t//this counts wells\n\t\timagesetcount=0; //this counts imagesets\n\t\t\tfor (r = 0; r &lt; rowlist.length; r++) {\n\t\t\t\t\t//r=10;\n\t\t\t\t\trow=rowlist[r];\n\t\t\t\tif(row&lt;10){\n\t\t\t\t\trow=\"r0\"+row;\n\t\t\t\t}else{\n\t\t\t\t\trow=\"r\"+row;\n\t\t\t\t}\n\t\t\t\tprint(\"row=\"+row);\n\t\t\t\tfor (c = 0; c &lt;collist.length; c++) {\n\t\t\t\t\t\tcol=collist[c];\n\t\t\t\t\tif(col&lt;10){\n\t\t\t\t\t\tcol=\"c0\"+col;\n\t\t\t\t\t}else{\n\t\t\t\t\t\tcol=\"c\"+col;\n\t\t\t\t\t}\n\t\t\t\t\tcount++;\n\t\t\t\t\tfor (f = 1; f &lt;= max_fields; f++) {\n\t\t\t\t\t\t//print(\"f=\"+f);\n\t\t\t\t\t\tif(f&lt;10){\n\t\t\t\t\t\tfield=\"f0\"+f;\n\t\t\t\t\t\t\n\t\t\t\t\t\t}else{\n\t\t\t\t\t\t\tfield=\"f\"+f;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfor (p = 1; p &lt;= max_planes; p++) {\n\t\t\t\t\t\t\t//print(\"p=\"+p);\n\t\t\t\t\t\t\tif(p&lt;10){\n\t\t\t\t\t\t\tplane=\"p0\"+p;\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t}else{\n\t\t\t\t\t\t\t\tplane=\"p\"+p;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t//print(row+col+field); //This is the UID for an image\n\t\t\t\t\t\t\tFile.openSequence(dir, \" filter=(\"+row+col+field+plane+\"-ch[0-9]sk1fk1fl1)\");\n\t\t\t\t\t\t\trun(\"Stack to Hyperstack...\", \"order=xyczt(default) channels=\"+max_channels+\" slices=\"+max_planes+\" frames=1 display=Color\");\n\t\t\t\t\t\t\trename(row+col+field+\"_Well\"+count);\n\t\t\t\t\t\t\t//save(path);\n\t\t\t\t\t\t\trun(\"Set Scale...\", \"distance=1 known=\"+sizeX+\" pixel=1 unit=micron\");\n\t\t\t\t\t\t\tsetMetadata(\"Info\", \"Plate name: \"+ID);\n\t\t\t\t\t\t\tsaveAs(\"tif\", dest+File.separator+getTitle());\n\t\t\t\t\t\t\tclose(\"*\");\n\t\t\t\t\t\t\timagesetcount++;\n\t\t\t\t\t\t\tshowProgress(imagesetcount, Image_Sets);\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t//waitForUser;\n\t\t\t\t\t\t\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t}\n\n\t\t\t}\n\t\tprint(imagesetcount+\" images processed\");\n\n\t}//end function process images\n\n }\n</code></pre>", "<p><a class=\"mention\" href=\"/u/jimmib_78\">@jimmib_78</a><br>\nDo you have an area which you think might be causing a problem? Does the program actually have an issue with processing the files, or is it other processes that are slow. I would recoomend timing certain sections of code with the <a href=\"https://imagej.nih.gov/ij/macros/GetDateAndTime.txt\" rel=\"noopener nofollow ugc\">timing</a> function to check how long the take.<br>\nPersonally, if you\u2019re program is slowing down over time, I would suspect functions which check the files in folders, such as <code>getFileList()</code> to be the culprit.</p>", "<p>Hi Christian,<br>\nThanks for your response.<br>\nThe macro is just opening a set of files as a series and then saving these.<br>\nThe issue seems to be actually reading and opening the images in a series.<br>\nI actually wonder if this is an issue with the PC (eg disk drive problem or something).  I will try on another system to compare.</p>", "<p>Hi all,<br>\nI have tested this across multiple systems now and across multiple file types.  I only seem to encounter the issue when using bioformats.<br>\nJ</p>", "<p>I am experiencing the same issue using a macro to split .lif files into individual .tiff files and running MIPs on them. Were you able to figure out a solution?</p>", "<p>No Sorry Marcos.</p>", "<p>hi James,<br>\nI found out that in my case opening all series at once and then processing each series individually is orders of magnitude faster than opening each one of the series individually in sequence. It went from taking a couple of mins processing a single series to processing several series/second) The only limitation is that you need to run the macro in a system with enough free RAM, which doesn\u2019t seems to be an issue in your system.</p>", "<p>Hey Marcos,<br>\nVery clever.  Can you share your code?<br>\nCheers,<br>\nJAmes</p>"], "78426": ["<p>Hi I have segmentation labels I created with cellpose and labels a tissue expert created.</p>\n<p>What would be a quick and dirty way to score the accuracy of the machine generated labels?</p>\n<p>Something like b) in this figure from the cellpose paper:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441.webp\" data-download-href=\"/uploads/short-url/spOh1Z3gG2o5wbDlXH8HKkrl1fj.webp?dl=1\" title=\"\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441_2_645x499.webp\" alt=\"\" data-base62-sha1=\"spOh1Z3gG2o5wbDlXH8HKkrl1fj\" role=\"presentation\" width=\"645\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441_2_645x499.webp, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441.webp 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c727cb3ea67dfd650037c9a858c75f64c2bf1441.webp 2x\" data-dominant-color=\"CBD3CF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\"></span><span class=\"informations\">685\u00d7531 90.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThank you!</p>\n<p>EDIT: Found this <a href=\"https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics/notebook\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">All the segmentation metrics! | Kaggle</a> which contains exactly what I want</p>", "<p>Hi <a class=\"mention\" href=\"/u/zeratoss\">@zeratoss</a>,</p>\n<p>General info and overview:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://arxiv.org/abs/2206.01653\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bac99e0da013a3e1e7e363ea82e1aff26e8bf11b.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://arxiv.org/abs/2206.01653\" target=\"_blank\" rel=\"noopener\">arXiv.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_500x500.png\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_1000x1000.png 2x\" data-dominant-color=\"865F5C\">\n\n<h3><a href=\"https://arxiv.org/abs/2206.01653\" target=\"_blank\" rel=\"noopener\">Metrics reloaded: Pitfalls and recommendations for image analysis validation</a></h3>\n\n  <p>Increasing evidence shows that flaws in machine learning (ML) algorithm\nvalidation are an underestimated global problem. Particularly in automatic\nbiomedical image analysis, chosen performance metrics often do not reflect the\ndomain interest, thus...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>An IJ script (disclaimer: I have not tested it):</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://imagej.net/tutorials/segmentation-evaluation-metrics\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/17cd920cc3af5597be55618d6d6ea4a54809d2eb.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://imagej.net/tutorials/segmentation-evaluation-metrics\" target=\"_blank\" rel=\"noopener\">ImageJ Wiki</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d255a5176a70445402413f516b0a0776a37504cd.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://imagej.net/tutorials/segmentation-evaluation-metrics\" target=\"_blank\" rel=\"noopener\">Segmentation evaluation metrics - Script</a></h3>\n\n  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thanks Biovoxxel, that looks quite helpful</p>", "<p>Dear <a class=\"mention\" href=\"/u/zeratoss\">@zeratoss</a>,<br>\nCould I suggest the newest plugin from Cedric Messaoudi:</p>\n<p><strong>MIC plugin Fiji :</strong><br>\nmenu &gt;Help&gt;Update\u2026</p>\n<p>click the button \u201cManage update sites\u201d</p>\n<p>select MiC mask comparator</p>\n<p>if it is not available directly you can add it (button add update site) with the folowing URL <a href=\"https://sites.imagej.net/MiC-mask-comparator/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Index of /MiC-mask-comparator</a></p>\n<p>MiC is an ImageJ plugin to compare segmentation masks.</p>\n<ul>\n<li>It computes the number of <strong>true positive (TP)</strong>, <strong>false positives (FP)</strong> and <strong>false negatives (FN)</strong>\n<ul>\n<li>at pixel level</li>\n<li>at object level with an overlap (or intersection over union - IoU) of 0.5\n<ul>\n<li>possibility of varying IoU</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>It computes metrics\n<ul>\n<li>\n<strong>Precision</strong> defined as</li>\n</ul>\n</li>\n</ul>\n<p><strong>Recall</strong> (or sensitivity) defined as <strong>Jaccard index</strong> (or global perecision) defined as <strong>F1-measure</strong> (or Sorensen Dice Coefficient - DSC) defined as * It displays the <strong>superposition of the two masks</strong> with the ground truth in green and the mask to evaluate in red for pixel level, for Object level the truth is in green, the mask to evaluate in red, TP are thus yellow, FP blue, FN dark green.</p>\n<ul>\n<li>It displays the <strong>plots</strong> corresponding to metrics as function of IoU</li>\n<li>All computed values are stored in <strong>result tables</strong> that can be exported in excel or csv format</li>\n<li>possibility to work on <strong>stacks</strong>. It works only in <strong>2D</strong> for now, each slice will be compared to corresponding slice. If varying IoU, an additional plot is displayed with metrics corresponding to the sum of TPs, FNs and FPs on all images.</li>\n<li>the plugin is <strong>macro recordable</strong>\n</li>\n</ul>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/Multimodal-Imaging-Center/MiC\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/Multimodal-Imaging-Center/MiC\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f6a257137ed304f47cd9f907cbc879fd58d5314.png 2x\" data-dominant-color=\"F5F1F1\"></div>\n\n<h3><a href=\"https://github.com/Multimodal-Imaging-Center/MiC\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - Multimodal-Imaging-Center/MiC: MiC is an ImageJ plugin to compare...</a></h3>\n\n  <p>MiC is an ImageJ plugin to compare segmentation masks  - GitHub - Multimodal-Imaging-Center/MiC: MiC is an ImageJ plugin to compare segmentation masks</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "57948": ["<p>The colors of the SVS images (GT450) shown in Qupath are not as accurate as the colors shown in Leica Imagescope. Is it possible to correct this using configurations?<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/8469719af260584400e4bd34d464e23503c07ba5.png\" alt=\"qupath copy\" data-base62-sha1=\"iTmWaXGO8y86Cgo3M6rlqW8Cq0J\" width=\"431\" height=\"368\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7abf2f33608d4dc3d1bc34d865cbbde391a5c1c4.png\" alt=\"imagescope copy\" data-base62-sha1=\"hvRRrF3sWWWNavVnef0hrFjihjC\" width=\"478\" height=\"441\"></p>", "<p>I believe ImageScope uses embedded ICC profiles for SVS images, and you can turn this on/off within ImageScope.</p>\n<p>QuPath doesn\u2019t, and rather assumes that the RGB data returned by the reader is \u2018ready for display\u2019.</p>\n<p>I looked into this some time ago, and there is some hidden code in QuPath\u2019s viewer to activate the ICC profile via scripting (if found within the current image), but it wasn\u2019t updated in a long time and so became quietly broken. I\u2019ve just pushed a fix <a href=\"https://github.com/petebankhead/qupath/commit/1df694aa41bec0b00fbc0092b4916e7227c1c3d7\">here</a>, but even with it performance remains pretty bad.</p>\n<p>I\u2019m not sure what the right behaviour is for QuPath here. Leica have an advantage in that they have their own viewing software to work with images from their own scanners in a format that they have written. QuPath, on the other hand, tries to be vendor-agnostic and rely only on open libraries to read images. This is troublesome because behavior isn\u2019t standardised. For example:</p>\n<ul>\n<li>Hamamatsu images don\u2019t (as far as I know) embed an ICC profile, but rather their viewer <em>does</em> use a specific gamma adjustment by default.</li>\n<li>The documentation for Philips\u2019 iSyntax format (not supported by QuPath) specifies a <em>lot</em> of other expected processing prior to viewing <a href=\"https://www.openpathology.philips.com/isyntax/\">here</a>.</li>\n<li>I don\u2019t know what other scanner formats require / expect from viewing software\u2026 they are generally proprietary and I don\u2019t have access to the information or permission to incorporate it into open software.</li>\n</ul>", "<p>Thank you for making the changes. How can I test this fix? A copy and paste in the right file would be enough or do I need to build from source?</p>", "<p>You\u2019d need to build from source. You can download the code as a zip file (under the green code button) from <a href=\"https://github.com/petebankhead/qupath/tree/icc_profiles\">here</a> and building instructions are <a href=\"https://qupath.readthedocs.io/en/stable/docs/reference/building.html\">here</a>.</p>", "<p>build from source, called function, but did not work.<br>\nBelow is the metadata from a slide and images to illustrate.<br>\nThank you very much</p>\n<p>{\u201ctiffslide.comment\u201d: \u201cAperio Image Library v12.0.16 \\r\\n149040x72420 [0,100 145800x72320] (240x240) J2K/KDU Q=70|AppMag = 40|StripeWidth = 1840|ScanScope ID = XXXXXXX|Filename = xxx|Date = xxx|Time = 12:43:35|Time Zone = GMT-04:00|User = xxx|MPP = 0.2538|Left = 11.933033|Top = 24.104160|LineCameraSkew = 0.000193|LineAreaXOffset = 0.012448|LineAreaYOffset = -0.001674|Focus Offset = -0.000500|ImageID = xxx|Exposure Time = 109|Exposure Scale = 0.000001|DisplayColor = 0|SessonMode = NR|OriginalWidth = 149040|OriginalHeight = 72420|ICC Profile = ScanScope v1\u201d, \u201ctiffslide.vendor\u201d: \u201caperio\u201d, \u201ctiffslide.quickhash-1\u201d: null, \u201ctiffslide.background-color\u201d: null, \u201ctiffslide.objective-power\u201d: 40, \u201ctiffslide.mpp-x\u201d: 0.2538, \u201ctiffslide.mpp-y\u201d: 0.2538, \u201ctiffslide.bounds-x\u201d: null, \u201ctiffslide.bounds-y\u201d: null, \u201ctiffslide.bounds-width\u201d: null, \u201ctiffslide.bounds-height\u201d: null, \u201caperio.AppMag\u201d: 40, \u201caperio.Date\u201d: \u201cxxx\u201d, \u201caperio.DisplayColor\u201d: 0, \u201caperio.Exposure Scale\u201d: 1e-06, \u201caperio.Exposure Time\u201d: 109, \u201caperio.Filename\u201d: xxx, \u201caperio.Focus Offset\u201d: -0.0005, \u201caperio.Header\u201d: \u201cAperio Image Library v12.0.16 \\r\\n149040x72420 [0,100 145800x72320] (240x240) J2K/KDU Q=70\u201d, \u201caperio.ICC Profile\u201d: \u201cScanScope v1\u201d, \u201caperio.ImageID\u201d: xxx, \u201caperio.Left\u201d: 11.933033, \u201caperio.LineAreaXOffset\u201d: 0.012448, \u201caperio.LineAreaYOffset\u201d: -0.001674, \u201caperio.LineCameraSkew\u201d: 0.000193, \u201caperio.MPP\u201d: 0.2538, \u201caperio.OriginalHeight\u201d: 72420, \u201caperio.OriginalWidth\u201d: 149040, \u201caperio.ScanScope ID\u201d: \u201cXXXXXXX\u201d, \u201caperio.SessonMode\u201d: \u201cNR\u201d, \u201caperio.StripeWidth\u201d: 1840, \u201caperio.Time\u201d: \u201c12:43:35\u201d, \u201caperio.Time Zone\u201d: \u201cGMT-04:00\u201d, \u201caperio.Top\u201d: 24.10416, \u201caperio.User\u201d: \u201cXXXXX\u201d, \u201ctiffslide.level[0].downsample\u201d: 1.0, \u201ctiffslide.level[0].height\u201d: 72320, \u201ctiffslide.level[0].width\u201d: 145800, \u201ctiffslide.level[0].tile-height\u201d: 240, \u201ctiffslide.level[0].tile-width\u201d: 240, \u201ctiffslide.level[1].downsample\u201d: 4.0, \u201ctiffslide.level[1].height\u201d: 18080, \u201ctiffslide.level[1].width\u201d: 36450, \u201ctiffslide.level[1].tile-height\u201d: 240, \u201ctiffslide.level[1].tile-width\u201d: 240, \u201ctiffslide.level[2].downsample\u201d: 16.000438975540916, \u201ctiffslide.level[2].height\u201d: 4520, \u201ctiffslide.level[2].width\u201d: 9112, \u201ctiffslide.level[2].tile-height\u201d: 240, \u201ctiffslide.level[2].tile-width\u201d: 240, \u201ctiffslide.level[3].downsample\u201d: 64.00175590216367, \u201ctiffslide.level[3].height\u201d: 1130, \u201ctiffslide.level[3].width\u201d: 2278, \u201ctiffslide.level[3].tile-height\u201d: 240, \u201ctiffslide.level[3].tile-width\u201d: 240, \u201ctiff.ImageDescription\u201d: \u201cAperio Image Library v12.0.16 \\r\\n149040x72420 [0,100 145800x72320] (240x240) J2K/KDU Q=70|AppMag = 40|StripeWidth = 1840|ScanScope ID = XXXXXXX|Filename = xxx|Date = xxx|Time = 12:43:35|Time Zone = GMT-04:00|User = xxxxxx|MPP = 0.2538|Left = 11.933033|Top = 24.104160|LineCameraSkew = 0.000193|LineAreaXOffset = 0.012448|LineAreaYOffset = -0.001674|Focus Offset = -0.000500|ImageID = xxx|Exposure Time = 109|Exposure Scale = 0.000001|DisplayColor = 0|SessonMode = NR|OriginalWidth = 149040|OriginalHeight = 72420|ICC Profile = ScanScope v1\u201d}</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/8/685bd6e63f5625ff84fdcd666731869e47295893.png\" alt=\"example2\" data-base62-sha1=\"eTcqakyHPOzXS5dgguANEQBONt9\" width=\"335\" height=\"374\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1a9aaf7f2311b47b4e6bab25ad5cd5684b34585.png\" alt=\"example\" data-base62-sha1=\"plG1u4BCwnw1uta6XFt7vFdUm8d\" width=\"382\" height=\"373\"></p>", "<p>Sorry, it looks like downloading the zip doesn\u2019t actually download the code for the active branch after all \u2013 I thought it would, but I\u2019ve just tried it and I see the zip doesn\u2019t seem to include the changes.</p>\n<p>You\u2019ll either need to get a bit deeper into git to clone the repository and find the right branch, or alternatively replace the <code>QuPathViewer.java</code> file contents with the file from <a href=\"https://raw.githubusercontent.com/petebankhead/qupath/1df694aa41bec0b00fbc0092b4916e7227c1c3d7/qupath-gui-fx/src/main/java/qupath/lib/gui/viewer/QuPathViewer.java\">here</a>.</p>\n<p>Note that this isn\u2019t really an intended QuPath feature, but really a hack to test out the impact of using an ICC profile. However if you get it to work I\u2019d be curious as to whether it results in a similar appearance to ImageScope.</p>", "<p>I build from source.<br>\nNo error when called function, but no change in color.<br>\nThe ICC profile doesn\u2019t appear to be embed in the images.<br>\nUsed libvips to check.<br>\nDoes someone have a file that contains the profile \u201caperio.ICC Profile\u201d: \u201cScanScope v1\u201d?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/727094934051c999f16d91dfc58620440e8a0f75.png\" data-download-href=\"/uploads/short-url/gknDiXJzzdt1Jf0tHUBupA70kZL.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/727094934051c999f16d91dfc58620440e8a0f75_2_690x121.png\" alt=\"image\" data-base62-sha1=\"gknDiXJzzdt1Jf0tHUBupA70kZL\" width=\"690\" height=\"121\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/727094934051c999f16d91dfc58620440e8a0f75_2_690x121.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/727094934051c999f16d91dfc58620440e8a0f75.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/727094934051c999f16d91dfc58620440e8a0f75.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/727094934051c999f16d91dfc58620440e8a0f75_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">876\u00d7154 38.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>It worked for me with some of the Aperio OpenSlide images: <a href=\"https://qupath.readthedocs.io/en/stable/docs/intro/acknowledgements.html\" class=\"inline-onebox\">Acknowledgements \u2014 QuPath 0.3.0 documentation</a></p>", "<p>I\u2019ve run into this too. Leica use very large embedded ICC profiles, you can see them with <code>tiffinfo</code>:</p>\n<pre><code class=\"lang-auto\">$ tiffinfo  H2022013912S1-1-22_0e16f82f-9ca3-7b7f-6b6b-d360ad89bf1e_100821.svs\n=== TIFF directory 0 ===\nTIFF Directory at offset 0x59ccc0be (1506590910)\n  Subfile Type: (0 = 0x0)\n  Image Width: 149882 Image Length: 86028 Image Depth: 1\n  Tile Width: 256 Tile Length: 256\n  Bits/Sample: 8\n  Compression Scheme: JPEG\n  Photometric Interpretation: YCbCr\n  Samples/Pixel: 3\n  Planar Configuration: single image plane\n  ImageDescription: Aperio Leica Biosystems GT450 v1.0.1 \n149882x86028 [0,0,149882x86028] (256x256) JPEG/YCC Q=91|AppMag = 40|Date = 05/25/2022|Exposure Scale = 0.000001|Exposure Time = 8|Filtered = 3|Focus Offset = 0.000000|Gamma = 2.2|Left = 40.077018737793|MPP = 0.264565|Rack = 1|ScanScope ID = SS45050|Slide = 10|StripeWidth = 4096|Time = 10:08:22|Time Zone = GMT+0200|Top = 23.170736312866\n  ICC Profile: &lt;present&gt;, 13113264 bytes\n</code></pre>\n<p>You can see this slide is from a GT450, and includes a 13 megabyte (!!! GOOD LORD !!! this is insane) profile.</p>\n<p>libvips uses openslide to load this type of TIFF, and openslide has no ICC profile support, so unfortunately you don\u2019t see the profile if you do a default load. You can see it if you force libvips to use its standard TIFF loader:</p>\n<pre><code class=\"lang-auto\">$ python3\nPython 3.10.7 (main, Mar 10 2023, 10:47:39) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import pyvips\n&gt;&gt;&gt; x = pyvips.Image.tiffload(\"H2022013912S1-1-22_0e16f82f-9ca3-7b7f-6b6b-d360ad89bf1e_100821.svs\")\n&gt;&gt;&gt; len(x.get(\"icc-profile-data\"))\n13113264\n&gt;&gt;&gt; \n</code></pre>\n<p>Applying the profile in the standard way does indeed fix the colour.</p>", "<p>\u2026 I meant to add, openslide is probably getting ICC profile support this year, so perhaps this issue will go away.</p>"], "78428": ["<p>This might be a noob question, but I am struggling to find area distribution based on colors.<br>\nI am attaching the original image and a sketch of what I intend to have in the processed image, the enclosed region of a specific color, and its area.<br>\n<a href=\"https://shorturl.at/ryPVY\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://shorturl.at/ryPVY</a></p>\n<p>I would appreciate any help regarding this.</p>"], "62046": ["<p>When performing video registration, I use \u201cImage Stabilizer\u201d which is a Plugin of ImageJ.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://imagej.net/plugins/image-stabilizer\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/a/dac2223c0c306ed411dd167f5f897e347916f4e8.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://imagej.net/plugins/image-stabilizer\" target=\"_blank\" rel=\"noopener nofollow ugc\">ImageJ Wiki</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5d2c5e23c9685fc566d532e71e8fd567a481edc1.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://imagej.net/plugins/image-stabilizer\" target=\"_blank\" rel=\"noopener nofollow ugc\">Image Stabilizer</a></h3>\n\n  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>However, there is no description of the detailed menu anywhere, making it difficult to make detailed settings.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7e57fbdbcb11ebec83b53b3407113e4a15f82cd.jpeg\" data-download-href=\"/uploads/short-url/uNUm2oGEhdarLYJYgydlb63K0Dr.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/7/d7e57fbdbcb11ebec83b53b3407113e4a15f82cd_2_612x500.jpeg\" alt=\"image\" data-base62-sha1=\"uNUm2oGEhdarLYJYgydlb63K0Dr\" width=\"612\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/7/d7e57fbdbcb11ebec83b53b3407113e4a15f82cd_2_612x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7e57fbdbcb11ebec83b53b3407113e4a15f82cd.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d7e57fbdbcb11ebec83b53b3407113e4a15f82cd.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/7/d7e57fbdbcb11ebec83b53b3407113e4a15f82cd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">738\u00d7602 56.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nDoes anyone know the details?</p>\n<p>Best regards</p>", "<p>Hey <a class=\"mention\" href=\"/u/itk_lighting\">@ITK_Lighting</a> ,</p>\n<p>Im also facing the same problem\ud83d\ude13</p>\n<p>Cant find documentation on the different parameters. Did you in the end find a source? Or are you using a different tool for drift-correction?</p>\n<p>best,<br>\nAlex</p>", "<p>Hey <a class=\"mention\" href=\"/u/itk_lighting\">@ITK_Lighting</a>  <a class=\"mention\" href=\"/u/alexjov\">@alexjov</a> ,<br>\nI am also having the same issue. Were you able to find out what the parameters mean?<br>\n<a class=\"mention\" href=\"/u/christlet\">@christlet</a> seems to be the maintainer. Christopher, can you help us?<br>\nAll the best!<br>\nSarah</p>", "<p>I have no insight into the way this plugin works and what the parameters mean. What I know is that the \u201cupdate template coefficient\u201d in the menu is the \u201ca\u201d value in the formula on the ImageJ wiki website. There are also some explanations on the Wikipedia page for the Lucas-Kanade algorithm (<a href=\"https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method\" class=\"inline-onebox\">Lucas\u2013Kanade method - Wikipedia</a>), which helped me in the past when the algorithm failed because there were shifts that were too big compared to the pixel size - in that case downsampling prior to correction helped.</p>"], "78431": ["<p>Hey <a class=\"mention\" href=\"/u/melvingelbard\">@melvingelbard</a>,</p>\n<p>Would you know how to create two annotations each taking half the screen? Would be really helpful, thank you!</p>", "<p>Hi <a class=\"mention\" href=\"/u/moreauem\">@MoreauEM</a> I\u2019ve moved this to its own topic since it seems a different question.</p>\n<p>It\u2019s not clear what you mean by \u2018half the screen\u2019. Do you mean over half the image? And with which orientation (top/bottom, left/right)?</p>\n<p>(Explaining why you want this would also help understand it better, and reduce the risk of anyone writing scripts that don\u2019t actually solve the real problem.)</p>", "<p>Hi <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> ,</p>\n<p>Sorry if I was not clear, I don\u2019t have much experience in coding. So what I am trying to do using scripts is naming multiple annotations in a full image. Ideally the left side of the image would have a different name than the annotations on the left side of the image. Is there a way to script this? Or to be more clear, is there a way for the script to automatically recognize where the half of the image is (using the X coordinate)?</p>\n<p>Thank you</p>", "<p>Possibly something like</p>\n<pre><code class=\"lang-auto\">\nmidpoint = getCurrentServer().getWidth()/2\nfor (annotation in getAnnotationObjects()){\n\n  if (annotation.getROI().getCentroidX() &gt;midpoint){\n    annotation.setName(\"Right side\")\n  } else {\n    annotation.setName(\"Left side\")\n  }\n}\n</code></pre>\n<p>Winging it, haven\u2019t tested the code.</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"4\" data-topic=\"78431\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">midpoint = getCurrentImageServer().getWidth()/2\nfor (annotation in getAnnotationObjects()){\n\n  if (annotation.getROI().getCentroidX() &gt;midpoint){\n    annotation.setName(\"Right side\")\n  } else {\n    annotation.setName(\"Left side\")\n  }\n}\n</code></pre>\n</blockquote>\n</aside>\n<p>Juste had to change ImageServer to Server and it works perfectly! Thank you!</p>", "<p>Thanks, fixed</p>"], "78434": ["<p>Hi.</p>\n<p>I have been trying to generate analysed videos with DeepLabCut using trained models I\u2019ve received from my Lab. I\u2019m trying to process one video at a time. The video gets processed and plot-poses, pickle, and h5 files are created. The csv is generated when I try to run the script manually but not for some reason when I run the script through a process spawned by NodeJS.</p>\n<p>Python Code:</p>\n<pre><code class=\"lang-auto\">import os\nos.environ[\"DLClight\"]=\"True\"\n\nimport deeplabcut\nimport argparse\n\nimport argparse\nparser = argparse.ArgumentParser(description='MobileNet 5 marker models')\nparser.add_argument('dir', type=str, help='absolute path for the directory for this analysis. must end with a /')\nparser.add_argument('video', type=str, help='name of the video file. tested only with mp4, supply that.')\nparser.add_argument('dirpath', type=str, help='name of the dir path to the config file')\nargs = parser.parse_args()\nprint(args.__dict__)\n\nvideofile_path = [args.dir  + args.video]\nvideofile = args.dir \npath_config_file = args.dirpath +'/config.yaml'\n\nprint('NEW project_path: ' + args.dir)\nprint('NEW path_config_file: ' + path_config_file)\nprint('NEW videofile_path: ' + videofile_path[0])\n\nif __name__ == \"__main__\":\n    deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='mp4')\n\n    deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype='mp4')\n\n    deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype='mp4')\n\n    deeplabcut.analyze_videos_converth5_to_csv(videofile,'.mp4')\n</code></pre>\n<p>NodeJS code:</p>\n<pre><code class=\"lang-auto\">const model = spawn('python', [\n      `models/model.py`, // refers to the above python code\n      `uploads/${fileDir}/`, // dir where the video is supposed to be located\n      `${fileName}`, // the name of the video file\n      `${modelDir}` // the dir where the model's config file is present\n    ])\n</code></pre>\n<p>Things I\u2019ve tried:</p>\n<ol>\n<li>I thought it could be an issue with the file name containing <code>-</code> but removing that did no change, the video was still not generated.</li>\n<li>I tried putting the code and the model in a separate folder to test and remove arguments and statically provided values via variables. The frames are again analysed successfully but no video is rendered or csv file is generated. Code below.</li>\n</ol>\n<p>Another thing that happens always after the video is analysed, I get the following error:</p>\n<pre><code class=\"lang-auto\">Starting to process video: videos/v.mp4\nLoading videos/v.mp4 and data.\n[Errno 2] No such file or directory: 'videos'\nTraceback (most recent call last):\n  File \"/path/to/models/model.py\", line 32, in &lt;module&gt;\n    deeplabcut.analyze_videos_converth5_to_csv(videofile,'.mp4')\n  File \"/Users/mk/Downloads/DeepLabCut/deeplabcut/utils/conversioncode.py\", line 130, in analyze_videos_converth5_to_csv\n    h5_files = list(\n  File \"/Users/mk/Downloads/DeepLabCut/deeplabcut/utils/auxiliaryfunctions.py\", line 436, in grab_files_in_folder\n    for file in os.listdir(folder):\nFileNotFoundError: [Errno 2] No such file or directory: 'v.mp4'\n</code></pre>\n<p>in this case, v.mp4 is the video it just analysed and is trying to analyse again and failing with this error.</p>\n<p>Code for the second case of what I tried</p>\n<pre><code class=\"lang-auto\">import os\nos.environ[\"DLClight\"]=\"True\"\n\nimport deeplabcut\n# import argparse\n\n# import argparse\n# parser = argparse.ArgumentParser(description='MobileNet 5 marker models')\n# parser.add_argument('dir', type=str, help='absolute path for the directory for this analysis. must end with a /')\n# parser.add_argument('video', type=str, help='name of the video file. tested only with mp4, supply that.')\n# parser.add_argument('dirpath', type=str, help='name of the dir path to the config file')\n# args = parser.parse_args()\n# print(args.__dict__)\n\nvideofile_path = ['videos/' + 'v.mp4']\nvideofile = 'v.mp4'\npath_config_file = 'config.yaml'\n\n# print('NEW project_path: ' + args.dir)\nprint('NEW path_config_file: ' + path_config_file)\nprint('NEW videofile_path: ' + videofile_path[0])\n\nif __name__ == \"__main__\":\n    deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='mp4')\n\n    deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype='mp4')\n\n    deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype='mp4')\n\n    deeplabcut.analyze_videos_converth5_to_csv(videofile,'.mp4')\n</code></pre>\n<p>Thank you reading the question. All help is highly appreciated.<br>\nThank you.</p>"], "78436": ["<p>Hello to everyone.I have a problem to create a new project with deeplabcut gui. I can\u2019t add my video when creating.<br>\ncant use the choose button.<br>\nI\u2019m using win10 and deeplabcut 2.3.0<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2.png\" data-download-href=\"/uploads/short-url/lVt5ELjtJyUR4rXFIt8Rhc2q1Kq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2_2_690x468.png\" alt=\"image\" data-base62-sha1=\"lVt5ELjtJyUR4rXFIt8Rhc2q1Kq\" width=\"690\" height=\"468\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2_2_690x468.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99acb6cfd020dec1b8ea7f4ca0e0f8e5eb41b5d2.png 2x\" data-dominant-color=\"2B3640\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">995\u00d7675 37.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nI\u2019m wondering if someone can help me. <img src=\"https://emoji.discourse-cdn.com/twitter/sob.png?v=12\" title=\":sob:\" class=\"emoji\" alt=\":sob:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sob.png?v=12\" title=\":sob:\" class=\"emoji\" alt=\":sob:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sob.png?v=12\" title=\":sob:\" class=\"emoji\" alt=\":sob:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/sob.png?v=12\" title=\":sob:\" class=\"emoji\" alt=\":sob:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>You should add the whole folder of videos. Then in the gui you can tick off the videos you don\u2019t want to add to the project</p>", "<p>thanks a lot\uff01</p>"], "78439": ["<p>I use this script, made by a colleague, to convert my IF images from the proprietary Leica .LIF file type to .TIF files, while preserving all the information from each image. Meaning it exports all of the images out of a single .lif project file into separate .tif files, and also a montage of each channel side-by-side in a jpg.</p>\n<p>For some reason however, if a .lif file has too many images in it, it only converts the first handful of images and then ignores the rest. Splitting up the .lif file solves the issue, but is rather inconvenient. Any ideas as to why this might be happening and how to address it?</p>\n<pre><code class=\"lang-javascript\">// \"lif2tif\"\n// Saves all open image windows as tif file, in the specified directory\n\nsetBatchMode(true); \ninput = getDirectory(\"Input directory, folder where your .lif file is\");\noutput = getDirectory(\"Output directory, where you'd like your adjusted .tif files to go\");\n\n//Dialog.create(\"File type\");\n//Dialog.addString(\"File suffix: \", \".tif\", 5);\n//Dialog.show();\nsuffix = \".lif\";//Dialog.getString();\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n}\n\nfunction processFile(input, output, file) {\n\t// do the processing here by replacing\n\t// the following two lines by your own code\n\tprint(\"Processing: \" + input + file);\n\topenLif(input+file);\n\tprint(\"Saving to: \" + output);\n\tlistImages();\n}\n\nfunction openLif(input){\n\taa = seriesN(80);\n\tprint(aa);\n\tprint(input+\" color_mode=Default view=[Standard ImageJ] stack_order=XYZCT \"+aa);\n\trun(\"Bio-Formats Importer\", \"open=\"+input+\" color_mode=Default view=[Standard ImageJ] stack_order=XYZCT \"+aa);\n}\n\n//Create string \"series_1 series_2 series_3 series_4....\"\nfunction seriesN(num){\n\tstr = \"\";\n\tfor (i=0; i&lt;num; i++){\n\t\tii = i+1;\n        str = str+\"series_\"+ii+\" \";\n     }\n     return str;\n}\n\n//Save\nfunction listImages(){\n\timageList = getList(\"image.titles\");\n\tif (imageList.length==0)\n\t     print(\"No image windows are open\");\n\telse {\n    \tprint(\"Image windows:\");\n     \tfor (i=0; i&lt;imageList.length; i++){\n        \tprint(\"   \"+imageList[i]);\n        \tsaveTiff(imageList[i]);\n     \t}\n\t}\n\tprint(\"\");\n}\n\n\nfunction saveTiff(winName){\n\tselectWindow(winName); \n\tsaveAs(\"Tiff\", output + winName);\n\tclose();\n}\n</code></pre>", "<p>Hi James,</p>\n<p>your script fixes the number of images/series in the lif file to 80.<br>\nSince I assume that their number actually changes from file to file, you need to use an extension of the bio-formats library that allows you to extract information from the file, like the number of images in it contained, before actually opening the single images.<br>\nSpecifically, you need to add at the beginning of the script the following line</p>\n<pre><code class=\"lang-javascript\">run(\"Bio-Formats Macro Extensions\");`\n</code></pre>\n<p>Then you modify the two functions <code>ProcessFile</code> and <code>openLif</code> as follow</p>\n<pre><code class=\"lang-javascript\">function processFile(input, output, file) {\n\t// do the processing here by replacing\n\t// the following two lines by your own code\n\tExt.setId(input + file);\n\tExt.getSeriesCount(count);\n\tprint(\"Processing: \" + input + file+ \" - \"+d2s(count,0)+\" images\");\n\n\tfor (f=0;f&lt;count;f++) {\n\t\topenLif(input+file,f);\n\t\tprint(\"Saving to: \" + output);\n\t\tlistImages();\n\t}\n}\n\nfunction openLif(input,f){\n\tExt.setSeries(f);\n\trun(\"Bio-Formats Importer\", \"open=[\" +input +\"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_\" + d2s(f+1,0));\n}\n</code></pre>\n<p>Notice that in this way the images in the lif file are opened one by one, which is a safer procedure.</p>\n<p>I hope it helps.<br>\nGiovanni</p>", "<p>Beautiful, and yes I do see how looping through is a better idea!</p>\n<p>One other side note, I see in the console as this is running, I get these warnings:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f.png\" data-download-href=\"/uploads/short-url/tIfdK8JnPQaTVU61QSRLDwRuPkX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f_2_690x229.png\" alt=\"image\" data-base62-sha1=\"tIfdK8JnPQaTVU61QSRLDwRuPkX\" width=\"690\" height=\"229\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f_2_690x229.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d03f6f3c8d2f7507b734253bb25bbacbf9ded67f.png 2x\" data-dominant-color=\"F3DCDC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">868\u00d7289 140 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The message is self-explanatory, but I was wondering if there\u2019s a way to just prevent them from showing up as they\u2019re not really relevant to anything I\u2019m doing.</p>", "<p>Hi James, those messages can largely be ignored. Unfortunately there isn\u2019t a simple way of turning them off, though a number of users have now requested a way of controlling the logging levels in the plugin.</p>\n<p>Currently the only way to change the logging levels is through the DebugTools class in the Bio-Formats API as per the <a href=\"https://bio-formats.readthedocs.io/en/stable/developers/logging.html\">logging docs</a>. If you are using a jython or groovy script then it can be set using the below:</p>\n<pre><code class=\"lang-auto\">from loci.plugins import BF\nfrom loci.common import DebugTools\n\nDebugTools.setRootLevel(\"OFF\")\n</code></pre>", "<p>It would be nice to change this whole ijm macro to a groovy script, but trying to just change the file extension and running that on FIJI doesn\u2019t seem to work.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb.png\" data-download-href=\"/uploads/short-url/wreYNjfxpUmVGo1ESj8AY3mFM15.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_690x50.png\" alt=\"image\" data-base62-sha1=\"wreYNjfxpUmVGo1ESj8AY3mFM15\" width=\"690\" height=\"50\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_690x50.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_1035x75.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e359e49901b5e61b46b231b315292c9df24b3ebb_2_1380x100.png 2x\" data-dominant-color=\"E6E6E6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1886\u00d7138 81.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I essentially get this unexpected input error for every line. In the window that pops up when dragging the groovy file onto FIJI (the console?), the language tab at the top left does correctly indicate that it\u2019s a groovy file that is being run.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/22a0fac488de941112ff661a0e931189e0104065.png\" alt=\"image\" data-base62-sha1=\"4Wl880lu40GydrOtJeSqf51IFVz\" width=\"330\" height=\"489\"></p>", "<p>The groovy syntax is largely the same but there will be some differences in the script. For the imports in groovy it should look like:</p>\n<pre><code class=\"lang-auto\">import loci.plugins.BF\nimport loci.common.DebugTools\nimport loci.formats.ImageReader\n</code></pre>\n<p>You can see an example of a Groovy script using Bio-Formats <a href=\"https://github.com/dgault/bio-formats-examples/blob/6cdb11e8c64566611b18f384b3a257dab5037e90/src/main/macros/groovy/OverlappedTiledPyramidConversion.groovy\">here</a>. It isn\u2019t using the DebugTools but it might help with the syntax.</p>"], "78441": ["<p>I would like to train the Stardist model using a custom dataset, I\u2019ve installed the packages needed using the documentation from the <a href=\"https://github.com/stardist/stardist\" rel=\"noopener nofollow ugc\">repo\u2019s README file</a>.<br>\nI\u2019ve gathered all the data and created 3 independent datasets (train/validation/test) each of these contain patches of images that I would like to use as input data for the model.<br>\nAs the size of these files would be too big to fit in memory all at once, I was thinking if it was possible to train using batches as I\u2019ve done previously using <code>pytorch</code>. I\u2019ve read in the <a href=\"https://stardist.net/docs/faq.html#what-if-my-training-dataset-does-not-fit-into-cpu-memory\" rel=\"noopener nofollow ugc\">FAQs</a> that it is possible and that I need to create my own <code>tf.keras.utils.Sequence</code> class for each of my datasets so this is what my code looks like right now:</p>\n<p>My <code>Sequence</code> class:</p>\n<pre><code class=\"lang-auto\">class StardistSequence(tf.keras.utils.Sequence):\n\n    def __init__(self, csv_path, name, batch_size=BATCH_SIZE, key=\"raw\", type_=\"train\"):\n        super().__init__()\n        df = pd.read_csv(csv_path, index_col=False)\n        self._filenames = df[key]\n        self._key = key\n        self._name = name\n        self._batch_size = batch_size\n        print(f\"{name} {type_} - {key} Batch size: {batch_size} dataset created!\")\n\n    def __len__(self):\n        return np.ceil(len(self._filenames) / self._batch_size).astype(np.uint8)\n\n    def preprocess_x(self, path):\n        axis_norm = (0, 1)\n        im = io.imread(path, as_gray=True)\n        im = exposure.adjust_gamma(im, .5)\n        im = normalize(im, 1, 99.8, axis=axis_norm)\n        return im\n    \n    def preprocess_y(self, path):\n        im = io.imread(path, as_gray=True)\n        return fill_label_holes(im)\n    \n    @lru_cache(100)\n    def __getitem__(self, idx):\n        batch = self._filenames.iloc[idx * self._batch_size:(idx + 1) * self._batch_size]\n        preprocess_fn = self.preprocess_x if self._key == \"raw\" else self.preprocess_y\n        batch = np.array([preprocess_fn(file_name) for file_name in batch])\n        return batch\n</code></pre>\n<p>Sequence creation:</p>\n<pre><code class=\"lang-auto\">name = \"stardist-test\"\nX_train_seq = StardistSequence(train_csv_path, name, key=\"raw\", type_=\"train\")\ny_train_seq = StardistSequence(train_csv_path, name, key=\"mask\", type_=\"train\")\nX_val_seq = StardistSequence(val_csv_path, name, key=\"raw\", type_=\"val\")\ny_val_seq = StardistSequence(val_csv_path, name, key=\"mask\", type_=\"val\")\n</code></pre>\n<p>All my input images are grayscale images with values between <code>0.</code> and <code>1.</code> and all my masks are binary with values between <code>0</code> or <code>255</code>. For example, here we can see that I have 47 batches and each contains 64 patches of 256x256.</p>\n<pre><code class=\"lang-auto\">print(len(X_train_seq), X_train_seq[0].shape)\n&gt;&gt;&gt; (47, (64, 256, 256))\n</code></pre>\n<p>Configuring the model:</p>\n<pre><code class=\"lang-auto\">n_rays = 32  #Number of radial directions for the star-convex polygon.\n\n# Use OpenCL-based computations for data generator during training (requires 'gputools')\nuse_gpu = True #False and gputools_available()\n\n# Predict on subsampled grid for increased efficiency and larger field of view\ngrid = (2,2)\n\nconf = Config2D (\n    n_rays       = n_rays,\n    grid         = grid,\n    use_gpu      = use_gpu,\n    n_channel_in = n_channel,\n    train_sample_cache = False,\n    train_batch_size= BATCH_SIZE, # BATCH_SIZE = 64\n)\n\nvars(conf)\n&gt;&gt;&gt; {'n_dim': 2,\n 'axes': 'YXC',\n 'n_channel_in': 1,\n 'n_channel_out': 33,\n 'train_checkpoint': 'weights_best.h5',\n 'train_checkpoint_last': 'weights_last.h5',\n 'train_checkpoint_epoch': 'weights_now.h5',\n 'n_rays': 32,\n 'grid': (2, 2),\n 'backbone': 'unet',\n 'n_classes': None,\n 'unet_n_depth': 3,\n 'unet_kernel_size': (3, 3),\n 'unet_n_filter_base': 32,\n 'unet_n_conv_per_depth': 2,\n 'unet_pool': (2, 2),\n 'unet_activation': 'relu',\n 'unet_last_activation': 'relu',\n 'unet_batch_norm': False,\n 'unet_dropout': 0.0,\n 'unet_prefix': '',\n 'net_conv_after_unet': 128,\n 'net_input_shape': (None, None, 1),\n 'net_mask_shape': (None, None, 1),\n 'train_shape_completion': False,\n 'train_completion_crop': 32,\n 'train_patch_size': (256, 256),\n 'train_background_reg': 0.0001,\n 'train_foreground_only': 0.9,\n 'train_sample_cache': False,\n 'train_dist_loss': 'mae',\n 'train_loss_weights': (1, 0.2),\n 'train_class_weights': (1, 1),\n 'train_epochs': 400,\n 'train_steps_per_epoch': 100,\n 'train_learning_rate': 0.0003,\n 'train_batch_size': 64,\n 'train_n_val_patches': None,\n 'train_tensorboard': True,\n 'train_reduce_lr': {'factor': 0.5, 'patience': 40, 'min_delta': 0},\n 'use_gpu': True}\n</code></pre>\n<p>Creating a model instance using such configuration and attempting to train (I\u2019m also using an augmenter function):</p>\n<pre><code class=\"lang-auto\">model = StarDist2D(conf, name='stardist_test', basedir='stardist_models')\nmodel.train(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), augmenter=augmenter, epochs=10, steps_per_epoch=len(X_val_seq))\n</code></pre>\n<p>I get the following error:</p>\n<pre><code class=\"lang-auto\">---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [75], in &lt;cell line: 1&gt;()\n----&gt; 1 model.train(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), augmenter=augmenter, epochs=10, steps_per_epoch=len(X_val_seq))\n\nFile ~/.local/lib/python3.10/site-packages/stardist/models/model2d.py:423, in StarDist2D.train(self, X, Y, validation_data, classes, augmenter, seed, epochs, steps_per_epoch, workers)\n    421 n_take = self.config.train_n_val_patches if self.config.train_n_val_patches is not None else n_data_val\n    422 _data_val = StarDistData2D(validation_data[0],validation_data[1], classes=classes_val, batch_size=n_take, length=1, **data_kwargs)\n--&gt; 423 data_val = _data_val[0]\n    425 # expose data generator as member for general diagnostics\n    426 self.data_train = StarDistData2D(X, Y, classes=classes, batch_size=self.config.train_batch_size,\n    427                                  augmenter=augmenter, length=epochs*steps_per_epoch, **data_kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/stardist/models/model2d.py:51, in StarDistData2D.__getitem__(self, i)\n     49 def __getitem__(self, i):\n     50     idx = self.batch(i)\n---&gt; 51     arrays = [sample_patches((self.Y[k],) + self.channels_as_tuple(self.X[k]),\n     52                              patch_size=self.patch_size, n_samples=1,\n     53                              valid_inds=self.get_valid_inds(k)) for k in idx]\n     55     if self.n_channel is None:\n     56         X, Y = list(zip(*[(x[0][self.b],y[0]) for y,x in arrays]))\n\nFile ~/.local/lib/python3.10/site-packages/stardist/models/model2d.py:53, in &lt;listcomp&gt;(.0)\n     49 def __getitem__(self, i):\n     50     idx = self.batch(i)\n     51     arrays = [sample_patches((self.Y[k],) + self.channels_as_tuple(self.X[k]),\n     52                              patch_size=self.patch_size, n_samples=1,\n---&gt; 53                              valid_inds=self.get_valid_inds(k)) for k in idx]\n     55     if self.n_channel is None:\n     56         X, Y = list(zip(*[(x[0][self.b],y[0]) for y,x in arrays]))\n\nFile ~/.local/lib/python3.10/site-packages/stardist/models/base.py:202, in StarDistDataBase.get_valid_inds(self, k, foreground_prob)\n    200 else:\n    201     patch_filter = (lambda y,p: self.max_filter(y, self.maxfilter_patch_size) &gt; 0) if foreground_only else None\n--&gt; 202     inds = get_valid_inds(self.Y[k], self.patch_size, patch_filter=patch_filter)\n    203     if self.sample_ind_cache:\n    204         with self.lock:\n\nFile ~/.local/lib/python3.10/site-packages/stardist/sample_patches.py:47, in get_valid_inds(img, patch_size, patch_filter)\n     34 def get_valid_inds(img, patch_size, patch_filter=None):\n     35     \"\"\"\n     36     Returns all indices of an image that \n     37     - can be used as center points for sampling patches of a given patch_size, and\n   (...)\n     44         a function with signature patch_filter(img, patch_size) returning a boolean mask \n     45     \"\"\"\n---&gt; 47     len(patch_size)==img.ndim or _raise(ValueError())\n     49     if not all(( 0 &lt; s &lt;= d for s,d in zip(patch_size,img.shape))):\n     50         raise ValueError(\"patch_size %s negative or larger than image shape %s along some dimensions\" % (str(patch_size), str(img.shape)))\n\nFile ~/.local/lib/python3.10/site-packages/csbdeep/utils/utils.py:91, in _raise(e)\n     89 def _raise(e):\n     90     if isinstance(e, BaseException):\n---&gt; 91         raise e\n     92     else:\n     93         raise ValueError(e)\n\nValueError: \n</code></pre>\n<p>Clearly we don\u2019t have a message for it but this comparison seems to be the problem: <code>len(patch_size)==img.ndim</code> where <code>len(patch_size)</code> is <code>2</code> as it is <code>(256, 256)</code> but <code>img.ndim</code> is <code>3</code> as it is <code>(64, 256, 256)</code>.</p>\n<p>Unfortunately I couldn\u2019t find an example that suited my needs nor an issue created in the repo. I was about to create one but it showed me an option to find the answer in this forum. I\u2019ve searched for something similar here but was unable to find anything useful so that is why I created this thread.<br>\nThe questions I would like to ask would be: what am I missing?, did I forgot to configure something?, is my <code>StardistSequence</code> class well defined? or is it something else?</p>\n<p>Thanks in advance!</p>", "<p>Hi,</p>\n<p><code>StardistSequence</code> should return a single <code>image, mask</code> pair (not a batch).</p>\n<p>Hope that helps,</p>\n<p>M</p>", "<p>Thanks for your suggestion <a class=\"mention\" href=\"/u/mweigert\">@mweigert</a>, I\u2019ve updated my <code>StardistSequence</code> to look like this:</p>\n<pre><code class=\"lang-auto\">class StardistSequence(tf.keras.utils.Sequence):\n\n    def __init__(self, csv_path, name, batch_size=BATCH_SIZE, type_=\"train\"):\n        super().__init__()\n        df = pd.read_csv(csv_path, index_col=False)\n        self._imgs = df[\"raw\"]\n        self._masks = df[\"mask\"]\n        self._name = name\n        self._batch_size = batch_size\n        print(f\"{name} {type_} - Batch size: {batch_size} dataset created!\")\n\n    def __len__(self):\n        return np.ceil(len(self._masks) / self._batch_size).astype(np.uint8)\n\n    def preprocess_x(self, path):\n        axis_norm = (0, 1)\n        im = io.imread(path, as_gray=True)\n        im = exposure.adjust_gamma(im, .5)\n        im = normalize(im, 1, 99.8, axis=axis_norm)\n        return im\n    \n    def preprocess_y(self, path):\n        im = io.imread(path, as_gray=True)\n        im = fill_label_holes(im)\n        im = (im / 255).astype(np.float32)\n        return im\n    \n    @lru_cache(100)\n    def __getitem__(self, idx):\n        img_path = self._imgs.iloc[idx]\n        mask_path = self._masks.iloc[idx]\n        img = self.preprocess_x(img_path)\n        mask = self.preprocess_y(mask_path)\n        return img, mask\n</code></pre>\n<p>So every time I call <code>__getitem__()</code> through indexation I get a tuple <code>(img, mask)</code>.</p>\n<p>Then my sequence instance creation looks like this:</p>\n<pre><code class=\"lang-auto\">train_seq = StardistSequence(train_csv_path, name, type_=\"train\")\nval_seq = StardistSequence(val_csv_path, name, type_=\"val\")\n</code></pre>\n<p>But how can I separate <code>X_train</code> and <code>Y_train</code> from <code>train_seq</code>  to pass them to <code>model.train()</code> as <code>X</code> and <code>Y</code>? (and similar for <code>val_seq</code>).</p>", "<p>In this case, creating a Sequence for X and Y separately should work. See <a href=\"https://github.com/stardist/stardist/issues/107\" class=\"inline-onebox\">CPU memory usage keeps on increasing during training \u00b7 Issue #107 \u00b7 stardist/stardist \u00b7 GitHub</a></p>", "<p><a class=\"mention\" href=\"/u/mweigert\">@mweigert</a>, thanks again for your help! I based my sequence class on one of the comments in that issue and previously had mixed it with the class in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\" rel=\"noopener nofollow ugc\"><code>tf.keras.utils.Sequence</code></a> docs thinking that it would be possible to get batches directly from the <code>Sequence</code>.</p>\n<p>Now I updated my <code>StardistSequence</code> class to look like this:</p>\n<pre><code class=\"lang-auto\">class StardistSequence(tf.keras.utils.Sequence):\n\n    def __init__(self, csv_path, name, batch_size=BATCH_SIZE, key=\"raw\", type_=\"train\"):\n        super().__init__()\n        df = pd.read_csv(csv_path, index_col=False)\n        self._filenames = df[key]\n        self._name = name\n        self._key = key\n        self._batch_size = batch_size\n        print(f\"{name} {type_} {key} - Batch size: {batch_size} dataset created!\")\n\n    def __len__(self):\n        return np.ceil(len(self._filenames) / self._batch_size).astype(np.uint8)\n\n    def preprocess_x(self, path):\n        axis_norm = (0, 1)\n        im = io.imread(path, as_gray=True)\n        im = exposure.adjust_gamma(im, .5)\n        im = normalize(im, 1, 99.8, axis=axis_norm)\n        return im\n    \n    def preprocess_y(self, path):\n        im = io.imread(path, as_gray=True)\n        return fill_label_holes(im)\n    \n    @lru_cache(100)\n    def __getitem__(self, idx):\n        file_name = self._filenames.iloc[idx]\n        preprocess_fn = self.preprocess_x if self._key == \"raw\" else self.preprocess_y\n        im = preprocess_fn(file_name) \n        return im\n\n</code></pre>\n<p>And create my sequences like this:</p>\n<pre><code class=\"lang-auto\">name = \"test\"\nX_train_seq = StardistSequence(train_csv_path, name, key=\"raw\", type_=\"train\")\ny_train_seq = StardistSequence(train_csv_path,  name, key=\"mask\", type_=\"train\")\nX_val_seq = StardistSequence(val_csv_path, name, key=\"raw\", type_=\"val\")\ny_val_seq = StardistSequence(val_csv_path, name, key=\"mask\", type_=\"val\")\n</code></pre>\n<p>Finally, I\u2019m able to start training without issues. I\u2019m leaving the updated code for future reference. Once again, thank you so much for your help <a class=\"mention\" href=\"/u/mweigert\">@mweigert</a>!</p>"], "78448": ["<p>Good Afternoon,</p>\n<p>I am currently engaged in the task of quantifying the number of fluorescent markers present in my images, along with determining the degree of overlap between these markers. While the summary window for the overlap analysis is functioning efficiently and presenting results stacked with filenames, the individual \u201cAnalyze Particles\u201d summary tables are generated for each image separately. I would prefer combining the \u201cAnalyze Particles\u201d summary tables for each individual image into a single, comprehensive table that includes the respective image names as the first column. This would greatly facilitate downstream analysis and interpretation of my findings.</p>\n<p>Here\u2019s what my \u201cAnalyze Particles\u201d output looks like right now: <a href=\"https://www.dropbox.com/s/b2cvl8n6jsy0onm/ImageJ_bDM1TPc3qp%20-%20Copy.png?dl=0\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Dropbox - ImageJ_bDM1TPc3qp - Copy.png - Simplify your life</a></p>\n<p>Here is my core code:</p>\n<pre><code class=\"lang-auto\">//markers in each channel\nrun(\"Z Project...\", \"projection=[Max Intensity]\");\nrun(\"Auto Threshold\", \"method=Otsu white\");\nrun(\"Analyze Particles...\", \"size=3-Infinity circularity=0.20-0.8 show=[Overlay Masks] summarize stack\");\n\n//overlapping markers\nrun(\"Split Channels\");\nimageCalculator(\"AND create\", \"c1\",\"c2\");\nrun(\"Auto Threshold\", \"method=Otsu white\");\nrun(\"Analyze Particles...\", \"size=3.00-Infinity circularity=0.20-0.80 show=[Overlay Masks] summarize stack\");\n</code></pre>\n<p>I tried following this tutorial (<a href=\"https://forum.image.sc/t/combining-several-summary-of-image-title-into-a-single-results-table/40152/6\" class=\"inline-onebox\">Combining several \"Summary of [image title]\" into a single results table - #6 by smith6jt</a>), specifically Antinos\u2019s comments in Nov 2020. The \u201cAnalysis\u201d table generated by the script is yielding unexpected results. Specifically, it only reports the number of markers detected in one of the channels, and occasionally shows a count of zero. In order to confirm the presence of markers, I reviewed the original images and verified their presence.</p>\n<p>I really appreciate any help you can provide.</p>", "<p>Hi <a class=\"mention\" href=\"/u/nopitynope\">@nopitynope</a></p>\n<p>Would this help at all: <a href=\"https://forum.image.sc/t/summarizing-results-in-one-table/57877\" class=\"inline-onebox\">Summarizing results in one table?</a></p>\n<p>Best wishes,<br>\nMarie</p>"], "78451": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/8892f61dc3e7afb680e18e3d589b1e356e591709.png\" data-download-href=\"/uploads/short-url/jubNyy6dyCZt4XpDRlHGwEc2SHn.png?dl=1\" title=\"Screenshot from 2023-03-12 08-01-57\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/8892f61dc3e7afb680e18e3d589b1e356e591709.png\" alt=\"Screenshot from 2023-03-12 08-01-57\" data-base62-sha1=\"jubNyy6dyCZt4XpDRlHGwEc2SHn\" width=\"593\" height=\"500\" data-dominant-color=\"898988\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-12 08-01-57</span><span class=\"informations\">798\u00d7672 10.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I am trying to use ImageJ to automatically count and measure the area of bacterial colonies. I followed <a href=\"https://www.youtube.com/watch?v=zwjXbeiviH0\" rel=\"noopener nofollow ugc\">this video</a>.</p>\n<p>The procedure, in essence, works, and the results are shown above. I converted the selection to 16 bits, applied the threshold, and counted the particles.</p>\n<p>But I have two problems:</p>\n<ol>\n<li>How to properly set the dimensions of the colonies? Here I get either no counts for well-defined spots or multiple counts on the same spot.</li>\n<li>How can I compare two different plates? I imagine the images should be exactly the same dimension otherwise the area (in pixel, I imagine) will differ one from another.</li>\n</ol>\n<p>Thank you</p>\n<h3>\n<a name=\"analysis-goals-1\" class=\"anchor\" href=\"#analysis-goals-1\"></a>Analysis goals</h3>\n<ul>\n<li>I am looking to automatically count the colonies and measure their area.</li>\n</ul>\n<h3>\n<a name=\"challenges-2\" class=\"anchor\" href=\"#challenges-2\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding? Counts of wrong items</li>\n<li>What have you tried already? Different size/pixel limits</li>\n<li>What software packages and/or plugins have you tried? only ImageJ</li>\n</ul>", "<p>Am l correct that you want to analyze the properties of each of the 18 colonies in this image?</p>\n<p>Did you set you define what you want to measure in <code>Analyze &gt; Set Measurements</code>?</p>\n<p>And when you want to obtain the result, do you choose <code>Analyze &gt; Measure</code> or <code>Analyze &gt; Analyze Particles</code>? I think the latter option should give you want you want (provided you did set the desired measurements in the <code>Set Measurements</code> menu).</p>\n<p>Optionally, you can record your actions to keep track of your steps, this may help in communicating what you have tried and where it may go wrong (<code>Plugins &gt; Macros &gt; Record...</code>, a sort of text window will pop up where each action is recorded).</p>", "<p>Thank you for the replies.<br>\nThe Measurements settings were these (which are, I guess, suitable for my task):<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/0/50b960c063953d66bce756a38ccfcceb5224d085.png\" alt=\"Screenshot from 2023-03-13 14-01-41\" data-base62-sha1=\"bw7mOI9EteryVxZDcg236xobyoB\" width=\"349\" height=\"480\"></p>\n<p>The procedure carried out was:</p>\n<pre><code class=\"lang-auto\">open(\"~/Documents/LAB/Growth/exp_7/CT_e-4.jpg\");\n//setTool(\"oval\");\nmakeOval(632, 334, 618, 586);\nmakeOval(572, 334, 678, 586);\nmakeOval(572, 334, 598, 586);\nmakeOval(552, 288, 618, 632);\nsetBackgroundColor(0, 0, 0);\nrun(\"Clear Outside\");\nrun(\"16-bit\");\nsetAutoThreshold(\"Default dark no-reset\");\n//run(\"Threshold...\");\nsetOption(\"BlackBackground\", true);\nrun(\"Convert to Mask\");\nrun(\"Close\");\n//run(\"Threshold...\");\n//setThreshold(229, 255);\nrun(\"Convert to Mask\");\nrun(\"Close\");\nrun(\"Analyze Particles...\", \"size=2-Infinity display clear summarize add\");\n</code></pre>\n<p>Which gave:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/58d3d13d768866a746d0d609307e023cd99100c8.png\" alt=\"Screenshot from 2023-03-13 14-08-26\" data-base62-sha1=\"cFNPHLvSp0kkBvCZcIXMSORKC0g\" width=\"333\" height=\"348\"></p>\n<p>How can I improve the analysis?<br>\nThanks</p>", "<p>Regarding your initial problems:<br>\n1- I suspect your problem is that the colonies are not well separated so are not adequately segmented. You may want to try something a bit more elaborate than simple thresholding. If you showed the original image, people may come up with suggestions.<br>\n2- The simplest way of comparing colonie sizes across plates is to image all plates under the same conditions, not just magnification but also illumination as differences could also affect segmentation.</p>", "<p>I don\u2019t think there is a problem of segmentation: the colonies are correctly outlined by the threshold tool. If I apply segmentation I get an error instead.<br>\nThis is the original image:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000.jpeg\" data-download-href=\"/uploads/short-url/lUM4sXeN8MjmqGpT15eDoW33KAU.jpeg?dl=1\" title=\"CT_e-4\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg\" alt=\"CT_e-4\" data-base62-sha1=\"lUM4sXeN8MjmqGpT15eDoW33KAU\" width=\"375\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_750x1000.jpeg 2x\" data-dominant-color=\"5D4019\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">CT_e-4</span><span class=\"informations\">1200\u00d71600 346 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>and this is the watershed:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f6695be843c9939ae3ebc4a08d942dec6d195b0e.png\" alt=\"Screenshot from 2023-03-13 15-14-39\" data-base62-sha1=\"z9RbG6TmzKvWlCX8bYXqiINNNUO\" width=\"333\" height=\"348\"></p>\n<p>The issue is that a single colony is counted several times\u2026</p>", "<aside class=\"quote no-group quote-modified\" data-username=\"Luigi_Marongiu\" data-post=\"5\" data-topic=\"78451\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/luigi_marongiu/40/68914_2.png\" class=\"avatar\"> Luigi Marongiu:</div>\n<blockquote>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f6695be843c9939ae3ebc4a08d942dec6d195b0e.png\" alt=\"Screenshot from 2023-03-13 15-14-39\" data-base62-sha1=\"z9RbG6TmzKvWlCX8bYXqiINNNUO\" width=\"333\" height=\"348\"></p>\n</blockquote>\n</aside>\n<p>Hi <a class=\"mention\" href=\"/u/luigi_marongiu\">@Luigi_Marongiu</a>,<br>\nJust to touch on one point, this \u201cspider web\u201d effect in watershed is quite a common issue when working with Binary masks and results from background not being set correctly. Can you check in <code>[Process &gt; Binary &gt; Options]</code> and make sure ther checkbox \u201cBlack Background\u201d is checked.</p>\n<p>This is equivalent to the line in your script: <code>setOption(\"BlackBackground\", true);</code></p>", "<p>Thank you, that option was already checked. It should have been unchecked:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/c/3cfd5fe3f5734caa2674fa9db10e07ae9f1d4ad7.png\" alt=\"Screenshot from 2023-03-14 12-55-35\" data-base62-sha1=\"8Hxv2RlfFzaOdQSqjVBOpSkJnSf\" width=\"481\" height=\"499\"></p>\n<p>But I think the issue is about defining the size of the things to count and their circularity.<br>\nI changed a bit the values with <code>run(\"Analyze Particles...\", \"size=5-10 display clear summarize add\");</code> and I got the results above.<br>\nBetter but not good enough. Is there a systematic approach to how to define the size of the colonies for counting?</p>", "<p>Hi <a class=\"mention\" href=\"/u/luigi_marongiu\">@Luigi_Marongiu</a>,</p>\n<p>The following macro might help. It works on the original image you posted here. If your original images are different (e.g. in size) you will need to adapt it, but at least can orient on it.</p>\n<pre><code class=\"lang-auto\">setOption(\"BlackBackground\", true);\noriginal = getTitle();\nrun(\"Duplicate...\", \" \");\ncopy = getTitle();\nrun(\"8-bit\");\nrun(\"Gaussian Blur...\", \"radius=50\");\nsetAutoThreshold(\"Minimum dark\");\nrun(\"Convert to Mask\");\nrun(\"Create Selection\");\nselectWindow(original);\nrun(\"Restore Selection\");\nrun(\"Enlarge...\", \"enlarge=-20\");\nrun(\"Crop\");\nrun(\"Duplicate...\", \" \");\ncopy2 = getTitle();\nrun(\"Select None\");\nrun(\"8-bit\");\nrun(\"Invert\");\nrun(\"Enhance Contrast...\", \"saturated=1 normalize\");\nrun(\"Median...\", \"radius=3\");\nrun(\"Top Hat...\", \"radius=100\");\nsetAutoThreshold(\"Li dark\");\nrun(\"Convert to Mask\");\nrun(\"Analyze Particles...\", \"size=0-Infinity show=Masks exclude in_situ\");\nrun(\"Watershed\");\nrun(\"Analyze Particles...\", \"  show=Nothing display clear summarize add\");\nrun(\"Set Measurements...\", \"area mean standard modal min centroid center perimeter bounding fit shape feret's integrated median skewness kurtosis area_fraction display redirect=None decimal=3\")\nrun(\"Analyze Particles...\", \"  show=Nothing display clear summarize add\");\nclose(copy);\nselectWindow(original);\nroiManager(\"show all without labels\");\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c.jpeg\" data-download-href=\"/uploads/short-url/q22XpKL4Ej9wiISnEsiVAawN6ri.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_690x353.jpeg\" alt=\"image\" data-base62-sha1=\"q22XpKL4Ej9wiISnEsiVAawN6ri\" width=\"690\" height=\"353\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_690x353.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_1035x529.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b673c7d25436577df8510a0ec0a3b8dd9dab996c_2_1380x706.jpeg 2x\" data-dominant-color=\"92836B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1526\u00d7782 285 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Thank you, that is very kind. I am trying to replicate the procedure but I can\u2019t find the command <code> setAutoThreshold</code>; is that an additional plugin? Also, a gaussian blur of 50 was too much, but it worked for me at 2.5.</p>", "<p>Hi <a class=\"mention\" href=\"/u/luigi_marongiu\">@Luigi_Marongiu</a>,</p>\n<p>the gaussian 50 was just to get the background area extracted to restrict the analysis, which is optional.<br>\nthe <code>setAutothreshold</code> method is recorded if you specify in the normal thresholding dialog from the dropdown menu a specific automatic thresholding algorithm. This has the advantage, that it adapts better to slight differences in your imaging setup.<br>\nBest check the <code>Plugins &gt;Macros...&gt;Record...</code> function which records all your clicking steps.</p>", "<p>Thank you but I still cannot get anything from the image.<br>\nI tried with the following macro:</p>\n<pre><code class=\"lang-auto\">open(\"CT_e-4.tif\");\nrun(\"Gaussian Blur...\", \"sigma=2.50\");\nsetAutoThreshold(\"Default no-reset\");\n//run(\"Threshold...\");\nsetAutoThreshold(\"Minimum dark no-reset\");\nsetAutoThreshold(\"Default dark no-reset\");\n//setThreshold(155, 255);\nsetOption(\"BlackBackground\", true);\nrun(\"Convert to Mask\");\nrun(\"Close\");\nrun(\"Convert to Mask\");\nrun(\"Watershed\");\nsaveAs(\"Tiff\", \"/watershed.tif\");\nrun(\"Undo\");\nsetAutoThreshold(\"Default dark no-reset\");\n//run(\"Threshold...\");\n//setThreshold(0, 128);\nrun(\"Convert to Mask\");\nrun(\"Close\");\nrun(\"Invert\");\nrun(\"Watershed\");\nrun(\"Undo\");\nrun(\"Find Maxima...\", \"prominence=10 output=[Point Selection]\");\nsaveAs(\"Tiff\", \"maxima.tif\");\n</code></pre>\n<p>I get only one dot. Is there a video tutorial more advanced than this?<br>\n<a href=\"https://www.youtube.com/watch?v=zwjXbeiviH0\" rel=\"noopener nofollow ugc\">https://www.youtube.com/watch?v=zwjXbeiviH0</a></p>\n<p>Thank you.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/dEBp3eDxyfLkuuObdd7QsL1ID7S.tif\">maxima.tif</a> (1.83 MB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/qxoEu366qbfr9HQyQoeDxrzMgC9.tif\">watershed.tif</a> (1.83 MB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/luigi_marongiu\">@Luigi_Marongiu</a> ,</p>\n<p>The only way to adapt the macro in a helpful way we would need the original image. The macro I posted was created for the posted image from you which most likely is different from the original and therefore the macro does not do exactly what it is supposed to do.</p>", "<p>Sure, thank you. Here is the original<br>\nBest regards<br>\nLuigi</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000.jpeg\" data-download-href=\"/uploads/short-url/lUM4sXeN8MjmqGpT15eDoW33KAU.jpeg?dl=1\" title=\"CT_e-4.jpg\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg\" alt=\"CT_e-4.jpg\" data-base62-sha1=\"lUM4sXeN8MjmqGpT15eDoW33KAU\" width=\"375\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_375x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_562x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/9/9998a29222daf2a354a122cb53a18c5647597000_2_750x1000.jpeg 2x\" data-dominant-color=\"5D4019\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">CT_e-4.jpg</span><span class=\"informations\">1200\u00d71600 346 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>So, when I run my macro I posted <a href=\"https://forum.image.sc/t/how-to-properly-count-and-measure-colonies/78451/8\">above</a>, it perfectly works.</p>\n<p>Have you tried to run exactly that macro (unchanged!) on the image you posted here. It should give you exactly what it shows in the image in my post.</p>"], "78461": ["<p>I set a reference system on the image. Drew a segmented line and extracted the x-y coordinates of the points on the segmented line by a macro code. I used these x-y coordinates to tune the parameters of my equation. Then by these parameters and the equation, I calculated the x-y values, which should be close to the initial x-y coordinate.<br>\nNow, I want to draw a curve by the calculated x-y values on the calibrated initial image to find how fit the curve is with the drawn segmented line. Please help me in this matter.</p>", "<p>Use the makeSelection(\u201cpolyline\u201d, xpoints, ypoints) macro function to convert the calculated x-y values into a line selection.</p>\n<p>The following example creates a segmented line, saves it in an overlay, uses the \u201cFit Spline\u201d and \u201cInterpolate\u201d commands to generate a set of x-y values, and creates a line selection from the values.</p>\n<pre><code class=\"lang-auto\">  newImage(\"Untitled\", \"8-bit black\", 350, 300, 1);\n  makeLine(61,43,165,111,239,54,272,172,197,224);\n  run(\"Add Selection...\");\n  run(\"Fit Spline\");\n  run(\"Interpolate\", \"interval=75 smooth adjust\");\n  getSelectionCoordinates(xpoints, ypoints);\n  makeSelection(\"polyline\", xpoints, ypoints)\n  run(\"Add Selection...\");\n</code></pre>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cbb9890c67eb01c5b9df97fb8bbad9e060bfbdea.png\" alt=\"Screenshot\" data-base62-sha1=\"t4eqUR4UwhaiQebNSXepa8yvv5g\" width=\"377\" height=\"368\"></p>", "<p>Thank you for the reply. Could you please explain where I can put my x-y values into the code?</p>\n<p>The values are in an Excel file and are around 500 points. Sorry, but I don\u2019t have any experience in using ImageJ. So, I would appreciate it if you could explain each step.</p>", "<p>In Excel, save the x-y values in .csv (coma-separated values) format. In ImageJ, write a macro that opens the .csv file and uses</p>\n<p>xvalues =Table.getColumn(\u201cx\u201d);<br>\nyvalues =Table.getColumn(\u201cy\u201d);</p>\n<p>to retrieve the values.</p>", "<p>I saved x-y values in .csv. Then, opened it in ImageJ and used the two lines macro code you wrote, but it resulted in this: undefined variables xvalues=Table.getColumn(&lt;\u201cx\u201d&gt;)<br>\nPlease explain to me with which macro and how exactly I can use the x-y values in the csv file to plot a curve on the initial image.</p>", "<aside class=\"quote no-group\" data-username=\"Ali_Amininejad\" data-post=\"5\" data-topic=\"78461\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ali_amininejad/40/68919_2.png\" class=\"avatar\"> Ali :</div>\n<blockquote>\n<p>I saved x-y values in .csv. Then, opened it in ImageJ and used the two lines macro code you wrote, but it resulted in this: undefined variables xvalues=Table.getColumn(&lt;\u201cx\u201d&gt;)</p>\n</blockquote>\n</aside>\n<p>Change the curly quotes to straight quotes. Post the .csv file and I will try to create an example macro that uses the x-y values in the .csv file to plot a curve on an image.</p>", "<p>I have posted the .csv file and also the original image. Thank you very much for your help.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/iapedURsvfQZM1zhwLnbaSyKnhH.csv\">fitted_points15.csv</a> (47.7 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/AajR9p6SnqkPJAy5VwfBHF5qyR0.tif\">final1.tif</a> (8.7 MB)</p>", "<p>Here is a macro that changes the color of the line selection on the image to red, adds it to an overlay, gets the x-y points from the table, converts the points from scaled to unscaled and creates a new selection from them.</p>\n<pre><code class=\"lang-auto\">Roi.setStrokeColor(\"red\");\nrun(\"Add Selection...\");\nxpoints = Table.getColumn(\"x_points\");\nypoints = Table.getColumn(\"y_points\"); \ntoUnscaled(xpoints, ypoints);\nmakeSelection(\"freeline\", xpoints, ypoints);\n</code></pre>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/d/dda600b889d8aff051dab709e72ef6887e549280.png\" alt=\"Screenshot\" data-base62-sha1=\"vCNbbjhKgmu1QN2kgaEhfhhlafK\" width=\"687\" height=\"409\"></p>", "<p>Thank you very much for your follow-up. Unfortunately, I am a beginner and probably incorrectly using the code. This is the error which is appeared.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8.jpeg\" data-download-href=\"/uploads/short-url/xQUy4LarnVW4xvjB8KxNwcoZoEg.jpeg?dl=1\" title=\"Screenshot 2023-03-17 174253\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_690x463.jpeg\" alt=\"Screenshot 2023-03-17 174253\" data-base62-sha1=\"xQUy4LarnVW4xvjB8KxNwcoZoEg\" width=\"690\" height=\"463\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_690x463.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_1035x694.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed42ed27e451a917b2adacc2c6039bc44014a3b8_2_1380x926.jpeg 2x\" data-dominant-color=\"D4D4D4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-17 174253</span><span class=\"informations\">1563\u00d71050 243 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>This looks like a Fiji problem. Try using ImageJ.</p>", "<p>The same error (no selection in line 1) appeared by using ImageJ.<br>\nIs it possible to add x-y values manually to draw the curve on the initial picture (with the<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062.jpeg\" data-download-href=\"/uploads/short-url/9z5STzBpXYOD76Nj6psvidhtYJ4.jpeg?dl=1\" title=\"a\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062_2_689x404.jpeg\" alt=\"a\" data-base62-sha1=\"9z5STzBpXYOD76Nj6psvidhtYJ4\" width=\"689\" height=\"404\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062_2_689x404.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062_2_1033x606.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/430b272238c18dbc3b8121d0af0f7ce048d56062.jpeg 2x\" data-dominant-color=\"CECECE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">a</span><span class=\"informations\">1231\u00d7722 176 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nreference system)?</p>", "<p>Is there a line selection on the Image? (The image you sent had one.) If not, delete the first 2 lines of the macro.</p>", "<p>Now it works. Thank you very much for your guidance.</p>", "<p>You wrote a macro code to read the x-y values from a .csv file and draw the related curve on the initial calibrated image. Here is the code:</p>\n<p>xpoints = Table.getColumn(\u201cx_points\u201d);<br>\nypoints = Table.getColumn(\u201cy_points\u201d);<br>\ntoUnscaled(xpoints, ypoints);<br>\nmakeSelection(\u201cfreeline\u201d, xpoints, ypoints);</p>\n<p>I need to edit the code. So, I can input two different x-y values from two .csv files and draw both of them. So, I would be able to compare two curves. I also need to be able to change the color and thickness of the drawn curves. For example, one curve with yellow dash lines, and the other continuous thick black line. Could you please guide me on how I can change the code for this purpose?</p>\n<p>Thank you very much.</p>"], "78462": ["<p>Hello everyone!<br>\nA new user of imageJ here wish to get your help. I am conducting research on CT angiography images, but I have encountered some problems due to my lack of knowledge of image processing and my unfamiliar use of imageJ.  First of all, my CT image is compressed, and there is no negative HU threshold when it opened through plugins <strong>BIO-formats</strong>. I converted it to carry minus sign through plugins <strong>Convert to signed 16</strong>, and the HU threshold range was still too large after passing process-math-sbtracted 1024. I think it is very important to solve this problem, so I first put forward, The processed threshold distribution range can be seen in the attachment. After solving the first question, I would like to analyze the volume occupied by different components of vascular plaques by ROI, as Professor Erik Meijering has done, but since I am new to imageJ, could someone please guide me on how to process the image afterwards?<br>\nthank you very much!<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/2274cb002376175c2a908a6005dabcead4c0f9a1.png\" alt=\"threshold\" data-base62-sha1=\"4UOsD5pIJpzrxjO3JpbxiwEthQJ\" width=\"390\" height=\"409\"></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5.png\" data-download-href=\"/uploads/short-url/qUXyDKFWW3MprOGjukBfgHCjiMB.png?dl=1\" title=\"analysis\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5_2_690x345.png\" alt=\"analysis\" data-base62-sha1=\"qUXyDKFWW3MprOGjukBfgHCjiMB\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bca8ddc2ee92ed981031cb4c9b38f3af039b64f5.png 2x\" data-dominant-color=\"B8B693\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">analysis</span><span class=\"informations\">1170\u00d7585 313 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d79bc6aaf29dd547510f826a3406435a572480c.png\" data-download-href=\"/uploads/short-url/1Vd1zfua2W9xS3DI3wM98wxEtWY.png?dl=1\" title=\"ways\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d79bc6aaf29dd547510f826a3406435a572480c.png\" alt=\"ways\" data-base62-sha1=\"1Vd1zfua2W9xS3DI3wM98wxEtWY\" width=\"690\" height=\"225\" data-dominant-color=\"EFECE1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ways</span><span class=\"informations\">1059\u00d7346 43.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78463": ["<p>I opened a new <a href=\"https://github.com/ome/ngff/issues/174\" rel=\"noopener nofollow ugc\">github issue</a> proposing that we look to the <a href=\"http://cfconventions.org/Data/cf-conventions/cf-conventions-1.8/cf-conventions.html\" rel=\"noopener nofollow ugc\">Climate and Forecast (CF) Metadata Conventions</a> as a direct model for subsequent versions of OME-NGFF.</p>\n<p>Basically, the climate science field has many of the same metadata problems we have, and they have pretty good solutions for them, and it would probably be wise to use those solutions as much as possible.</p>\n<p>I\u2019m happy to discuss the proposal further here or on github.</p>", "<p>Thanks for raising this - I replied on on the github issue\u2026</p>"], "78464": ["<p>Dear all,</p>\n<p>I am Isaac Vieco-Mart\u00ed, PhD student at the University of Val\u00e8ncia, Spain. Along the last year I had to do a lot of image analysis with QuPath for my PhD and fortunately I find this community. Thanks to you all I have been able to learn a lot and I discovered this amazing field of Image Analysis. So first, I would thank you all for your help!</p>\n<p>This week I faced myself with a new challenge: to create a hexagonal grid to crop an image.<br>\nThe purpose of doing this trimming is to perform a topological analysis like the one that appears in this <a href=\"https://pubmed.ncbi.nlm.nih.gov/31173338/\" rel=\"noopener nofollow ugc\">paper</a>. I find this <a href=\"https://forum.image.sc/t/hexagonal-grid-roi-macro/31465/2\">post</a> by <a class=\"mention\" href=\"/u/mountain_man\">@mountain_man</a> in which he designs a script to do it in Image J. However, I didn\u2019t see anything about hexagonal grids in QuPath. So, I developed some scripts to create hexagonal grids in QuPath. Here is the link to the <a href=\"https://github.com/iviecomarti/Hexagons_QuPath\" rel=\"noopener nofollow ugc\">repository</a> (so exciting to attach the link for the first time).</p>\n<p>I do not have a lot of background in Computer Science (I am a Biotechnologist), so maybe the scripts are not as perfect as they could be, but they work.  For sure we all can improve them <img src=\"https://emoji.discourse-cdn.com/twitter/blush.png?v=12\" title=\":blush:\" class=\"emoji\" alt=\":blush:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>The first I have done is to adapt the script of <a class=\"mention\" href=\"/u/mountain_man\">@mountain_man</a> to make it work in QuPath. You can find the script <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagonal_grid.groovy\" rel=\"noopener nofollow ugc\">here</a>.  Then, I  realised that the hexagons are arranged horizontally, and perhaps one needs the grid in a vertical arrangement. Understanding the hexagon geometry, I modified the script to obtain the <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagonal_grid.groovy\" rel=\"noopener nofollow ugc\">vertical</a> arrangement. Here you can see the results of both grids.<br>\n<strong>Figure 1</strong></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7.png\" data-download-href=\"/uploads/short-url/1YSvLk2suqAoVSxsjwEGMUf7zYb.png?dl=1\" title=\"Diapositiva1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7_2_690x388.png\" alt=\"Diapositiva1\" data-base62-sha1=\"1YSvLk2suqAoVSxsjwEGMUf7zYb\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0de3ea2ff84579cfba4717c700a43ae3cced2dd7.png 2x\" data-dominant-color=\"D4B3B3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Diapositiva1</span><span class=\"informations\">1280\u00d7720 125 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Since we usually work with Whole Slide Images, create such large hexagonal grid could be demanding for our pc. It would be easier to create an hexagonal grid overlaying just the annotation.</p>\n<p>This <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagons_multiple_annotations.groovy\" rel=\"noopener nofollow ugc\">script</a> to gets the width and the height of the bounds of the annotation, and create a hexagonal grid of those dimensions.  Since we can obtain the xCoord and yCoord of the bounds of the original annotation, we can translate the grid to that position to have the overlay (1st column of the figure 2).  With the Geometry tools (this <a href=\"https://www.imagescientist.com/editing-object-shapes-or-types\" rel=\"noopener nofollow ugc\">link</a> is incredible for learning them) we can decide if we want the \u201cintersecting\u201d hexagons (2nd column of the figure 2) or the \u201cwithin\u201d hexagons (3rd column of the figure 2). We can also obtain the same with the <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagons_multiple_annotations.groovy\" rel=\"noopener nofollow ugc\">vertical</a> arrangement.</p>\n<p><strong>Figure 2</strong></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a.png\" data-download-href=\"/uploads/short-url/agYvMH3yKIOfNWBq1aXymXdIluO.png?dl=1\" title=\"Diapositiva2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a_2_690x388.png\" alt=\"Diapositiva2\" data-base62-sha1=\"agYvMH3yKIOfNWBq1aXymXdIluO\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4800ff14a050640b99594ba5096be721cbe8525a.png 2x\" data-dominant-color=\"C8C0BB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Diapositiva2</span><span class=\"informations\">1280\u00d7720 182 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Once you learn the geometry of the hexagon, it is easy to develop other tools. For example, these scripts can create a <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagons_from_centroids.groovy\" rel=\"noopener nofollow ugc\">horizonal</a> or <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagon_from_centroids.groovy\" rel=\"noopener nofollow ugc\">vertical</a> hexagon of the desired size taking the reference of the centroids of the green annotations (1st column of figure 3).  This perhaps can be useful to create hexagons from a coords list in a csv file, as discussed in this <a href=\"https://forum.image.sc/t/contruct-voronoi-polygons-from-a-set-of-points-groovy-script/70998/4\">post</a>. We can also take a circle and create a hexagon which fits <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Horizontal_hexagons/horizontal_hexagon_inside_outside_circle.groovy\" rel=\"noopener nofollow ugc\">inside</a> or <a href=\"https://github.com/iviecomarti/Hexagons_QuPath/blob/615a48f588739ec2024934c93618b7584c8f3816/Vertical_hexagons/vertical_hexagons_inside_outside_circle.groovy\" rel=\"noopener nofollow ugc\">outside</a> the circle (2nd column of figure 3).</p>\n<p><strong>Figure 3</strong></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2.png\" data-download-href=\"/uploads/short-url/uZkvBAiN3PgJG8pQzbI3vCSEQWS.png?dl=1\" title=\"Diapositiva3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2_2_690x388.png\" alt=\"Diapositiva3\" data-base62-sha1=\"uZkvBAiN3PgJG8pQzbI3vCSEQWS\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9300711c47063e0cc4805ed17fbb3d47d5ab0b2.png 2x\" data-dominant-color=\"B7B3B1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Diapositiva3</span><span class=\"informations\">1280\u00d7720 101 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I hope that these scripts can be useful for you.</p>\n<p>Thank you again for all your help!</p>\n<p>Best,</p>\n<p>Isaac</p>", "<p>This looks really nice, thanks for sharing \u2013 and documenting!</p>"], "78467": ["<p>Hi,</p>\n<p>I am trying to solve a curious problem with some VSI files I have from an Olympus slide scanner. When I try to open certain regions of certain series I get an error (attached as <code>Exception.txt</code>). So for example, I can open Series <span class=\"hashtag\">#10</span> without a problem, but not Series <span class=\"hashtag\">#9</span>. And when I feed in XYWH coordinates for Series <span class=\"hashtag\">#9</span> I can open some regions but not other regions.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/u4jY0JpdkPNHmsUyDPaDVaho6UA.txt\">Exception.txt</a> (3.4 KB)</p>\n<p>This makes me think the problem might be corrupted data. But another possibly helpful clue, all the VSI files that have this problem were acquired in the same scanning session. Files from different scanning sessions on different days are not affected.</p>\n<p>I have the same problem either when using Bio-Formats Import in ImageJ or <code>python-bioformats</code>.</p>\n<p>It is tricky to share the data as the VSI files are considerable (~5GB) but I have attached XML data that I read out using python-bioformats package.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/AfuL6dfJV017GClZZwagvyPcKJD.xml\">FT155_metadata.xml</a> (221.0 KB)</p>\n<p>Any ideas would be greatly appreciated!</p>\n<p>Jaime</p>", "<p>Hi Jaime,</p>\n<p>Is the issue happening only when opening a sub-resolution level ? If yes, then your issue could be related to <a href=\"https://forum.image.sc/t/qupath-omero-weird-pyramid-levels/65484/7\" class=\"inline-onebox\">QuPath - OMERO - Weird Pyramid levels - #7 by NicoKiaru</a>.</p>\n<p>If that\u2019s the case, the error will show up when trying to load the right edge of the image - and only for certain vsi files.</p>", "<p>Hi <a class=\"mention\" href=\"/u/jaimemcc\">@jaimemcc</a>, can you give us some examples of regions that are failing for a particular series?</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a> and <a class=\"mention\" href=\"/u/dgault\">@dgault</a>,</p>\n<p>Apologies for the delay in responding and thanks for the questions. I had to spend some time trying to work out if there was anything special about the ROIs that wouldn\u2019t open (e.g. were they always in the same location on the slide?) and concluded that, no, there was nothing special about to ROIs that failed.</p>\n<p>But in trying to work out why I could replicate the issue on one machine but not another I discovered that the md5 checksums of different copies of the same file did not match.</p>\n<p>So in conclusion, the (perhaps boring) solution is that the data were corrupted during transfer from one of our instrument servers to our cloud storage. I have managed to re-upload the data and now everything is working as intended.</p>\n<p>Thanks for your engagement!</p>\n<p>Jaime</p>"], "78482": ["<p>Hi all,</p>\n<p>i try to open a OME-ZARR by using Drag &amp; Drop or from CMD in the latest napari but I always get errors. Any idea what I am doing wrong here?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb.png\" data-download-href=\"/uploads/short-url/y6PBvy8B4PWaOQKSuZS0sS5zRzB.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb_2_690x213.png\" alt=\"image\" data-base62-sha1=\"y6PBvy8B4PWaOQKSuZS0sS5zRzB\" width=\"690\" height=\"213\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb_2_690x213.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb_2_1035x319.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/ef0fa191d442e628d6cd0ae259ca5770dd2c60eb.png 2x\" data-dominant-color=\"393A3F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1363\u00d7422 12 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-plaintext\">napari: 0.4.17\nPlatform: Windows-10-10.0.19045-SP0\nPython: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]\nQt: 5.15.6\nPyQt5: 5.15.7\nNumPy: 1.23.5\nSciPy: 1.10.0\nDask: 2023.1.1\nVisPy: 0.11.0\nmagicgui: 0.6.1\nsuperqt: unknown\nin-n-out: 0.1.6\napp-model: 0.1.1\nnpe2: 0.6.2\n\nOpenGL:\n- GL version: 4.6.0 NVIDIA 512.36\n- MAX_TEXTURE_SIZE: 32768\n\nScreens:\n- screen 1: resolution 1920x1080, scale 2.0\n- screen 2: resolution 1920x1080, scale 1.0\n\n</code></pre>\n<pre><code class=\"lang-bash\">(ia39) PS C:\\Users\\m1srh&gt; napari \"f:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr\" --plugin napari-ome-zarr\nTraceback (most recent call last):\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\Scripts\\napari.exe\\__main__.py\", line 7, in &lt;module&gt;\n    sys.exit(main())\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\__main__.py\", line 561, in main\n    _run()\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\__main__.py\", line 341, in _run\n    viewer._window._qt_viewer._qt_open(\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\_qt\\qt_viewer.py\", line 830, in _qt_open\n    self.viewer.open(\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\components\\viewer_model.py\", line 1014, in open\n    self._add_layers_with_plugins(\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\components\\viewer_model.py\", line 1216, in _add_layers_with_plugins\n    layer_data, hookimpl = read_data_with_plugins(\n  File \"F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\plugins\\io.py\", line 104, in read_data_with_plugins\n    raise ValueError(\nValueError: There is no registered plugin named 'napari-ome-zarr'.\nNames of plugins offering readers are: {'bfio', 'ome-types'}\n</code></pre>\n<p>But the napari-ome-zarr plugin is installed an up-to-date</p>\n<pre><code class=\"lang-plaintext\">MultipleReaderError: Multiple plugins found capable of reading F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr. Select plugin from {'napari': 'napari builtins', 'napari-ome-zarr': 'napari-ome-zarr'} and pass to reading function e.g. `viewer.open(..., plugin=...)`.\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\_qt\\qt_viewer.py:1191, in QtViewer.dropEvent(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, event=&lt;PyQt5.QtGui.QDropEvent object&gt;)\n   1188     else:\n   1189         filenames.append(url.toString())\n-&gt; 1191 self._qt_open(\n        self = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;\n        filenames = ['F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr']\n        shift_down = &lt;PyQt5.QtCore.Qt.KeyboardModifiers object at 0x000000007F46D350&gt;\n        alt_down = &lt;PyQt5.QtCore.Qt.KeyboardModifiers object at 0x000000007F46D2E0&gt;\n   1192     filenames,\n   1193     stack=bool(shift_down),\n   1194     choose_plugin=bool(alt_down),\n   1195 )\n\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\_qt\\qt_viewer.py:848, in QtViewer._qt_open(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, filenames=[r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr'], stack=False, choose_plugin=False, plugin=None, layer_type=None, **kwargs={})\n    838     handle_gui_reading(\n    839         filenames,\n    840         self,\n   (...)\n    845         **kwargs,\n    846     )\n    847 except MultipleReaderError:\n--&gt; 848     handle_gui_reading(filenames, self, stack, **kwargs)\n        filenames = ['F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr']\n        self = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;\n        stack = False\n        kwargs = {}\n\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\_qt\\dialogs\\qt_reader_dialog.py:199, in handle_gui_reading(paths=[r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr'], qt_viewer=&lt;napari._qt.qt_viewer.QtViewer object&gt;, stack=False, plugin_name=None, error=None, plugin_override=False, **kwargs={})\n    197 display_name, persist = readerDialog.get_user_choices()\n    198 if display_name:\n--&gt; 199     open_with_dialog_choices(\n        display_name = 'napari-ome-zarr'\n        persist = True\n        readerDialog = &lt;napari._qt.dialogs.qt_reader_dialog.QtReaderDialog object at 0x000000007F4728B0&gt;\n        readerDialog._extension = 'F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr\\\\'\n        readers = {'napari': 'napari builtins', 'napari-ome-zarr': 'napari-ome-zarr'}\n        paths = ['F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr']\n        stack = False\n        qt_viewer = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;\n        kwargs = {}\n    200         display_name,\n    201         persist,\n    202         readerDialog._extension,\n    203         readers,\n    204         paths,\n    205         stack,\n    206         qt_viewer,\n    207         **kwargs,\n    208     )\n\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\_qt\\dialogs\\qt_reader_dialog.py:292, in open_with_dialog_choices(display_name='napari-ome-zarr', persist=True, extension=r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr\\', readers={'napari': 'napari builtins', 'napari-ome-zarr': 'napari-ome-zarr'}, paths=[r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr'], stack=False, qt_viewer=&lt;napari._qt.qt_viewer.QtViewer object&gt;, **kwargs={})\n    288 plugin_name = [\n    289     p_name for p_name, d_name in readers.items() if d_name == display_name\n    290 ][0]\n    291 # may throw error, but we let it this time\n--&gt; 292 qt_viewer.viewer.open(paths, stack=stack, plugin=plugin_name, **kwargs)\n        plugin_name = 'napari-ome-zarr'\n        paths = ['F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr']\n        qt_viewer.viewer = Viewer(axes=Axes(visible=False, labels=True, colored=True, dashed=False, arrows=True), camera=Camera(center=(0.0, 0.0, 0.0), zoom=1.0, angles=(0.0, 0.0, 90.0), perspective=0.0, interactive=True), cursor=Cursor(position=(1.0, 1.0), scaled=True, size=1, style=&lt;CursorStyle.STANDARD: 'standard'&gt;), dims=Dims(ndim=2, ndisplay=2, last_used=0, range=((0, 2, 1), (0, 2, 1)), current_step=(0, 0), order=(0, 1), axis_labels=('0', '1')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[], scale_bar=ScaleBar(visible=False, colored=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, ticks=True, position=&lt;Position.BOTTOM_RIGHT: 'bottom_right'&gt;, font_size=10.0, box=False, box_color=&lt;class 'numpy.ndarray'&gt; (4,) float32, unit=None), text_overlay=TextOverlay(visible=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, font_size=10.0, position=&lt;TextOverlayPosition.TOP_LEFT: 'top_left'&gt;, text=''), overlays=Overlays(interaction_box=InteractionBox(points=None, show=False, show_handle=False, show_vertices=False, selection_box_drag=None, selection_box_final=None, transform_start=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684790&gt;, transform_drag=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526847F0&gt;, transform_final=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684850&gt;, transform=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526848B0&gt;, allow_new_selection=True, selected_vertex=None)), help='', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=True, mouse_move_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_move at 0x00000000617C1700&gt;], mouse_drag_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_drag at 0x00000000617AE8B0&gt;], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[&lt;function dims_scroll at 0x000000004F253F70&gt;], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={'Shift': &lt;function InteractionBoxMouseBindings.initialize_key_events.&lt;locals&gt;.hold_to_lock_aspect_ratio at 0x00000000617AE700&gt;, 'Control-Shift-R': &lt;function InteractionBoxMouseBindings._reset_active_layer_affine at 0x00000000610CB1F0&gt;, 'Control-Shift-A': &lt;function InteractionBoxMouseBindings._transform_active_layer at 0x00000000610CB280&gt;})\n        stack = False\n        kwargs = {}\n        qt_viewer = &lt;napari._qt.qt_viewer.QtViewer object at 0x00000000527429D0&gt;\n    294 if persist:\n    295     if not extension.endswith(os.sep):\n\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\components\\viewer_model.py:1014, in ViewerModel.open(self=Viewer(axes=Axes(visible=False, labels=True, col...._transform_active_layer at 0x00000000610CB280&gt;}), path=[r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr'], stack=False, plugin='napari-ome-zarr', layer_type=None, **kwargs={})   1011 _path = [_path] if not isinstance(_path, list) else _path\n   1012 if plugin:\n   1013     added.extend(\n-&gt; 1014         self._add_layers_with_plugins(\n        added = []\n        self = Viewer(axes=Axes(visible=False, labels=True, colored=True, dashed=False, arrows=True), camera=Camera(center=(0.0, 0.0, 0.0), zoom=1.0, angles=(0.0, 0.0, 90.0), perspective=0.0, interactive=True), cursor=Cursor(position=(1.0, 1.0), scaled=True, size=1, style=&lt;CursorStyle.STANDARD: 'standard'&gt;), dims=Dims(ndim=2, ndisplay=2, last_used=0, range=((0, 2, 1), (0, 2, 1)), current_step=(0, 0), order=(0, 1), axis_labels=('0', '1')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[], scale_bar=ScaleBar(visible=False, colored=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, ticks=True, position=&lt;Position.BOTTOM_RIGHT: 'bottom_right'&gt;, font_size=10.0, box=False, box_color=&lt;class 'numpy.ndarray'&gt; (4,) float32, unit=None), text_overlay=TextOverlay(visible=False, color=&lt;class 'numpy.ndarray'&gt; (4,) float32, font_size=10.0, position=&lt;TextOverlayPosition.TOP_LEFT: 'top_left'&gt;, text=''), overlays=Overlays(interaction_box=InteractionBox(points=None, show=False, show_handle=False, show_vertices=False, selection_box_drag=None, selection_box_final=None, transform_start=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684790&gt;, transform_drag=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526847F0&gt;, transform_final=&lt;napari.utils.transforms.transforms.Affine object at 0x0000000052684850&gt;, transform=&lt;napari.utils.transforms.transforms.Affine object at 0x00000000526848B0&gt;, allow_new_selection=True, selected_vertex=None)), help='', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=True, mouse_move_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_move at 0x00000000617C1700&gt;], mouse_drag_callbacks=[&lt;function InteractionBoxMouseBindings.initialize_mouse_events.&lt;locals&gt;.mouse_drag at 0x00000000617AE8B0&gt;], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[&lt;function dims_scroll at 0x000000004F253F70&gt;], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={'Shift': &lt;function InteractionBoxMouseBindings.initialize_key_events.&lt;locals&gt;.hold_to_lock_aspect_ratio at 0x00000000617AE700&gt;, 'Control-Shift-R': &lt;function InteractionBoxMouseBindings._reset_active_layer_affine at 0x00000000610CB1F0&gt;, 'Control-Shift-A': &lt;function InteractionBoxMouseBindings._transform_active_layer at 0x00000000610CB280&gt;})\n        _path = ['F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr']\n        kwargs = {}\n        plugin = 'napari-ome-zarr'\n        layer_type = None\n        _stack = False\n   1015             _path,\n   1016             kwargs=kwargs,\n   1017             plugin=plugin,\n   1018             layer_type=layer_type,\n   1019             stack=_stack,\n   1020         )\n   1021     )\n   1022 # no plugin choice was made\n   1023 else:\n   1024     layers = self._open_or_raise_error(\n   1025         _path, kwargs, layer_type, _stack\n   1026     )\n\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\components\\viewer_model.py:1216, in ViewerModel._add_layers_with_plugins(self=Viewer(axes=Axes(visible=False, labels=True, col...._transform_active_layer at 0x00000000610CB280&gt;}), paths=[r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr'], stack=False, kwargs={}, plugin='napari-ome-zarr', layer_type=None)\n   1214 else:\n   1215     assert len(paths) == 1\n-&gt; 1216     layer_data, hookimpl = read_data_with_plugins(\n        paths = ['F:\\\\Testdata_Zeiss\\\\OME_ZARR_Testfiles\\\\w96_A1+A2_test_zarr']\n        stack = False\n        plugin = 'napari-ome-zarr'\n   1217         paths, plugin=plugin, stack=stack\n   1218     )\n   1220 # glean layer names from filename. These will be used as *fallback*\n   1221 # names, if the plugin does not return a name kwarg in their meta dict.\n   1222 filenames = []\n\nFile F:\\Documents\\miniconda3\\envs\\ia39\\lib\\site-packages\\napari\\plugins\\io.py:104, in read_data_with_plugins(paths=[r'F:\\Testdata_Zeiss\\OME_ZARR_Testfiles\\w96_A1+A2_test_zarr'], plugin='napari-ome-zarr', stack=False)\n    102 if plugin not in plugin_manager.plugins:\n    103     names = {i.plugin_name for i in hook_caller.get_hookimpls()}\n--&gt; 104     raise ValueError(\n        trans = &lt;napari.utils.translations.TranslationBundle object at 0x00000000257266D0&gt;\n        plugin = 'napari-ome-zarr'\n        names = {'ome-types', 'bfio'}\n    105         trans._(\n    106             \"There is no registered plugin named '{plugin}'.\\nNames of plugins offering readers are: {names}\",\n    107             deferred=True,\n    108             plugin=plugin,\n    109             names=names,\n    110         )\n    111     )\n    112 reader = hook_caller._call_plugin(plugin, path=npe1_path)\n    113 if not callable(reader):\n\nValueError: There is no registered plugin named 'napari-ome-zarr'.\nNames of plugins offering readers are: {'ome-types', 'bfio'}\n</code></pre>", "<p>Hmm, this looks like a manifestation of this open issue:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/ome/napari-ome-zarr/issues/71\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/ome/napari-ome-zarr/issues/71\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/ome/napari-ome-zarr</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/ome/napari-ome-zarr/issues/71\" target=\"_blank\" rel=\"noopener nofollow ugc\">Support bioformats2raw.layout</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-10-18\" data-time=\"09:52:33\" data-timezone=\"UTC\">09:52AM - 18 Oct 22 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/will-moore\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"will-moore\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c3b4abae5c68aa3acff11ae5b78492a1ed3968a9.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          will-moore\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">As noted in previous discussion at #21, the data generated with:\n\n```$ bioform<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">ats2raw image directory.zarr```\n\ncannot be opened with:\n\n```$ napari directory.zarr```.\n\nWhat should be the expected behaviour in this case?\n\n - Open ALL the images under `directory.zarr/` in multiple layers? The user could then turn off or delete the layers that they don't want to use. This should probably be the default behaviour in the case where there is only 1 image. But what if there are 3, or 100?\n - In the case of an OME-NGFF plate, we show a grid of thumbnails for the first image in each Well. We could try to do the same for non-plate bioformats2raw.layout, but this doesn't give users an easy way to open the Images themselves.\n - Any other way that a napari file-reader plugin can direct users to the images within the `directory.zarr`?</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Doesn\u2019t look like there is any good solution yet, just some draft PR?</p>\n<p>Certainly the error message could be improved though!</p>", "<p>I have also seen the same issue with unknown errors resulting in an unrelated stacktrace and the incorrect <code>There is no registered plugin named 'napari-ome-zarr'</code> message.</p>\n<p>It has also been reported at <a href=\"https://github.com/ome/ome-zarr-py/issues/258\" class=\"inline-onebox\">use declarative representation of the spec \u00b7 Issue #258 \u00b7 ome/ome-zarr-py \u00b7 GitHub</a><br>\nI can\u2019t tell the cause of the issue in your case.</p>\n<p>Can you try to open in <code>ome-ngff-validator</code>?<br>\nYou need to serve the data with CORs headers - I use this python script:</p>\n<pre><code class=\"lang-auto\"># From https://stackoverflow.com/questions/21956683/enable-access-control-on-simple-http-server\n\nfrom http.server import HTTPServer, SimpleHTTPRequestHandler, test\nimport sys\n\nclass CORSRequestHandler (SimpleHTTPRequestHandler):\n    def end_headers (self):\n        self.send_header('Access-Control-Allow-Origin', '*')\n        SimpleHTTPRequestHandler.end_headers(self)\n\nif __name__ == '__main__':\n    test(CORSRequestHandler, HTTPServer, port=int(sys.argv[1]) if len(sys.argv) &gt; 1 else 8000)\n</code></pre>\n<p>then go to <a href=\"https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/path/to/OME_ZARR_Testfiles%5Cw96_A1+A2_test_zarr\">https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/path/to/OME_ZARR_Testfiles\\w96_A1+A2_test_zarr</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a></p>\n<p>Can you maybe give me a hint how to \"construct the correct path?</p>\n<p>So the local path is : c:\\w96_A1+A2_test_zarr\\</p>\n<p>But I struggle with how to add this to:</p>\n<p><a href=\"https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/???\" rel=\"noopener nofollow ugc\">https://ome.github.io/ome-ngff-validator/?source=http://localhost:8000/???</a></p>\n<p>I truied to append the local path in various ways but nothing worked so far. The validator could not find my path so far.</p>", "<p>Sorry, yes - It all depends on where you save the python script.<br>\nIf you save it into <code>\\Testdata_Zeiss\\OME_ZARR_Testfiles\\simple-cors.py</code> then run it from there: <code>python simple-cors.py</code>, the URL will be <code>http://localhost8000/w96_A1+A2_test_zarr</code>.</p>\n<p>I hope I\u2019m not missing something that\u2019s different on Windows here?</p>", "<p>Ok, now it works and i give me:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb.jpeg\" data-download-href=\"/uploads/short-url/zz3QDvZCBWi0woRbpnlkqSVtYzp.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb_2_690x257.jpeg\" alt=\"image\" data-base62-sha1=\"zz3QDvZCBWi0woRbpnlkqSVtYzp\" width=\"690\" height=\"257\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb_2_690x257.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb_2_1035x385.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f942b9be242d7c5c51752134bb2ee1d07955cfeb.jpeg 2x\" data-dominant-color=\"DF8E5F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1091\u00d7407 80.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>OK, so there doesn\u2019t appear to be an image under <code>w96_A1_A2_test_zarr/0/</code> which is where it should be for <code>bioformats2raw</code> layout.</p>\n<p>How was the image generated?<br>\nVia <code>bioformats2raw</code>? or via NGFF-converter? Or custom code?</p>", "<p>I used the NGFF converter (latest version). The structure looks like this:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7d0c0ac6a9154536a9fd7722fee066cd80f4c42.png\" alt=\"image\" data-base62-sha1=\"nWyZfVnEP28qKRE77DCR1ouYo4G\" width=\"316\" height=\"165\"></p>", "<p>Ah, sorry, I should have realised you\u2019re exporting a Plate, not an image.</p>\n<p>The <code>w96_A1_A2_test_zarr/.zattrs</code> file that you\u2019re seeing there with <code>{\"bioformats2raw.layout\":3}</code> should also contain `\u201cplate\u201d: {}`` data. E.g.<br>\n<a href=\"https://ome.github.io/ome-ngff-validator/?source=https://cellpainting-gallery.s3.amazonaws.com/cpg0004-lincs/broad/images/2016_04_01_a549_48hr_batch1/images_zarr/SQ00014812__2016-05-23T20_44_31-Measurement1.ome.zarr\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://ome.github.io/ome-ngff-validator/?source=https://cellpainting-gallery.s3.amazonaws.com/cpg0004-lincs/broad/images/2016_04_01_a549_48hr_batch1/images_zarr/SQ00014812__2016-05-23T20_44_31-Measurement1.ome.zarr</a></p>\n<p>So something seems to have failed during conversion.</p>\n<p>Did the conversion run to completion without errors?<br>\nIf you happen to have a sample plate that you could share, that would be helpful so we can test (I understand this isn\u2019t so easy with plate data).</p>\n<p>I actually haven\u2019t tried NGFF converter with Plates myself. cc <a class=\"mention\" href=\"/u/melissa\">@melissa</a></p>", "<p>The file is here:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.dropbox.com/s/t04gmbku6knxilc/w96_A1%2BA2_test_zarr.zip?dl=0\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/9303f04ca4e54b7a86074e7efd6051d682695017.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.dropbox.com/s/t04gmbku6knxilc/w96_A1%2BA2_test_zarr.zip?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">Dropbox</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9f1f990cbac6b5571218c9046e9dead200fe6f1b.png\" class=\"thumbnail onebox-avatar\" width=\"160\" height=\"160\">\n\n<h3><a href=\"https://www.dropbox.com/s/t04gmbku6knxilc/w96_A1%2BA2_test_zarr.zip?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">w96_A1+A2_test_zarr.zip</a></h3>\n\n  <p>Shared with Dropbox</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thanks - I downloaded the NGFF plate, but I wondered if you could share the original file, so we can test the conversion?</p>", "<p>Here is the corresponding CZI:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.dropbox.com/s/ka6os6djp1iiuyk/w96_A1%2BA2_test.czi?dl=0\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b05b6a5aef581aef17d6d558f8dfa5ef3c697208.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.dropbox.com/s/ka6os6djp1iiuyk/w96_A1%2BA2_test.czi?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">Dropbox</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png\" class=\"thumbnail onebox-avatar\" width=\"160\" height=\"160\">\n\n<h3><a href=\"https://www.dropbox.com/s/ka6os6djp1iiuyk/w96_A1%2BA2_test.czi?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\">w96_A1+A2_test.czi</a></h3>\n\n  <p>Shared with Dropbox</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thanks <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a> for providing the original file and the converted NGFF file.</p>\n<p>I am unable to reproduce the issue with missing metadata using NGFF-Converter 1.1.4 and OS X as the top-level <code>.zattrs</code> contains all the expected elements</p>\n<pre><code class=\"lang-auto\">(base) sbesson@Sebastiens-MacBook-Pro-2 Downloads % head -n 5 w96_A1+A2_test.zarr/.zattrs \n{\n  \"bioformats2raw.layout\" : 3,\n  \"plate\" : {\n    \"columns\" : [ {\n      \"name\" : \"1\"\n</code></pre>\n<p>One difference I found is that the your NGFF seems to have been generated with Bio-Formats 6.12.0 while mine was generated with Bio-Formats 6.11.1:</p>\n<pre><code class=\"lang-auto\">(base) sbesson@Sebastiens-MacBook-Pro-2 Downloads % head -n 6 w96_A1+A2_test_zarr/A/1/0/.zattrs \n{\n  \"multiscales\" : [ {\n    \"metadata\" : {\n      \"method\" : \"loci.common.image.SimpleImageScaler\",\n      \"version\" : \"Bio-Formats 6.12.0\"\n    },\n(base) sbesson@Sebastiens-MacBook-Pro-2 Downloads % head -n 6 w96_A1+A2_test.zarr/A/1/0/.zattrs \n{\n  \"multiscales\" : [ {\n    \"metadata\" : {\n      \"method\" : \"loci.common.image.SimpleImageScaler\",\n      \"version\" : \"Bio-Formats 6.11.1\"\n    },\n</code></pre>\n<p>Which version of NGFF-Converter was used? And under which operating system?</p>", "<p>As far as I remember i used NGFF 1.1.4 under Windows 10.</p>\n<p>I just did run the NGFF converter again and dropped the ZARR into napari and this time I get a different error \u2026</p>\n<p>Log from conversion:</p>\n<pre><code class=\"lang-plaintext\">18:29:17 INFO  c.g.convert.App - Beginning file conversion...\n\n18:29:17 INFO  c.g.c.ConverterTask - Executing bioformats2raw with args [C:\\Temp\\zarr\\w96_A1+A2_test.czi, C:\\Temp\\zarr\\w96_A1+A2_test.zarr, --log-level=INFO]\n18:29:17 INFO  c.g.c.ConverterTask - Working on w96_A1+A2_test.czi\n18:29:18 INFO  l.f.ImageReader - ZeissCZIReader initializing C:\\Temp\\zarr\\w96_A1+A2_test.czi\n18:29:20 INFO  c.g.b.Converter - Using 4 pyramid resolutions\n18:29:20 INFO  c.g.b.Converter - Preparing to write pyramid sizeX 1960 (tileWidth: 1024) sizeY 1416 (tileWidth: 1024) sizeZ 1 (tileDepth: 1) imageCount 2\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 1024] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - chunk read complete 1/8\n18:29:20 INFO  c.g.b.Converter - chunk read complete 2/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760413] time[37] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 4/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760413] time[46] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 0] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 1024, 0] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 1024] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 1024, 1024] to A/1/0/0\n18:29:20 INFO  c.g.b.Converter - chunk read complete 5/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760589] time[33] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 6/8\n18:29:20 INFO  c.g.b.Converter - chunk read complete 7/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760589] time[36] tag[getChunk]\n18:29:20 INFO  o.p.TimingLogger - start[1678987760589] time[36] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 8/8\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Reducing active tileWidth to 980\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/1\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/1\n18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[111] tag[getTileDownsampled]\n18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[111] tag[getTileDownsampled]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 1/2\n18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[112] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 2/2\n18:29:20 INFO  o.p.TimingLogger - start[1678987760704] time[113] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=0 yy=0 zz=0 width=980 height=708 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=1 xx=0 yy=0 zz=0 width=980 height=708 depth=1\n18:29:20 INFO  c.g.b.Converter - Reducing active tileWidth to 490\n18:29:20 INFO  c.g.b.Converter - Reducing active tileHeight to 354\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/2\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/2\n18:29:20 INFO  o.p.TimingLogger - start[1678987760835] time[10] tag[getTileDownsampled]\n18:29:20 INFO  o.p.TimingLogger - start[1678987760835] time[10] tag[getTileDownsampled]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 1/2\n18:29:20 INFO  o.p.TimingLogger - start[1678987760835] time[10] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 2/2\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=0 xx=0 yy=0 zz=0 width=490 height=354 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=1 xx=0 yy=0 zz=0 width=490 height=354 depth=1\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/1/0/3\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/1/0/3\n18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getTileDownsampled]\n18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getTileDownsampled]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 1/2\n18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 2/2\n18:29:20 INFO  o.p.TimingLogger - start[1678987760860] time[3] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=1 xx=0 yy=0 zz=0 width=245 height=177 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=0 xx=0 yy=0 zz=0 width=245 height=177 depth=1\n18:29:20 INFO  c.g.b.Converter - Using 4 pyramid resolutions\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 1024] to A/2/0/0\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/2/0/0\n18:29:20 INFO  c.g.b.Converter - chunk read complete 1/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760876] time[19] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 2/8\n18:29:20 INFO  c.g.b.Converter - chunk read complete 3/8\n18:29:20 INFO  c.g.b.Converter - chunk read complete 4/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760876] time[24] tag[getChunk]\n18:29:20 INFO  o.p.TimingLogger - start[1678987760876] time[25] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=0 zz=0 width=936 height=1024 depth=1\n18:29:20 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 1024, 1024] to A/2/0/0\n18:29:20 INFO  c.g.b.Converter - chunk read complete 5/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760948] time[11] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - chunk read complete 6/8\n18:29:20 INFO  c.g.b.Converter - chunk read complete 8/8\n18:29:20 INFO  o.p.TimingLogger - start[1678987760948] time[13] tag[getChunk]\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=1 xx=0 yy=1024 zz=0 width=1024 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=1024 zz=0 width=936 height=392 depth=1\n18:29:20 INFO  c.g.b.Converter - Reducing active tileWidth to 980\n18:29:20 INFO  c.g.b.Converter - Reducing active tileHeight to 708\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/1\n18:29:21 INFO  o.p.TimingLogger - start[1678987761003] time[49] tag[getTileDownsampled]\n18:29:21 INFO  o.p.TimingLogger - start[1678987761003] time[49] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=0 yy=0 zz=0 width=980 height=708 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=1 xx=0 yy=0 zz=0 width=980 height=708 depth=1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/2\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/2\n18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[10] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - chunk read complete 1/2\n18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[10] tag[getChunk]\n18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[11] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - chunk read complete 2/2\n18:29:21 INFO  o.p.TimingLogger - start[1678987761069] time[12] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=1 xx=0 yy=0 zz=0 width=490 height=354 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=0 xx=0 yy=0 zz=0 width=490 height=354 depth=1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 1, 0, 0, 0] to A/2/0/3\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/2/0/3\n18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getTileDownsampled]\n18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - chunk read complete 1/2\n18:29:21 INFO  c.g.b.Converter - chunk read complete 2/2\n18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getChunk]\n18:29:21 INFO  o.p.TimingLogger - start[1678987761088] time[3] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=1 xx=0 yy=0 zz=0 width=245 height=177 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=0 xx=0 yy=0 zz=0 width=245 height=177 depth=1\n18:29:21 INFO  c.g.b.Converter - Using 5 pyramid resolutions\n18:29:21 INFO  c.g.b.Converter - Preparing to write pyramid sizeX 3066 (tileWidth: 1024) sizeY 2030 (tileWidth: 1024) sizeZ 1 (tileDepth: 1) imageCount 1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 2048] to A/3/0/0\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/0\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/3/0/0\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 0] to A/3/0/0\n18:29:21 INFO  c.g.b.Converter - chunk read complete 1/6\n18:29:21 INFO  c.g.b.Converter - chunk read complete 2/6\n18:29:21 INFO  c.g.b.Converter - chunk read complete 3/6\n18:29:21 INFO  o.p.TimingLogger - start[1678987761108] time[9] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - chunk read complete 4/6\n18:29:21 INFO  o.p.TimingLogger - start[1678987761108] time[9] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=0 zz=0 width=1024 height=1024 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=0 zz=0 width=1024 height=1024 depth=1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 1024] to A/3/0/0\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 1024, 2048] to A/3/0/0\n18:29:21 INFO  c.g.b.Converter - chunk read complete 5/6\n18:29:21 INFO  c.g.b.Converter - chunk read complete 6/6\n18:29:21 INFO  o.p.TimingLogger - start[1678987761129] time[10] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=0 yy=1024 zz=0 width=1024 height=1006 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=2048 yy=0 zz=0 width=1018 height=1024 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=2048 yy=1024 zz=0 width=1018 height=1006 depth=1\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=0 plane=0 xx=1024 yy=1024 zz=0 width=1024 height=1006 depth=1\n18:29:21 INFO  c.g.b.Converter - Reducing active tileHeight to 1015\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 1024] to A/3/0/1\n18:29:21 INFO  o.p.TimingLogger - start[1678987761172] time[26] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=1024 yy=0 zz=0 width=509 height=1015 depth=1\n18:29:21 INFO  o.p.TimingLogger - start[1678987761172] time[37] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=1 plane=0 xx=0 yy=0 zz=0 width=1024 height=1015 depth=1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/2\n18:29:21 INFO  o.p.TimingLogger - start[1678987761222] time[14] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=2 plane=0 xx=0 yy=0 zz=0 width=766 height=507 depth=1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/3\n18:29:21 INFO  o.p.TimingLogger - start[1678987761248] time[2] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - chunk read complete 1/1\n18:29:21 INFO  o.p.TimingLogger - start[1678987761248] time[3] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=3 plane=0 xx=0 yy=0 zz=0 width=383 height=253 depth=1\n18:29:21 INFO  c.g.b.Converter - requesting tile to write at [0, 0, 0, 0, 0] to A/3/0/4\n18:29:21 INFO  o.p.TimingLogger - start[1678987761296] time[1] tag[getTileDownsampled]\n18:29:21 INFO  c.g.b.Converter - chunk read complete 1/1\n18:29:21 INFO  o.p.TimingLogger - start[1678987761296] time[1] tag[getChunk]\n18:29:21 INFO  c.g.b.Converter - Successfully processed chunk; resolution=4 plane=0 xx=0 yy=0 zz=0 width=191 height=126 depth=1\n18:29:21 INFO  c.g.c.ConverterTask - Successfully created: w96_A1+A2_test.zarr\n\n18:29:21 INFO  c.g.c.ConverterTask - Completed conversion of 1 files.\n\n</code></pre>\n<p>The I open napari (up-to-date) and try to drag an drop the zarr folder</p>\n<pre><code class=\"lang-plaintext\">napari: 0.4.17\nPlatform: Windows-10-10.0.19045-SP0\nPython: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]\nQt: 5.15.6\nPyQt5: 5.15.7\nNumPy: 1.23.5\nSciPy: 1.10.0\nDask: 2023.1.1\nVisPy: 0.11.0\nmagicgui: 0.6.1\nsuperqt: unknown\nin-n-out: 0.1.6\napp-model: 0.1.1\nnpe2: 0.6.2\n\nOpenGL:\n  - GL version:  4.6.0 NVIDIA 512.36\n  - MAX_TEXTURE_SIZE: 32768\n\nScreens:\n  - screen 1: resolution 1920x1080, scale 1.0\n\nPlugins:\n  - bfio: 2.3.0 (4 contributions)\n  - napari: 0.4.17 (77 contributions)\n  - napari-accelerated-pixel-and-object-classification: 0.12.3 (36 contributions)\n  - napari-aicsimageio: 0.7.2 (2 contributions)\n  - napari-assistant: 0.4.4 (4 contributions)\n  - napari-brightness-contrast: 0.1.8 (2 contributions)\n  - napari-console: 0.0.7 (0 contributions)\n  - napari-crop: 0.1.7 (2 contributions)\n  - napari-cupy-image-processing: 0.3.1 (38 contributions)\n  - napari-czann-segment: 0.0.16 (2 contributions)\n  - napari-griottes: 0.3.9 (12 contributions)\n  - napari-label-interpolator: 0.1.0 (2 contributions)\n  - napari-layer-details-display: 0.1.5 (2 contributions)\n  - napari-ome-zarr: 0.5.2 (2 contributions)\n  - napari-plugin-search: 0.1.4 (2 contributions)\n  - napari-pyclesperanto-assistant: 0.22.1 (51 contributions)\n  - napari-simpleitk-image-processing: 0.4.5 (104 contributions)\n  - napari-skimage-regionprops: 0.10.0 (2 contributions)\n  - napari-svg: 0.1.6 (2 contributions)\n  - napari-time-slicer: 0.4.9 (2 contributions)\n  - napari-tools-menu: 0.1.19 (0 contributions)\n  - ome-types: 0.3.3 (4 contributions)\n</code></pre>\n<p>and this is what I get:</p>\n<p>ValueError: (\u2018Shapes do not align: %s\u2019, [(177, 245), (177, 245), (253, 383), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245), (177, 245)])</p>", "<p>What si also confusing me is this layout:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e56ec0042c3bcef628622dba2a2755a26216b036.png\" data-download-href=\"/uploads/short-url/wJECxCA7pEfQgA2ycmnr09b0foq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e56ec0042c3bcef628622dba2a2755a26216b036.png\" alt=\"image\" data-base62-sha1=\"wJECxCA7pEfQgA2ycmnr09b0foq\" width=\"690\" height=\"99\" data-dominant-color=\"F6F5F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">730\u00d7105 3.69 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Shouldn\u2019t there only be two folder \u201c1\u201d and \u201c2\u201d because the file has two wells (A1 &amp; A2)?</p>", "<p>Hi <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a></p>\n<p>I found the same as <a class=\"mention\" href=\"/u/s.besson\">@s.besson</a> - and I think the same as you got this time. I see the same error when I try to open in napari.</p>\n<p>The output from my conversion is public and can be browsed at:</p>\n<p><a href=\"https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr</a></p>\n<p>I see a few issues:</p>\n<p>The validator fails since wells have <code>row_index</code> and <code>column_index</code> instead of 'rowIndex<code>and</code>columnIndex`. This looks like a bug in the NGFF validator.</p>\n<p>The <code>plate</code> is missing a <code>version</code> - so it\u2019s being validated with the v0.4 schema.</p>\n<p>Strangely the validator reports that some images in the plate are invalid, but when each Well and Image are opened alone, they seem to be valid.</p>\n<p>E.g. first image: <a href=\"https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr/0/0/0/\" class=\"inline-onebox\">OME-NGFF validator</a><br>\nand third image: <a href=\"https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr/0/0/2/\" class=\"inline-onebox\">OME-NGFF validator</a></p>\n<p>The images themselves are <code>v0.2</code> and missing <code>omero</code> so e.g. Vizarr shows both channels on a single slider: <a href=\"https://hms-dbmi.github.io/vizarr/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1_A2_test.zarr/0/0/0\" class=\"inline-onebox\">vizarr</a></p>\n<p>The error that you\u2019re seeing in <code>napari</code> is that it assumes ALL images are the same size, so it fails to stitch them together.<br>\nThere\u2019s actually a PR at <a href=\"https://github.com/ome/ome-zarr-py/pull/241\" class=\"inline-onebox\">[WIP] Support plate loading for varying well sizes (ref #240) by jluethi \u00b7 Pull Request #241 \u00b7 ome/ome-zarr-py \u00b7 GitHub</a> to address that issue.<br>\nUsing that branch of ome-zarr-py, I can open the plate in napari:</p>\n<p><code>$ napari --plugin napari-ome-zarr w96_A1_A2_test.zarr/</code></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee.png\" data-download-href=\"/uploads/short-url/npfW7SlMPut1fLVwfY9ungk4QGi.png?dl=1\" title=\"Screenshot 2023-03-16 at 23.09.09\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_690x433.png\" alt=\"Screenshot 2023-03-16 at 23.09.09\" data-base62-sha1=\"npfW7SlMPut1fLVwfY9ungk4QGi\" width=\"690\" height=\"433\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_690x433.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_1035x649.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a40ce5b6b8cc28f8a98148ad337be427e419c5ee_2_1380x866.png 2x\" data-dominant-color=\"1A1C1F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-16 at 23.09.09</span><span class=\"informations\">1886\u00d71186 217 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a> and <a class=\"mention\" href=\"/u/s.besson\">@s.besson</a> ,</p>\n<p>ok, that are interesting finds.</p>\n<p>Clearly one issue is that the NGFF converter cannot deal with the preview images, because the CZI contains 2 FoV from 2 wells and an overview of the sample (taken from a preview camera for automatic sample carrier detection).</p>\n<p>Obviously this one has totally different dimensions and metadata compared to the actual image data and should be ignores when creating the OME-ZARR (or stored different).</p>\n<p>When using BioFormats it works fine since it allows to ignore such preview images. Maybe this should become an option for the NGFF converter as well?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0.png\" data-download-href=\"/uploads/short-url/hIKoAVoo7bZyGSTdjjbQCHQBKo0.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_435x500.png\" alt=\"image\" data-base62-sha1=\"hIKoAVoo7bZyGSTdjjbQCHQBKo0\" width=\"435\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_435x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_652x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c33e5d16eb0d7cbb2c023509523c0684d5c48c0_2_870x1000.png 2x\" data-dominant-color=\"E4E4E4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">900\u00d71033 144 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>So the issue with the different sizes at least is now explainable, but the other issue with the not registered plugin is still unclear, correct? Because I have several OME-ZARR where this \u201cunregistered plugin error\u201d comes up, while the same plugin &amp; napari work fine for others.</p>", "<p>Hey <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a><br>\nGreat to see you so active in the OME-Zarr community and I hope the other issues can also get figured out.</p>\n<p>Regarding the PR that <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a> linked to above (<a href=\"https://github.com/ome/ome-zarr-py/pull/241\" rel=\"noopener nofollow ugc\">that one here</a>), it will load plates where wells have irregular sizes (the spec allows that, so eventually the viewers should support that as well). It currently doesn\u2019t scale great with plate sizes though (WIP), so more useful for testing and smaller datasets.</p>", "<p>Cross-linking to <a href=\"https://github.com/glencoesoftware/bioformats2raw/issues/194\" class=\"inline-onebox\">NGFF-Converter 1.1.4 fails to convert CZI file which opens normally in Fiji using BioFormats \u00b7 Issue #194 \u00b7 glencoesoftware/bioformats2raw \u00b7 GitHub</a> where this issue has also been raised and discussed. And re-emphasizing <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a>\u2019s point, both the OME-NGFF specification and the converter currently do not support the conversion of preview images alongside HCS data.</p>\n<p>With the current version of NGFF-Converter (1.1.4), it is possible to ignore these preview images during the conversion of CZI files by passing</p>\n<pre><code class=\"lang-auto\">--options=zeissczi.attachments=false\n</code></pre>\n<p>in the <code>Extra arguments</code> window. This is the command-line equivalent of the checkbox in the configuration window above and this will create a NGFF with two multiscales images (<code>A/1/0</code> and <code>A/2/0</code>) following the 0.4 version of the specification.</p>", "<p>Apologies, I was using an out-of-date version of NGFF converter above.<br>\nWith the latest I get valid NGFF - see <a href=\"https://ome.github.io/ome-ngff-validator/?source=https://minio-dev.openmicroscopy.org/idr/image_sc/w96_A1%2BA2_test.zarr\" class=\"inline-onebox\">OME-NGFF validator</a></p>"], "78484": ["<p>Hi all,</p>\n<p>I have been experiencing difficulties in running AnalyseSkeleton in the ImageJ Extension of QuPath. I came across some very useful post about polygonal measurement with Skeletonizing/Distance Mapping and has successful experience in Fiji. The images were annotated with QuPath so at a certain level I am using them in conjunction.</p>\n<p>Soon I saw some other topics regarding automated (via scripting/macros) measurements and results return to QuPath that involved similar techniques so I attempted to set up a similar macro. However, I was unable to install AnalyzeSkeleton into the QuPath version of ImageJ. I am not sure is the plugin on the Fiji side utilizing ImageJ2. I have visited the <a href=\"http://imagej.net\" rel=\"noopener nofollow ugc\">imagej.net</a> page and the link to the Image J 1.x seemed to be dead. May I ask that is there any way to workaround this issue? Thank you very much.</p>\n<p>Alvin</p>", "<p>There are some documentation on how to install ImageJ plugins or change the ImageJ plugin directory for QuPath at <a href=\"https://qupath.readthedocs.io/en/stable/docs/advanced/imagej.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ImageJ \u2014 QuPath 0.4.3 documentation</a>.</p>\n<p>Assuming that you managed to install the plugin without issues, I have had success with the following QuPath script to run ImageJ macros (in this case, using the Analyze Skeleton (2D/3D) plugin). Note that you will need to adapt some parts for your project (e.g. \u201cClass Name\u201d to whatever class your annotations of interest is; or the parameters as you see fit).</p>\n<pre><code class=\"lang-auto\">//// Run ImageJ macros in QuPath\nimport qupath.imagej.gui.ImageJMacroRunner\n\nparams = new ImageJMacroRunner(getQuPath()).getParameterList()\n\n// Change the value of a parameter, using the JSON to identify the key\nparams.getParameters().get('downsampleFactor').setValue(1)\nparams.getParameters().get('sendROI').setValue(true)\nparams.getParameters().get('sendOverlay').setValue(false)\nparams.getParameters().get('getOverlay').setValue(false)\nparams.getParameters().get('getOverlayAs').setValue('Annotations') // or 'Detections'\nparams.getParameters().get('getROI').setValue(false)\nparams.getParameters().get('clearObjects').setValue(false)\n\nmacro = \"\"\"\nselectWindow(\"Class Name\");\nrun(\"Create Mask\");\nselectWindow(\"Mask\");\nrun(\"Skeletonize (2D/3D)\");\nrun(\"Analyze Skeleton (2D/3D)\", \"prune=none\");\n\"\"\"\n\ndef imageData = getCurrentImageData()\ndef annotations = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Class Name\")}\n\n// Loop through the annotations and run the macro\nfor (annotation in annotations) {\n    ImageJMacroRunner.runMacro(params, imageData, null, annotation, macro)\n//    ij.IJ.run(\"Close All\", \"\");\n}\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"Alvin\" data-post=\"1\" data-topic=\"78484\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png\" class=\"avatar\"> Alvin Lam:</div>\n<blockquote>\n<p>I have visited the <a href=\"http://imagej.net\">imagej.net</a> page and the link to the Image J 1.x seemed to be dead.</p>\n</blockquote>\n</aside>\n<p>Which link do you mean? The IJ links should all point to <a href=\"https://imagej.net/software/imagej/\" class=\"inline-onebox\">ImageJ</a> and seem to be working.</p>", "<p>Hi Yau Mun,</p>\n<p>Thank you very much for the suggestion. That was indeed one of the scripts that I tried. The issue seemed to be the \"run(\u201cAnalyze Skeleton (2D/3D)\u201d part in the ImageJ macro. Manually, I cannot find nor install the plugin hence the script cannot run through.</p>", "<p>Hi Research_Associate,</p>\n<p>This is the page I am referring to.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://imagej.net/plugins/analyze-skeleton/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/17cd920cc3af5597be55618d6d6ea4a54809d2eb.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://imagej.net/plugins/analyze-skeleton/\" target=\"_blank\" rel=\"noopener nofollow ugc\">ImageJ Wiki</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d255a5176a70445402413f516b0a0776a37504cd.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://imagej.net/plugins/analyze-skeleton/\" target=\"_blank\" rel=\"noopener nofollow ugc\">AnalyzeSkeleton</a></h3>\n\n  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nRight at the top where it says \u201cFor the ImageJ 1.x plugin, see [this page]\u201d. I have troubles entering the dedicated link.</p>", "<p>Yep, that whole domain does seem to be gone. It sounded like you meant that <a href=\"http://imagej.net\">imagej.net</a> was down for you.</p>", "<aside class=\"quote no-group\" data-username=\"Alvin\" data-post=\"4\" data-topic=\"78484\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png\" class=\"avatar\"> Alvin Lam:</div>\n<blockquote>\n<p>Manually, I cannot find nor install the plugin hence the script cannot run through.</p>\n</blockquote>\n</aside>\n<p>You shouldn\u2019t need to, it is included in Fiji (if not ImageJ as well), so following the instructions to point to that plugins directory should allow you to use it.</p>", "<p>Hi again Research_Associate,</p>\n<p>I should have mentioned that I have set the plugin folder for QuPath-ImageJ extention same as the standalone Fiji directory. The Default ImageJ plugin list doesn\u2019t seem to have analyse skeleton. It is because of that I have the question of plugin capability of ImageJ 1 vs ImageJ2/Fiji. I assume if the <a href=\"http://image.net\" rel=\"noopener nofollow ugc\">image.net</a> page has a dedicated link for the installation of the plugin for ImageJ1, the Fiji default version should be something that utilizes the counterpart ImageJ2.</p>\n<p>I ran a few test and here are the results</p>\n<ol>\n<li>Copied analyse skeleton .jar file from Fiji directory to ImageJ1 standalone plugin directory &gt; works fine</li>\n<li>Set Fiji plugin directory as QuPath-ImageJ plugin directory &gt; i. Manual operation, nothing popped up; ii. by script, error message</li>\n<li>Set ImageJ1 standalone plugin directory (as of test 1) as QuPath-ImageJ plugin directory &gt; error message</li>\n</ol>", "<p>to be precise, this is the message from test 3</p>\n<p>ImageJ 1.53v; Java 17.0.6 [64-bit]; Mac OS X 13.2.1; 76MB of 8192MB (&lt;1%)</p>\n<p>java.awt.IllegalComponentStateException: The dialog is decorated<br>\nat java.desktop/java.awt.Dialog.setBackground(Unknown Source)<br>\nat ij.gui.GenericDialog.(GenericDialog.java:113)<br>\nat ij.gui.GenericDialog.(GenericDialog.java:97)<br>\nat sc.fiji.analyzeSkeleton.AnalyzeSkeleton_.createSettingsDialog(AnalyzeSkeleton_.java:3434)<br>\nat sc.fiji.analyzeSkeleton.AnalyzeSkeleton_.run(AnalyzeSkeleton_.java:286)<br>\nat ij.plugin.filter.PlugInFilterRunner.processOneImage(PlugInFilterRunner.java:266)<br>\nat ij.plugin.filter.PlugInFilterRunner.(PlugInFilterRunner.java:114)<br>\nat ij.IJ.runUserPlugIn(IJ.java:241)<br>\nat ij.IJ.runPlugIn(IJ.java:205)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.base/java.lang.Thread.run(Unknown Source)</p>\n<p>I am not in anyway a scripting language person.</p>", "<p>Hi <a class=\"mention\" href=\"/u/alvin\">@Alvin</a></p>\n<p>If it helps, I\u2019ve put the Analyze Skeleton and Skeletonize3D plugins I used with QuPath into a ZIP file for you to download.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/fjPe2zp0zcVA8cooaiC7jRuiXXB.zip\">AnalyzeSkeleton_-3.4.2.zip</a> (57.0 KB)</p>", "<aside class=\"quote no-group\" data-username=\"Alvin\" data-post=\"1\" data-topic=\"78484\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png\" class=\"avatar\"> Alvin Lam:</div>\n<blockquote>\n<p>However, I was unable to install AnalyzeSkeleton into the QuPath version of ImageJ. I am not sure is the plugin on the Fiji side utilizing ImageJ2</p>\n</blockquote>\n</aside>\n<p>Just in case, stepping back, AnalyzeSkeleton does not show up in the Plugins menu. That doesn\u2019t explain errors, but it is a part of Fiji <strong>and</strong> ImageJ, not a plugin (well, not in the plugins menu, it is default included for both).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6.jpeg\" data-download-href=\"/uploads/short-url/rrhu09BGEuto3T47Q9NcA9A6Vx4.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_690x365.jpeg\" alt=\"image\" data-base62-sha1=\"rrhu09BGEuto3T47Q9NcA9A6Vx4\" width=\"690\" height=\"365\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_690x365.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_1035x547.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0503108db5ede2e8b3714e4d49392ca3ccc60d6_2_1380x730.jpeg 2x\" data-dominant-color=\"A5AFAE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1759\u00d7931 252 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>ImageJ2 is not Fiji, it\u2019s a coding framework that some parts of Fiji use, but no parts of ImageJ1 should use. Just because something is in Fiji does not mean it uses ImageJ2.</p>\n<aside class=\"quote no-group\" data-username=\"Alvin\" data-post=\"8\" data-topic=\"78484\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/a/b5a626/40.png\" class=\"avatar\"> Alvin Lam:</div>\n<blockquote>\n<p>I assume if the <a href=\"http://image.net\">image.net</a> page has a dedicated link for the installation of the plugin for ImageJ1, the Fiji default version should be something that utilizes the counterpart ImageJ2.</p>\n</blockquote>\n</aside>\n<p>Reasonable, but no. Just an older version that doesn\u2019t seem to be maintained.</p>\n<p>The script as <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a> works for me using the Fiji plugins directory, or the default ImageJ. Not sure if you have changed your Fiji in some way for it not to work.</p>", "<p>Hi <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a> ,</p>\n<p>I will for sure have a try. Once again thank you very much.</p>", "<p>Hi <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>,</p>\n<p>Thank you very much for the explanation. I believe the error was due to something wrong in my QuPath or ImageJ build then. I don\u2019t mind to do things manually if necessary. Maybe a clean install may help.</p>"], "78485": ["<p>Hi Everyone,</p>\n<p>Now I\u2019m trying to get the label and macro image in a czi file using python. However, I searched many libraries and I cannot solve this problem. Is there anyone know how to access the label image in a czi file like the below screenshot in Zen Blue?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919.png\" data-download-href=\"/uploads/short-url/t8G41E0MYO50WkOLnxbTATlugsh.png?dl=1\" title=\"Label in Zen Blue\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919_2_690x253.png\" alt=\"Label in Zen Blue\" data-base62-sha1=\"t8G41E0MYO50WkOLnxbTATlugsh\" width=\"690\" height=\"253\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919_2_690x253.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc3a2f61d18976b0bfd0a4571d00458a97957919.png 2x\" data-dominant-color=\"92938D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Label in Zen Blue</span><span class=\"informations\">1016\u00d7373 351 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks a lot even if tell me it cannot be accessed now <img src=\"https://emoji.discourse-cdn.com/twitter/melting_face.png?v=12\" title=\":melting_face:\" class=\"emoji\" alt=\":melting_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>With Bioformats the label &amp; macro images of the slide are the last two resolutions of the image.</p>\n<pre><code class=\"lang-auto\">b.imRead = new ImageReader();\nint macro = b.imRead.getSeriesCount() - 1;\nint label = b.imRead.getSeriesCount() - 2;\nb.imRead.setSeries(macro);\nbyte[] bts = b.imRead.openBytes(0, tilex, tiley, tilesx, tilesy);\n</code></pre>"], "78488": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25.jpeg\" data-download-href=\"/uploads/short-url/7cA4lkgPiZsxqFvoiETwig5KbiZ.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_690x388.jpeg\" alt=\"image\" data-base62-sha1=\"7cA4lkgPiZsxqFvoiETwig5KbiZ\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/327b50164c972a77cc84e092b90cac2002da5c25_2_1380x776.jpeg 2x\" data-dominant-color=\"B9B8BB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71080 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hello, guys! I\u00b4m trying to understand what I am doing wrong. It is the second time it happens to me and I cannot access the annotations I have done in my project.</p>\n<p>I got a message saying that I was out of memmory after running a cell detection and then a classifier. I could see the detections and also the categories. Then when I tried to save, it crashed and now this message appears and I cannot access the image.</p>\n<p>The last time I had to redo the annotations that took me a long time\u2026 Now it happens again\u2026 Any inputs about how to avoid this?</p>", "<p>Hi Leonardo,</p>\n<p>Sorry to hear you are running into issues and having to repeat steps. Can I ask how much RAM is in the computer you are using?</p>\n<p>There is the option to increase the memory available to QuPath via the preferences but I believe the default is 50% of the RAM in the machine. It\u2019s wise to not increase it much more than this for the computers sake if you don\u2019t have a lot of RAM in the first place (better QuPath grinds to a halt instead of the PC).</p>\n<p>Thanks,</p>\n<p>Fiona</p>", "<p>It is a 8Gb but I used to keep about 80% for QuPath. And indeed, never have issues with other softwares, even QuPath when crashed. With the new version, not sure how to set the memmory to be used\u2026</p>", "<p>When QuPath crashes, if you copy the information from <em>View \u2192 Show log</em> then that can help us track down the source of the problem.</p>\n<p>I\u2019m afraid the error message you see now suggests that the data for that image is unavailable, but it can\u2019t explain why (since that depends upon the original crash).</p>\n<aside class=\"quote no-group\" data-username=\"llordello\" data-post=\"3\" data-topic=\"78488\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png\" class=\"avatar\"> Leonardo:</div>\n<blockquote>\n<p>And indeed, never have issues with other softwares, even QuPath when crashed.</p>\n</blockquote>\n</aside>\n<p>There are lots of details that might be relevant. QuPath is likely working with huge images. And if you run a pixel classifier at a high resolution across the entire image, then crashes are quite likely. The \u2018workaround\u2019 is usually to run a pixel classifier at a lower resolution, or only within smaller annotations.</p>\n<aside class=\"quote no-group\" data-username=\"llordello\" data-post=\"1\" data-topic=\"78488\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png\" class=\"avatar\"> Leonardo:</div>\n<blockquote>\n<p>It is the second time it happens to me and I cannot access the annotations I have done in my project.</p>\n</blockquote>\n</aside>\n<p>Do you mean you lose all annotations for all images in the project? I wouldn\u2019t expect a crash when saving one image to affect the others.</p>\n<aside class=\"quote no-group\" data-username=\"llordello\" data-post=\"1\" data-topic=\"78488\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png\" class=\"avatar\"> Leonardo:</div>\n<blockquote>\n<p>running a cell detection and then a classifier.</p>\n</blockquote>\n</aside>\n<p>I\u2019m not sure how you are doing that \u2013 interactively, or via a script (using either \u2018Run\u2019 or \u2018Run for project\u2019).</p>\n<aside class=\"quote no-group\" data-username=\"llordello\" data-post=\"1\" data-topic=\"78488\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/l/ed8c4c/40.png\" class=\"avatar\"> Leonardo:</div>\n<blockquote>\n<p>The last time I had to redo the annotations that took me a long time\u2026 Now it happens again\u2026 Any inputs about how to avoid this?</p>\n</blockquote>\n</aside>\n<p>If you do time-consuming manual annotations, it\u2019s a good idea to back these up regularly \u2013 especially before doing any complex processing that has crashed before. One way is to create a zip file of the entire project folder. Alternatively, you can use <em>File \u2192 Export objects as GeoJSON</em> to store the annotations in a text file. You can then import them again later by dragging them on top of QuPath when the image is open.</p>", "<p>I vaguely remember having this issue happening to me once recently. Fortunately, I managed to recover the hierarchy data from an automatic backup created within the data subfolder for the particular project entry.</p>\n<p>Edit: I cannot remember the project and entry that this happened, so cannot show a screenshot example\u2026 I recall that the data.qpdata had a backup (I think appended as .bak or similar), which I renamed and got my hierarchy data back.</p>", "<p>Dear Pete,</p>\n<p>I\u2019ll keep the log in case it happens again. I increased the memmory, so it is working well again. Unfortunately, I dont know how to recover the backup files that are generated by qupath. What I have done is to keep a duplicate of the folders I have done many annotations.</p>\n<p>Thank you for your support.</p>", "<p>I had this happen to me a few times - as far as I could tell QuPath was running out of memory while trying to save the image data. I noticed that the data file was smaller than the backup even though I\u2019d done more to the image. Replacing the data file with the backup seemed to fix the hierarchy.</p>\n<p>I\u2019ve upgraded to a machine with x4 the memory so <img src=\"https://emoji.discourse-cdn.com/twitter/crossed_fingers.png?v=12\" title=\":crossed_fingers:\" class=\"emoji\" alt=\":crossed_fingers:\" loading=\"lazy\" width=\"20\" height=\"20\"> fingers crossed this shouldn\u2019t be happening again!</p>\n<p>(P.S. It was also an 8Gb machine that was giving me problems - maybe QuPath needs a minimum of 8Gb to itself to process very large images?)</p>", "<p>To restore the backup, right click on the broken image. <em>Open directory</em> \u2192 <em>Project entry\u2026</em> This will take you to the correct directory for that project entry. Delete the <code>data</code> file and rename the the <code>data.bkup</code> file by deleting the extension.</p>\n<p>(You may want to move the broken data file to another location rather than delete it entirely. <em>Or</em> renaming the directory would mean that QuPath would create a new directory when you open the image again. You could copy the backup into the new folder then rename it. That should hopefully fix things!)</p>", "<aside class=\"quote no-group\" data-username=\"ChrisStarling\" data-post=\"7\" data-topic=\"78488\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png\" class=\"avatar\"> Chris Starling:</div>\n<blockquote>\n<p>(P.S. It was also an 8Gb machine that was giving me problems - maybe QuPath needs a minimum of 8Gb to itself to process very large images?)</p>\n</blockquote>\n</aside>\n<p>Vaguely also recall having issues with qptiffs from Vectra not being pyramidal, which stressed the memory far more. Less for QuPath to use for other stuff.</p>", "<p>Hi <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> ,</p>\n<p>This just happened to me again, and I managed to log it.</p>\n<p>What happened to me was that I ran a script for several project entries that took an unusually long time (it was only <code>println getProjectEntry().getImageName()</code>).</p>\n<p>So I did what I normally do to stop the running script by closing QuPath (see <a href=\"https://github.com/qupath/qupath/issues/1167\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Cancelling a script run takes too long to regain control of QuPath \u00b7 Issue #1167 \u00b7 qupath/qupath \u00b7 GitHub</a> and <a href=\"https://forum.image.sc/t/feature-suggestion-option-to-revert-to-last-state-on-cancelling-script-run/74486\" class=\"inline-onebox\">Feature suggestion: option to revert to last state on cancelling script run</a>). This somehow now causes the data.qpdata file to become corrupted and losing hierarchy data, probably due to QuPath unable to read the .qpdata properly.</p>\n<p>I managed to restore the data from the automatic backup as mentioned in the posts above. Screenshot of the automatic backup:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754.jpeg\" data-download-href=\"/uploads/short-url/7sJP8za5Q1axJRQaARH1YO18vjK.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754_2_517x154.jpeg\" alt=\"image\" data-base62-sha1=\"7sJP8za5Q1axJRQaARH1YO18vjK\" width=\"517\" height=\"154\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754_2_517x154.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754_2_775x231.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/4/344ee0e383c6e3214762a7d0a325bb849bdb2754.jpeg 2x\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1028\u00d7307 50.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The full log of the error from QuPath trying to read the corrupted .qpdata file:</p>\n<pre><code class=\"lang-auto\">ERROR: Reached end of file...\nERROR: null\njava.io.EOFException: null\n    at java.base/java.io.ObjectInputStream$BlockDataInputStream.peekByte(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)\n    at java.base/java.io.ObjectInputStream$FieldValues.&lt;init&gt;(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)\n    at qupath.lib.objects.PathCellObject.readExternal(PathCellObject.java:94)\n    at java.base/java.io.ObjectInputStream.readExternalData(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)\n    at qupath.lib.objects.PathObject.readExternal(PathObject.java:1201)\n    at java.base/java.io.ObjectInputStream.readExternalData(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)\n    at java.base/java.io.ObjectInputStream$FieldValues.&lt;init&gt;(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)\n    at java.base/java.io.ObjectInputStream.readObject(Unknown Source)\n    at qupath.lib.io.PathIO.readImageDataSerialized(PathIO.java:353)\n    at qupath.lib.io.PathIO.readImageData(PathIO.java:488)\n    at qupath.lib.projects.DefaultProject$DefaultProjectImageEntry.readImageData(DefaultProject.java:696)\n    at qupath.lib.gui.QuPathGUI.openImageEntry(QuPathGUI.java:2991)\n    at qupath.lib.gui.panes.ProjectBrowser.lambda$new$4(ProjectBrowser.java:201)\n    at com.sun.javafx.event.CompositeEventHandler.dispatchBubblingEvent(CompositeEventHandler.java:86)\n    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:234)\n    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:191)\n    at com.sun.javafx.event.CompositeEventDispatcher.dispatchBubblingEvent(CompositeEventDispatcher.java:59)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:58)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.EventUtil.fireEventImpl(EventUtil.java:74)\n    at com.sun.javafx.event.EventUtil.fireEvent(EventUtil.java:54)\n    at javafx.event.Event.fireEvent(Event.java:198)\n    at javafx.scene.Scene$ClickGenerator.postProcess(Scene.java:3599)\n    at javafx.scene.Scene$MouseHandler.process(Scene.java:3903)\n    at javafx.scene.Scene.processMouseEvent(Scene.java:1887)\n    at javafx.scene.Scene$ScenePeerListener.mouseEvent(Scene.java:2620)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:411)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:301)\n    at java.base/java.security.AccessController.doPrivileged(Unknown Source)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler.lambda$handleMouseEvent$2(GlassViewEventHandler.java:450)\n    at com.sun.javafx.tk.quantum.QuantumToolkit.runWithoutRenderLock(QuantumToolkit.java:424)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler.handleMouseEvent(GlassViewEventHandler.java:449)\n    at com.sun.glass.ui.View.handleMouseEvent(View.java:551)\n    at com.sun.glass.ui.View.notifyMouse(View.java:937)\n    at com.sun.glass.ui.win.WinApplication._runLoop(Native Method)\n    at com.sun.glass.ui.win.WinApplication.lambda$runLoop$3(WinApplication.java:184)\n    at java.base/java.lang.Thread.run(Unknown Source)\nERROR: Load ImageData: Cannot invoke \"qupath.lib.objects.hierarchy.PathObjectHierarchy.getRootObject()\" because \"hierarchy\" is null\nERROR: Load ImageData\njava.lang.NullPointerException: Cannot invoke \"qupath.lib.objects.hierarchy.PathObjectHierarchy.getRootObject()\" because \"hierarchy\" is null\n    at qupath.lib.objects.hierarchy.PathObjectHierarchy.setHierarchy(PathObjectHierarchy.java:832)\n    at qupath.lib.io.PathIO.readImageDataSerialized(PathIO.java:412)\n    at qupath.lib.io.PathIO.readImageData(PathIO.java:488)\n    at qupath.lib.projects.DefaultProject$DefaultProjectImageEntry.readImageData(DefaultProject.java:696)\n    at qupath.lib.gui.QuPathGUI.openImageEntry(QuPathGUI.java:2991)\n    at qupath.lib.gui.panes.ProjectBrowser.lambda$new$4(ProjectBrowser.java:201)\n    at com.sun.javafx.event.CompositeEventHandler.dispatchBubblingEvent(CompositeEventHandler.java:86)\n    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:234)\n    at com.sun.javafx.event.EventHandlerManager.dispatchBubblingEvent(EventHandlerManager.java:191)\n    at com.sun.javafx.event.CompositeEventDispatcher.dispatchBubblingEvent(CompositeEventDispatcher.java:59)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:58)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.BasicEventDispatcher.dispatchEvent(BasicEventDispatcher.java:56)\n    at com.sun.javafx.event.EventDispatchChainImpl.dispatchEvent(EventDispatchChainImpl.java:114)\n    at com.sun.javafx.event.EventUtil.fireEventImpl(EventUtil.java:74)\n    at com.sun.javafx.event.EventUtil.fireEvent(EventUtil.java:54)\n    at javafx.event.Event.fireEvent(Event.java:198)\n    at javafx.scene.Scene$ClickGenerator.postProcess(Scene.java:3599)\n    at javafx.scene.Scene$MouseHandler.process(Scene.java:3903)\n    at javafx.scene.Scene.processMouseEvent(Scene.java:1887)\n    at javafx.scene.Scene$ScenePeerListener.mouseEvent(Scene.java:2620)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:411)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler$MouseEventNotification.run(GlassViewEventHandler.java:301)\n    at java.base/java.security.AccessController.doPrivileged(Unknown Source)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler.lambda$handleMouseEvent$2(GlassViewEventHandler.java:450)\n    at com.sun.javafx.tk.quantum.QuantumToolkit.runWithoutRenderLock(QuantumToolkit.java:424)\n    at com.sun.javafx.tk.quantum.GlassViewEventHandler.handleMouseEvent(GlassViewEventHandler.java:449)\n    at com.sun.glass.ui.View.handleMouseEvent(View.java:551)\n    at com.sun.glass.ui.View.notifyMouse(View.java:937)\n    at com.sun.glass.ui.win.WinApplication._runLoop(Native Method)\n    at com.sun.glass.ui.win.WinApplication.lambda$runLoop$3(WinApplication.java:184)\n    at java.base/java.lang.Thread.run(Unknown Source)\n</code></pre>", "<p>Thanks <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a> is the the log from when QuPath was force-quit, or is it from the attempt to read the .qpdata later?</p>\n<p>It looks like it\u2019s from the attempt to read the data (at which time the data is already corrupted), but I\u2019m not sure.</p>", "<p>It is when QuPath is attempting to read the .qpdata later, when the data is already corrupted.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a> I\u2019ve created an issue for this at <a href=\"https://github.com/qupath/qupath/issues/1252\" class=\"inline-onebox\">PathIO doesn't restore backup if writing ImageData fails \u00b7 Issue #1252 \u00b7 qupath/qupath \u00b7 GitHub</a></p>\n<p>Specially QuPath <em>should</em> automatically read from the backup file if the original is missing or broken. The stack trace in your message is really helpful and shows me why that doesn\u2019t always work.</p>"], "78490": ["<p>Where can I download the unformatted data in the starfish instance</p>"], "78493": ["<p>Sharing and working with large 3D image data can be very tedious without the right software. <a href=\"https://webknossos.org/\" rel=\"noopener nofollow ugc\">WEBKNOSSOS</a> makes this very easy. Free accounts on <a href=\"http://webknossos.org\" rel=\"noopener nofollow ugc\">webknossos.org</a> include some storage space to get your started. However, if you already have storage resources, you may not want to upgrade just for storage. With its OME-Zarr support, WEBKNOSSOS can access datasets that are stored externally. In this tutorial, we\u2019ll explain how to convert data into OME-Zarr and how to set up a static file server for use with WEBKNOSSOS.</p>\n<p>Using <a href=\"https://webknossos.org/\" rel=\"noopener nofollow ugc\">webknossos.org</a> instead of a self-hosted WEBKNOSSOS instance has the benefit that we maintain the server, install updates frequently, and backup your annotations. Also, you can upgrade to paid features of WEBKNOSSOS at any time.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df.png\" data-download-href=\"/uploads/short-url/guYxuHlODIU4OHEvafji2udwlVJ.png?dl=1\" title=\"Architecture diagram of WEBKNOSSOS with internal storage and remote storage (e.g. your own server and cloud storage)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df_2_673x500.png\" alt=\"Architecture diagram of WEBKNOSSOS with internal storage and remote storage (e.g. your own server and cloud storage)\" data-base62-sha1=\"guYxuHlODIU4OHEvafji2udwlVJ\" width=\"673\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df_2_673x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73a3306c2c97147e00a30b2bbd9da7c4132711df.png 2x\" data-dominant-color=\"F4F4F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Architecture diagram of WEBKNOSSOS with internal storage and remote storage (e.g. your own server and cloud storage)</span><span class=\"informations\">700\u00d7519 46.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h1>\n<a name=\"getting-your-data-ready-1\" class=\"anchor\" href=\"#getting-your-data-ready-1\"></a>Getting your data ready</h1>\n<p>WEBKNOSSOS supports a range of chunked file formats for external access including OME-Zarr, N5 and Neuroglancer precomputed. We strongly recommend OME-Zarr, because it is best supported. Other file formats, such as TIFF, CZI, IMS or HDF5, need to be converted to OME-Zarr. There are multiple tools available to do that, e.g. bioformats2raw or NGFF-Converter. If you are savvy in Python, you can use the <strong><code>webknossos</code></strong> package:</p>\n<pre><code class=\"lang-python\">import webknossos as wk\n\nds = wk.Dataset.from_images(\n  \"path/to/tiff/stack\",\n  \"dataset_wkw\",\n  voxel_size=(4, 4, 40),\n  data_format=\"zarr\"\n)\nds.compress()\nds.downsample()\n</code></pre>\n<p>The Python library also has features for creating layers and mags (resolution levels) from arbitrary numpy arrays. You can easily integrate that into your existing scripts. Make sure to check out the <a href=\"https://docs.webknossos.org/webknossos-py/examples/dataset_usage.html\" rel=\"noopener nofollow ugc\">examples in the documentation</a>.</p>\n<h1>\n<a name=\"set-up-your-own-storage-server-2\" class=\"anchor\" href=\"#set-up-your-own-storage-server-2\"></a>Set up your own storage server</h1>\n<p>You can also use your own server as storage for WEBKNOSSOS. You need to have a server that is publicly available on the Internet and has a (sub)domain name attached to it. It needs to run a server application with HTTPS support. We also recommend basic auth to prevent unauthorized access to your data.</p>\n<p>In this tutorial, we show how to set up <a href=\"https://caddyserver.com/\" rel=\"noopener nofollow ugc\">Caddy</a> on an Ubuntu server. Caddy is a great choice because it includes automatic HTTPS configuration and is easy to use. Other software such as Apache, nginx or traefik are also great options and there are many tutorials available on how to set them up.</p>\n<p>First, you need to install Caddy as explained in the documentation: <a href=\"https://caddyserver.com/docs/install\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Install \u2014 Caddy Documentation</a>. Next, you need to assign a folder from where the data is served. In our example, we\u2019ll use <code>/opt/webknossos</code>. Go ahead and create that folder.</p>\n<p>Now, we need to configure Caddy. It should be located under <code>/etc/caddy/Caddyfile</code>. Depending on your Linux distribution, it might be located somewhere else. Copy the following content into your Caddyfile. Change the domain name in the first line and generate your own password for basic auth. You can use the <code>caddy hash-password</code> command to generate the password.</p>\n<pre><code class=\"lang-auto\">example.cloud.scm.io {\n  root * /opt/webknossos\n  file_server browse\n  basicauth * {\n    webknossos $2a$14$uGab5vbFo/VH1Jubz39/yOW57uQlPmhT//mbGvT85dDn.xIiqRJam\n  }\n}\n\n</code></pre>\n<p>Reload the Caddy service using <code>sudo systemctl reload caddy</code>. After a few seconds, Caddy should serve your data.</p>\n<p>Now you are ready to add your data to this folder. If you don\u2019t have data of your own, you can download and extract the following dataset to test your setup: <a href=\"https://static.webknossos.org/data/l4_sample.zarr.zip\" rel=\"noopener nofollow ugc\">https://static.webknossos.org/data/l4_sample.zarr.zip</a></p>\n<p>The folder structure should look something like this:</p>\n<pre><code class=\"lang-auto\">/opt/webknossos/\n\u2514\u2500\u2500 l4_sample\n    \u251c\u2500\u2500 .zgroup\n    \u251c\u2500\u2500 color\n    \u2502   \u251c\u2500\u2500 .zattrs\n    \u2502   \u251c\u2500\u2500 .zgroup\n    \u2502   \u251c\u2500\u2500 1\n    \u2502   \u251c\u2500\u2500 2-2-1\n    \u2502   \u251c\u2500\u2500 4-4-1\n    \u2502   \u251c\u2500\u2500 8-8-2\n    \u2502   \u2514\u2500\u2500 16-16-4\n    \u251c\u2500\u2500 datasource-properties.json\n    \u2514\u2500\u2500 segmentation\n        \u251c\u2500\u2500 .zattrs\n        \u251c\u2500\u2500 .zgroup\n        \u251c\u2500\u2500 1\n        \u251c\u2500\u2500 2-2-1\n        \u251c\u2500\u2500 4-4-1\n        \u251c\u2500\u2500 8-8-2\n        \u2514\u2500\u2500 16-16-4\n</code></pre>\n<p>Your server is now fully prepared to serve data to WEBKNOSSOS. Head over to your account on <a href=\"http://webknossos.org\" rel=\"noopener nofollow ugc\">webknossos.org</a> and add your dataset. Go to <a href=\"https://webknossos.org/datasets/upload#remote\" rel=\"noopener nofollow ugc\">\u201cAdd Dataset\u201d &gt; \u201cAdd Remote Dataset</a>\u201d and enter the URL to your dataset, e.g. <a href=\"https://example.cloud.scm.io/l4_sample\" rel=\"noopener nofollow ugc\">https://example.cloud.scm.io/l4_sample</a>. Click add layer and voil\u00e1 your data should be imported and ready to be visualized, annotated and shared.</p>\n<p><div class=\"large-image-placeholder\"><a href=\"https://miro.medium.com/v2/resize:fit:700/1*sjx5CSylNzqiMx8aDaf9jA.gif\" target=\"_blank\" rel=\"noopener nofollow ugc\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"url\">https://miro.medium.com/v2/resize:fit:700/1*sjx5CSylNzqiMx8aDaf9jA.gif</span><span class=\"help\">(image larger than 20 MB)</span></a></div></p>\n<h1>\n<a name=\"cloud-storage-3\" class=\"anchor\" href=\"#cloud-storage-3\"></a>Cloud storage</h1>\n<p>If you don\u2019t want to manage your own storage servers, you can also buy storage from one of the many cloud providers. Amazon S3 is the most popular choice, but can be quite pricey. Especially costs for egress traffic add up quickly. While Google Cloud storage is a popular alternative, pricing is in the same ballpark. Cheaper alternatives include Backblaze B2, Cloudflare R2, and Scaleway Object Storage.</p>\n<p>To use that, simply create an account with the provider of your choice, create a storage bucket, upload some datasets, and fetch the credentials. Now, you can import the data into WEBKNOSSOS using the URL, e.g. s3://webknossos-zarr/demodata/l4_sample, and the corresponding credentials.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867.png\" data-download-href=\"/uploads/short-url/4gEpxHii4ulwGiG3fgIrqYK82WP.png?dl=1\" title=\"\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867_2_690x498.png\" alt=\"\" data-base62-sha1=\"4gEpxHii4ulwGiG3fgIrqYK82WP\" width=\"690\" height=\"498\" role=\"presentation\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867_2_690x498.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1dea9217a16a50030ad98005e3418f3b92a27867.png 2x\" data-dominant-color=\"E7E8EB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\"></span><span class=\"informations\">700\u00d7505 69.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>To learn more about updates to WEBKNOSSOS, follow us on <a href=\"https://twitter.com/webknossos\" rel=\"noopener nofollow ugc\">Twitter</a> or <a href=\"https://mstdn.science/@webknossos\" rel=\"noopener nofollow ugc\">Mastodon</a>. If you haven\u2019t already, go to <a href=\"https://webknossos.org/\" rel=\"noopener nofollow ugc\">webknossos.org</a> and sign up for a free account.</p>"], "78495": ["<p>Hello,</p>\n<p>Has anyone used the Quantile Based Normalization plugin in Fiji? I have tried all kinds of thing and keep getting Null Point Exception errors, and don\u2019t know enough about programming to know what to do with it\u2026</p>\n<blockquote>\n<p>java.lang.NullPointerException<br>\nat util.Quantile_Based_Normalization.processToDirectory(Quantile_Based_Normalization.java:529)<br>\nat util.Quantile_Based_Normalization.run(Quantile_Based_Normalization.java:862)<br>\nat ij.IJ.runUserPlugIn(IJ.java:237)<br>\nat ij.IJ.runPlugIn(IJ.java:203)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.lang.Thread.run(Thread.java:750)</p>\n</blockquote>\n<p>I\u2019ve checked Fiji is up to date, removed spaces from file names and file paths. Both images are identical in size.</p>\n<p>Any thoughts would be appreciated! Thanks!</p>", "<p>Hi <a class=\"mention\" href=\"/u/kda9\">@kda9</a> ,<br>\nthe plugin works without problems for me (on ubuntu).<br>\nDo you get the error before or after the gui comes up?<br>\nBest,<br>\nVolker</p>"], "78497": ["<p><a class=\"attachment\" href=\"/uploads/short-url/24p37YnrV3VOxEOl71pF3uXdG6O.tif\">February 2022_RPG_BCAS-CC1-Olig2 staining 5 week con_5weekcon_left3_3_MIP.ome.tif (rearranged).tif</a> (4.0 MB)</p>\n<p>Hi everybody,</p>\n<p>I am quite new to Qupath. I have images of mouse brain and I am trying to quantify the oligodendrocyte cell population in these images.<br>\nMy images have four channels:</p>\n<ul>\n<li>DAPI</li>\n<li>Olig2 (oligodendrocyte lineage marker)</li>\n<li>CC1 (mature oligodendrocyte marker)</li>\n<li>BCAS (immature oligodendrocyte marker)</li>\n</ul>\n<p>I trained classifiers to recognize Olig2, CC1 and BCAS. However, as BCAS is expressed in oligodendrocyte processes, cells that are not BCAS+ sometimes/often are counted as BCAS+ when an oligodendrocyte process \u2018accidentaly\u2019 lies close to the nucleus of that cell. I can recognize this by eye, but the classifier only gets about 70% of the BCAS+ cells correct, no matter how much I train it, how small I make the cell circumference and which parameters I use (I tried a bunch of different things).</p>\n<p>So, I want to go through all the pictures and correct the assigned populations manually (mostly for BCAS, but also to correct misstakes of the other markers).</p>\n<p>To ease the correction process, I would like to convert the detected cells to dots (one cell type = one annotation) for each category that I find relevant:</p>\n<ul>\n<li>DAPI+ cells</li>\n<li>Olig2+ cells</li>\n<li>Olig2+CC1+ cells</li>\n<li>Olig2+BCAS+ cells</li>\n<li>Olig2+CC1+BCAS+ cells</li>\n</ul>\n<p>It works to create a group of dots for the DAPI+ cells using this macro:</p>\n<pre><code class=\"lang-auto\">selectAllObjects();\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImage\":\"DAPI\",\"requestedPixelSizeMicrons\":0.3,\"backgroundRadiusMicrons\":8.0,\"backgroundByReconstruction\":true,\"medianRadiusMicrons\":0.0,\"sigmaMicrons\":1.5,\"minAreaMicrons\":10.0,\"maxAreaMicrons\":400.0,\"threshold\":15.0,\"watershedPostProcess\":true,\"cellExpansionMicrons\":1.5,\"includeNuclei\":true,\"smoothBoundaries\":true,\"makeMeasurements\":true}');\n\nimport qupath.lib.roi.ROIs\nimport qupath.lib.objects.PathAnnotationObject\n\nxs = []\nys = []\n\ngetCellObjects().forEach {\n    xs &lt;&lt; it.getROI().getCentroidX()\n    ys &lt;&lt; it.getROI().getCentroidY()\n}\n\ndef roi = ROIs.createPointsROI(xs as double[], ys as double[], getCurrentViewer().getImagePlane())\ndef newAnn = new PathAnnotationObject(roi, null)\naddObject(newAnn)\n\nfor (var ann: getAnnotationObjects()) {\n    ann.setName(\"DAPI+cells\")\n}\n</code></pre>\n<p>Then, it also works to select the Olig2+cells using this macro:<br>\nselectAllObjects();</p>\n<pre><code class=\"lang-auto\">runPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImage\":\"DAPI\",\"requestedPixelSizeMicrons\":0.3,\"backgroundRadiusMicrons\":8.0,\"backgroundByReconstruction\":true,\"medianRadiusMicrons\":0.0,\"sigmaMicrons\":1.5,\"minAreaMicrons\":10.0,\"maxAreaMicrons\":400.0,\"threshold\":15.0,\"watershedPostProcess\":true,\"cellExpansionMicrons\":1.5,\"includeNuclei\":true,\"smoothBoundaries\":true,\"makeMeasurements\":true}');\nrunObjectClassifier(\"Olig2\");\nselectObjectsByClassification(\"Olig2\");\n\nimport qupath.lib.roi.ROIs\nimport qupath.lib.objects.PathAnnotationObject\n\nxs = []\nys = []\n\ngetSelectedObjects().forEach {\n    xs &lt;&lt; it.getROI().getCentroidX()\n    ys &lt;&lt; it.getROI().getCentroidY()\n}\n\ndef roi = ROIs.createPointsROI(xs as double[], ys as double[], getCurrentViewer().getImagePlane())\ndef newAnn = new PathAnnotationObject(roi, null)\naddObject(newAnn)\n\n}\n</code></pre>\n<p>However, when I apply the following macro, which attempts to label the Olig2+CC1+ subsets by first deleting the \u2018not Olig2+ cells\u2019, my DAPI+cells annotation and my Olig2+ cell annotations are removed. Does anybody know how to prevent this from happening? I don\u2019t have much coding experience and despite checking all blogposts related to this topic I haven\u2019t managed to work around this. Thank you so much for your help!</p>\n<pre><code class=\"lang-auto\">selectAllObjects();\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImage\":\"DAPI\",\"requestedPixelSizeMicrons\":0.3,\"backgroundRadiusMicrons\":8.0,\"backgroundByReconstruction\":true,\"medianRadiusMicrons\":0.0,\"sigmaMicrons\":1.5,\"minAreaMicrons\":10.0,\"maxAreaMicrons\":400.0,\"threshold\":15.0,\"watershedPostProcess\":true,\"cellExpansionMicrons\":1.5,\"includeNuclei\":true,\"smoothBoundaries\":true,\"makeMeasurements\":true}');\n\nrunObjectClassifier(\"Olig2\");\nselectObjectsByClassification(null);\nclearSelectedObjects(true);\n\nrunObjectClassifier(\"CC1\");\nselectObjectsByClassification(\"CC1\");\n\nimport qupath.lib.roi.ROIs\nimport qupath.lib.objects.PathAnnotationObject\n\nxs = []\nys = []\n\ngetSelectedObjects().forEach {\n    xs &lt;&lt; it.getROI().getCentroidX()\n    ys &lt;&lt; it.getROI().getCentroidY()\n}\n\ndef roi = ROIs.createPointsROI(xs as double[], ys as double[], getCurrentViewer().getImagePlane())\ndef newAnn = new PathAnnotationObject(roi, null)\naddObject(newAnn)\n\n}\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/rianne\">@Rianne</a> I don\u2019t have an answer to your question, but I edited your post to format the code so it is more readable for others (using <code>&lt;/&gt;</code> in the toolbar when editing).</p>", "<p>Thank you! <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Weird, totally missed this one, but I\u2019d recommend using <code>resetSelection()</code> before selecting new objects.</p>"], "78499": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503.jpeg\" data-download-href=\"/uploads/short-url/cpy0Ps2y4BNWIgg7eM2PetbwFCr.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_499x499.jpeg\" alt=\"image\" data-base62-sha1=\"cpy0Ps2y4BNWIgg7eM2PetbwFCr\" width=\"499\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_499x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_748x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503_2_998x998.jpeg 2x\" data-dominant-color=\"82B2BF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1800\u00d71801 275 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The <a href=\"https://quarep.org/\">QUAREP-LiMi</a> (<em>\u201cQuality Assessment and Reproducibility for Instruments &amp; Images in Light Microscopy\u201d</em>) is a group of enthusiastic light microscopists from Academia and Industry all interested in improving quality assessment (QA) and quality control (QC) in light microscopy.</p>", ""], "78502": ["<p>Keep getting this error:</p>\n<p>Error using flim_omero_logon_manager/Omero_logon (line 125)<br>\nJava exception occurred:<br>\nIce.UnmarshalOutOfBoundsException</p>\n<pre><code>reason = \"\"\n                                                                  \nat IceInternal.BasicStream.readString(BasicStream.java:2038)\n                    \nat omero.sys.EventContext.__readImpl(EventContext.java:166)\n                     \nat Ice.ObjectImpl.__read(ObjectImpl.java:368)\n                                   \nat IceInternal.BasicStream$EncapsDecoder.unmarshal(BasicStream.java:3132)\n       \nat IceInternal.BasicStream$EncapsDecoder10.readInstance(BasicStream.java:3513)\n  \nat IceInternal.BasicStream$EncapsDecoder10.readPendingObjects(BasicStream.java:3440)\nat IceInternal.BasicStream.readPendingObjects(BasicStream.java:566)\n             \nat omero.api.IAdminPrxHelper.end_getEventContext(IAdminPrxHelper.java:4062)\n     \nat omero.api.IAdminPrxHelper.getEventContext(IAdminPrxHelper.java:3935)\n         \nat omero.api.IAdminPrxHelper.getEventContext(IAdminPrxHelper.java:3922)\n</code></pre>\n<p>Error in front_end_menu_controller/menu_login_callback (line 369)</p>\n<p>Error in front_end_menu_controller&gt;@(varargin)obj.menu_login_callback(varargin{:})</p>\n<p>Error in EC (line 7)</p>\n<p>Error in front_end_menu_controller&gt;@(x,y)EC(fcn) (line 263)</p>", "<p>the error is due to the fact that the version of the library used by FlimFIT to connect to the OMERO server is out-of-date.<br>\nFLIMFit connection to OMERO has not been updated for a while now.</p>\n<p>Could you add the <code>flimfit</code> tag to the post so people using or working on FlimFit are aware?<br>\nThanks<br>\nCheers<br>\nJmarie</p>"], "78504": ["<p>Hi,</p>\n<p>I am wondering what is the reason behind very long analysis times in CP when analysing multiple folders in one run. A single plate takes 30-40 minutes, but when I put all 27 folders (=plates), the analysis is much longer than 40 min * 27. It goes on for. 3-4 days, instead &lt;1 day.</p>\n<p>Anyone knows any explanation for this?</p>\n<p>Best,<br>\nBartek</p>", "<p>It looks like you\u2019re running out of memory. Check if this is the case by monitoring memory usage while running your analysis. If that\u2019s not the case, you\u2019ll have to provide more info.</p>", "<p>Thanks for your reply!</p>\n<p>I\u2019ve restarted the analysis to now monitor the memory usage. When I saw the analysis running slow today, there was still plenty memory left (out of 512 GB, less than 5% was being used). I\u2019ll report back how does the resource utilisation look like when it\u2019s actively processing the entire set.</p>\n<p>What kind of info will be useful in this case?</p>", "<p>Within the first 30 minutes, the high point of usage was 99% for CPU and 17% memory (around 70 GB). My pipeline isn\u2019t very complex (attached screenshot), but I wonder whether the ExportToSpreadhseet isn\u2019t the cause for slow processing. It\u2019s over 35000 image sets in the end\u2026</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b.jpeg\" data-download-href=\"/uploads/short-url/dvn0zXIQZZEXD6tHvJQ3hJQSLEL.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_690x340.jpeg\" alt=\"image\" data-base62-sha1=\"dvn0zXIQZZEXD6tHvJQ3hJQSLEL\" width=\"690\" height=\"340\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_690x340.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_1035x510.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_1380x680.jpeg 2x\" data-dominant-color=\"EEEEED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7948 153 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<blockquote>\n<p>What kind of info will be useful in this case?</p>\n</blockquote>\n<p>Operating system and CP version but also possibly some details of the workflow.</p>", "<blockquote>\n<p>whether the ExportToSpreadhseet isn\u2019t the cause for slow processing</p>\n</blockquote>\n<p>If that\u2019s the case, try exporting to database instead.</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5.jpeg\" data-download-href=\"/uploads/short-url/5HG9zIs3zlq9G1doLBjgQ2hXdbv.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_690x199.jpeg\" alt=\"image\" data-base62-sha1=\"5HG9zIs3zlq9G1doLBjgQ2hXdbv\" width=\"690\" height=\"199\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_690x199.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_1035x298.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_1380x398.jpeg 2x\" data-dominant-color=\"F8F8F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1830\u00d7528 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>CP 4.2.1</p>\n<p>Workflow screenshot in the previous post.</p>", "<p>Tried it. Usage of resources was the same, but in this case \u2018CellProfiler.exe stopped working\u2019 after 10-15 minutes from the moment it started running modules</p>", "<p>If the database set up is fine then it looks like CP may have some issue writing to disk. Unfortunately I don\u2019t know much about Windows to help with troubleshooting this kind of issue. Maybe someone more knowledgeable will come along.</p>"], "78506": ["<p><a class=\"attachment\" href=\"/uploads/short-url/jQePmHyYW6r8Evkll47Ttihr2ZQ.tif\">DIESTRUS All runs.lif_Run 3 F55 Diestrus A 10_ch03.tif</a> (616.6 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7DHjXTkuinN7E1ikMtkE3wzaqct.tif\">DIESTRUS All runs.lif_Run 3 F55 Diestrus B 32_ch00.tif</a> (878.4 KB)</p>\n<p>Background:<br>\nMarked sections with four stains: Nuclei, Dnmt, Vasopressin and Cre (The last three are just three different proteins). Images attached show two of these stains on the same section as an example.</p>\n<p>Analysis goal:<br>\nI\u2019m trying to understand whether these proteins coexpress with eachother. For example, the cells that are expressing Dnmt, do they also express Cre? Do they also express Vasopressin? The nuclei stain is just to mark cells.</p>\n<p>Challenges:<br>\nI dont know which cell profiler modules I should use for this type of analysis. I tried doing MaskImages and then MeasureObjectIntensity, but I don\u2019t understand what the resulting intensity values mean. Should I use Relate objects instead? Is Cell profiler the right software for this type of analysis?</p>"], "78510": ["<p>Dear all,</p>\n<p>I am trying to do the following:</p>\n<p>This code needs to process Ki67 and aSMA images in a specified directory. It takes each Ki67 and aSMA image and its corresponding ROI (region of interest) file, opens the image, performs several image processing steps such as background subtraction, thresholding, and particle analysis, and then searches for the next corresponding aSMA image to finally calculate the aSMA positive region within the Ki-67 particles.</p>\n<p>The issue is that, from the second loop onwards, the macro searches for the previous \u201c4.tif\u201d  image, instead of continuing the loop, resulting in an error. Please find the code below, any help is appreciated:</p>\n<pre><code class=\"lang-auto\">run(\"Close All\");\nrun(\"Clear Results\");\n\n//kernen input\n\n//aSMA input\ndir = getDirectory(\"Choose Directory 4.\"); \n\nlist=getFileList(dir);\nfor (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder \n\tif(endsWith(list[i], \"4.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\tinput = dir + list[i];\n\t\tinput_ROI = replace(input,\"4.tif\",\"4.zip\");\n\n\t\tprint(\"now processing: \"+input);\n\t\tprint(\"together with: \"+input_ROI);\n\n\t\topen(input);\n\t\t\n\t\t//get image title for later use\n\t\taSMA = getTitle();\n\t\t\n\t\troiManager(\"reset\");\n\t\trun(\"Select None\");\n\n\t\troiManager(\"open\", input_ROI);\n\t\troiManager(\"Show All\");\n\t\t\nroiManager(\"Select\", 2);\nsetBackgroundColor(0, 0, 0);\nrun(\"Clear Outside\");\nrun(\"8-bit\");\n//get image title for later use\n\t\taSMA = getTitle();\n\n//Ki67 input\ndir = getDirectory(\"Choose Directory 3.\"); \n\nlist=getFileList(dir);\nfor (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder \n\tif(endsWith(list[i], \"3.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\tinput = dir + list[i];\n\t\tinput_ROI = replace(input,\"3.tif\",\"3.zip\");\n\n\t\tprint(\"now processing: \"+input);\n\t\tprint(\"together with: \"+input_ROI);\n\n\t\topen(input);\n\t\t\n\t\t//get image title for later use\n\t\tKi67 = getTitle();\n\t\t\n\t\troiManager(\"reset\");\n\t\trun(\"Select None\");\n\n\t\troiManager(\"open\", input_ROI);\n\t\troiManager(\"Show All\");\n\t\nroiManager(\"Select\", 2);\nsetBackgroundColor(0, 0, 0);\nrun(\"Clear Outside\");\nrun(\"8-bit\");\n\n//get image title for later use\n\t\tKi67 = getTitle();\n\t\t\n\t\tStack.setXUnit(\"pixel\");\n\t\trun(\"Properties...\", \"channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000\");\n\t\t\n\t\t\n\t\t\n\t\t//select the window with ki67 staining\n\t\tselectWindow(Ki67);\n\t\t//correct for background in ki67 staining\n\t\trun(\"Subtract Background...\", \"rolling=50\");\n\t\t//threshold based on the ki67 staining and convert to a binary image\n\t\tsetAutoThreshold(\"Default dark\");\n\t\tsetOption(\"BlackBackground\", true);\n\t\trun(\"Convert to Mask\");\n\t\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\t\trun(\"Close-\");\n\t\trun(\"Open\");\n\t\t//watershed to split up nuclei that are lying against each other\n\t\trun(\"Watershed\");\n\t\trun(\"Select All\");\n\t\troiManager(\"Delete\");\n\t\trun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\n\t\t\n\t\t//select the window with aSMA staining\n\t\tselectWindow(aSMA);\n\t\t\n\t\t//threshold based on the aSMA staining and convert to a binary image\n\t\tsetAutoThreshold(\"Default dark\");\n\t\tsetOption(\"BlackBackground\", true);\n\t\trun(\"Convert to Mask\");\n\t\t\n\t\t//select all ROIs from ki67 and transfer to aSMA\n\t\trun(\"Select All\");\n\t\troiManager(\"XOR\");\n\t\tcount = roiManager(\"count\");\n\t\troiManager(\"select\", count-1);\n\t\troiManager(\"Add\");\n\t\t\t\t\t\n\t\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\t\trun(\"Close-\");\n\t\trun(\"Open\");\n\t\t//select all ROIs and dilate in order to catch all potential aSMA  signal\n\t\trun(\"Select All\");\n\t\trun(\"Dilate\");\n\t\trun(\"Dilate\");\n\t\t//watershed to split up dilated ROIs that are lying against each other\n\t\trun(\"Watershed\");\n\n\n//calculate overlap\nimageCalculator(\"AND\", Ki67, aSMA);\nrun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\nwaitForUser(\"Press OK to continue\",\"press ok to continue!\");\nprint(\"aSMA has run\");\n\t\tselectWindow(aSMA);\n\t\tclose();\n\t\tselectWindow(Ki67);\n\t\tclose();\n\t\tList.clear()\n\t}\n}\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/19cent\">@19cent</a>,</p>\n<p>It\u2019s really helpful if you can format your code, not only is it easier to read but also it prevents errors when others want to copy your code. There should be a \u201cPreformatted text\u201d button in the editor.</p>\n<p>Anyway, regarding your issue, from a brief look I would guess it\u2019s a result of using <code>i</code> as your iterating variable in both an outside and inside loop. eg.</p>\n<pre><code class=\"lang-auto\"> for (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder\n</code></pre>\n<p>For each loop when nested, you need to pick a new variable name (eg. <code>j</code>,<code>k</code>, or ideally give it a descriptive name like <code>ki67_loop</code>). Otherwise, the loops will interfere with each other.</p>\n<p>Hope it\u2019s as simple a fix as that!</p>", "<p>Thanks for the swift reply and the suggestion! Unfortunately, the same error keeps popping up</p>\n<pre><code class=\"lang-auto\">// Close all open images and clear previous results\nrun(\"Close All\");\nrun(\"Clear Results\");\n\n//kernen input\n\n//aSMA input\ndir = getDirectory(\"Choose Directory 4.\"); \n\nlist=getFileList(dir);\nfor (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder\nif(endsWith(list[i], \"4.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\tinput = dir + list[i];\n\t\tinput_ROI = replace(input,\"4.tif\",\"4.zip\");\n\n\n\tprint(\"now processing: \"+input);\n\tprint(\"together with: \"+input_ROI);\n\n\topen(input);\n\t\n\t//get image title for later use\n\taSMA = getTitle();\n\t\n\troiManager(\"reset\");\n\trun(\"Select None\");\n\n\troiManager(\"open\", input_ROI);\n\troiManager(\"Show All\");\nroiManager(\"Select\", 2);\nsetBackgroundColor(0, 0, 0);\nrun(\"Clear Outside\");\nrun(\"8-bit\");\n//get image title for later use\naSMA = getTitle();\n\n//Ki67 input\ndir = getDirectory(\"Choose Directory 3.\"); \n\nlist=getFileList(dir);\nfor (j=0; j&lt;list.length; j++){ // for loop to parse through names in main folder\nif(endsWith(list[j], \"3.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\tinput = dir + list[j];\n\t\tinput_ROI = replace(input,\"3.tif\",\"3.zip\");\n\n\n\tprint(\"now processing: \"+input);\n\tprint(\"together with: \"+input_ROI);\n\n\topen(input);\n\t\n\t//get image title for later use\n\tKi67 = getTitle();\n\t\n\troiManager(\"reset\");\n\trun(\"Select None\");\n\n\troiManager(\"open\", input_ROI);\n\troiManager(\"Show All\");\nroiManager(\"Select\", 2);\nsetBackgroundColor(0, 0, 0);\nrun(\"Clear Outside\");\nrun(\"8-bit\");\n\n//get image title for later use\nKi67 = getTitle();\n\nStack.setXUnit(\"pixel\");\n\trun(\"Properties...\", \"channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000\");\n\t\n\t\n\t\n\t//select the window with ki67 staining\n\tselectWindow(Ki67);\n\t//correct for background in ki67 staining\n\trun(\"Subtract Background...\", \"rolling=50\");\n\t//threshold based on the ki67 staining and convert to a binary image\n\tsetAutoThreshold(\"Default dark\");\n\tsetOption(\"BlackBackground\", true);\n\trun(\"Convert to Mask\");\n\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\trun(\"Close-\");\n\trun(\"Open\");\n\t//watershed to split up nuclei that are lying against each other\n\trun(\"Watershed\");\n\trun(\"Select All\");\n\troiManager(\"Delete\");\n\trun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\n\t\n\t//select the window with aSMA staining\n\tselectWindow(aSMA);\n\t\n\t//threshold based on the aSMA staining and convert to a binary image\n\tsetAutoThreshold(\"Default dark\");\n\tsetOption(\"BlackBackground\", true);\n\trun(\"Convert to Mask\");\n\t\n\t//select all ROIs from ki67 and transfer to aSMA\n\trun(\"Select All\");\n\troiManager(\"XOR\");\n\tcount = roiManager(\"count\");\n\troiManager(\"select\", count-1);\n\troiManager(\"Add\");\n\t\t\t\t\n\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\trun(\"Close-\");\n\trun(\"Open\");\n\t//select all ROIs and dilate in order to catch all potential aSMA  signal\n\trun(\"Select All\");\n\trun(\"Dilate\");\n\trun(\"Dilate\");\n\t//watershed to split up dilated ROIs that are lying against each other\n\trun(\"Watershed\");\n//calculate overlap\nimageCalculator(\"AND\", Ki67, aSMA);\nrun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\nwaitForUser(\"Press OK to continue\",\"press ok to continue!\");\nprint(\"aSMA has run\");\nselectWindow(aSMA);\nclose();\nselectWindow(Ki67);\nclose();\nList.clear()\n}\n}\n\n\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"19cent\" data-post=\"3\" data-topic=\"78510\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/19cent/40/68978_2.png\" class=\"avatar\"> 19cent:</div>\n<blockquote>\n<p>the same error keeps popping up</p>\n</blockquote>\n</aside>\n<p>Can you clarify what is the error message? Also, if you can provide even one of each image, then it would enable other people to replicate your analysis and more easily solve the problem.</p>\n<p>Thanks!</p>", "<p>sure! thanks again for the quick reply. The following error message is popping up:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/1/115735aee8706756bd3c57062b8dafaca083779d.png\" alt=\"image\" data-base62-sha1=\"2toXHsqvcy91dZ5tzvOM0TI1l5X\" width=\"340\" height=\"118\"><br>\nThis error messgae is caused by line 92 (selectwindow(asma)) in the second run</p>\n<p>and the two images:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/qKrNlnCHmC03x3G15yW7EvxMyXE.tif\">M1_70_c3.tif</a> (5.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/rHRtmPpGxyjHH5dNZ5n0ih3aQo9.tif\">M1_70_c4.tif</a> (13.2 MB)</p>\n<p>please note that the macro starts with the image called m11_34 and then proceeds onto M11_45</p>", "<p>I can\u2019t replicate the problem <em>exactly</em> but it looks like you\u2019re missing a pair of closing braces.</p>\n<p>Generally, it\u2019s good practice (although not required) to indent your code blocks for each nested loop to keep track of when you close each block. With the addition of two closing braces, the code below seems to work:</p>\n<pre><code class=\"lang-auto\">// Close all open images and clear previous results\nrun(\"Close All\");\nrun(\"Clear Results\");\n\n//kernen input\n\n//aSMA input\ndir = getDirectory(\"Choose Directory 4.\"); \nfor (i=0; i&lt;list.length; i++){ // for loop to parse through names in main folder\n\tif(endsWith(list[i], \"4.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\tinput = dir + list[i];\n\t\tinput_ROI = replace(input,\"4.tif\",\"4.zip\");\n\t\tprint(\"now processing: \"+input);\n\t\tprint(\"together with: \"+input_ROI);\n\n\t\topen(input);\t\n\t\t//get image title for later use\n\t\taSMA = getTitle();\n\t\n\t\troiManager(\"reset\");\n\t\trun(\"Select None\");\n\n\t\troiManager(\"open\", input_ROI);\n\t\troiManager(\"Show All\");\n\t\troiManager(\"Select\", 2);\n\t\tsetBackgroundColor(0, 0, 0);\n\t\trun(\"Clear Outside\");\n\t\trun(\"8-bit\");\n\t\t//get image title for later use\n\t\taSMA = getTitle();\n\n\t\t//Ki67 input\n\t\tdir = getDirectory(\"Choose Directory 3.\"); \n\n\t\tlist=getFileList(dir);\n\t\t\tfor (j=0; j&lt;list.length; j++){ // for loop to parse through names in main folder\n\t\t\t\tif(endsWith(list[j], \"3.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\t\t\tinput = dir + list[j];\n\t\t\t\tinput_ROI = replace(input,\"3.tif\",\"3.zip\");\n\n\n\t\t\t\tprint(\"now processing: \"+input);\n\t\t\t\tprint(\"together with: \"+input_ROI);\n\n\t\t\t\topen(input);\n\t\n\t\t\t\t//get image title for later use\n\t\t\t\tKi67 = getTitle();\n\t\n\t\t\t\troiManager(\"reset\");\n\t\t\t\trun(\"Select None\");\n\n\t\t\t\troiManager(\"open\", input_ROI);\n\t\t\t\troiManager(\"Show All\");\n\t\t\t\troiManager(\"Select\", 2);\n\t\t\t\tsetBackgroundColor(0, 0, 0);\n\t\t\t\trun(\"Clear Outside\");\n\t\t\t\trun(\"8-bit\");\n\n\t\t\t\t//get image title for later use\n\t\t\t\tKi67 = getTitle();\n\n\t\t\t\tStack.setXUnit(\"pixel\");\n\t\t\t\trun(\"Properties...\", \"channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000\");\n\t\n\t\n\t\n\t\t\t\t//select the window with ki67 staining\n\t\t\t\tselectWindow(Ki67);\n\t\t\t\t//correct for background in ki67 staining\n\t\t\t\trun(\"Subtract Background...\", \"rolling=50\");\n\t\t\t\t//threshold based on the ki67 staining and convert to a binary image\n\t\t\t\tsetAutoThreshold(\"Default dark\");\n\t\t\t\tsetOption(\"BlackBackground\", true);\n\t\t\t\trun(\"Convert to Mask\");\n\t\t\t\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\t\t\t\trun(\"Close-\");\n\t\t\t\trun(\"Open\");\n\t\t\t\t//watershed to split up nuclei that are lying against each other\n\t\t\t\trun(\"Watershed\");\n\t\t\t\trun(\"Select All\");\n\t\t\t\troiManager(\"Delete\");\n\t\t\t\trun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\n\t\n\t\t\t\t//select the window with aSMA staining\n\t\t\t\tselectWindow(aSMA);\n\t\n\t\t\t\t//threshold based on the aSMA staining and convert to a binary image\n\t\t\t\tsetAutoThreshold(\"Default dark\");\n\t\t\t\tsetOption(\"BlackBackground\", true);\n\t\t\t\trun(\"Convert to Mask\");\n\t\t\n\t\t\t\t//select all ROIs from ki67 and transfer to aSMA\n\t\t\t\trun(\"Select All\");\n\t\t\t\troiManager(\"XOR\");\n\t\t\t\tcount = roiManager(\"count\");\n\t\t\t\troiManager(\"select\", count-1);\n\t\t\t\troiManager(\"Add\");\n\t\t\t\t\n\t\t\t\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\t\t\t\trun(\"Close-\");\n\t\t\t\trun(\"Open\");\n\t\t\t\t//select all ROIs and dilate in order to catch all potential aSMA  signal\n\t\t\t\trun(\"Select All\");\n\t\t\t\trun(\"Dilate\");\n\t\t\t\trun(\"Dilate\");\n\t\t\t\t//watershed to split up dilated ROIs that are lying against each other\n\t\t\t\trun(\"Watershed\");\n\t\t\t\t//calculate overlap\n\t\t\t\timageCalculator(\"AND\", Ki67, aSMA);\n\t\t\t\trun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\n\t\t\t\twaitForUser(\"Press OK to continue\",\"press ok to continue!\");\n\t\t\t\tprint(\"aSMA has run\");\n\t\t\t\tselectWindow(aSMA);\n\t\t\t\tclose();\n\t\t\t\tselectWindow(Ki67);\n\t\t\t\tclose();\n\t\t\t\tList.clear();\n\t\t\t} //-- ends with 3.tif\n\t\t} //-- j loop to find matching file\n\t} //-- ends with 4.tif\n} //- i loop all files in main folder\n</code></pre>\n<p>This is very much a personal choice, but I also like to label the closing braces so I know what loop is closing:</p>\n<pre><code class=\"lang-auto\">\t\t\t} //-- ends with 3.tif\n\t\t} //-- j loop to find matching file\n\t} //-- ends with 4.tif\n} //- i loop all files in main folder\n</code></pre>\n<p>Can you try that and see if it works for you?</p>", "<pre><code class=\"lang-auto\">// Close all open images and clear previous results\nrun(\"Close All\");\nrun(\"Clear Results\");\n\n//kernen input\n\n//aSMA input\ndir4 = getDirectory(\"Choose Directory 4.\"); \nlist4=getFileList(dir4);\nfor (i=0; i&lt;list4.length; i++){ // for loop to parse through names in main folder\n\tif(endsWith(list4[i], \"4.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\tinput = dir4 + list4[i];\n\t\tinput_ROI = replace(input,\"4.tif\",\"4.zip\");\n\t\tprint(\"now processing: \"+input);\n\t\tprint(\"together with: \"+input_ROI);\n\n\t\topen(input);\t\n\t\t//get image title for later use\n\t\taSMA = getTitle();\n\t\n\t\troiManager(\"reset\");\n\t\trun(\"Select None\");\n\n\t\troiManager(\"open\", input_ROI);\n\t\troiManager(\"Show All\");\n\t\troiManager(\"Select\", 2);\n\t\tsetBackgroundColor(0, 0, 0);\n\t\trun(\"Clear Outside\");\n\t\trun(\"8-bit\");\n\t\t//get image title for later use\n\t\taSMA = getTitle();\n\n\t\t//Ki67 input\n\t\tdir3 = getDirectory(\"Choose Directory 3.\"); \n\t\tlist3=getFileList(dir3);\n\t\t\tfor (j=0; j&lt;list3.length; j++){ // for loop to parse through names in main folder\n\t\t\t\tif(endsWith(list3[j], \"3.tif\")){   // if the filename ends with 4.tif file, we enter the following part:\n\t\t\t\tinput = dir3 + list3[j];\n\t\t\t\tinput_ROI = replace(input,\"3.tif\",\"3.zip\");\n\n\n\t\t\t\tprint(\"now processing: \"+input);\n\t\t\t\tprint(\"together with: \"+input_ROI);\n\n\t\t\t\topen(input);\n\t\n\t\t\t\t//get image title for later use\n\t\t\t\tKi67 = getTitle();\n\t\n\t\t\t\troiManager(\"reset\");\n\t\t\t\trun(\"Select None\");\n\n\t\t\t\troiManager(\"open\", input_ROI);\n\t\t\t\troiManager(\"Show All\");\n\t\t\t\troiManager(\"Select\", 2);\n\t\t\t\tsetBackgroundColor(0, 0, 0);\n\t\t\t\trun(\"Clear Outside\");\n\t\t\t\trun(\"8-bit\");\n\n\t\t\t\t//get image title for later use\n\t\t\t\tKi67 = getTitle();\n\n\t\t\t\tStack.setXUnit(\"pixel\");\n\t\t\t\trun(\"Properties...\", \"channels=1 slices=1 frames=1 pixel_width=1 pixel_height=1 voxel_depth=1.0000000\");\n\t\n\t\n\t\n\t\t\t\t//select the window with ki67 staining\n\t\t\t\tselectWindow(Ki67);\n\t\t\t\t//correct for background in ki67 staining\n\t\t\t\trun(\"Subtract Background...\", \"rolling=50\");\n\t\t\t\t//threshold based on the ki67 staining and convert to a binary image\n\t\t\t\tsetAutoThreshold(\"Default dark\");\n\t\t\t\tsetOption(\"BlackBackground\", true);\n\t\t\t\trun(\"Convert to Mask\");\n\t\t\t\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\t\t\t\trun(\"Close-\");\n\t\t\t\trun(\"Open\");\n\t\t\t\t//watershed to split up nuclei that are lying against each other\n\t\t\t\trun(\"Watershed\");\n\t\t\t\trun(\"Select All\");\n\t\t\t\troiManager(\"Delete\");\n\t\t\t\trun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\n\t\n\t\t\t\t//select the window with aSMA staining\n\t\t\t\tselectWindow(aSMA);\n\t\n\t\t\t\t//threshold based on the aSMA staining and convert to a binary image\n\t\t\t\tsetAutoThreshold(\"Default dark\");\n\t\t\t\tsetOption(\"BlackBackground\", true);\n\t\t\t\trun(\"Convert to Mask\");\n\t\t\n\t\t\t\t//select all ROIs from ki67 and transfer to aSMA\n\t\t\t\trun(\"Select All\");\n\t\t\t\troiManager(\"XOR\");\n\t\t\t\tcount = roiManager(\"count\");\n\t\t\t\troiManager(\"select\", count-1);\n\t\t\t\troiManager(\"Add\");\n\t\t\t\t\n\t\t\t\t//erode and dilate (for both Close- and Open) for better signal -&gt; make smoother\n\t\t\t\trun(\"Close-\");\n\t\t\t\trun(\"Open\");\n\t\t\t\t//select all ROIs and dilate in order to catch all potential aSMA  signal\n\t\t\t\trun(\"Select All\");\n\t\t\t\trun(\"Dilate\");\n\t\t\t\trun(\"Dilate\");\n\t\t\t\t//watershed to split up dilated ROIs that are lying against each other\n\t\t\t\trun(\"Watershed\");\n\t\t\t\t//calculate overlap\n\t\t\t\timageCalculator(\"AND\", Ki67, aSMA);\n\t\t\t\trun(\"Analyze Particles...\", \"size=20-5000 pixel show=Overlay summarize add\");\n\t\t\t\twaitForUser(\"Press OK to continue\",\"press ok to continue!\");\n\t\t\t\tprint(\"aSMA has run\");\n\t\t\t\tselectWindow(aSMA);\n\t\t\t\tclose();\n\t\t\t\tselectWindow(Ki67);\n\t\t\t\tclose();\n\t\t\t\tList.clear();\n\t\t\t} //-- ends with 3.tif\n\t\t} //-- j loop to find matching file\n\t} //-- ends with 4.tif\n} //- i loop all files in main folder\n</code></pre>\n<p>Thanks for the suggestions, i have implemented them. Unfortunately, the script remains to have the same error. The first loop with the first pair of 3.tif and 4.tif images goes as planned, but it does not correctly identify the second \u201c4.tif\u201d  image in the directory, causing the above error in this part of the code:</p>\n<p>//select the window with aSMA staining<br>\nselectWindow(aSMA);</p>\n<p>I think it is because of the macro not opening the second image in the 4. directory, hence it is searching for the previous window (M11_34)</p>", "<p>OK, here\u2019s your problem (I think). Your nested loop structure is causing problems because it doesn\u2019t break out of the loop once it finds a match.</p>\n<p>So if you have folders that look like this:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/987e858f6f9d6104660f587b0a81ef90d26ea09b.png\" alt=\"2023-03-14\" data-base62-sha1=\"lL1E7esGn0iDYu48BVaEGL5pglt\" width=\"312\" height=\"123\"></p>\n<p><code>list4</code> will look like this: <code>M1_70_c4.tif, M1_70_c4.zip, M1_71_c4.tif, M1_71_c4.zip</code></p>\n<p>The first <code>endsWith</code> check will pick up <code>M1_70_c4</code> then will go to <code>list3</code> and find the first matching image. First round goes OK.</p>\n<p>The problem is when you run <code>j++</code> (having closed both your aSMA and Ki67 images at the end of the last <code>j</code>\u2019 loop), you will eventually find another Ki67 image ending in <code>3.tif</code>, open it and try to process it against an <code>aSMA</code> image which you already closed!</p>\n<p>The not terribly elegant solution, is to break the loop once you\u2019ve found a match by artifically iterating your count variable.</p>\n<pre><code class=\"lang-auto\">\t\t\t\tselectWindow(aSMA);\n\t\t\t\tclose();\n\t\t\t\tselectWindow(Ki67);\n\t\t\t\tclose();\n\t\t\t\tList.clear();\n\t\t\t\tj=1000000;\n\t\t\t} //-- ends with 3.tif\n\t\t} //-- j loop to find matching file\n\t} //-- ends with 4.tif\n} //- i loop all files in main folder\n</code></pre>\n<p>Note the addition of the <code>j=1000000;</code> line. This basically prevents any more searches on the file list and kicks you back to the next <code>i</code> iteration.</p>\n<p>Super! Problem solved right?</p>\n<p><strong>BUT</strong> \u2026 there is a bigger problem, while you are iterating through your list4, you are always selecting the first match in <code>list3</code> that ends with <code>3.tif</code>. This is evidenced by the output table which shows every time that the image is <code>M1_70_c3.tif</code>.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/239211967e4a1528d6c71433fc7748abc5317602.png\" alt=\"2023-03-14-table\" data-base62-sha1=\"54FF3z1sBxdr8dmd8CQzBL0WjYu\" width=\"496\" height=\"293\"></p>\n<p>So, what you actually need to do is look for a matching string to your <code>list[i]</code> filename, that ends in <code>3.tif</code>. Something like</p>\n<pre><code class=\"lang-auto\">replace(list[i],\"4.tif\",\"3.tif\");\n</code></pre>\n<p>on line ~37.</p>\n<p>Hopefully that should get you back on track!</p>"], "78511": ["<p>Hello,</p>\n<p>I am want to run inference on WSI, but the images are gigantic, so I thought to use lower, upsampled level ( I will go with level 1 from openslide with is 4x downsample). Now I run the inference, but I want to upsample without interpolation or having float values.</p>\n<p>I tried the following for skimage without success.</p>\n<pre><code class=\"lang-auto\">patch = skimage.transform.pyramid_expand(patch,4, preserve_range = True)\n</code></pre>\n<p>Any help is appreciated.</p>\n<p>Thanks in advance</p>", "<p>Hi <a class=\"mention\" href=\"/u/pathai\">@pathAI</a>,</p>\n<p>if I got you right and you want to upsample a label map, one simple approach would be to use nearest neighbor interpolation:</p>\n<pre><code class=\"lang-python\">image_upsampled = skimage.transform.rescale(image, scale=4, order=0)\n</code></pre>\n<p>However, this creates relatively coarse boundaries. Alternatively, the next simplest way might be to use linear interpolation (or higher order, quadratic, cubic etc.) and reconstruct each label independently:</p>\n<pre><code class=\"lang-python\">image_upsampled =\\\n     np.max([(skimage.transform.rescale((image==l).astype(float),\n                                        scale=4, order=1) &gt; 0.5).astype(np.uint64) * l\n             for l in np.unique(image)], axis=0)\n</code></pre>\n<p>I might be misunderstanding your question though!</p>"], "78512": ["<p>Hello,<br>\nI am trying to measure the intensity of point selection in all slices of a stack:</p>\n<ul>\n<li>In \u201cSet Measurements\u201d I choose \u201cMean gray value\u201d.</li>\n<li>I place one or more point selections on the stack and add the selections to the ROI Manager (either automatically in the Point Tool options menu or manually by pressing \u201ct\u201d after each point selection).</li>\n<li>Then I choose \u201cMulti measure\u201d with \u201cMeasure all slices\u201d and \u201cOne row per slice\u201d in the ROI Manager menu.<br>\nThe Results table shows the same intensity value for all slices which is not correct. It works nicely when I try the same thing with eg circle ROIs or probably everything that is larger than a single pixel.<br>\nMy stacks are 8-bit or 16-bit gray scale and I can reproduce this also with the \u201cT1 Head\u201d sample stack.<br>\nCould someone help me in finding out if the problem is in the computer or before the computer?</li>\n</ul>\n<p>Best,<br>\nKlaus</p>", "<p>This bug is fixed in the ImageJ 1.54d6 daily build.</p>", "<p>Thank you very much!!</p>"], "78514": ["<p>Dear Histo analysts out here,</p>\n<p>We are quite new to the analysis of histology tissues using quPath.  We have stainings with Hematoxylin and Eosin. But in France (mostly) we have an additional marker : Saffron which stains mostly collagen (and other things). Our issue is that we want to separate the stainings, and in quPath our Saffron is not separated properly (a mix between Eosin and the residuals)  here are the images of the problem :<br>\nOriginal staining that shows saffron as a peachy color :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792.png\" data-download-href=\"/uploads/short-url/94PU59b1zGEqmj12Omr7XSAFSSK.png?dl=1\" title=\"Original\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792_2_690x320.png\" alt=\"Original\" data-base62-sha1=\"94PU59b1zGEqmj12Omr7XSAFSSK\" width=\"690\" height=\"320\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792_2_690x320.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792_2_1035x480.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/f/3f9f8b8f8adb0be36c96254558a9b21220cfc792.png 2x\" data-dominant-color=\"D494BB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Original</span><span class=\"informations\">1355\u00d7630 206 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When separating Eosin (for intracellular/cytoplasm staining) you can see long and thick fibers of collagen  due to Saffron mixing :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f.png\" data-download-href=\"/uploads/short-url/xNaB90QBzSz3Uqy4xjik8URSSPZ.png?dl=1\" title=\"Eosin\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f_2_690x339.png\" alt=\"Eosin\" data-base62-sha1=\"xNaB90QBzSz3Uqy4xjik8URSSPZ\" width=\"690\" height=\"339\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f_2_690x339.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f_2_1035x508.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/c/ecd6aa0e14661ce03f88e42433c0c845bc17740f.png 2x\" data-dominant-color=\"DB92C7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Eosin</span><span class=\"informations\">1353\u00d7665 192 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And lastly part of the Saffron is also detected in the residuals :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728.png\" data-download-href=\"/uploads/short-url/4qe9H7jrs0tHbP9J9idsCIJoE64.png?dl=1\" title=\"Residual\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728_2_690x385.png\" alt=\"Residual\" data-base62-sha1=\"4qe9H7jrs0tHbP9J9idsCIJoE64\" width=\"690\" height=\"385\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728_2_690x385.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728_2_1035x577.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1effb2e7fe47f2e7b1605402f5a64590bc36d728.png 2x\" data-dominant-color=\"FBFA94\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Residual</span><span class=\"informations\">1360\u00d7760 174 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is there a way to correctly separates saffron from the rest of the HE staining?<br>\nThank you in advance,</p>\n<p>Zeinab and Guil</p>", "<p>Unclear if you are using the predefined H&amp;E stain vectors (which we would nor expect to accomplish what you want) or you set stain vectors yourself as described in the <a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/separating_stains.html#setting-stain-vectors\" rel=\"noopener nofollow ugc\">documentation</a>.</p>\n<p>You can set vectors for up to 3 stains, and in theory you may be able to separate hematoxylin, eosin, and saffron. In practice, I\u2019m not sure how well it will separate eosin and saffron, because in my experience with this stain they are different but not very far in color from each other (eosin: pink, saffron: pinkish orange). I have worked with this stain but never attempted to separate it by deconvolution.</p>", "<p>Thank you for the insight. We were indeed trying to set vectors, however, it seems that Saffron is mixed with Eosin and we wanted to know whether there would be a \u2018trick\u2019 or procedure to unmix the two. <img src=\"https://emoji.discourse-cdn.com/twitter/smiling_face_with_tear.png?v=12\" title=\":smiling_face_with_tear:\" class=\"emoji\" alt=\":smiling_face_with_tear:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I think the only thing to try is setting the stain vector for the saffron itself using Brightfield(other). You can sometimes get the stain vectors more easily using ImageJ plugins on extracted regions of the image.<br>\nSome of the information here might help <a href=\"https://forum.image.sc/t/double-positive-cell-detection-in-triple-stained-tissues/40218/2\" class=\"inline-onebox\">Double-positive cell detection in triple-stained tissues - #2 by Research_Associate</a><br>\nNote the use of color inspector 3D etc.</p>", "<p>If setting image type to Brightfield(other) and setting the 3 stain vectors still doesn\u2019t separate the stain well, perhaps the next step should be to try training a pixel classifier.</p>", "<p>Interesting, I had not known that about saffron. <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8009589/\" class=\"inline-onebox\">Modification of a Haematoxylin, Eosin, and Natural Saffron Staining Method for The Detection of Connective Tissue - PMC</a><br>\nIt looks like depending on the method you may get more or less separable stains. Stains on different color axes (red, green, blue) will be more accurately and usefully separable. Pink and red, for example, would not be since they are mostly <em>along the same color axis</em>, differing only in the intensity of the red.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f.jpeg\" data-download-href=\"/uploads/short-url/s9Tj3kg7GIITELArsei3RFl2txZ.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_439x500.jpeg\" alt=\"image\" data-base62-sha1=\"s9Tj3kg7GIITELArsei3RFl2txZ\" width=\"439\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_439x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_658x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c55b21576781d24645a347e137b34c12abc7638f_2_878x1000.jpeg 2x\" data-dominant-color=\"D5A789\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1208\u00d71375 187 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The images on the right would be much easier to separate.</p>", "<p>Exactly! this is what we were afraid of\u2026Eosin and Saffron are quite similar only that Saffron has more \u201corange\u201d and peachy tones than Eosin \u2026 guess we will have no choice but pixel classification and training for separating the 2 stainings\u2026 Our images are unfortunately more similar to your left pannel\u2026</p>"], "78515": ["<p>Dea Qupath user, I would like to quantify my cells(detetcions) in my whole tissue by annotation. I have different annotations (merge by type) with the following hierarchy : spleen tissue<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc.jpeg\" data-download-href=\"/uploads/short-url/k3Ib9bub6tiBeg27Tkn0mgUYQ3i.jpeg?dl=1\" title=\"Clipboard-1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc_2_578x500.jpeg\" alt=\"Clipboard-1\" data-base62-sha1=\"k3Ib9bub6tiBeg27Tkn0mgUYQ3i\" width=\"578\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc_2_578x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c96ea3ef5cb09a371216c1e8c17ff3a753e59bc.jpeg 2x\" data-dominant-color=\"8B8D8F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Clipboard-1</span><span class=\"informations\">842\u00d7728 143 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If I launch the cell detection module on the whole tissue, my annotations disapear\u2026How can I do to quantify the cells number in each annotation ?<br>\nI can launch in the MZ/B cell zone and T cell zonea and red pulp that should be exclusice but infortunaltely due to little errors in the annotation they are overlap a little so it clear the older detections\u2026</p>\n<p>One way is to save the annotations in Geojson and to reload them after the cells detection ?<br>\nTks, Mathieu</p>", "<p>If the annotations are not in a hierarchy but all at the same level, I believe the annotations will not be deleted.</p>", "<p>yes sure (if they are not overlapping otherwise even if the annoation are not in a hirerachy but overlap Qupath  delete them I think by creating the hirerchy before detect the cells ). if I want the statistics in the whole annotations do you think the best way is to reload the annoations after the detections (not scriptable ?)  ?</p>", "<p>Maintain hierarchy: Select on the annotations you want to run the cell detection inside of, if they are on a lower level. If you run a cell detection on, for example Whole spleen, all of your other annotations will be deleted.</p>\n<pre><code class=\"lang-auto\">selectObjectsByClassification(\"a\", \"b\", \"c\")\n//run cell detection\n</code></pre>\n<p>Destroy Hierarchy quick and dirty:</p>\n<pre><code class=\"lang-auto\">annotations = getAnnotationObjects()\nselectAnnotations()\n//run cell detection here, annotations will be deleted\nclearAnnotations() //not necessary, but will end up with duplicated parent annotations otherwise\ncreateObjects(annotations)\n</code></pre>\n<p>Destroy Hierarchy 2:<br>\nIt would be project dependent, but follow nearly the same steps as above, but use getLevel and some print statements to figure out which annotations to store.</p>\n<p><code>annotationsToRestore = getAnnotationObjects().findAll{it.getLevel !=1}</code><br>\nor similar.</p>", "<p>ok great seems to be what I need !! Thanks</p>"], "78519": ["<p>Hello</p>\n<p>I am trying to use the \u201cWell plates\u201d pluging to open my data that are coming from a scanR instrument. It used to work perfectly but now I cannot get it to work. Each time I find my folder and click on the \u201cplay\u201d button an error message appears \u201cjava.lang.NullPointerException\u201d<br>\nDo you have any idea where it could come from?</p>\n<p>Thank you for your help</p>\n<p>Best,</p>\n<p>Alix</p>", "<p>Hi <a class=\"mention\" href=\"/u/alix_boucharlat\">@Alix_Boucharlat</a></p>\n<p>Can you share a sample image that we can use with this plugin ? It looks like it needs a very specific format for the image (I made some tests but my images weren\u2019t recognized).<br>\nThat would be really useful in understanding the problem, thanks !</p>\n<p>\u2013 Stephane</p>", "<p>Hello <a class=\"mention\" href=\"/u/stephane\">@Stephane</a></p>\n<p>Unfortunately now none of my image are working with that plugin ! It used to work (back in 2018).<br>\nMaybe it is because of the upgraded version of ICY?<br>\nI have many ScanR images from well plates that I could send you if you need a sample to try.</p>\n<p>Thank you for your help,</p>\n<p>Best,</p>\n<p>Alix</p>", "<p>I think something broke the plugin in the recent changes indeed.<br>\nAnd yeah just having a sample image to test would help (a small one is better) as i don\u2019t remember the format required by the plugin and i\u2019m not sure to have one of these with me. Thanks !</p>\n<p>You can send me a link to a sample by email (or directly joined in the mail if it\u2019s small enough) <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Best,</p>\n<p>\u2013 Stephane</p>", "<p>Hello Stephane,</p>\n<p>I sent you some images yesterday. Can you confirm that you received them ?<br>\nThank you,</p>\n<p>Alix</p>"], "78522": ["<p>Hi! I am trying to export regions based on annotations, I am currently using this code, which works well to export them one by one but I constantly have to change the name in the script.</p>\n<p>def roi = getSelectedROI()<br>\ndef requestROI = RegionRequest.createInstance(server.getPath(), 1, roi)<br>\nwriteImageRegion(server, requestROI, \u2018/path/to/export/region.tif\u2019)</p>\n<p>I have already named all of my annotations on Qupath; how could I get to export all annotations once on the Image using the names that I gave them?</p>\n<p>Thank you!</p>", "<p>Could increment using a value</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.imagescientist.com/scripting-export-images#regions\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c6b0cf7d4866291864036815c968bbc0536bd810.png\" class=\"site-icon\" width=\"100\" height=\"100\">\n\n      <a href=\"https://www.imagescientist.com/scripting-export-images#regions\" target=\"_blank\" rel=\"noopener\">Image Scientist</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://www.imagescientist.com/scripting-export-images#regions\" target=\"_blank\" rel=\"noopener\">Exporting images from QuPath \u2014 Image Scientist</a></h3>\n\n  <p>There are many ways to export many different types of image information from QuPath. Here you can find ways to export masks, areas with indexed masks (individual cells), measurement maps, pixel classifier predictions, and more. Also a section on...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nOr replace the increment using the annotation name itself. In your code you aren\u2019t using any annotations, so it would need something like<br>\n<code>getSelectedObject().getName()</code><br>\nassuming you only had one object selected. In general, selecting objects to do things is slow and not really a great way to do things unless you are using plugin commands like cell detection.</p>"], "78523": ["<p>Hi I had a somewhat similar question to the one raised here: <a href=\"https://forum.image.sc/t/custom-atlas-in-abba/77206\" class=\"inline-onebox\">Custom Atlas in ABBA</a>, but instead of a different atlas I just want to update the annotations. Specifically I have edited the labels of the ccf2017 atlas to divide a region into subregions, giving them unique id values. Ultimately, I want to use these custom labels to extract regions cell counts in qupath. In order to use these labels in abba what alterations need to be made?</p>\n<p>I assume I need to update the ontology to include my new regions as children of the parent region. When setting the id value (and other parameters) is enough that it is unique or do they need to be inserted in order such that all subsequent ids are incremented? It would be easier if I can just make the ids (40,000, 40,001 \u2026) something far outside the range.<br>\ne.g. in the file \u201c1.json\u201d in ./abba_atlases</p>\n<pre><code class=\"lang-auto\">{\n                     \"id\": 1085,\n                     \"atlas_id\": 1125,\n                     \"ontology_id\": 1,\n                     \"acronym\": \"MOs6b\",\n                     \"name\": \"Secondary motor area, layer 6b\",\n                     \"color_hex_triplet\": \"1F9D5A\",\n                     \"graph_order\": 29,\n                     \"st_level\": 11,\n                     \"hemisphere_id\": 3,\n                     \"parent_structure_id\": 993,\n                     \"children\": [\n                        # proposed subdivisions\n                        {\"id\": 1086, \"atlas_id\":1126, ...}, \n                        {\"id\":1087, \"atlas_id\":1127, ...}, \n]\n                    }\n</code></pre>\n<p>As for the atlas images can I just \u201cswap out\u201d the existing labels for my own, update the region outlines (maybe unnecessary if only utilized for visualization), and recompile the images into a new .h5 with multiple resolutions? Then replace the atlas and ontologies located in the abba_atlases folder?</p>\n<p>Or is there a simpler way such as just exporting the transformations applied by abba and applying to my label image? If I can extract the transforms applied in abba to my own custom atlas, I can apply it to my section images to extract the regions in python.</p>", "<p>Replacing the atlas is going to be complicated unfortunately\u2026 Not impossible, but complicated.</p>\n<p>The reason is because the Allen Brain Atlas uses gigantic numbers for their labels. What I mean by gigantic is 32 bits integers. They do not fit in 16-bits unsigned integers. (That does not make a lot of sense for me since the total number of labels do not exceed a few thousands).</p>\n<p>BigDataViewer do not very easily support 32 bits integer labels, and there\u2019s a passage using an ImageJ ImagePlus structure which do not support 32 bits integer at all anyway. So I had to cheat.</p>\n<p>I noticed that I can remap the labels by doing a modulo 65000 on all labels, and I do not have any overlap, that\u2019s why I packaged this new modulo atlas (<a href=\"https://zenodo.org/record/4173229#.ZA9Qeh_MI70\" class=\"inline-onebox\">Allen Mouse CCF v3 - Labels Modulo 65000 | Zenodo</a>).</p>\n<p>Now if you had or resample labels, you will need to take care of this. I think it\u2019s going to be a pain.</p>\n<blockquote>\n<p>Or is there a simpler way such as just exporting the transformations applied by abba and applying to my label image? If I can extract the transforms applied in abba to my own custom atlas, I can apply it to my section images to extract the regions in python.</p>\n</blockquote>\n<p>Yes, so I think this makes sense. Basically, using ABBA you can map slices to Allen CCFv3, and extract the coordinates in space. You can then use this coordinates in your atlas to know the corresponding labels.</p>\n<p>I\u2019ve updated <a href=\"https://github.com/NicoKiaru/ABBA-Python\">ABBA-Python</a> (use the dev branch) and there are <a href=\"https://github.com/NicoKiaru/ABBA-Python/tree/dev/notebooks\">a few notebooks</a> demoing how to access the coordinates of any pixel in the atlas after the registration. Maybe that can be helpful ?</p>\n<p>Have a quick look and let me know what you think.</p>", "<p>Hi Nicolas,</p>\n<p>Thanks for your response. I wanted to clarify that I made my adjustments to the modulo 65000 atlas you referenced, by extracting the label image from the file \u201cccf2017-mod65000-border-centered-mm-bc.h5\u201d and painting over those regions I wanted to add with unique ids. It might be easier if I can put the edited labels back into the .h5 file (by creating a new one) and replace the one located in the \u201c.\\abba_atlases\u201d folder?</p>\n<p>Do you think this will break anything?</p>\n<p>Thanks,<br>\nPascal</p>", "<p>No risk to break anything! Worst case scenario you have to re-install ABBA. Or just delete the <code>.\\abba_atlases</code> folder and it will be re-downloaded from scratch.</p>", "<p>Any guidance for creating this file? The key elements seem to be the 4 3d images corresponding to channels downsampled 4 times:</p>\n<pre><code class=\"lang-auto\">        s00\n        &lt;HDF5 dataset \"cells\": shape (1140, 800, 1320), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (570, 400, 660), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (285, 200, 330), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (142, 100, 165), type \"&lt;i2\"&gt;\n\n        s01\n        &lt;HDF5 dataset \"cells\": shape (1140, 800, 1320), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (570, 400, 660), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (285, 200, 330), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (142, 100, 165), type \"&lt;i2\"&gt;\n\n        s02\n        &lt;HDF5 dataset \"cells\": shape (1140, 800, 1320), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (570, 400, 660), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (285, 200, 330), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (142, 100, 165), type \"&lt;i2\"&gt;\n\n        s03\n        &lt;HDF5 dataset \"cells\": shape (1140, 800, 1320), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (570, 400, 660), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (285, 200, 330), type \"&lt;i2\"&gt;\n        &lt;HDF5 dataset \"cells\": shape (142, 100, 165), type \"&lt;i2\"&gt;\n</code></pre>\n<p>Is there anything else I need to include in the .h5 file besides these images?<br>\nWhat do I need to modify in the ontology? Do I just need to update the \u201c1.json\u201d file with the new subregions as children of the parent region, do I need to change anything in \u201cmouse_brain_ccfv3.xml\u201d?</p>", "<p>I was able to add my custom atlas labels doing the following:</p>\n<ol>\n<li>extracted the label image at highest resolution from the ccf2017-mod65000-border-centered-mm-bc.h5 file</li>\n<li>load that image into napari</li>\n<li>create a new layer where I painted in my subregions</li>\n<li>overwrite the existing label with my new ones</li>\n<li>save as a tif (\u2018int16\u2019)</li>\n<li>used the following code to overwrite the .h5 file with my labels at various resolutions</li>\n</ol>\n<pre><code class=\"lang-auto\">img_dir_atlas = r'D:\\...'\nimg_path_atlas = os.path.join(img_dir_atlas, 'ccf2017-mod65000-border-centered-mm-bc.h5')\nimg_labels_path = os.path.join(img_dir_atlas, 'mylabels.tif')\n\n\nmylabels = imread(img_labels_path)\nmylabels2x = rescale(mylabels, 1/2, anti_aliasing=False, preserve_range=True).astype('int16')\nmylabels4x = rescale(mylabels, 1/4, anti_aliasing=False, preserve_range=True).astype('int16')\nmylabels8x = rescale(mylabels, 1/8, anti_aliasing=False, preserve_range=True).astype('int16')\n\n# overwrite the data at a given resolution\nwith h5py.File(img_path_atlas,'r+') as atlas_h5:\n    atlas_h5['t00000']['s03']['0']['cells'][...] = mylabels\n    atlas_h5['t00000']['s03']['1']['cells'][...] = mylabels2x\n    atlas_h5['t00000']['s03']['2']['cells'][...] = mylabels4x\n    atlas_h5['t00000']['s03']['3']['cells'][...] = mylabels8x\n</code></pre>\n<ol start=\"7\">\n<li>updated the ontology in the \u20181.json\u2019 file located in .\\ABBA\\abba_atlases by adding my subregions as children of the parent region. I gave them unique ids and graph orders and setting the atlas id to null.</li>\n<li>replacing the \u2018ccf2017-mod65000-border-centered-mm-bc.h5\u2019 and \u20181.json\u2019 files with my new versions</li>\n</ol>\n<p>Did not seem to break anything, as I was able to do the registrations and extract the cells in my new regions running the cells_in_regions.groovy script.</p>"], "78529": ["<p>Hi,</p>\n<p>I\u2019m working on version 2 of my plugin, here: <a href=\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/tree/dev-v.0.2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - HelmholtzAI-Consultants-Munich/napari-organoid-counter at dev-v.0.2</a></p>\n<p>I would like that the events.data event (here: <a href=\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/96843870fd8a7e942711737621c8d4f9be5cfac9/napari_organoid_counter/_widget.py#L138\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">napari-organoid-counter/_widget.py at 96843870fd8a7e942711737621c8d4f9be5cfac9 \u00b7 HelmholtzAI-Consultants-Munich/napari-organoid-counter \u00b7 GitHub</a>) is triggered only when the user adds or removes a box. Currently it also triggered when the user plays around with the sliders to change the model parameters (which also changes the data of the layer).</p>\n<p>Any help would much appreciated <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThanks,<br>\nChristina</p>", "<p>I don\u2019t think we have an event specifically for that\u2014there\u2019s a long open issue:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/napari/napari/issues/720\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/napari/napari/issues/720\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/napari/napari</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/napari/napari/issues/720\" target=\"_blank\" rel=\"noopener nofollow ugc\">Receive events on shape create and delete</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2019-11-22\" data-time=\"03:38:36\" data-timezone=\"UTC\">03:38AM - 22 Nov 19 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/cudmore\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"cudmore\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/486cc9305fb394d18456198c9f60433d4ce94eae.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          cudmore\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          feature\n        </span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          good first issue\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">## \ud83d\ude80 Feature\nCan we get events when shape layer shapes are created and deleted?<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">\n\n## Motivation\n\nI am implementing a shape analysis plugin. It would be great if my code could receive an event when a shape is added and a shape is deleted.\n\nI want to display the current shapes in list, something like a Qt TreeView. I need to append and delete from this list as shapes are added and deleted from the Napari interface. To do this, I need to receive events when a new shape is created and a shape is deleted.\n\n## Pitch\n\nBecause there can be many shapes (e.g. ROIs), it is super useful to display them as a list. In a Qt TreeView for example. With this, the user can see the number of shapes, the properties of each (including my analysis results), can select them in the list and have them selected in the viewer, can delete them, etc. etc.\n\nWhen there are many shapes, it is not useful to only display them on top of an image. This results in **user fatigue**. It is useful to have them in a list so the user can browse shapes from the list as compared to a visual search on top of the image. Thus, I need Napari to trigger events on shape addition and deletion.\n\nWhen shapes/ROIs are only on top of the image, it becomes similar to [where is Waldo](https://www.google.com/search?q=where+is+waldo&amp;sxsrf=ACYBGNSnqjLq7LwPJcsMs8nydf4f6HKJ2Q:1574393515050&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwi9xs3x8PzlAhVIGDQIHYWWCBoQ_AUoAXoECA4QAw&amp;biw=1258&amp;bih=1231#imgrc=0W6qdOm2E9qAdM:)!\n\n## Alternatives\n\n\n\nI can find shape and shape layer events triggered when properties of a shape change, things like (edge_color, face_color, edge_width). But no code that generates an event when a shape is created or deleted?\n\n - New shape\n\nFor new shapes I am catching mouse down events using the `mouse_drag_callbacks` decorator. But this requires me to keep a backend list of current shapes and compare to the list when the mouse is clicked.\n\n```\n@self.shapeLayer.mouse_drag_callbacks.append\ndef shape_mouse_move_callback(layer, event):\n    if event.type == 'mouse_press':\n        #compare list of shapes in my backend shape layer\n        #if there are more than in my backend then the last one is a new one\n        pass\n```\n\n - Delete shape\n\nI am not sure how to do this? I found `napari.layers.shapes.shapeList.remove()` which does not trigger an event and seems to be called repeatedly as shape properties are updated? Properties like (edge_color, face_color, edge_width). Maybe it should be `_remove()`. I also found `remove_selected()` but it also does not trigger an event?\n\n## Additional context\n\n\nI am storing the analysis for each shape in the shape `metadata` data member. With this, my code does not need to know about added or deleted shapes. But I would like to keep a backend list of shapes to be displayed in a Tree View so the user can select shapes from the list as compared to selecting in the image.\n\n![cudmore-napari-plugin-1](https://user-images.githubusercontent.com/1009168/69394311-825a4780-0c90-11ea-91f9-9b839b1c2f3c.gif)\n\nOn the left is the Napari viewer with two shapes, a line in blue and a polygon/rectangle in orange. On the right is my PyQtGraph interface for shape analysis. It has 4 plots., (i) a line intensity profile (white) along with my analysis including a gaussian fit (red) and my diameter estimate (blue). This is all updated in real time as the user drags a selected line shape and/or adjusts the viewed image plane/frame, (ii) my meta analysis of the diameter along the line, (iii) a plot of all the line profiles through the time series, and (iv) a plot of the mean intensity within the polygon/rectangle napari shape.\n\nI would like to add a list of current shapes/ROIs to this interface. Something like a Qt TreeView. To do this, I need events that tell me when a shape is added and a shape is deleted...</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nBut, you can try using a different event <code>set_data</code> instead of <code>data</code> which is not well documented but does fire when a shape is added or deleted (but also when shapes are modified, which might not be what you want\u2026).<br>\nShapes <code>data</code> is a list of the shapes, so you could maybe check the len() of the layer data?</p>\n<p>For debugging and exploring the UI you can try this nice plugin:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/dalthviz/napari-ui-tracer\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/dalthviz/napari-ui-tracer\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/6951e08dc02a6dbe3017d8ca260981601a523b1f.png 2x\" data-dominant-color=\"E9EAEC\"></div>\n\n<h3><a href=\"https://github.com/dalthviz/napari-ui-tracer\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - dalthviz/napari-ui-tracer: A plugin to help understand Napari UI...</a></h3>\n\n  <p>A plugin to help understand Napari UI components and check their source code definition - GitHub - dalthviz/napari-ui-tracer: A plugin to help understand Napari UI components and check their source...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nSee more here:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/napari/docs/issues/93#issuecomment-1456590480\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/napari/docs/issues/93#issuecomment-1456590480\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/napari/docs</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/napari/docs/issues/93#issuecomment-1456590480\" target=\"_blank\" rel=\"noopener nofollow ugc\">Add documentation mapping visual UI with code structure, modules and classes</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2023-01-18\" data-time=\"18:13:36\" data-timezone=\"UTC\">06:13PM - 18 Jan 23 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/dalthviz\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"dalthviz\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce9210e6fcf089db7e955f4e310d823aac2eb5a0.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          dalthviz\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          content\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">## \ud83d\udcda New content request\n\nAfter helping around fixing bugs, I was thinking tha<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">t maybe having some sort of mapping from the visual elements/UI to its related source code (directory structure, modules and classes) could be a good way to document the Napari GUI. This could help with the task of onboarding contributors and also document the general way things are currently working in terms of the GUI.\n\nI think it could be nice to create documentation per interface section (layers controls, layers list, viewer, menus, dialogs, etc) and show for each of them the corresponding subset of the Napari source code directory layout: only the most related directories/files as well as some sort of dependency diagram(s) to show how the modules and classes depend between each other in that subset of the Napari source code.\n\nChecking, seems like some exploration to do mappings of the source code have been done in a more or less automatic manner as well as an exploration of tooling to create diagrams (https://github.com/napari/napari/issues/1389). I would say that finding a way to at least semiautomatically create this documentation should be explored.\n\nAlso, talking about this with @melissawm a suggestion I got is asking core developers about the current code layout and the decisions behind the current code layout to document this, and seems like @psobolewskiPhD and @goanpeca did some notes about things that could be nice to document related to the Napari source code that could also apply to the idea here of creating some documentation for the visual UI/frontend (https://hackmd.io/2iq_CmjHR8mK0UL7e4bYaw#napari-codebase-related).\n\nThis effort could be related also with https://github.com/napari/docs/issues/49\n\n### Outline\n\n* UI sections:\n  * Application menus\n  * Application status bar\n  * Layer controls\n  * Layer list\n  * Viewer\n  * Console (probably plugins dockwidgets in general using the Console plugin as example)\n  * Dialogs:\n    * Preferences\n    * Plugins\n    * About\n    * Warning/Error/Information notifications dialogs\n\nThe sections above come from what I have explored from the source code but probably there are more UI sections.\n\nCreating an issue for this as discussed with @potating-potato</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<aside class=\"quote no-group\" data-username=\"psobolewskiPhD\" data-post=\"2\" data-topic=\"78529\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\"> Peter Sobolewski:</div>\n<blockquote>\n<p>But, you can try using a different event <code>set_data</code> instead of <code>data</code> which is not well documented</p>\n</blockquote>\n</aside>\n<p>Agreed that <code>set_data</code> is not well documented, but I think it\u2019s probably not what you\u2019re looking for here either. Based on where it is emitted (i.e. in <code>Layer.refresh</code>), I\u2019ve always interpreted it as indicating that new data is ready to be rendered either because the data itself changed or the view/slice of the data changed (e.g. the dimension slider positions moved).</p>\n<p>As suggested, I think using the <code>data</code> event in combination with the <code>len()</code> of <code>Layer.data</code> is probably the best solution/workaround right now.</p>", "<p>HI both,</p>\n<p>thank you very much for your comments and fast replies <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Indeed, that is what I did initially, but I actually want the number of boxes (<code>len(data)</code>) to be able to change also from the backend, e.g. one of the user sliders represents the model\u2019s confidence in the predictions - so as you move it to the right the number of boxes should reduce, while moving it to the left the number of boxes should increase. This should be dealt differently than when a user manually adds or removes a box.</p>\n<p>I was wondering if somehow combining the <code>data</code> event with some user event, e.g. <code>mouse_over_canvas</code> would work. Do you know if that would be possible? Thanks again!</p>", "<p>I\u2019ve thought about this for about a minute, so this may not be the best solution, but here\u2019s one idea: layers have a <code>.metadata</code> attribute that is a dictionary explicitly for \u201carbitrary\u201d use. You could do something like:</p>\n<pre><code class=\"lang-python\">from contextlib import contextmanager\n\n@contextmanager\ndef set_dict_key(dictionary, key, value):\n    dictionary[key] = value\n    yield\n    del dictionary[key]\n\n# ...\n# now in your slider widget class/function\n\ndef update_confidence_threshold(\n        shapes_layer, confidence_level, boxes\n        ):\n    with set_dict_key(\n            shapes_layer.metadata,\n            'my-plugin-name:update_confidence_threshold',\n            True\n            ):\n        shapes_layer.data = filter_by_confidence(boxes, confidence_level)\n</code></pre>\n<p>Then in the function that you\u2019re hooking up to events:</p>\n<pre><code class=\"lang-python\">def respond_to_layer_data(ev):\n    key = 'my-plugin-name:update_confidence_threshold'\n    if key in shapes_layer.metadata:\n        return\n    # rest of function goes here\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/jni\">@jni</a>,</p>\n<p>looks like one minute was all you need - this solution worked out for me <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thanks so much,<br>\nChristina</p>"], "78531": ["<p>We are looking for a bioimage scientist.<br>\nRemote meetings, participation in development, provision of data, product evaluation, introduction of customers, compliance with regulations.<br>\nPlease contact me directly if you are interested.</p>"], "78533": ["<p>Hello everyone,</p>\n<p>I am new at using CellProfiler and I\u2019m trying to quantify signal intensity for p16, a nuclear protein, in an artificial skin construct in 2D. I understand that this may be a simple request but I have tried following several approaches without good results. The following images are the DAPI channel (ch00), the p16 channel (ch01) and the image with all overlaid channels.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/6vY482qrshx8ZujGF1mLaA9ILu1.tif\">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1_ch00.tif</a> (1.3 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hdLoMpGIe3zPJPRxduYSoXTiywi.tif\">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1_ch01.tif</a> (1.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/f6zNMjhxiXL6WEMSh2wmEzSrKV6.tif\">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1.tif</a> (2.6 MB)</p>\n<p>My initial idea on how to go on about doing this was the following:</p>\n<ol>\n<li>Covert all channels to greyscale</li>\n<li>Identify Primary Objects, in the DAPI channel, i.e. nuclei</li>\n<li>Mask everything that is not identified as a nucleus in the DAPI channel image</li>\n<li>Apply this mask to the p16 channel image</li>\n<li>Define this as an object</li>\n<li>Measure the signal intensity of this object in the p16 channel</li>\n</ol>\n<p>I could not get this to work so I did the following:</p>\n<ol>\n<li>Covert all channels to greyscale</li>\n<li>Identify Primary Objects in the DAPI channel, i.e. nuclei</li>\n<li>Identify Primary Objects in the p16 channel as \u201cp16 signal\u201d</li>\n<li>Relate nuclei as the \u201cparent\u201d object to the p16 signal \u201cchildren\u201d object and define this as new object named \u201cp16 positive nuclei\u201d</li>\n<li>Measure signal intensity of p16 positive nuclei</li>\n</ol>\n<p>Looking at the images this has not worked as I wanted to, as the object \u201cp16 positive nuclei\u201d does not seem to be defined by the area occupied by the object \u201cnuclei\u201d.</p>\n<p>Essentially, what I want to do is define the area occupied by the nuclei in the DAPI channel and measure the intensity of the p16 signal from the p16 channel in that area, as p16 is a nuclear stain. Below you can find my pipeline.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/mYeZDoco9za6pAjSiy0uOHdftwP.cpproj\">UG3SEN_p16_53BP1.cpproj</a> (1.6 MB)</p>\n<p>I apologize for the long post. I would be grateful for any help.</p>\n<p>Thank you,<br>\nPeriklis</p>"], "78534": ["<p>I\u2019d love to combine two ImageJ/FIJI macros into one, but I am having trouble with the bio-formats portion.</p>\n<p>The first macro converts images in a .lif to .tiff, the second takes .tiff images and allows for adjustment of brightness, then outputs the adjusted .tiffs. The first thing I tried was to just put the two scripts sequentially after the other, and line up the output file directory of the first to the input of the second, but that didn\u2019t go well.</p>\n<p>Here\u2019s the convert lif to a tif script</p>\n<pre><code class=\"lang-auto\">run(\"Bio-Formats Macro Extensions\");\nsetBatchMode(true); \ninput = getDirectory(\"Input directory, folder where your .lif file is\");\noutput = getDirectory(\"Output directory, where you'd like your raw .tiff files to go\");\n\n\nsuffix = \".lif\";\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n}\n\nfunction processFile(input, output, file) {\n\tExt.setId(input + file);\n\tExt.getSeriesCount(count);\n\tprint(\"Processing: \" + input + file+ \" - \"+d2s(count,0)+\" images\");\n\n\tfor (f=0;f&lt;count;f++) {\n\t\topenLif(input+file,f);\n\t\tprint(\"Saving to: \" + output);\n\t\tlistImages();\n\t}\n}\n\nfunction openLif(input,f){\n\tExt.setSeries(f);\n\trun(\"Bio-Formats Importer\", \"open=[\" +input +\"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_\" + d2s(f+1,0));\n}\n\n//Create string \"image_1 image_2 image_3 image_4....\"\nfunction seriesN(num){\n\tstr = \"\";\n\tfor (i=0; i&lt;num; i++){\n\t\tii = i+1;\n        str = str+\"image_\"+ii+\" \";\n     }\n     return str;\n}\n\n//Save\nfunction listImages(){\n\timageList = getList(\"image.titles\");\n\tif (imageList.length==0)\n\t     print(\"No image windows are open\");\n\telse {\n    \tprint(\"Image windows:\");\n     \tfor (i=0; i&lt;imageList.length; i++){\n        \tprint(\"   \"+imageList[i]);\n        \tsaveTiff(imageList[i]);\n     \t}\n\t}\n\tprint(\"\");\n}\n\n\nfunction saveTiff(winName){\n\tselectWindow(winName); \n\tsaveAs(\"Tiff\", output + winName);\n\tclose();\n}\n</code></pre>\n<p>Here\u2019s the brightness adjustment script, judging by the comments, this is likely a public script made by someone somewhere and handed down to me.</p>\n<pre><code class=\"lang-auto\">print(\"\\\\Clear\");\nGetTime();\nsetBatchMode(true); \nrun(\"Input/Output...\", \"jpeg=100 gif=-1 file=.xls use_file copy_row save_column save_row\");\n\n\ninput = getDirectory(\"Input directory, where your raw .tiff files are\");\noutput = getDirectory(\"Output directory, where you'd like your adjusted .tiff files to go\");\n\nDialog.create(\"File type\");\nDialog.addString(\"File suffix: \", \".tif\", 5);\nDialog.addNumber(\"Ch1:\", 0);\nDialog.addNumber(\"Ch1:\", 65535);\nDialog.addNumber(\"Ch2:\", 0);\nDialog.addNumber(\"Ch2:\", 65535);\nDialog.addNumber(\"Ch3:\", 0);\nDialog.addNumber(\"Ch3:\", 65535);\nDialog.addNumber(\"Ch4:\", 0);\nDialog.addNumber(\"Ch4:\", 65535);\nDialog.show();\nsuffix = Dialog.getString();\nRange1 = Dialog.getNumber();\nRange2 = Dialog.getNumber();\nRange3 = Dialog.getNumber();\nRange4 = Dialog.getNumber();\nRange5 = Dialog.getNumber();\nRange6 = Dialog.getNumber();\nRange7 = Dialog.getNumber();\nRange8 = Dialog.getNumber();\n\nBC_range = newArray(Range1,Range2,Range3,Range4,Range5,Range6,Range7,Range8);\n\nprint(\"Brightness and contrast range is: \");\nArray.print(BC_range);\nprint(\"Gray, Green, Red, Blue\");\n\n\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n\tGetTime();\n\tselectWindow(\"Log\");  \n\tsaveAs(\"Text\", output + \"log.txt\"); \n}\n\nfunction processFile(input, output, file) {\n\n\tprint(\"Processing: \" + input + file);\n\t\n\t\n\topen(input+file);\n\trenderColor(file);\n\tbrightnessNcontrast(file);\n\tdeleteSlices(file);\n\n\tsaveAs(\"Tiff\", output + file + \"_BCadjusted\");\n\trename(file);\n\t\n\tRGBmerge(file);\n\tscaleBar();\n\tmakeMontage(file);\n\n\t//selectWindow(\"Montage1to5\");\n\t//saveAs(\"Jpeg\", output + file + \"_montage1to5\");\n\tselectWindow(\"MontageHorizontal\");\n\tsaveAs(\"Jpeg\", output + file + \"_BCmontage\");\n\n\tprint(\"Saving to: \" + output);\n\trun(\"Close All\");\n}\n\n\n\n// Give colors for each slice\nfunction renderColor(file){\n\tcolor = newArray(\"Blue\",\"Green\",\"Red\",\"Grays\");\n\t//color = newArray(\"Blue\",\"Green\",\"Red\");\n\n\trun(\"Make Composite\", \"display=Color\");\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\trun(color[i]);\n\t}\n}\n// Change brightness and contrast\nfunction brightnessNcontrast(file){\n\t//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\tsetMinAndMax(BC_range[2*i],BC_range[2*i+1]);\n\t}\n\t\n}\n\n// Delete slices. You want to delete slice a, b\nfunction deleteSlices(file){\n\tdeleteA = 0;\n\tdeleteB = 0;\n\t//deleteB &gt; deleteA. deleteB should be larger than deleteA\n\n\tprint(\"deleting the slice \"+deleteA+\" and \"+deleteB);\n\tselectWindow(file);\n\tif(deleteB&gt;0){\n\t\tsetSlice(deleteB);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t\t}\n\tif(deleteA&gt;0){\n\t\tsetSlice(deleteA);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t}\t\n}\n\n// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.\nfunction RGBmerge(file){\n\trun(\"Duplicate...\", \"title=4channels duplicate\");\n\trun(\"RGB Color\");\n\t\t\n\tselectWindow(file);\n\trun(\"Duplicate...\", \"title=Merge duplicate\");\n\tStack.setDisplayMode(\"composite\");\n\trun(\"RGB Color\");\n\trun(\"Copy\");\n\n\tselectWindow(\"4channels (RGB)\");\n\tsetSlice(nSlices);\n\trun(\"Add Slice\"); \n\trun(\"Paste\"); \n\n\tclose(\"4channels\");\n\tclose(\"Merge\");\n\tclose(\"Merge (RGB)\");\n\t\n\t// \"4channels (RGB)\" is made.\n}\n\n// Add scale bar\nfunction scaleBar(){\n\tsetSlice(nSlices);\n\trun(\"Set Scale...\", \"distance=311.0016 known=100 pixel=1 unit=\u00b5m\");\n\t//run(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] hide\");\n\trun(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] bold\");\n}\n\nfunction makeMontage(input){\n\t//selectWindow(\"4channels (RGB)\");\n\t//run(\"Make Montage...\", \"columns=1 rows=5 scale=0.25 border=2\");\n\t//rename(\"Montage1to5\");\n\tselectWindow(\"4channels (RGB)\");\n\trun(\"Make Montage...\", \"columns=\"+nSlices+\" rows=1 scale=0.5 border=2\");\n\trename(\"MontageHorizontal\");\n}\nfunction GetTime(){\n     MonthNames = newArray(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\");\n     DayNames = newArray(\"Sun\", \"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\");\n     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);\n     TimeString =\"Date: \"+DayNames[dayOfWeek]+\" \";\n     if (dayOfMonth&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+dayOfMonth+\"-\"+MonthNames[month]+\"-\"+year+\"\\nTime: \";\n     if (hour&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+hour+\":\";\n     if (minute&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+minute+\":\";\n     if (second&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+second;\n     print(TimeString);\n}\n</code></pre>\n<p>Ideally, I would like to dialog the user as to whether or not they would also like to save the unadjusted .tiff files (and if so, where to do that, separately from the adjusted ones). But I am not as familiar with ImageJ macro language as I am with QuPath/groovy and don\u2019t know how to go about doing that</p>", "<p>Hi James, I have roughly tried to combine the two as below though I haven\u2019t tested it and you could likely tidy it up to make the code much neater. It simply skips the processing of the folder in the second script and jumps straight to processing the file (now renamed processTiff) after saving the unadjusted tiffs. I have left a TODO where you could add the extra user dialog at a later point.</p>\n<pre><code class=\"lang-auto\">run(\"Bio-Formats Macro Extensions\");\nsetBatchMode(true); \ninput = getDirectory(\"Input directory, folder where your .lif file is\");\noutput = getDirectory(\"Output directory, where you'd like your raw .tiff files to go\");\n\n\nsuffix = \".lif\";\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n}\n\nfunction processFile(input, output, file) {\n\tExt.setId(input + file);\n\tExt.getSeriesCount(count);\n\tprint(\"Processing: \" + input + file+ \" - \"+d2s(count,0)+\" images\");\n\n\tfor (f=0;f&lt;count;f++) {\n\t\topenLif(input+file,f);\n\t\tprint(\"Saving to: \" + output);\n\t\tlistImages();\n\t}\n}\n\nfunction openLif(input,f){\n\tExt.setSeries(f);\n\trun(\"Bio-Formats Importer\", \"open=[\" +input +\"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_\" + d2s(f+1,0));\n}\n\n//Create string \"image_1 image_2 image_3 image_4....\"\nfunction seriesN(num){\n\tstr = \"\";\n\tfor (i=0; i&lt;num; i++){\n\t\tii = i+1;\n        str = str+\"image_\"+ii+\" \";\n     }\n     return str;\n}\n\n//Save\nfunction listImages(){\n\timageList = getList(\"image.titles\");\n\tif (imageList.length==0)\n\t     print(\"No image windows are open\");\n\telse {\n    \tprint(\"Image windows:\");\n     \tfor (i=0; i&lt;imageList.length; i++){\n        \tprint(\"   \"+imageList[i]);\n        \t// TODO: Add condition for saving unadjusted files\n                saveTiff(imageList[i]);\n                \n                // Process the files as per the second script\n                processTiff(output, output, imageList[i]);\n     \t}\n\t}\n\tprint(\"\");\n}\n\n\nfunction saveTiff(winName){\n\tselectWindow(winName); \n\tsaveAs(\"Tiff\", output + winName);\n\tclose();\n}\n\nfunction processTiff(input, output, file) {\n\n\tprint(\"Processing: \" + input + file);\n\t\n\t\n\topen(input+file);\n\trenderColor(file);\n\tbrightnessNcontrast(file);\n\tdeleteSlices(file);\n\n\tsaveAs(\"Tiff\", output + file + \"_BCadjusted\");\n\trename(file);\n\t\n\tRGBmerge(file);\n\tscaleBar();\n\tmakeMontage(file);\n\n\t//selectWindow(\"Montage1to5\");\n\t//saveAs(\"Jpeg\", output + file + \"_montage1to5\");\n\tselectWindow(\"MontageHorizontal\");\n\tsaveAs(\"Jpeg\", output + file + \"_BCmontage\");\n\n\tprint(\"Saving to: \" + output);\n\trun(\"Close All\");\n}\n\n\n\n// Give colors for each slice\nfunction renderColor(file){\n\tcolor = newArray(\"Blue\",\"Green\",\"Red\",\"Grays\");\n\t//color = newArray(\"Blue\",\"Green\",\"Red\");\n\n\trun(\"Make Composite\", \"display=Color\");\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\trun(color[i]);\n\t}\n}\n// Change brightness and contrast\nfunction brightnessNcontrast(file){\n\t//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\tsetMinAndMax(BC_range[2*i],BC_range[2*i+1]);\n\t}\n\t\n}\n\n// Delete slices. You want to delete slice a, b\nfunction deleteSlices(file){\n\tdeleteA = 0;\n\tdeleteB = 0;\n\t//deleteB &gt; deleteA. deleteB should be larger than deleteA\n\n\tprint(\"deleting the slice \"+deleteA+\" and \"+deleteB);\n\tselectWindow(file);\n\tif(deleteB&gt;0){\n\t\tsetSlice(deleteB);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t\t}\n\tif(deleteA&gt;0){\n\t\tsetSlice(deleteA);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t}\t\n}\n\n// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.\nfunction RGBmerge(file){\n\trun(\"Duplicate...\", \"title=4channels duplicate\");\n\trun(\"RGB Color\");\n\t\t\n\tselectWindow(file);\n\trun(\"Duplicate...\", \"title=Merge duplicate\");\n\tStack.setDisplayMode(\"composite\");\n\trun(\"RGB Color\");\n\trun(\"Copy\");\n\n\tselectWindow(\"4channels (RGB)\");\n\tsetSlice(nSlices);\n\trun(\"Add Slice\"); \n\trun(\"Paste\"); \n\n\tclose(\"4channels\");\n\tclose(\"Merge\");\n\tclose(\"Merge (RGB)\");\n\t\n\t// \"4channels (RGB)\" is made.\n}\n\n// Add scale bar\nfunction scaleBar(){\n\tsetSlice(nSlices);\n\trun(\"Set Scale...\", \"distance=311.0016 known=100 pixel=1 unit=\u00b5m\");\n\t//run(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] hide\");\n\trun(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] bold\");\n}\n\nfunction makeMontage(input){\n\t//selectWindow(\"4channels (RGB)\");\n\t//run(\"Make Montage...\", \"columns=1 rows=5 scale=0.25 border=2\");\n\t//rename(\"Montage1to5\");\n\tselectWindow(\"4channels (RGB)\");\n\trun(\"Make Montage...\", \"columns=\"+nSlices+\" rows=1 scale=0.5 border=2\");\n\trename(\"MontageHorizontal\");\n}\nfunction GetTime(){\n     MonthNames = newArray(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\");\n     DayNames = newArray(\"Sun\", \"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\");\n     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);\n     TimeString =\"Date: \"+DayNames[dayOfWeek]+\" \";\n     if (dayOfMonth&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+dayOfMonth+\"-\"+MonthNames[month]+\"-\"+year+\"\\nTime: \";\n     if (hour&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+hour+\":\";\n     if (minute&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+minute+\":\";\n     if (second&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+second;\n     print(TimeString);\n}\n\n</code></pre>", "<p>Thank you for the response.</p>\n<p>I seem to be getting an error</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e.png\" data-download-href=\"/uploads/short-url/1A4M5ltasL10ls2u4sg2ghPHmtU.png?dl=1\" title=\"Untitled\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_690x240.png\" alt=\"Untitled\" data-base62-sha1=\"1A4M5ltasL10ls2u4sg2ghPHmtU\" width=\"690\" height=\"240\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_690x240.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_1035x360.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0b162e6f9753ad3e272560db59cee26cc217a94e_2_1380x480.png 2x\" data-dominant-color=\"CED0D0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Untitled</span><span class=\"informations\">1751\u00d7611 117 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>In the output folder, the first image in .lif file is successfully converted to a raw .tiff, but nothing else is present.</p>\n<p>I would guess this error is happening because the script is getting through the first part, converting one image from .lif to a raw .tiff, but then the error occurs when this first image is attempted to be adjusted. No prompt comes up for any adjustment parameters, so the crash is likely occurring before that.</p>", "<p>That looks like it has likely failed to find the first of the tiff files. You should see a log message \u201cProcessing: fileName\u201d. If that filename is incorrect, or I suspect it may be missing the correct tiff extension, then you need to modify the processTiff function to ensure that the <code>open(input+file);</code> command has the correct filename and extension included.</p>", "<p>So I do get that log message, perhaps it\u2019s a bit hard to see in the screenshot I sent but the log message doesn\u2019t include .tiff at the end of it, it just ends with the name of the image.</p>\n<p>In the code, I tried adding a</p>\n<pre><code class=\"lang-auto\">suffix = \".tif\"\n</code></pre>\n<p>immediately prior to the proccessTiff function, but that didn\u2019t change anything.</p>\n<p>If what I have to adjust is the <code>file</code> then I don\u2019t know how to do that. I don\u2019t see a <code>file =</code> anywhere and the first reference to it comes as an input to the processFile function after the processFolder function (where it was presumably created).</p>\n<p>Also, should the processTiff function be taking in <code>input</code>? Since the saveTiff function is saving to <code>output</code> where presumably the raw/unadjusted tiffs are located. To get around this for now I\u2019ve just been selecting the same folder for both <code>input</code> and <code>output</code><br>\nEdit: Probably nevermind on this input / output thing, I may have misunderstood the code</p>", "<p>I think it should then work if you change the open command in processTiff to the below:<br>\n<code>open(input+file+\".tiff\");</code></p>\n<p>The use of input as a parameter in processTiff can be updated in the future if you wanted. Currently when the function is called (as below) it is using the output folder from the initial conversion for both input and  output, so everything is just using the same directory for now.</p>\n<p><code>processTiff(output, output, imageList[i]);</code></p>", "<p>That worked to fix the file not found error. Now the script runs through entirely, but only doing the function of the first macro (converting .lif to .tiff) without actually giving any of the prompts to adjust the .tiff files to do the job of the second macro.</p>\n<p>When I uncomment the</p>\n<pre><code class=\"lang-auto\">  //processTiff(output, output, imageList[i]);\n</code></pre>\n<p>line, I get this error<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/db5277d59146c8aa8cc41e0f2bd83ccace551407.png\" alt=\"image\" data-base62-sha1=\"vidfH0kCnAJfos7PPXEH9MdqYOX\" width=\"518\" height=\"430\"></p>\n<p>After successfully getting through the prompts for selecting the input/output directories, and after the log shows processing the first image file</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be.png\" data-download-href=\"/uploads/short-url/aNOabuSHXybglUrqlQSMa0AWdn8.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_690x194.png\" alt=\"image\" data-base62-sha1=\"aNOabuSHXybglUrqlQSMa0AWdn8\" width=\"690\" height=\"194\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_690x194.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_1035x291.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4bb7214105ad670b2351f384f52339b6d30fe3be_2_1380x388.png 2x\" data-dominant-color=\"C7DADB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">3224\u00d7910 113 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Edit: since this is a distinct issue with just this new script, I created a <a href=\"https://forum.image.sc/t/undefined-identifier-in-line-61-called-from-line/78781\">new topic</a></p>"], "78535": ["<p>Hello everyone,</p>\n<p>I\u2019m trying to quantify RNA FISH in different cell types with 3D z-stack images that have 4 channels.<br>\nC1 is RNA<br>\nC2 is YFP+ cells<br>\nC3 is RFP+ cells<br>\nC4 is DNA</p>\n<p>In order to do this, I coded a Macros pipeline in FIJI for 3D cell/spot segmentation. Now I want to use the <strong>3D MultiColoc</strong> of the ImageJ 3D Suite plugin to analyze colocalization between C2/3, C3/2, C2/1, and C3/1. However, I noticed that some of the labels counted 2+ cells as a single object, and I need to split the cells before I analyze the co-expression of YFP and RFP in different channels. Therefore, I used <strong>3D Manager</strong> from ImageJ 3D Suite to split objects and saved the new 3D ROIs as zip files ending with \u201cfilter_3droi\u201d. However, I couldn\u2019t find a way to convert these 3D ROIs into labeled images for the 3D MultiColoc analysis.</p>\n<p><strong>Here are some sample images:</strong><br>\n<a href=\"https://drive.google.com/drive/folders/1jwLyhxJHNeC8kzoFsRJx33zxryUft9Kd?usp=share_link\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://drive.google.com/drive/folders/1jwLyhxJHNeC8kzoFsRJx33zxryUft9Kd?usp=share_link</a></p>\n<p>The most relevant posts I found were all using 2D ROIs:</p><aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"4256\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/imagejan/40/14151_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/creating-labeled-image-from-rois-in-the-roi-manager/4256/2\">Creating Labeled Image from ROIs in the ROI Manager?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hi Ofra, \nwhat you\u2019re asking for is the general concept of a Labeling image. \nAs far as I know, there\u2019s no built-in way to do exactly what you want (converting ROI Manager entries to a labeling image), but there are several tools available from <a href=\"http://imagej.net/List_of_update_sites\" rel=\"noopener nofollow ugc\">update sites</a>: \n\n\nthe LOCI update site provides a ROI Map command (probably the closest to what you want to do), that produces an 8-bit indexed color image for up to 255 ROIs, or a 16-bit image when the number of ROIs is higher, \n\n\nthe 3D ImageJ Suite upda\u2026\n  </blockquote>\n</aside>\n<aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"55582\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/imagejan/40/14151_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/segmentation-masks-from-rois-plugins/55582/2\">Segmentation masks from rois plugins</a> <a class=\"badge-wrapper  bullet\" href=\"/c/announcements/10\"><span class=\"badge-category-bg\" style=\"background-color: #AB9364;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for announcements, such as new software releases and upcoming community events.\">Announcements</span></a>\n  </div>\n  <blockquote>\n    Thanks a lot for sharing, <a class=\"mention\" href=\"/u/lthomas\">@LThomas</a>! \nI agree there\u2019s definitely a need for such tool, but I\u2019m afraid that many people are re-creating almost identical functionality in various places: \n\n\n\nMaybe we could make a community effort to centralize this kind of stuff, and directly ship an implementation with Fiji? \nI opened an issue a while ago (albeit originally for the other direction, label image =&gt; ROI, but we should have both of course):\n  </blockquote>\n</aside>\n\n<p>I also found plugins that convert ROIs into binary masks, but that ruins the whole point of splitting the objects in the first place. I would really appreciate it if someone could help me out with converting 3D ROI into labeled images.</p>\n<p>Here are the websites for 3D Manager and 3D Multicoloc:<br>\n<a href=\"https://mcib3d.frama.io/3d-suite-imagej/plugins/3DManager/3D-Manager/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://mcib3d.frama.io/3d-suite-imagej/plugins/3DManager/3D-Manager/</a><br>\n<a href=\"https://mcib3d.frama.io/3d-suite-imagej/plugins/Analysis/Relationships/3D-MultiColoc/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://mcib3d.frama.io/3d-suite-imagej/plugins/Analysis/Relationships/3D-MultiColoc/</a></p>\n<p>Thank you very much!</p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a> ,</p>\n<p>In the new version of 3D Manage, called <strong>3D Manager V4</strong>, there is the option to <em>export</em> your roi list. The split option is not implemented yet in this new version, so in the meanwhile you can save your roi list from the legacy 3D Manager, load it into the new version and export it.</p>\n<p>It seems what would be ideal for your case is to have a macro function <em>export</em> for the legacy 3D Manager, I will try to implement it.</p>\n<p>Best,</p>\n<p>Thomas</p>", "<p>Hi <a class=\"mention\" href=\"/u/thomasboudier\">@ThomasBoudier</a></p>\n<p>Thank you so much for the recommendation! I was able to create a labeled image with V4, but I couldn\u2019t record the macros code for all the steps. Do you know if there is a reference page for batch analysis with 3d Manager V4? Thank you!</p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a> and <a class=\"mention\" href=\"/u/thomasboudier\">@ThomasBoudier</a>,</p>\n<p>if I understood correctly what you want to do, the following macro should no the job:</p>\n<pre><code class=\"lang-java\">//the image you got the ROIs from needs to be active, so IJ can catch its dimensions\nnewImage(\"Splitted ROIs\", \"16-bit black\", getWidth(), getHeight(), nSlices);\n\nrun(\"3D Manager\");\n\nExt.Manager3D_Count(nb_obj);\n\nfor (i = 0; i &lt; nb_obj; i++) {\n\tExt.Manager3D_Select(i);\n\tExt.Manager3D_FillStack(i, i, i);\n\t\n}\nsetMinAndMax(0, nb_obj);\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a></p>\n<p>Thank you so much for helping me with the code! I\u2019m also trying to run a size exclusion filter on imageJ to remove ROIs that are too large or too small. Do know if there is a way to extract measurements from 3D measure on 3D manager and delete the corresponding ROIs?</p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a>,</p>\n<p>Not at the computer right now but you can check all macro commands here: <a href=\"https://mcib3d.frama.io/3d-suite-imagej/uploads/MacrosFunctionsRoiManager3D.pdf\">https://mcib3d.frama.io/3d-suite-imagej/uploads/MacrosFunctionsRoiManager3D.pdf</a></p>\n<p>With the <code>Measure3D</code> function you can get the volume of an object and based on it decide if you want to draw it into the image or not.</p>", "<p>Hi <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a></p>\n<p>Thank you so much for sharing this resource with me! That is exactly what I needed!</p>\n<p>Have a great rest of your day <img src=\"https://emoji.discourse-cdn.com/twitter/laughing.png?v=12\" title=\":laughing:\" class=\"emoji\" alt=\":laughing:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Thanks <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a> , sometimes I forget my own development <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Just to complete, there is another method, not documented yet, called <strong>Manager3D_FillStack2</strong> that will not require to explicitly select the object, so you can replace the loop by :</p>\n<pre><code class=\"lang-auto\">for (i = 0; i &lt; nb_obj; i++) {\n\tExt.Manager3D_FillStack2(i,i+1, i+1, i+1);\n}\n</code></pre>\n<p>Best,</p>\n<p>Thomas</p>", "<p>Thank you <a class=\"mention\" href=\"/u/thomasboudier\">@ThomasBoudier</a> ! That\u2019s great to know!</p>", "<p>Hi <a class=\"mention\" href=\"/u/thomasboudier\">@ThomasBoudier</a>,</p>\n<p>first thanks for the new options in the 3D Suite (e.g. the direct measurements in standard results tables).</p>\n<aside class=\"quote no-group\" data-username=\"ThomasBoudier\" data-post=\"8\" data-topic=\"78535\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/thomasboudier/40/17506_2.png\" class=\"avatar\"> Thomas Boudier:</div>\n<blockquote>\n<p>sometimes I forget my own development</p>\n</blockquote>\n</aside>\n<p>I know that too well.</p>\n<p>Yesterday I just had a quick look at the new V4 3D Roi Manager and figured out that the live ROI button might not work correctly. At least I could not manage to get the ROIs displayed on the image. However, I might have missed something.<br>\nIf you prefer that, I can also create a GitHub issue.</p>", "<p>Hi <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a>,</p>\n<p>Thanks for testing the new version of 3D Manager, the procedure to display the objects  is, at the moment :</p>\n<ul>\n<li>Import Image</li>\n<li>Compute Rois</li>\n<li>Live Roi</li>\n<li>Select all</li>\n</ul>\n<p>Note that if you select another image, you need to <em>refresh</em> the display by selecting/deselecting some objects.</p>\n<p>Best,</p>\n<p>Thomas</p>", "<p><a class=\"mention\" href=\"/u/thomasboudier\">@ThomasBoudier</a>,</p>\n<p>ah ok, that worked. Thanks!</p>"], "72394": ["<p>Hello everyone,<br>\nFor my new job, I got an <strong>Ubuntu 22.04.1 LTS</strong> work laptop and I couldn\u2019t find much information on installing Cellprofiler on this system.<br>\nI\u2019ve tried a few variations of <a href=\"https://gist.github.com/GenevieveBuckley/d17a94eac2293d0540d5881bc358dd6a\" rel=\"noopener nofollow ugc\">this</a> and <a href=\"https://iqbalnaved.wordpress.com/2022/05/22/installing-cellprofiler-4-2-1-on-ubuntu-20-04/\" rel=\"noopener nofollow ugc\">this</a>, but neither worked.</p>\n<p>Before I start posting error messages, do you know if there is proper documentation for this installation that I might have missed?</p>\n<p>If not, did someone manage to install it and is willing to share some instructions?</p>\n<p>Thank you for the help!</p>", "<p>I\u2019m not aware of any installation instructions for Ubuntu 22 specifically - we would be happy to help you troubleshoot getting some working though!</p>", "<p>Hello <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a>!<br>\nThank you for the reply. I will try to describe what went wrong below!</p>\n<p>Following <a href=\"https://gist.github.com/GenevieveBuckley/d17a94eac2293d0540d5881bc358dd6a\" rel=\"noopener nofollow ugc\">this guide</a>:</p>\n<h2>\n<a name=\"at-point-3-1\" class=\"anchor\" href=\"#at-point-3-1\"></a>At point 3</h2>\n<p>after having added the link to the list of sources:</p>\n<p>I get the following issue:</p>\n<pre><code class=\"lang-auto\">Get:12 http://cz.archive.ubuntu.com/ubuntu bionic InRelease [242 kB]                \nErr:12 http://cz.archive.ubuntu.com/ubuntu bionic InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3B4FE6ACC0B21F32\nReading package lists... Done\nW: GPG error: http://cz.archive.ubuntu.com/ubuntu bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3B4FE6ACC0B21F32\nE: The repository 'http://cz.archive.ubuntu.com/ubuntu bionic InRelease' is not signed.\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\n</code></pre>\n<p>So when I try to install libwebkitgtk-1.0-0 of course I get:</p>\n<pre><code class=\"lang-auto\">E: Package 'libwebkitgtk-1.0-0' has no installation candidate\n</code></pre>\n<h2>\n<a name=\"in-point-7-2\" class=\"anchor\" href=\"#in-point-7-2\"></a>In point 7</h2>\n<p>There seems not to be a GitHub/CellProfiler directory. So I tried to install directly in the CellProfiler directory using:</p>\n<pre><code class=\"lang-auto\">pip install -e .\n</code></pre>\n<h2>\n<a name=\"try-to-import-3\" class=\"anchor\" href=\"#try-to-import-3\"></a>Try to import</h2>\n<p>After points 7, 8 and 9, if I try to import wx I get the following error:</p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; import wx\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/wx/__init__.py\", line 17, in &lt;module&gt;\n    from wx.core import *\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/wx/core.py\", line 12, in &lt;module&gt;\n    from ._core import *\nImportError: /lib/x86_64-linux-gnu/libgio-2.0.so.0: undefined symbol: g_module_open_full\n</code></pre>\n<h2>\n<a name=\"calling-cellprofiler-4\" class=\"anchor\" href=\"#calling-cellprofiler-4\"></a>Calling cellprofiler</h2>\n<p>If I try to call cellprofiler from the terminal I get:</p>\n<pre><code class=\"lang-auto\">(cellprofiler-dev) dallavem@PP4-1004-B:~/Downloads$ cellprofiler\nTraceback (most recent call last):\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/bin/cellprofiler\", line 5, in &lt;module&gt;\n    from cellprofiler.__main__ import main\n  File \"/home/dallavem/Downloads/CellProfiler/cellprofiler/__main__.py\", line 22, in &lt;module&gt;\n    from cellprofiler_core.measurement import Measurements\n  File \"/home/dallavem/Downloads/core/cellprofiler_core/measurement/__init__.py\", line 1, in &lt;module&gt;\n    from ._measurements import Measurements\n  File \"/home/dallavem/Downloads/core/cellprofiler_core/measurement/_measurements.py\", line 15, in &lt;module&gt;\n    from ..constants.image import CT_OBJECTS\n  File \"/home/dallavem/Downloads/core/cellprofiler_core/constants/image.py\", line 1, in &lt;module&gt;\n    from bioformats import READABLE_FORMATS\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/bioformats/__init__.py\", line 21, in &lt;module&gt;\n    import javabridge\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/javabridge/__init__.py\", line 38, in &lt;module&gt;\n    from .jutil import start_vm, kill_vm, vm, activate_awt, deactivate_awt\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/javabridge/jutil.py\", line 162, in &lt;module&gt;\n    import javabridge._javabridge as _javabridge\n  File \"_javabridge.pyx\", line 1, in init _javabridge\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n</code></pre>\n<p>I hope this info can help pointing at the problem, thank you!</p>", "<p>General note - I might try instead of cloning CellProfiler from GitHub, just pip installing it, as stuff is subject to change as we move to CP5. But ultimately your call! Definitely do clone and install core from GitHub if installing CellProfiler from GitHub</p>\n<blockquote>\n<p>At point 3</p>\n</blockquote>\n<p>Have you tried <a href=\"https://chrisjean.com/fix-apt-get-update-the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/\">this</a>? I\u2019m not certain webkit-gtk 1 is necessary but this should hopefully help you get it.</p>\n<blockquote>\n<p>At point 7</p>\n</blockquote>\n<p>Yup, looks like the person who wrote that guide had cloned inside a folder called GitHub</p>\n<blockquote>\n<p>Try to import</p>\n</blockquote>\n<p>See if the webgtk thing fixes the issue, if not, can you confirm whether or not you installed with the <a href=\"https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/\">latest wheels</a>, and if not can you try them? (CellProfiler might complain about 4.2.0 being too new a version of wx but it shouldn\u2019t prevent run in my experience)</p>\n<blockquote>\n<p>Calling cellprofiler<br>\nUsually that happens if the version of numpy is changed after the version of python-javabridge is installed; it\u2019s fussy like that. Using the code in the comment below (minus installing cellpose, unless you also wan that!) typically solves it.</p>\n</blockquote>\n<aside class=\"quote\" data-post=\"4\" data-topic=\"68994\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/cannot-use-cellpose-plugin-with-cellprofiler/68994/4\">Cannot use cellpose plugin with cellprofiler</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    It\u2019s been a while, but I figured I would update this to say that we do now have a .bat file for easing the install on Windows. Details can be found <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/tree/master/Instructions/Windows_batch_file\">here</a>. \nAlso, have you tried something like this? \npip uninstall -y numpy python-javabridge centrosome\npip install numpy==1.21\npip install --no-cache-dir --no-deps --no-build-isolation centrosome python-javabridge\npip install cellprofiler cellpose\n  </blockquote>\n</aside>\n", "<p>Hello,</p>\n<p>Update your envirnoment by following <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Conda-Installation\" rel=\"noopener nofollow ugc\">this</a>. It\u2019s work fine to install cellprofiler 4.2.4 on ubuntu 22.04.</p>\n<p>Try this command to update your environment conda :<br>\n<code>conda env update --name myenv --file local.yml --prune</code></p>\n<p>Best,</p>", "<p>Hi,</p>\n<p>the problem is that CellProfiler requires Python3.8 but Ubuntu 22.04 comes with Python3.10. In my case it helped to install Python3.8 and make a local environment with it. I made some new instructions for it, integrating some of the instructions I found on the GitHUB pages. It works, hopefully it will work for others as well. You can find the instructions on our lab web page: <a href=\"http://www.pfistererlab.org/tools.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Pfisterer Laboratory</a></p>\n<p><a href=\"https://www.pfistererlab.org/cubbli22_cellprofiler_4_installation.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://www.pfistererlab.org/cubbli22_cellprofiler_4_installation.html</a></p>"], "78539": ["<p>Hello! I am using the Vedo library in Python to continuously rotate a .obj file. Although my code is partially working, the window freezes when I try to interact with it or minimize it. I would appreciate any guidance on what I might be doing wrong in my implementation.</p>\n<pre><code class=\"lang-auto\">from vedo import *\nimport time\n\n# Load the object file\nobj = load(\"file.obj\")\n\nwhile True:\n    # Rotate the mesh around the y-axis by 10 degrees\n    obj.rotate_y(10)\n\n    # Wait for 0.1 seconds before rotating again\n    time.sleep(0.1)\n\n    obj.show(interactive=False)\n</code></pre>", "<p>Hi, try this:</p>\n<pre><code class=\"lang-python\">from vedo import *\nfrom time import time\n\ndef loop_func(event):\n    #print(event)\n    msh.rotate_z(0.1)\n    txt.text(f\"time: {time()-t0} sec\")\n    plt.render()\n\nmsh = Cube() #Mesh(\"file.obj\")\ntxt = Text2D(bg='yellow')\nt0 = time()\n\nplt = Plotter(axes=1)\nplt += [msh, txt]\nplt.add_callback(\"timer\", loop_func)\nplt.timer_callback(\"start\")\nplt.show()\nplt.close()\n</code></pre>\n<p>See also <a href=\"https://github.com/marcomusy/vedo/blob/8039c5ea96a61d6801a74be501a54c72ffc07419/examples/simulations/airplane1.py\">this example</a>.</p>"], "78541": ["<p>I am trying to write a macro that will build a table with the necessary image data and then record the measurements that I perform in the same table. This shouldn\u2019t be a big deal. I am opening the image in a for loop and then using the table functions to record the information from the image and the measurements as I go. However, I have been working with a simplified version of the code because my created table is empty once I introduce the for loop that opens images. Here is the code for comparison.</p>\n<p>//Working Code<br>\n//image already open<br>\nrowIndex = 0;<br>\nimageTitle = getTitle();<br>\nbasename = substring(imageTitle, 0, indexOf(imageTitle, \u201c.tif\u201d));<br>\nanimal = \u201c77R\u201d;<br>\nsection = \u201c2.3\u201d;<br>\nDialog.create(\u201cFill in animal name and section number\u201d); //title on window<br>\nDialog.addString(\u201cImage Name\u201d, basename); //correct image name<br>\nDialog.addString(\u201cAnimal Name\u201d, animal);<br>\nDialog.addString(\u201cSection Number\u201d, section);<br>\nDialog.show();<br>\nimageName = Dialog.getString();<br>\nanimal = Dialog.getString();<br>\nsection = Dialog.getString();<br>\nlobule = getNumber(\u201cWhich Lobule are you measuring?\u201d, 8);<br>\nTable.create(\u201cCortical Width Measurements\u201d);<br>\nTable.setLocationAndSize(240, 125, 600, 700);<br>\nTable.set(\u201cImage\u201d, rowIndex, imageName);<br>\nTable.set(\u201cAnimal\u201d, rowIndex, animal);<br>\nTable.set(\u201cSection\u201d, rowIndex, section);<br>\nTable.set(\u201cLobule\u201d, rowIndex, lobule);<br>\nselectWindow(imageTitle);<br>\nclose();</p>\n<p>//Code with empty table<br>\nimagePath = getDirectory(\u201cWhere are the images located?\u201d);<br>\nfilelist = getFileList(imagePath)<br>\nfor (rowIndex = 0; rowIndex &lt; lengthOf(filelist); rowIndex++) {<br>\nif (endsWith(filelist[rowIndex], \u201c.tif\u201d)) {<br>\n//open(imagePath + File.separator + filelist[rowIndex]);<br>\nwaitForUser;<br>\nimageTitle = getTitle();<br>\nbasename = substring(imageTitle, 0, indexOf(imageTitle, \u201c.tif\u201d));<br>\nanimal = \u201c77R\u201d;<br>\nsection = \u201c2.3\u201d;<br>\nDialog.create(\u201cFill in animal name and section number\u201d); //title on window<br>\nDialog.addString(\u201cImage Name\u201d, basename); //correct image name<br>\nDialog.addString(\u201cAnimal Name\u201d, animal);<br>\nDialog.addString(\u201cSection Number\u201d, section);<br>\nDialog.show();<br>\nimageName = Dialog.getString();<br>\nanimal = Dialog.getString();<br>\nsection = Dialog.getString();<br>\nlobule = getNumber(\u201cWhich Lobule are you measuring?\u201d, 8);<br>\nTable.create(\u201cCortical Width Measurements\u201d);<br>\nTable.setLocationAndSize(240, 125, 600, 700);<br>\nTable.set(\u201cImage\u201d, rowIndex, imageName);<br>\nTable.set(\u201cAnimal\u201d, rowIndex, animal);<br>\nTable.set(\u201cSection\u201d, rowIndex, section);<br>\nTable.set(\u201cLobule\u201d, rowIndex, lobule);<br>\nselectWindow(imageTitle);<br>\nclose();<br>\n}<br>\n}</p>\n<p>I thought I had read all the documentation to get this code to work but perhaps I missed something? Thank You!<br>\nJenny</p>", "<p>Hi <a class=\"mention\" href=\"/u/jennifer_fessler\">@Jennifer_Fessler</a>,</p>\n<p>the <code>Table.create</code> should not be inside of the loop. Just call it once before the loop starts, otherwise it will clear the table in each iteration.</p>\n<p>Best,<br>\nVolker</p>", "<p>Hey <a class=\"mention\" href=\"/u/volker\">@volker</a><br>\nThanks for getting back with me. I pulled the Table.create out of the for loop. I forgot last night in playing around with the code I left it in there. Here is the altered code. The table does not update. I even inserted a bit of code to select the table window to avoid confusion on which window I am updating and it still does not update the table. I am playing with this simplified code because the table was not updating in larger macro I had set up and I was trying to isolate the issue. Thanks for the help!</p>\n<p>Table.create(\u201cCortical Width Measurements\u201d);<br>\nTable.setLocationAndSize(240, 125, 600, 700);<br>\nimagePath = getDirectory(\u201cWhere are the images located?\u201d);<br>\nfilelist = getFileList(imagePath)<br>\nfor (rowIndex = 0; rowIndex &lt; lengthOf(filelist); rowIndex++) {<br>\nif (endsWith(filelist[rowIndex], \u201c.tif\u201d)) {<br>\nopen(imagePath + File.separator + filelist[rowIndex]);<br>\nwaitForUser;<br>\nimageTitle = getTitle();<br>\nbasename = substring(imageTitle, 0, indexOf(imageTitle, \u201c.tif\u201d));<br>\nanimal = \u201c77R\u201d;<br>\nsection = \u201c2.3\u201d;<br>\nDialog.create(\u201cFill in animal name and section number\u201d); //title on window<br>\nDialog.addString(\u201cImage Name\u201d, basename); //correct image name<br>\nDialog.addString(\u201cAnimal Name\u201d, animal);<br>\nDialog.addString(\u201cSection Number\u201d, section);<br>\nDialog.show();<br>\nimageName = Dialog.getString();<br>\nanimal = Dialog.getString();<br>\nsection = Dialog.getString();<br>\nlobule = getNumber(\u201cWhich Lobule are you measuring?\u201d, 8);<br>\nselectWindow(\u201cCortical Width Measurements\u201d);<br>\nTable.set(\u201cImage\u201d, rowIndex, imageName);<br>\nTable.set(\u201cAnimal\u201d, rowIndex, animal);<br>\nTable.set(\u201cSection\u201d, rowIndex, section);<br>\nTable.set(\u201cLobule\u201d, rowIndex, lobule);<br>\nselectWindow(imageTitle);<br>\nclose();<br>\n}<br>\n}</p>", "<p>Hi <a class=\"mention\" href=\"/u/jennifer_fessler\">@Jennifer_Fessler</a>,<br>\ndoes this (even more simplified code) work ?</p>\n<pre><code class=\"lang-auto\">Table.setLocationAndSize(240, 125, 600, 700);\nfor (rowIndex = 0; rowIndex &lt; 10; rowIndex++) {\n    if (true) {\n        selectWindow(\"Cortical Width Measurements\");\n        Table.set(\"Image\", rowIndex, 1);\n        Table.set(\"Animal\", rowIndex, 2);\n        Table.set(\"Section\", rowIndex, 3);\n        Table.set(\"Lobule\", rowIndex, 4);    \n    }\n}\n</code></pre>\n<p>For me it does.</p>\n<p>Instead of selecting the window you can also add the name of the table as the last parameter to all Table-commands. You can also try to call <code>Table.update</code>at the end or after each row.</p>\n<p>Best,<br>\nVolker</p>", "<p>Hello <a class=\"mention\" href=\"/u/volker\">@volker</a><br>\nYes, this code does work. I will try your suggestions, but what about opening an image is causing the Table functions to stall?<br>\nThanks, Jenny</p>", "<p>Hey <a class=\"mention\" href=\"/u/volker\">@volker</a><br>\nSo I tried your update suggestions by adding Table.update and calling the table at the end of each Table.set call.<br>\nThis worked. Thank you so much for your help!<br>\n//working code</p>\n<p>Table.create(\u201cCortical Width Measurements\u201d);<br>\nTable.setLocationAndSize(240, 125, 600, 700);<br>\nimagePath = getDirectory(\u201cWhere are the images located?\u201d);<br>\nfilelist = getFileList(imagePath)<br>\nfor (rowIndex = 0; rowIndex &lt; lengthOf(filelist); rowIndex++) {<br>\nif (endsWith(filelist[rowIndex], \u201c.tif\u201d)) {<br>\nopen(imagePath + File.separator + filelist[rowIndex]);<br>\nwaitForUser;<br>\nimageTitle = getTitle();<br>\nbasename = substring(imageTitle, 0, indexOf(imageTitle, \u201c.tif\u201d));<br>\nanimal = \u201c77R\u201d;<br>\nsection = \u201c2.3\u201d;<br>\nDialog.create(\u201cFill in animal name and section number\u201d); //title on window<br>\nDialog.addString(\u201cImage Name\u201d, basename); //correct image name<br>\nDialog.addString(\u201cAnimal Name\u201d, animal);<br>\nDialog.addString(\u201cSection Number\u201d, section);<br>\nDialog.show();<br>\nimageName = Dialog.getString();<br>\nanimal = Dialog.getString();<br>\nsection = Dialog.getString();<br>\nlobule = getNumber(\u201cWhich Lobule are you measuring?\u201d, 8);</p>\n<p>Table.set(\u201cImage\u201d, rowIndex, imageName,\u201cCortical Width Measurements\u201d);<br>\nTable.update;<br>\nTable.set(\u201cAnimal\u201d, rowIndex, animal,\u201cCortical Width Measurements\u201d);<br>\nTable.update;<br>\nTable.set(\u201cSection\u201d, rowIndex, section,\u201cCortical Width Measurements\u201d);<br>\nTable.update;<br>\nTable.set(\u201cLobule\u201d, rowIndex, lobule, \u201cCortical Width Measurements\u201d);<br>\nTable.update;<br>\nselectWindow(imageTitle);<br>\nclose();<br>\n}<br>\n}</p>", "<p>Hi <a class=\"mention\" href=\"/u/jennifer_fessler\">@Jennifer_Fessler</a>,</p>\n<p>you could use a variable for the name of the table, that way if ever you want to change the name you have to do it only in one place.</p>\n<p>I don\u2019t think that you need to call the <code>Table.update</code>after each <code>set</code>. Either once at the end or once per row might be enough, but I admit that the updating behavior of the ImageJ tables is a mystery to me.</p>\n<p>Best,<br>\nVolker</p>"], "78544": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/f/cfbff39ee1889cc23973dd01d2711de1fc7fe7e6.jpeg\" data-download-href=\"/uploads/short-url/tDQ5DlCCKSAjv0CoTceioYU7p30.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/f/cfbff39ee1889cc23973dd01d2711de1fc7fe7e6.jpeg\" alt=\"image\" data-base62-sha1=\"tDQ5DlCCKSAjv0CoTceioYU7p30\" width=\"690\" height=\"325\" data-dominant-color=\"989276\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">881\u00d7416 54.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nA bit new at using this great library, so be gentle.<br>\nI\u2019m using the felzenszwalb segmentation functions to create a mask for images, and it does great at that.   I was wondering if it\u2019s possible to get the coordinate values for all (or a subset of) the lines making up the mask so I have a list of vertices I can feed to other applications?</p>", "<p>Hey <a class=\"mention\" href=\"/u/pccross\">@pccross</a> and welcome!</p>\n<p>I\u2019m curious whether you are interested in the coordinates of the yellow mask you showed, which I think are the <em>boundaries</em> between segments found by Felzenszwalb, or whether you want the coordinates of the pixels making up each segment?</p>\n<p>I\u2019m also curious about exactly what features you\u2019re trying to find in the image, because it doesn\u2019t look to me like Felzenszwalb actually does a great job of finding the \u201cwalls\u201d and \u201cgaps\u201d in it. But I\u2019m not sure if that\u2019s exactly what you\u2019re after.</p>\n<p>In addition to answering those questions, would you be able to share (a) the source image itself, and (b) the code to get the figure you showed above? This can let us play around with it a bit more to see if we can come up with a solution to your problem.</p>", "<p>Thanks for the prompt response <a class=\"mention\" href=\"/u/jni\">@jni</a> !</p>\n<p>What I\u2019m trying to do is use the 2d \u2018blueprint\u2019 to get approximate coordinates that I can feed into Unity and build a 3d model from it.   So it doesn\u2019t have to be exact.    I think I can use the yellow mask for extracting coordinates, although as you mention Felzenszwalb may not be be doing a great job of finding \u2018walls and gaps\u2019, and it requires tweaking the scale quite a bit.    Since I\u2019m not CV or ML expert, I\u2019m not sure if this is best route to go, so if you have any suggestions for segmentation or coordinate extraction alternatives, I\u2019m completely open <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>As you requested, am including the code and image source.    Really appreciate the insights you can provide.<br>\nFor it to work, I created subdirectories under content in Collab:  \u2018content\\data\\dungeons*.png\u2019<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c51b89f03c75ed1a2b44ef9854a95c4b59200ab8.png\" data-download-href=\"/uploads/short-url/s7H3VrSV4XiFXWxgSt28DEawBYY.png?dl=1\" title=\"dung1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c51b89f03c75ed1a2b44ef9854a95c4b59200ab8.png\" alt=\"dung1\" data-base62-sha1=\"s7H3VrSV4XiFXWxgSt28DEawBYY\" width=\"500\" height=\"500\" data-dominant-color=\"79715F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">dung1</span><span class=\"informations\">512\u00d7512 463 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-auto\"># Importing the required libraries\nfrom skimage.segmentation import felzenszwalb\nfrom skimage.color import rgb2gray\nfrom skimage import data\nfrom skimage.filters import gaussian\nfrom skimage.segmentation import active_contour\nimport matplotlib.pyplot as plt\nimport os\nfrom skimage.color import label2rgb\nfrom pathlib import Path\nimport numpy as np\n\ndata_path = Path(\"data/\")\n\nfrom skimage.segmentation import slic, mark_boundaries\nfrom PIL import Image\n \n# Setting the figure size as 15, 15\nplt.figure(figsize=(15,15))\n \nimage_path = data_path /\"dungeons\"\nprint(image_path)\n\n# Get image class from path name\nimage_class = image_path.parent.stem\nprint(image_class)\n#Open image\nsuffix = '.png'\nbase_filename = 'dung1'\ndungImage = os.path.join(image_path, base_filename + suffix)\ninterDungeon = Image.open(dungImage)\ndungeon = np.asanyarray(interDungeon)\n#dungeon = dungImage()\ngray_dungeon = rgb2gray(dungeon)\n \n# computing the Felzenszwalb's\n# Segmentation with sigma = 5 and minimum\n# size = 100\ndungeon_segments = felzenszwalb(dungeon,\n                                  scale = 80,\n                                  sigma=5,\n                                  min_size=100)\n\n# Plotting the original image\nplt.subplot(1,2,1)\nplt.imshow(dungeon)\n \n# Marking the boundaries of\n# Felzenszwalb's segmentations\nplt.subplot(1,2,2)\nplt.imshow(mark_boundaries(dungeon,\n                           dungeon_segments))\nprint (dungeon_segments)\n</code></pre>", "<p>Ok, so here\u2019s an approach:</p>\n<p>My goal is to get the walls in the image. I\u2019ve noticed that there\u2019s a lot of brightness variation in the image <em>but</em> the sides of the walls are reliably dark. In order to make the inside of the walls dark as well, you can do an <em>opening</em>, which expands the dark regions, then expands the bright regions. The order of the operations means that if you have a bright gap smaller than the radius of the opening, it\u2019ll stay dark, because the bright gap will be \u201cobliterated\u201d by the darkness. Sounds bleak but I guess it matches the theme. <img src=\"https://emoji.discourse-cdn.com/twitter/joy.png?v=12\" title=\":joy:\" class=\"emoji\" alt=\":joy:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>I measured the walls to be about 10 pixels thick, so I tried an opening with a radius of 5:</p>\n<pre><code class=\"lang-python\">import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nfrom skimage import morphology, color, filters, measure\nimport imageio.v3 as iio\n\n\nimage = iio.imread('dungeon.png')\ngray = color.rgb2gray(image)\nopened = morphology.opening(gray, morphology.disk(5))\nfig, (ax0, ax1) = plt.subplots(1, 2)\nax0.imshow(gray, cmap='gray')\nax0.set_axis_off()\nax0.set_title('grayscale')\nax1.imshow(opened, cmap='gray')\nax1.set_axis_off()\nax1.set_title('opened')\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe.png\" data-download-href=\"/uploads/short-url/2zgHkf32jZOqPt2tv2CQJrqgQRo.png?dl=1\" title=\"opened-compare\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe_2_690x331.png\" alt=\"opened-compare\" data-base62-sha1=\"2zgHkf32jZOqPt2tv2CQJrqgQRo\" width=\"690\" height=\"331\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe_2_690x331.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe_2_1035x496.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1200fb713b59749273cdb71ff3757adb219e4abe.png 2x\" data-dominant-color=\"AFAFAF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">opened-compare</span><span class=\"informations\">1097\u00d7527 152 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Cool! Now the walls are completely dark! One thing I noticed is that there\u2019s a few wispy bits and, for example, the entrance to the central chamber thing still has a black line across it. We can do the reverse process now with a <em>closing</em>. As long as the radius is smaller than our walls, they should stay intact while thinner structures are removed.</p>\n<pre><code class=\"lang-python\">closed = morphology.closing(opened, morphology.disk(3))\n\nfig, (ax0, ax1) = plt.subplots(1, 2)\nax0.imshow(opened, cmap='gray')\nax0.set_axis_off()\nax0.set_title('opened')\nax1.imshow(closed, cmap='gray')\nax1.set_axis_off()\nax1.set_title('closed')\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58.png\" data-download-href=\"/uploads/short-url/dUrcOD33wBLluWdcYtI31mfkeV2.png?dl=1\" title=\"closed-compare\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58_2_690x331.png\" alt=\"closed-compare\" data-base62-sha1=\"dUrcOD33wBLluWdcYtI31mfkeV2\" width=\"690\" height=\"331\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58_2_690x331.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58_2_1035x496.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/1/617d8124eb1f84cf05a6a42a9cbcc8438566ca58.png 2x\" data-dominant-color=\"AAAAAA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">closed-compare</span><span class=\"informations\">1097\u00d7527 119 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Minor difference, but you\u2019ll notice the dark line closing off the chamber is gone. Now we need to threshold it \u2014 find only the dark bits. There\u2019s lots of thresholding algorithms in scikit-image, but there\u2019s a nice function to try them all in one go:</p>\n<pre><code class=\"lang-python\">from skimage import filters\n\nfilters.try_all_threshold(closed)\n</code></pre>\n<p>Which gives:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1.png\" data-download-href=\"/uploads/short-url/iAn9B2bi7qYAWL6H7y9cRnpjnxL.png?dl=1\" title=\"try-all-threshold\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_314x500.png\" alt=\"try-all-threshold\" data-base62-sha1=\"iAn9B2bi7qYAWL6H7y9cRnpjnxL\" width=\"314\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_314x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_471x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/2/8243b7a0b5218b3aef4b9af1fa90de055e275ce1_2_628x1000.png 2x\" data-dominant-color=\"CFCFCF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">try-all-threshold</span><span class=\"informations\">646\u00d71026 60.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Minimum is looking pretty good! Now we can threshold the image, and use <code>skimage.measure.find_contours</code> to find the coordinates around the walls:</p>\n<pre><code class=\"lang-python\">thresholded_min = closed &lt; filters.threshold_minimum(closed)\ncontours = measure.find_contours(thresholded_min)\n\nfig, ax = plt.subplots()\nax.imshow(gray, cmap='gray')\nax.set_axis_off()\n\nfor contour in contours:\n    contour_xy = contour[:, [1, 0]]  # matplotlib uses xy coordinates, not row/col\n    ax.add_patch(patches.Polygon(\n            contour_xy,\n            facecolor=(0, 0, 0, 0),\n            edgecolor='cornflowerblue',\n            linewdith=1))\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206.png\" data-download-href=\"/uploads/short-url/9T4BgMoMsr8kfcGEGENbg2stezY.png?dl=1\" title=\"contours\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206_2_597x500.png\" alt=\"contours\" data-base62-sha1=\"9T4BgMoMsr8kfcGEGENbg2stezY\" width=\"597\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206_2_597x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/454d51b8d2afe86ccec8e638033f21594b789206.png 2x\" data-dominant-color=\"B4B5B6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">contours</span><span class=\"informations\">891\u00d7746 327 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>So, that\u2019s something! I\u2019ll add two notes:</p>\n<ol>\n<li>If you\u2019re going to be processing lots of these, it might be useful to use some machine learning tools to draw on the walls and get a really high contrast between walls and background. I got the following result by painting for about a minute in <a href=\"https://ilastik.org\">Ilastik</a> using the pixel classification workflow:</li>\n</ol>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41.png\" data-download-href=\"/uploads/short-url/xd5mIiYxPrxPiDQTCzPwmjLbmBX.png?dl=1\" title=\"Screen Shot 2023-03-16 at 5.50.03 pm\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_690x446.png\" alt=\"Screen Shot 2023-03-16 at 5.50.03 pm\" data-base62-sha1=\"xd5mIiYxPrxPiDQTCzPwmjLbmBX\" width=\"690\" height=\"446\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_690x446.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_1035x669.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8c2718779e3dfeb89071de7ad79bf63f6629b41_2_1380x892.png 2x\" data-dominant-color=\"898F8D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-16 at 5.50.03 pm</span><span class=\"informations\">3824\u00d72474 1.97 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Looks pretty good I reckon! Check out some tutorials on their site. Once you\u2019re done you can export the resulting probability maps and repeat the workflow above, but with a much nicer signal to noise I should think\u2026</p>\n<ol start=\"2\">\n<li>If you\u2019re going to be doing a smaller batch, then before finding the contours, you might be able to refine the thresholded example in <a href=\"https://napari.org\">napari</a>:</li>\n</ol>\n<pre><code class=\"lang-python\">import napari\n\nviewer, layer = napari.imshow(gray)\nthresholded_edit = np.copy(thresholded_min)  # to preserve original\nlabels_layer = napari.add_labels(thresholded_edit)\n# now use some of the paintbrush/eraser tools in napari\n# the thresholded_edit array will be directly updated\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28.png\" data-download-href=\"/uploads/short-url/17W1i8LoTNjCbfolkCFufMhkizS.png?dl=1\" title=\"Screen Shot 2023-03-16 at 9.01.39 pm\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_613x500.png\" alt=\"Screen Shot 2023-03-16 at 9.01.39 pm\" data-base62-sha1=\"17W1i8LoTNjCbfolkCFufMhkizS\" width=\"613\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_613x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_919x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07e7d2b238b9691a7e180aa0673144798d9d2f28_2_1226x1000.png 2x\" data-dominant-color=\"646060\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-16 at 9.01.39 pm</span><span class=\"informations\">3028\u00d72468 2.19 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>(about 2 min of painting on napari \u2014 notice the fixed gaps around the central chamber.)</p>\n<p>I hope all of the above is helpful and inspirational! <img src=\"https://emoji.discourse-cdn.com/twitter/blush.png?v=12\" title=\":blush:\" class=\"emoji\" alt=\":blush:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<aside class=\"quote no-group\" data-username=\"jni\" data-post=\"4\" data-topic=\"78544\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jni/40/5991_2.png\" class=\"avatar\"> Juan Nunez Iglesias:</div>\n<blockquote>\n<p>skimage.measure.find_contours</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/jni\">@jni</a>  Wow!  I think this fits my needs perfectly!  Thanks for the insights, and awesome explanations <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>One question on the contours, since I\u2019m not completely clear on that part.<br>\nSo when I print out the contours variable, I get a long list where the following are first few arrays for it. How do I interpret each of the arrays?   Does each array correlate to a line segment?</p>\n<p>[array([[285.5,  51. ],<br>\n[285.5,  50. ],<br>\n[285. ,  49.5],<br>\n\u2026,<br>\n[284.5,  52. ],<br>\n[285. ,  51.5],<br>\n[285.5,  51. ]]), array([[466.5, 434. ],<br>\n[466. , 433.5],<br>\n[465.5, 433. ],<br>\n\u2026,<br>\n[465.5, 435. ],<br>\n[466. , 434.5],<br>\n[466.5, 434. ]]), array([[215.5, 375. ],<br>\n[215. , 374.5],<br>\n[214.5, 374. ],<br>\n\u2026,<br>\n[214.5, 376. ],<br>\n[215. , 375.5],<br>\n[215.5, 375. ]])</p>", "<p>Each contour is a closed loop around a contiguous bunch of \u201cTrue\u201d pixels. Does that make sense?</p>", "<p>Yep, makes total sense.   Thanks again so much for the detailed insights.  I\u2019m going to dive more deeply into testing examples over the next week, and will definitely come back if I have additional questions <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78549": ["<p>Hi,</p>\n<p>I have tried using pyclesperanto_prototype to perform segmentation and it works excellent on a demo image. However, I have problem when using my own images. I can show the images using imshow from matplotlib but when using the imshow from it pyclesperanto_prototype no image is shown.</p>\n<p>My code:</p>\n<pre><code class=\"lang-auto\">import sys\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pyclesperanto_prototype as cle\nfrom skimage import io\n\nimg1 = io.imread(\"Own image.jpg\")\nimg2 = io.imread(\"IXMtest_A02_s9_w1.tif\")\n\ncle.imshow(img1)          #Not shown\nplt.show()\n\nplt.imshow(img1, cmap='gray')      #Shown\nplt.show()\n\ncle.imshow(img2)          #Shown\nplt.show()\n\nplt.imshow(img2, cmap='gray')         #Shown\nplt.show()\n</code></pre>\n<p>Hope for help!<br>\nBR<br>\nFredrik Olsson</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/9I5Dr2fSECOebnYg8cO05IgX7sT.tif\">IXMtest_A02_s9_w1.tif</a> (709.3 KB)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f.jpeg\" data-download-href=\"/uploads/short-url/htpfHH2ziVLhTVRBoCYXhxCq87d.jpeg?dl=1\" title=\"Own image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f_2_669x500.jpeg\" alt=\"Own image\" data-base62-sha1=\"htpfHH2ziVLhTVRBoCYXhxCq87d\" width=\"669\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f_2_669x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a77f46bbfc5114064e19a4078ad0ecb1efe080f.jpeg 2x\" data-dominant-color=\"885A56\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Own image</span><span class=\"informations\">750\u00d7560 227 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hey!</p>\n<p>I\u2019m not familiar with clesperanto, but based on their <a href=\"https://github.com/clEsperanto/clesperanto.github.io\" rel=\"noopener nofollow ugc\">repo</a> it doesn\u2019t seem like an active project. I would be weary of adopting it (even though it seems super cool).</p>\n<p>Maybe if you explain what is your overall goal, as opposed to \u201cwhy doesn\u2019t X work?\u201d, someone could offer you an alternative. <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Leo</p>", "<p>Hi <a class=\"mention\" href=\"/u/fredrik\">@Fredrik</a> ,</p>\n<p>I just ran your code from a Python Jupyter lab and do see an image when executing it.</p>\n<p>When running this, it shows a very thin image:</p>\n<pre><code class=\"lang-auto\">import sys\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pyclesperanto_prototype as cle\nfrom skimage import io\n\nimg1 = io.imread(\"Own image.jpg\")\nimg2 = io.imread(\"IXMtest_A02_s9_w1.tif\")\n\ncle.imshow(img1)          #Not shown\nplt.show()\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b.png\" data-download-href=\"/uploads/short-url/eHMn5L1XIrpcA1CWlCyUu2slupZ.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b_2_294x500.png\" alt=\"image\" data-base62-sha1=\"eHMn5L1XIrpcA1CWlCyUu2slupZ\" width=\"294\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b_2_294x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67115c160747db12091d0cb5b1c512cea31d561b.png 2x\" data-dominant-color=\"F8F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">407\u00d7690 18.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The underlying reason is that clesperanto is not made for RGB images (such as jpeg). <a href=\"https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/12a_image_file_formats/image_file_formats.html#why-jpeg-should-be-avoided\">Read more why working with JPG is a bad idea in scientific image analysis</a>.</p>\n<p>You could show the individual channels of the RGB image like this:</p>\n<pre><code class=\"lang-auto\">cle.imshow(img1[...,0])\n</code></pre>\n<p>or this</p>\n<pre><code class=\"lang-auto\">cle.imshow(img1[:,:,0])\n</code></pre>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/b/cb341749a2825c5b03b88ec3a47c047c0d8dd3b6.jpeg\" alt=\"image\" data-base62-sha1=\"sZCwUIvwY7CE2z1vKpPIFaoxioC\" width=\"647\" height=\"498\"></p>\n<p>Btw. here is the <a href=\"https://github.com/clEsperanto/pyclesperanto_prototype/\">link to the actively maintained repo</a>, not the repo of its website.</p>\n<p>Let us know if this helps!</p>\n<p>Best,<br>\nRobert</p>", "<p>Hi,</p>\n<p>Yes it really helped, thanks a lot!! Thank you for explaining that it was not intended for RGB images and how I can show the single channels. I do work mainly with tiff and czi images but also ran into occasions where I only have jpg images available.</p>\n<p>Since it is very difficult to add single layer powders with no overlaps you ran into segmentation problems and I am interested to try it out to see how well it performs <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>BR<br>\nFredrik</p>"], "78551": ["<p>Is DeepLabCut useful for analysing animals with no rigid skeletons/animals with fully muscular compositions? If yes, any particular steps to be mindful of/procedures to follow to analyse animals with don\u2019t have bones/rigid skeletons?</p>", "<p>I know of some projects where DLC was used on octopi and cuttlefish so definitely yes. The rules are the same, keep your labelling as precise and as consistent as possible.</p>\n<p>The post pose estimation analysis depends on your goal, you should have one before you invest significant amounts of time into training a pose estimation model</p>", "<p>thank you very much for letting me know, I will keep your pointers in mind. appreciate the help!</p>"], "78552": ["<p>Hi everyone,<br>\nMay I ask for some help please? I plan to measure the thinnest part of dentin after performing mechanical instrument (MI) in CT image and would like to measure dentin thickness of the same area (before performing mechanical instrument).  I do not know which plugin should I use? Could you mind help suggest me please?</p>\n<p>Adjabhak</p>", "<p>Hi<br>\nIt is difficult to give you advice without seeing the study support. Can you upload two images:</p>\n<ul>\n<li>a precisely annotated</li>\n<li>one not annotated<br>\nThanks in advance.</li>\n</ul>\n<pre><code class=\"lang-auto\">macro \"Thinnest area of dentin measurement\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\nimg=getImageID();\n// Start batch mode\n//------------------------------\nselectImage(img);\nsetBatchMode(true);\nrun(\"Duplicate...\", \"title=1\");\nclose(\"\\\\Others\");\nrun(\"Duplicate...\", \"title=[The result]\");\nrun(\"Duplicate...\", \"title=2\");\n// Start processing\n//------------------------------\nrun(\"Convert to Mask\");\nrun(\"Duplicate...\", \"title=temp\");\nrun(\"Invert\");\n//------------------------------\nrun(\"Find Maxima...\", \"prominence=10 strict exclude output=[Point Selection]\");\nrun(\"Find Maxima...\", \"prominence=10 strict exclude output=List\");\nclose(\"temp\");\nTable.sort(\"X\");\n//------------------------------\n//distance between points\ndx=getResult(\"X\",0)-getResult(\"X\",1);\ndy=getResult(\"Y\",0)-getResult(\"Y\",1);\nd=sqrt(pow(dx,2)+pow(dy,2));\nR=d;// User choice\n//------------------------------\nfor(j=0;j&lt;2;j++){\n//------------------------------\nTable.create(\"Measure Results for point  n\u00b0: \"+(j+1));\n//------------------------------\nfor(i=0;i&lt;36;i++){\nt=i*10*(PI/180); // i*(10 degrees) \nangle=i*10;\n// add to a table\n//------------------------------\nTable.set(\"Idx\", i, i);\nTable.set(\"Angle_degrees\", i, angle);\nTable.set(\"Angle_rad\", i, t);\n//------------------------------\nselectWindow(\"2\");\nmakeLine(getResult(\"X\",j),getResult(\"Y\",j),getResult(\"X\",j)+d*cos(t),getResult(\"Y\",j)+d*sin(t));\nroiManager(\"Add\");\nprofile=getProfile();\n\tn=0;\n\t\tfor(b=0; b&lt;profile.length; b++){\n\t\tif(profile[b]==255)\n                        n++;\n\t\t}\n// add to a table\n//------------------------------\nTable.set(\"Minimum distance\",i, n);\nTable.update;\nTable.sort(\"Minimum distance\"); \n//------------------------------\n}\nprint(\"Point \"+(j+1)+\"\\nMinimal distance: \"+ Table.get(\"Minimum distance\", 0),\"\\nAngle: \"+Table.get(\"Angle_degrees\", 0)+\" degrees\");\nprint(\"-------\");\nselectWindow(\"The result\");\nrun(\"RGB Color\");\nt=Table.get(\"Angle_rad\", 0);\nsetColor(\"red\");\ndrawLine(getResult(\"X\",j),getResult(\"Y\",j),getResult(\"X\",j)+d*Math.cos(t),getResult(\"Y\",j)+d*Math.sin(t));\n//------------------------------\n//setTool(\"text\");\nx=getResult(\"X\",j)-50;\ny=getResult(\"Y\",j);\nsetFont(\"SansSerif\", 18, \" antialiased\");\nsetColor(\"green\");\nOverlay.drawString(\"\"+(j+1), x, y, 0.0);\nOverlay.show();\n}\n//------------------------------\nclose(\"Results\");\n// End of processing\n//End of batch mode\nsetBatchMode(false);\nrun(\"Tile\");\nexit();\n}\ntype or paste code here\n</code></pre>", "<p><a class=\"attachment\" href=\"/uploads/short-url/1eWjYG2N5N03AVm3kaXo6EDfQyn.tif\">2 mm after.tif</a> (177.4 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/59t0kg2XjEoUSbkOH6AKLsNIcAh.tif\">2 mm before.tif</a> (177.4 KB)</p>\n<p>Thank you very much for your reply. I really appreciate your help. Here are my images.</p>"], "78558": ["<p>Hello<br>\nI\u2019m trying to write a plugin to get the values from freeline drawn over a skeletonised image. The skeleton is from neurons where the idea is follow the pixel line and look for branch points. I\u2019ll be linking this to AnalyseSkeleton plugin which already does most of the heavy lifting but I need to be able to follow the \u201cskeleton\u201d. I cannot get the code to accept the polyline, I\u2019m getting the error: Cannot convert class Lij.gui.Roi; to MaskInterval. My guess is that even though the line is 15 pixels thick its not seen as an Roi but I can\u2019t get any polyline options to even pass the compile stage in Eclipse. This is my code so far:</p>\n<p>public void MeasureMinora(Img skeletonisedImage) {</p>\n<pre><code> //Select Line Tool and set width to 15 pixels\n\tIJ.setTool(\"freeline\");\n\tIJ.run(\"Line Width...\", \"line=15\");\n   \t\n\tRoiManager rm = new RoiManager();\n\tnew WaitForUserDialog(\"Draw Line\", \"Draw a line on the central neuron\").show();\n\trm.addRoi(null);\n\tRoi[] theLine = rm.getRoisAsArray();\n\tMaskInterval maskInterval = roiService.toMaskInterval(theLine);\n\t\n\t//Apply the MaskInterval to the skeleton image to get an IterableInterval just in the ROI\n   IterableInterval linePos = Views.interval(skeletonisedImage, maskInterval);\n\t\n    Cursor cur = linePos.cursor();\n    while (cur.hasNext()) {\n    \tObject val = cur.get();\n    \tcur.fwd();\n    }\t\n\t \n}\n</code></pre>\n<p>Any help appreciated</p>\n<p>Thanks</p>\n<p>David</p>", "<p>Hello,<br>\nI\u2019ve made a little progress on this, I can get the line coordinates for a single line using the code below</p>\n<pre><code>    IJ.setTool(\"freeline\");\n\tIJ.run(\"Line Width...\", \"line=1\");\n   \t\n\tRoiManager rm = new RoiManager();\n\tnew WaitForUserDialog(\"Draw Line\", \"Draw a line on the central neuron\").show();\n\trm.addRoi(null);\n\t\n\t//Works for single line\n\tPolygonRoi theRoi = (PolygonRoi) rm.getRoi(0);\n\tUnmodifiablePolylineRoiWrapper test = new UnmodifiablePolylineRoiWrapper(theRoi);\n\tList&lt;RealLocalizable&gt; coordVals = test.vertices(); \n</code></pre>\n<p>However, this wrapper isn\u2019t usable on thicker lines. There are other classes within the package</p>\n<h1>\n<a name=\"netimagejlegacyconvertroipolylinehttpsjavadocscijavaorgimagej2netimagejlegacyconvertroipolylinepackage-summaryhtml-1\" class=\"anchor\" href=\"#netimagejlegacyconvertroipolylinehttpsjavadocscijavaorgimagej2netimagejlegacyconvertroipolylinepackage-summaryhtml-1\"></a><a href=\"https://javadoc.scijava.org/ImageJ2/net/imagej/legacy/convert/roi/polyline/package-summary.html\" rel=\"noopener nofollow ugc\">net.imagej.legacy.convert.roi.polyline</a>\n</h1>\n<p>I can\u2019t seem to get any of the ones for thicker lines to provide any coordinates. Has anyone used them and can give advice.</p>\n<p>Thanks</p>\n<p>David</p>", "<p>Hello,<br>\nTried more approaches to this with no success, from what I can extract from the API the code below should work:</p>\n<pre><code>    IJ.setTool(\"freeline\");\n\tIJ.run(\"Line Width...\", \"line=15\");\n\tRoiManager rm = new RoiManager();\n\tnew WaitForUserDialog(\"Draw Line\", \"Draw a line on the central neuron\").show();\n\trm.addRoi(null);\n\n\tWritablePolyline testRoi = (WritablePolyline) roiService.getROIs(currentData);\n\tRoi[] theLine = rm.getRoisAsArray();\n\troiService.add(theLine[0], skeletonisedImage);\n\tROITree theRoi = roiService.getROIs((Dataset) skeletonisedImage);\n\tPolylineWrapper newRoi = new PolylineWrapper((WritablePolyline) theRoi);\n\tPoint[] testVals = newRoi.getContainedPoints();\n</code></pre>\n<p>Unfortunately I get the error  class ij.gui.FreehandRoi is not a supported ROI type. I\u2019m beginning to wonder if IJ1 ROI can be used at all with ImageJ2 and equally do the imgLib2 ROI\u2019s actually work as I can find no way to select them.</p>\n<p>The obvious workaround is to do all the ROI work in IJ1, however, if its an IJ2 plugin you cannot pass any IJ1 variables between functions as it causes errors. The IJ1 variables have to be kept in 1 function which leads to ugly, difficult to read code.</p>\n<p>It would be great if any of the imgLib2 developers could comment on this, regions of interest feature somewhere in just about every plugin I\u2019ve ever done. Some documentation would be great, something like the Jupyter notebooks documentation for ops which is very good. I realise documentation like that takes time and isn\u2019t a fun task but it would make the transition to IJ2 easier/possible for IJ1 users such as myself.</p>\n<p>Thank you</p>\n<p>David</p>"], "78559": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f.png\" data-download-href=\"/uploads/short-url/4DgXc2GoljqBABpBGWtgWoia7G7.png?dl=1\" title=\"qUPATH\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_690x15.png\" alt=\"qUPATH\" data-base62-sha1=\"4DgXc2GoljqBABpBGWtgWoia7G7\" width=\"690\" height=\"15\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_690x15.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_1035x22.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_1380x30.png 2x\" data-dominant-color=\"C9C9C9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">qUPATH</span><span class=\"informations\">1419\u00d732 20.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nHello everyone, I need your help in the sense that I do not fully understand the different Qupath parameters, what do they mean exactly<br>\nMax caliper/Min caliper<br>\nOD std dev<br>\nOD Range<br>\nAnd especially in term of interpretation (that is a good circularity of the core =1, but for the other parameters how can I know the reference values)<br>\nThank you for your HEEELP ! <span class=\"hashtag\">#URGENT</span></p>", "<p>I believe the calipers are the min and max lengths of the object, as measured by calipers, see <a href=\"https://en.wikipedia.org/wiki/Feret_diameter\" class=\"inline-onebox\">Feret diameter - Wikipedia</a><br>\nStd dev is Standard Deviation and Range is\u2026 range from min to max. OD ranges tend to be from 0 to ~1.5-3, but can change depending on the color vectors for specific stains.</p>"], "78560": ["<p>Hi all,</p>\n<p>I would like to apply a piecewise affine transformation to align two images from two set of coordinates that are chromatically shifted: mov \u2192 ref., and save the transformation map to correct other sets of coordinates. I\u2019m trying with the skimage.transform PiecewiseAffineTransform package, but I\u2019m not sure how to apply the transformation map to other sets of coordinates.<br>\nAny help will be appreciated!</p>\n<p>Thanks!</p>", "<p>Hi <a class=\"mention\" href=\"/u/altairch95\">@Altairch95</a>!</p>\n<p>Once you have computed the transform you can apply it to a set of coordinates by calling it:</p>\n<pre><code class=\"lang-python\">tf = PiecewiseAffineTransform()\ntf.estimate(src_points, dst_points)\nnew_dst = tf(new_src)\n</code></pre>"], "78561": ["<p>Hi there,</p>\n<p>I am trying to expand my islet annotations, and while some do get expanded, others do<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70.jpeg\" data-download-href=\"/uploads/short-url/yVGWnhc9IPuwFql5uAI29tA2tK8.jpeg?dl=1\" title=\"annot expansion error\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_571x500.jpeg\" alt=\"annot expansion error\" data-base62-sha1=\"yVGWnhc9IPuwFql5uAI29tA2tK8\" width=\"571\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_571x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_856x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4cf70b62dcde46d2b31a16f31e6f3d9851fdc70_2_1142x1000.jpeg 2x\" data-dominant-color=\"2E3031\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">annot expansion error</span><span class=\"informations\">1897\u00d71659 397 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nnot and the process goes on forever with QuPath non-responding. What is causing these exceptions? How can I get past them?</p>\n<p>Thank you,<br>\nEry</p>", "<p>I\u2019m afraid I don\u2019t know. It\u2019s a really surprising error, because it seems to be related to running out of memory when reading pixels from the image \u2013 but that wouldn\u2019t be relevant to expanding the annotations.</p>\n<p>Does the problem <em>only</em> occur when running the command to expand annotations?</p>\n<p>When it occurs, does it happen for <em>all</em> annotations in the image \u2013 or just for some annotations in the image?</p>\n<p>Are you using a pixel classifier at all, or is there anything else running that might use a lot of memory?</p>", "<p>I know it\u2019s surprising to me too, because I ran the same yesterday and it was totally fine.</p>\n<p>Yes this happens only to the expand annotations command and only to some of the annotations.</p>\n<p>Hmmm, I am not so sure what you mean with the last question. Yes I used a pixel classifier to detect my islets. Then I created 3 new classes because I wanted to be able to distinguish three types of islets, but this was done manually by selecting the islets and setting the new classes. Also some of the annotations were manually corrected, but I do it in all of my images, so I guess in the one of yesterday as well.</p>", "<aside class=\"quote no-group\" data-username=\"EPet\" data-post=\"3\" data-topic=\"78561\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/epet/40/46601_2.png\" class=\"avatar\"> Ery Petropoulou:</div>\n<blockquote>\n<p>Yes I used a pixel classifier to detect my islets. Then I created 3 new classes because I wanted to be able to distinguish three types of islets, but this was done manually by selecting the islets and setting the new classes.</p>\n</blockquote>\n</aside>\n<p>Was the pixel classifier still open in the background?</p>", "<p>No it was not. Anything else that might be happening?</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"4\" data-topic=\"78561\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>Was the pixel classifier still open in the background?</p>\n</blockquote>\n</aside>\n<p>Yeah, that would have been my question as well.</p>\n<aside class=\"quote no-group\" data-username=\"EPet\" data-post=\"5\" data-topic=\"78561\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/epet/40/46601_2.png\" class=\"avatar\"> Ery Petropoulou:</div>\n<blockquote>\n<p>No it was not. Anything else that might be happening?</p>\n</blockquote>\n</aside>\n<p>Hmmmm, does the problem ever occur whenever you <em>haven\u2019t</em> use the pixel classifier since (re)starting QuPath?</p>\n<p>I wonder if there could be a bug that means the pixel classifier is still being active even when it shouldn\u2019t be. If so, then pressing the \u2018C\u2019 button to turn off the previous might help.</p>\n<p>(I don\u2019t know of such a bug, and maybe the pixel classifier has nothing to do with the problem, it\u2019s just my best guess for now)</p>", "<p>Thanks Pete, I will try and let you know.</p>", "<p>Hi, so I tried with the \u2018C\u2019 button pressed and without and it still gave me all these errors and the process was going on forever. I am working on 0.2.3, as this is an old analysis. When I tried on 0.4.3 though it completed the process in 30 sec, without any problem.</p>"], "78562": ["<p>It would be useful for something I\u2019m working on to be able to distinguish tumour cells that form linear clusters rather than round nests:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b071800aff6fef77c072abdff6d139810d508669.jpeg\" alt=\"Classic\" data-base62-sha1=\"paTcSJtzXryXMmovY3pkpuL8OCZ\" width=\"302\" height=\"185\"> Linear</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/581f3ff48d7985d03331646f59f21abfbd976f4b.jpeg\" alt=\"Alveolar\" data-base62-sha1=\"czyY84nQG4ppsMTAKEoFXLefpFN\" width=\"215\" height=\"171\"> Round</p>\n<p>I was thinking that if I took the centroid of a tumour cell and the centroid of its nearest tumour cell neighbour then the angle between them could be calculated. Or the angle between three cells (two \u2018lines\u2019) If this was done for all tumour centroids within (say) 15um of each other then the tendency of each group to follow a linear or round pattern could be calculated\u2026</p>\n<p>The problem is that I have fallen at the first hurdle - I can\u2019t figure out how to calculate angles between centroids. Any ideas would be gratefully appreciated! <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Or any ideas of better ways to get this sort of data! Many thanks!</p>", "<p>If the clusters of tumor cells have been segmented, you can compute any number of shape-related features such as circularity, elongation or eccentricity.</p>", "<p>I am not interested in the individual cell shapes but how they relate to each other: In linear clusters the angles between cells will tend to be in the 90 - 180 degrees range (in green below):<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/939fd03b962a4e7820982e43677ddf9edabd1fbb.jpeg\" alt=\"linear1\" data-base62-sha1=\"l3WAnGkEbBLtnAZiWqhBgYhrfpV\" width=\"291\" height=\"177\"></p>\n<p>Whereas in round clusters the angles between cells will tend to be less than 90 degrees (in blue):<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/5407863e7d5e9dfe95726d251984ae97165f51d7.jpeg\" alt=\"cluster1\" data-base62-sha1=\"bZmeaaiBxOGgmSPpdh2zBa9PPDx\" width=\"269\" height=\"243\"></p>\n<p>I was thinking by averaging these angles I could work out if a cluster tends towards round or linear but I haven\u2019t been able to figure out how to calculate the angles\u2026</p>", "<aside class=\"quote no-group\" data-username=\"jkh1\" data-post=\"2\" data-topic=\"78562\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jkh1/40/20446_2.png\" class=\"avatar\"> Jean-Karim H\u00e9rich\u00e9:</div>\n<blockquote>\n<p>f the clusters of tumor cells have been segmented,</p>\n</blockquote>\n</aside>\n<p>I suspect they meant the clusters themselves, as in <a href=\"https://forum.image.sc/t/annotations-based-on-cell-density/51021/3\" class=\"inline-onebox\">Annotations Based on Cell Density - #3 by Research_Associate</a><br>\nOnce you have the outline of each cluster, you could take shape measurements.</p>\n<p>You <em>might</em> also be able to use density maps to detect small clusters and create annotations for them.</p>\n<p>Another option is to convert the objects into annotations, then dilate/erode them a certain distance so that they form a single object - <a href=\"https://forum.image.sc/t/create-annotation-from-selected-objects/77529/5\" class=\"inline-onebox\">Create annotation from selected objects - #5 by David_Garcia_Ros</a></p>\n<p>Or create a <em><strong>concave</strong></em> hull around them <a href=\"https://forum.image.sc/t/qupath-script-command-to-draw-polygon-annotation-from-points/76833/2\" class=\"inline-onebox\">Qupath - script command to draw polygon annotation from points? - #2 by petebankhead</a></p>", "<p>Thanks, I\u2019ll look into those and see if they get me anywhere. Thank you!</p>", "<p>I was trying to use this script from Pete:</p>\n<pre><code class=\"lang-auto\">import qupath.lib.objects.PathObject\nimport static qupath.lib.scripting.QP.*\n\ndef imageData = getCurrentImageData()\ndef hierarchy = imageData.getHierarchy()\n\n// Get cells\ndef cells = getCellObjects()\n\n// Extract Delaunay info (should have run clustering plugin already)\ndef connections = imageData.getProperties().get('OBJECT_CONNECTIONS')\n\n// Assign labels to clusters\nint label = 0\nfor (group in connections.getConnectionGroups()) {\n    def allObjects = group.getPathObjects() as Set\n    while (!allObjects.isEmpty()) {\n        def nextObject = allObjects.pop()\n        label++\n        def cluster = new HashSet()\n        def pending = [nextObject]\n        while (!pending.isEmpty()) {\n            nextObject = pending.pop()\n            addToCluster(group, nextObject, cluster, pending)\n        }\n        for (pathObject in cluster) {\n            // REMOVE THE OPTIONS YOU DON'T WANT!\n            // Show cluster as classification\n            //pathObject.setPathClass(getPathClass(\"Cluster \" + label))\n            // Show cluster as name\n            pathObject.setName(\"Cluster \" + label)\n            // Add cluster as measurement (but don't use this in an object classifier!)\n            pathObject.getMeasurementList().putMeasurement(\"Cluster\", label)\n            pathObject.getMeasurementList().close()        \n        }\n        allObjects.removeAll(cluster)\n        \n    }\n}\nfireHierarchyUpdate()\n\nvoid addToCluster(group, pathObject, cluster, pending) {\n    if (cluster.add(pathObject)) {\n        for (next in group.getConnectedObjects(pathObject)) {\n            if (!cluster.contains(next))\n                pending.add(next)\n        }\n    }\n}\n</code></pre>\n<p>To label the clusters but QuPath doesn\u2019t seem to like the <code>pop()</code>  command:</p>\n<pre><code class=\"lang-auto\">ERROR: No signature of method: java.util.LinkedHashMap$LinkedKeySet.pop() is applicable for argument types: () values: []\n</code></pre>\n<p>I\u2019ve googled about but I can\u2019t figure out what I need to import or what method I can substitute</p>", "<aside class=\"quote no-group\" data-username=\"ChrisStarling\" data-post=\"6\" data-topic=\"78562\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png\" class=\"avatar\"> Chris Starling:</div>\n<blockquote>\n<p>I was trying to use this script from Pete:</p>\n</blockquote>\n</aside>\n<p>Can you link to the source? I don\u2019t remember where or when I wrote it\u2026 <a href=\"https://docs.groovy-lang.org/latest/html/groovy-jdk/java/util/Set.html\">Groovy sets don\u2019t have a pop() method</a> but maybe using <code>as List</code> instead of <code>as Set</code> would work. Or maybe it will have horrible performance and give duplicate objects \u2013 I\u2019m not sure.</p>\n<hr>\n<p>Approaching it in a different way, this concave hull script might be worth a try:</p>\n<aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"76833\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-script-command-to-draw-polygon-annotation-from-points/76833/2\">Qupath - script command to draw polygon annotation from points?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hi <a class=\"mention\" href=\"/u/smalldogworld\">@SmallDogWorld</a> \nThis script should create a convex hull annotation around selected objects: \n\nNote that it wouldn\u2019t give an annotation quite like the one in your screenshot, because yours isn\u2019t convex. \nQuPath v0.4.x contains Java Topology Suite 1.19.0, which gives support to create a concave hull instead. There\u2019s a script showing that at <a href=\"https://gist.github.com/petebankhead/13e1d7ec8635b24e6fc0cd7ed3a44810\" class=\"inline-onebox\">Generate a concave hull using JTS in QuPath v0.4.0 \u00b7 GitHub</a> \nHere\u2019s an adapted version that works with selected objects again: \nimport org.locationtech.jts.\u2026\n  </blockquote>\n</aside>\n", "<p>Looks like it was from here <a href=\"https://forum.image.sc/t/cell-classification-within-a-segmented-region-in-histology-images/55507/50\" class=\"inline-onebox\">Cell classification within a segmented region in histology images - #50 by petebankhead</a> with a similar comment about pop.</p>", "<p>Hi, <a href=\"https://gist.github.com/petebankhead/bea34ee1716b67246c2308808dd3c025\" rel=\"noopener nofollow ugc\">this</a> was the script I was using.</p>", "<p>I\u2019ve been busy so only just got to test this - <code>as List</code> rather than <code>as Set</code> seems to fix that problem!</p>\n<p>I\u2019ll draw a hull around the clusters and get some shape measurements.</p>"], "78563": ["<p>I tried to classify objectives from whole slide image in ilastik (raw data, and segmented image), but I get the following error massage when I try to export the image by Blockwise Object Classification (as recommended in ilastik website for big image analysis <a href=\"https://www.ilastik.org/documentation/objects/objects\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ilastik - Object Classification</a>):</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b.jpeg\" data-download-href=\"/uploads/short-url/nFZqayVhLL7iuQf0RfNtTTu939V.jpeg?dl=1\" title=\"20230314_132752\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_666x500.jpeg\" alt=\"20230314_132752\" data-base62-sha1=\"nFZqayVhLL7iuQf0RfNtTTu939V\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5f123c5098957bbc6e82f66dfec3bb1ecee376b_2_1332x1000.jpeg 2x\" data-dominant-color=\"9A9B9C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">20230314_132752</span><span class=\"informations\">1920\u00d71440 237 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Do someone here know how to export big image with object classification in ilastik?</p>", "<p>Hello <a class=\"mention\" href=\"/u/yeshurun\">@yeshurun</a>,</p>\n<p>sorry you ran into problems using ilastik again. What version of ilastik are you using?</p>\n<p>So I take it you found good parameters for your Block Size and Halo on a representative, smaller, image and added the \u201cbig\u201d image along with the corresponding probability maps in the batch processing?</p>\n<p>The error itself is not super informative (unfortunately). ilastik writes a log file, however, that usually has more information. Would you be up for making it available? The cleanest solution would be to start ilastik, got to <em>Settings \u2192 Open Log Folder</em>, then delete the <code>log.txt</code> file you find there. Then restart ilastik, and go on processing the big image in the same project file in batch processing. After the crash, you could go to <em>Settings \u2192 Open log folder</em> again, open the txt file and post the contents here.</p>\n<p>Cheers<br>\nDominik</p>", "<p>thank you,<br>\nyou right about your assume.<br>\nI use in ilastik 1.4.orc8 (I tried GPU too).<br>\nhere the error message.<br>\nthank you<br>\n<a class=\"attachment\" href=\"/uploads/short-url/pn5YQVdaoVevPIpH2QM6It6kjOj.txt\">log.txt</a> (67.3 KB)</p>"], "78565": ["<p>Hi all,</p>\n<p>I have quite a simple problem I\u2019d like some help with.</p>\n<p>In my image analysis pipeline, I currently open an RGB image, split the channels, select the green channel and then run an image analysis macro on that channel alone.</p>\n<p>I currently do the first steps (1-3) manually:</p>\n<ol>\n<li>open image</li>\n<li>split channels</li>\n<li>select green channel</li>\n<li>run my analysis macro.</li>\n</ol>\n<p>How could I add a few lines into my current macro to automate steps 2 and 3?<br>\nI guess it\u2019ll be something like:</p>\n<p>run(\u201cSplit Channels\u201d);<br>\nselectWindow(\u201cgreen channel\u201d); \u2190 I\u2019m not sure how to properly write this line\u2026! (the window name would be \u201cfilename.jpg (green)\u201d)</p>\n<p>Hope that makes sense! Any help would be much appreciated, thanks in advance!</p>\n<p>Amadeus</p>", "<p>Hi <a class=\"mention\" href=\"/u/amadeus\">@amadeus</a>,</p>\n<p>This will do\u2026</p>\n<pre><code class=\"lang-auto\">original_image = getTitle();\nrun(\"Split Channels\");\nselectWindow(original_image + \" (green)\");\n\n</code></pre>", "<p>Thank you, that works perfectly!</p>"], "78567": ["<p>Hello again, Please i need your help<br>\nHow can I choose what I want to measure on qupath as a parameter with one line of code (script)</p>", "<p>Generally you can\u2019t. You first need to select the objects you want to measure, then perform the measurement. If you select the objects through the GUI (annotations, detections, some class) and then create the measurement through the GUI, the lines of script should show up in the workflow. <a href=\"https://qupath.readthedocs.io/en/stable/docs/scripting/workflows_to_scripts.html\" class=\"inline-onebox\">Workflows to scripts \u2014 QuPath 0.4.3 documentation</a></p>"], "78569": ["<p>Dear QuPath user, I am trying  to correct some cells identifiacation/classification. For some reason (cells overalpping), some cells are not well identified. Is there a way to change the cells class manualy to correct bad results ? Of course it is possible to do it after export. Thanks, Mathieu</p>", "<p>You can use the Set Class button in the Annotation tab I think. Or use a script.</p><aside class=\"quote quote-modified\" data-post=\"4\" data-topic=\"45382\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/fluorescent-image-manually-set-class-to-a-detected-cell/45382/4\">Fluorescent image: manually set class to a detected cell?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    The most straightforward method will be recreating the composite classifier with only the two classes that work, and then finding some way to add in the third positive class. The \u201cSet class\u201d will only work if you have a list of complex classes to set, I think. So you would have to choose from triple, two different double positive classes, or single positive, when manually assigning the class. Kind of awkward, but doable as long as you have created those classes in your Annotation tab. You could\u2026\n  </blockquote>\n</aside>\n\n<p><a href=\"https://gist.github.com/Svidro/5b016e192a33c883c0bd20de18eb7764#file-set-selected-object-class-groovy\">https://gist.github.com/Svidro/5b016e192a33c883c0bd20de18eb7764#file-set-selected-object-class-groovy</a></p>"], "78570": ["<p>This is the version of the ImageJ I am using:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png\" data-download-href=\"/uploads/short-url/wHHVEARNewpXphnJDKGeFnnveO6.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png\" alt=\"image\" data-base62-sha1=\"wHHVEARNewpXphnJDKGeFnnveO6\" width=\"674\" height=\"500\" data-dominant-color=\"4D4A1D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">758\u00d7562 103 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I am operating on Windows 10.<br>\nThe problem is when I open a colorized stack composite in 3D viewer plugin, I go to view, and take snapshot, and nothing is in frame, and a black screen results:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238.jpeg\" data-download-href=\"/uploads/short-url/g6LaNsmsUqFvLDHHVK3g9nw25As.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg\" alt=\"image\" data-base62-sha1=\"g6LaNsmsUqFvLDHHVK3g9nw25As\" width=\"690\" height=\"398\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1035x597.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1380x796.jpeg 2x\" data-dominant-color=\"171714\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1467\u00d7847 28.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg\" data-download-href=\"/uploads/short-url/bM3txLoZ9CSkkGWNbCPKzu7ylSh.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg\" alt=\"image\" data-base62-sha1=\"bM3txLoZ9CSkkGWNbCPKzu7ylSh\" width=\"690\" height=\"404\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_1035x606.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg 2x\" data-dominant-color=\"100E0D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1350\u00d7791 54.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It will take a snapshot of the bounding box, but the sample is missing from the snapshot. I tried on a mac laptop, and it worked fine, but I\u2019m wondering if anyone else is having this problem in ImageJ Windows 10? Is there a solution to this?</p>", "<p>I certainly see some issues, but not the ones you are seeing. Also Win10.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9.png\" data-download-href=\"/uploads/short-url/8QhvMjfFTK1aMSv6ywJT3pMBVsB.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_467x500.png\" alt=\"image\" data-base62-sha1=\"8QhvMjfFTK1aMSv6ywJT3pMBVsB\" width=\"467\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_467x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_700x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dfa5b5430f50ca045fe6ddb5ca1ab0ad1723ac9_2_934x1000.png 2x\" data-dominant-color=\"181716\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1035\u00d71107 83 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nUsing the sample image, one of the three test images worked, the other two stuck the sample down in the lower corner, despite the 3Dviewer being in the state as shown top left.</p>", "<p>I\u2019m having issues where only one quarter of the image shows up in the snapshot. It isn\u2019t shrunk into one corner as with your images, but looks like it tiled  four images, and three are blank. Reducing the snapshot size produces a correct image. My guess was that it had to do with screen resolution, but I haven\u2019t dwelved into it any further. It doesn\u2019t always happen, sometimes it is fine.</p>\n<p>I\u2019m having other issues with it, the rotation pivot is set to the origin, and I don\u2019t seem to be able to change it. I thought I just didn\u2019t do it right, but it doesn\u2019t happen on my laptop, so maybe this is somehow related?</p>\n<p>As far as I can tell, these are the same versions of the Fiji and 3D viewer, and both on Windows 10.</p>", "<p>What version of ImageJ are you using?</p>", "<p><a class=\"mention\" href=\"/u/jbehnsen\">@jbehnsen</a> What ImageJ version are you running?</p>", "<p>Ah, sorry, 1.53t. I didn\u2019t state since it was the same as yours.<br>\nThe built in sample image I used was the Organ of Corti.</p>", "<p>Same, 1.53t.</p>\n<p>I can\u2019t reproduce the tiling error now, so maybe that is fixed? I still have trouble with the rotation pivot though. Maybe that is just me being stupid, but I\u2019m sure it used to be in the centre, or at least you could set it to the centre.</p>"], "78571": ["<p>Hi all,</p>\n<p>I\u2019m trying to figure out the best way to count positive cells. I\u2019m working on a CD68 IHC-DAB stain as my initial stain to figure this out. I\u2019ve tried both using Qupath\u2019s native cell detection, then setting cell intensity based on cell-DAB stdv as it seems to separate the best, but what I\u2019ve noticed is that some of my cells seem to be oversegmented.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e.jpeg\" data-download-href=\"/uploads/short-url/zTyBUHqSetWRdyAxKXIUKrLEiiq.jpeg?dl=1\" title=\"Screenshot 2023-03-14 at 14.03.43\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_644x500.jpeg\" alt=\"Screenshot 2023-03-14 at 14.03.43\" data-base62-sha1=\"zTyBUHqSetWRdyAxKXIUKrLEiiq\" width=\"644\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_644x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_966x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb93d983823bbccbb9554b601ed27c966b18628e_2_1288x1000.jpeg 2x\" data-dominant-color=\"EDEDED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-14 at 14.03.43</span><span class=\"informations\">1676\u00d71300 136 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/ebBBbqFwpJ06UlW6A0Xb3UlgYLS.tif\">Screenshot - slide-2023-02-24T09-20-06-R9-S10-1.tif</a> (3.3 MB)</p>\n<p>I then tried running stardist, and the opposite seems to happen, where there is an inadequate separation of cells.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/haWqdlp81Ftxde3eE1Ryp5lnxRV.tif\">Screenshot - slide-2023-02-24T09-20-06-R9-S10.mrxs (1)-1.tif</a> (3.3 MB)</p>\n<pre><code class=\"lang-auto\">\n// Get current image - assumed to have color deconvolution stains set\nvar imageData = getCurrentImageData()\nvar stains = imageData.getColorDeconvolutionStains()\n\nimport qupath.ext.stardist.StarDist2D\n\n// Specify the model file (you will need to change this!)\nvar pathModel = '/Applications/QuPath.app/dsb2018_paper.pb'\n\nvar stardist = StarDist2D.builder(pathModel)\n        .preprocess( // Extra preprocessing steps, applied sequentially\n            ImageOps.Channels.deconvolve(stains),\n            ImageOps.Channels.extract(0),\n            ImageOps.Filters.median(2),\n            ImageOps.Core.divide(1.5)\n         )\n        .threshold(0.1)              // Probability (detection) threshold\n        .normalizePercentiles(1, 99) // Percentile normalization\n        .pixelSize(0.5)              // Resolution for detection\n        .cellExpansion(10)          // Approximate cells based upon nucleus expansion\n        .cellConstrainScale(2)     // Constrain cell expansion using nucleus size\n        .measureShape()              // Add shape measurements\n        .measureIntensity()          // Add cell measurements (in all compartments)\n        .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n\t\t .tileSize(1024)\n        .build()\n\n\n\n// Run detection for the selected objects\nvar pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nstardist.detectObjects(imageData, pathObjects)\n\n\nstardist.close() // This can help clean up &amp; regain memory\n\nsetCellIntensityClassifications(\"Hematoxylin: Nucleus: Mean\", 0.15)\n\n//Clean up bad objects\nremoval = getCellObjects().findAll{it.getPathClass().toString().contains(\"Negative\")}\nremoveObjects(removal, true)\n\nsetCellIntensityClassifications(\"Nucleus: Area \u00b5m^2\", 3)\n\n//Clean up bad objects\nremoval = getCellObjects().findAll{it.getPathClass().toString().contains(\"Negative\")}\nremoveObjects(removal, true)\n\nprintln 'Done!'\n</code></pre>\n<p>Just wondering whether anyone had any suggestions on how to improve this? I did then try running cellpose to see if it made a difference, but am completely new to programming and having much difficulty trying to get it up and running on a Mac M1.</p>\n<p>Also, I noticed that stardist was taking &gt;10mins per annotation vs 2-3 mins  using the native Qupath plugin, just wondering whether I missed out a command string somewhere to help with stardist processing time?</p>", "<aside class=\"quote no-group\" data-username=\"Li1234\" data-post=\"1\" data-topic=\"78571\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/li1234/40/68757_2.png\" class=\"avatar\"> Li Yenn Yong :</div>\n<blockquote>\n<p>Also, I noticed that stardist was taking &gt;10mins per annotation vs 2-3 mins using the native Qupath plugin, just wondering whether I missed out a command string somewhere to help with stardist processing time?</p>\n</blockquote>\n</aside>\n<p>Not really, StarDist takes longer. You can get minor speed increases using the GPU, sometimes, but it\u2019s mostly system dependent.<br>\nAdjusting the parameters for StarDist is probably your best bet. Over and under segmentation is sometimes dependent on how accurate the color channels passed to the analysis are or the pixel size predicted. Larger pixel size makes for more merged objects. Same thing goes for the built in cell detection.</p>\n<p>In general, if the DAB obscures the nucleus, you will get poorer segmentation. That\u2019s just the price you pay for using DAB as opposed to fluorescence, not much you can do about that part now.</p>", "<p>Thanks for your thoughts.</p>\n<p>Presumably if I increase resolution to 0.24, which is the native resolution it may help with segmentation? But the downside is it may increase the length of time of processing?</p>", "<aside class=\"quote no-group\" data-username=\"Li1234\" data-post=\"3\" data-topic=\"78571\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/li1234/40/68757_2.png\" class=\"avatar\"> Li Yenn Yong :</div>\n<blockquote>\n<p>Presumably if I increase resolution to 0.24, which is the native resolution it may help with segmentation?</p>\n</blockquote>\n</aside>\n<p>Not really. Most segmentation models are based on certain sized objects in pixels. So you want the size of the nucleus to match up with what the model or method expects. If you are getting too much splitting, you generally reduce that by increasing the effective pixel size (downsampling the image). The opposite if you are getting too much merging.</p>\n<p>At some point there should be a balance between oversplitting and merging.</p>", "<p>Thank you. I\u2019ll give a go and see how I get on</p>", "<aside class=\"quote no-group\" data-username=\"Li1234\" data-post=\"1\" data-topic=\"78571\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/li1234/40/68757_2.png\" class=\"avatar\"> Li Yenn Yong :</div>\n<blockquote>\n<p>I then tried running stardist, and the opposite seems to happen, where there is an inadequate separation of cells.</p>\n</blockquote>\n</aside>\n<p>Based on the picture you provided:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6.jpeg\" data-download-href=\"/uploads/short-url/hOuBaysMfvJmPDmhKkUqTLicqMe.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6_2_586x500.jpeg\" alt=\"image\" data-base62-sha1=\"hOuBaysMfvJmPDmhKkUqTLicqMe\" width=\"586\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6_2_586x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6_2_879x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6.jpeg 2x\" data-dominant-color=\"D7D1D0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1051\u00d7896 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nthe odd geometries could be caused by removing objects after the cells have been expanded with <code>.cellExpansion(10)</code>, as noted by the object removal excerpt from your script below:</p>\n<pre><code class=\"lang-auto\">//Clean up bad objects\nremoval = getCellObjects().findAll{it.getPathClass().toString().contains(\"Negative\")}\nremoveObjects(removal, true)\n</code></pre>\n<p>As for the undersegmentation, I\u2019d bet deconvolution issues are one major culprit. DSB2018 was trained on DAPI stained nuclei, and you\u2019re trying to extract what appears to be the hematoxylin channel from HDAB stained sections. You might want to try different color vectors and see if that improves segmentation.</p>\n<p>As for using DAPI-trained models to segment HDAB brightfield images, I discuss some of the challenges and potential solutions in this video:</p><div class=\"onebox lazyYT lazyYT-container\" data-youtube-id=\"UI_Sfv3rNo4\" data-youtube-title=\"HDAB Cell Detection using StarDist in QuPath\" data-parameters=\"feature=oembed&amp;wmode=opaque\">\n  <a href=\"https://www.youtube.com/watch?v=UI_Sfv3rNo4\" target=\"_blank\" rel=\"noopener\">\n    <img class=\"ytp-thumbnail-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/1/617d2846647d4f2c4fc36af1b463ed9b5a9a551f.jpeg\" title=\"HDAB Cell Detection using StarDist in QuPath\" width=\"480\" height=\"360\">\n  </a>\n</div>\n<p>\nIt\u2019ll still fail in high intensity CD68 DAB cells, because deconvolution typically fails under high optical density conditions (i.e. can\u2019t separate DAB from Hematoxylin when you have an almost black nucleus)</p>\n<p>And finally, here\u2019s a small collection of scripts that make parameter tuning of StarDist a bit simpler, especially when you want to apply models trained on different modalities from your own images. And doesn\u2019t require extensive programming knowledge: <a href=\"https://github.com/MarkZaidi/Universal-StarDist-for-QuPath\" class=\"inline-onebox\">GitHub - MarkZaidi/Universal-StarDist-for-QuPath: Transfer trained StarDist models across imaging modalities</a><br>\nIt\u2019s a bit of a \u201cjack of all trades, master of none\u201d script, so you\u2019ll likely have better odds playing around with the script used in the video, since that merges the hematoxylin and DAB channels together. Useful when there\u2019s overlap between your nuclear counterstain and your IHC marker. CD68 <em>should</em> be localized more to the cytoplasm/membrane, but it\u2019s still worth a shot. Script from video is pasted below:</p>\n<pre><code class=\"lang-auto\">import qupath.ext.stardist.StarDist2D\nvar imageData = getCurrentImageData()\ndef stains = imageData.getColorDeconvolutionStains()\n// Specify the model file (you will need to change this!)\ndef pathModel = 'C:/Users/Mark Zaidi/Documents/QuPath/Stardist Trained Models/dsb2018_heavy_augment.pb'\n\nvar stardist = StarDist2D.builder(pathModel)\n        .preprocess(\n            ImageOps.Channels.deconvolve(stains),\n            ImageOps.Channels.extract(0,1),\n            ImageOps.Channels.sum(),\n            ImageOps.Filters.median(2),\n            //ImageOps.Core.divide(2),\n         )\n        .threshold(0.5)              // Probability (detection) threshold\n        //.channels('DAPI')            // Specify detection channel\n        .normalizePercentiles(1, 99) // Percentile normalization\n        .pixelSize(0.5)              // Resolution for detection\n        .build()\n\n// Run detection for the selected objects\n\nvar pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nstardist.detectObjects(imageData, pathObjects)\nprintln 'Done!'\n</code></pre>", "<p>Hi<br>\nI have experience with using stardist and cellpose in qupath. It also has some challenges and potential solutions similar to stardist like Mark said.</p><aside class=\"quote quote-modified\" data-post=\"6\" data-topic=\"78571\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mark_zaidi/40/37001_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/stardist-vs-qupath-cell-detection-oversegmentation-and-inadequate-seperation-of-cells/78571/6#:~:text=As%20for%20using%20DAPI%2Dtrained%20models%20to%20segment%20HDAB%20brightfield%20images%2C%20I%20discuss%20some%20of%20the%20challenges%20and%20potential%20solutions%20in%20this%20video%3A\">Stardist vs Qupath cell detection, oversegmentation and inadequate seperation of cells</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Based on the picture you provided: \n <a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7cda2778a1b6526106f1831cb2e066884dd5ddb6.jpeg\" data-download-href=\"/uploads/short-url/hOuBaysMfvJmPDmhKkUqTLicqMe.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\">[image]</a> \nthe odd geometries could be caused by removing objects after the cells have been expanded with .cellExpansion(10), as noted by the object removal excerpt from your script below: \n//Clean up bad objects\nremoval = getCellObjects().findAll{it.getPathClass().toString().contains(\"Negative\")}\nremoveObjects(removal, true)\n\nAs for the undersegmentation, I\u2019d bet deconvolution issues are one major culprit. DSB2018 was trained on DAPI stained nuclei, and you\u2019r\u2026\n  </blockquote>\n</aside>\n\n<p>The advantage of using cellpose is it has lots of models you can use and you can even retrain the model. I already got some nice results in HE and IHC(CD8, Ki67,HER2 ect\u2026) slides. The disadvantage of it is also evident in that it needs python packages support and CUDA suppost. I\u2019m not sure it works well or not in Mac M1.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/BIOP/qupath-extension-cellpose\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/BIOP/qupath-extension-cellpose\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32ac1855a2235a59cc1edbeb03255d1129ea4a26.png 2x\" data-dominant-color=\"F2F0EC\"></div>\n\n<h3><a href=\"https://github.com/BIOP/qupath-extension-cellpose\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - BIOP/qupath-extension-cellpose: an extension that wraps a Cellpose...</a></h3>\n\n  <p>an extension that wraps a Cellpose environment such that WSI can be analyzed using Cellpose through QuPath. - GitHub - BIOP/qupath-extension-cellpose: an extension that wraps a Cellpose environment...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I can give you some cellpose scripts if you need.</p>\n<p>Best<br>\nYuan</p>", "<p>Thank you <a class=\"mention\" href=\"/u/mark_zaidi\">@Mark_Zaidi</a>, I will give it a go and see whether I can get improve on it.  For the over segmentation, I ran out without a script ie just using Qupath\u2019s native built-ins. I\u2019ve not actually observed this phenomenon before, not sure whether I accidentally changed a setting or two by alternating between scripting and running the inbuilt plugins.</p>\n<p><a class=\"mention\" href=\"/u/orchard\">@Orchard</a>, thank you for your kind offer. I\u2019ll see how far I get on with Mark\u2019s suggestions and whether I manage to get cellpose up and running on my Mac\u2026I may yet take you up on your offer!</p>"], "78572": ["<p>Hi <a class=\"mention-group notify\" href=\"/groups/ome\">@ome</a> ,<br>\nI would like to have the jar omero_ij on bioconda like <a href=\"https://github.com/bioconda/bioconda-recipes/tree/master/recipes/fiji-max_inscribed_circles\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">bioconda-recipes/recipes/fiji-max_inscribed_circles at master \u00b7 bioconda/bioconda-recipes \u00b7 GitHub</a>. Would you agree? Could I have a permanent URL for the version 5.8.0? What is the licence of the plugin?<br>\nThank you very much,<br>\nBest Regards,<br>\nLucille Delisle</p>", "<p>Hi <a class=\"mention\" href=\"/u/lldelisle\">@lldelisle</a><br>\nThat will certainly be useful<br>\nThe artifacts are built via GHA when a new tag is pushed. The permanent URL will the GitHub one. This is the license file for omero-insight <a href=\"https://github.com/ome/omero-insight/blob/0d1a4092ae773e505029a334eb96ae02086ad787/LICENSE.txt\" class=\"inline-onebox\">omero-insight/LICENSE.txt at 0d1a4092ae773e505029a334eb96ae02086ad787 \u00b7 ome/omero-insight \u00b7 GitHub</a>. It will also apply to the jar file.</p>\n<p>Will you be working on the packaging?</p>\n<p>Cheers<br>\nJean-Marie</p>", "<p>So cool, this is great news. Yes, I can do it. Which github account should I pin for approval?</p>", "<p>The pull request is here: <a href=\"https://github.com/bioconda/bioconda-recipes/pull/39932\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Add omero_ij plugin (ImageJ/Fiji) by lldelisle \u00b7 Pull Request #39932 \u00b7 bioconda/bioconda-recipes \u00b7 GitHub</a><br>\nPlease <a class=\"mention-group notify\" href=\"/groups/ome\">@ome</a> have a look and comment on it.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/lldelisle\">@lldelisle</a><br>\nComment added</p>"], "78578": ["<p>Hi, I\u2019m using ImageJ SNT to construct the paths from a .swc files and do the sholl analysis. I would like to have the color-coded paths and try the application : Path Mnager&gt;Analyze&gt;Color Coding&gt;Color Code Path(s). Color by: Sholl inters. (root centered). LUT: mpl-inferno.lut.<br>\nHowever, the colored path are not continuous. Some segments didn\u2019t follow the color codes, and just showed the color as \u2018Deselected\u2019 paths (magenta ones here, and those segments change color with color assigned for \u2018Deselected\u2019 in the SNT control panel).<br>\np.s. those \u2018deselected\u2019 segments seems to be the seeds(?) in original Imaris filaments.<br>\nIs there any possible way to solve this problem? Thank you.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e.png\" data-download-href=\"/uploads/short-url/4pOVfLhXXM73k6Zmoir9gkdo1z0.png?dl=1\" title=\"SNTquestion20230314\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e_2_375x500.png\" alt=\"SNTquestion20230314\" data-base62-sha1=\"4pOVfLhXXM73k6Zmoir9gkdo1z0\" width=\"375\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e_2_375x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ef3ebe58a646ac156ba3ccf8bdd28dbdfb7ab8e.png 2x\" data-dominant-color=\"050203\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">SNTquestion20230314</span><span class=\"informations\">400\u00d7532 27.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78579": ["<p>Following Henry Pinkard\u2019s advice (<a href=\"https://forum.image.sc/t/how-to-convert-fast-stream-of-images-to-stream-of-z-stacks/77670\">How to convert fast stream of images to stream of z stacks</a>) I wrote a Beanshell script to acquire images and sort them to stacks. I\u2019m also implementing a configurable Java plugin version of the same.</p>\n<p>Sometimes it works, but often image acquisition fails. From the logfile I can see that my device adapter reports result code 31 (DEVICE_INCOMPATIBLE_IMAGE) when trying to insert the captured image. Strangely it always works when I use Live-View.</p>\n<p>I also tried to acquire images from napari-micromanager and got snapshots but no live-view. Unfortunately no logfile is written but I suspect the error is the same as above.</p>\n<p>Any hints at what could cause this error? What does DEVICE_INCOMPATIBLE_IMAGE actually mean?</p>", "<p>One way that might happen if something changes the image properties, like binning, bit depth or crop size. Whenever that changes you need to <a href=\"https://valelab4.ucsf.edu/~MM/doc/mmcorej/mmcorej/CMMCore.html#initializeCircularBuffer--\" rel=\"noopener nofollow ugc\">initialize the buffer</a>.</p>", "<p>Close, but not the only problem. <em>\u201c\u2026 based on the current camera settings\u201d</em> made me aware that the camera device methods GetImageWidth() etc. MUST reflect what the images will be like. After fixing that some other issues also disappeared, so I\u2019ll call that a solution <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Unfortunately I still can\u2019t get a live-view in napari-micromanager, so this is probably not the same problem.</p>"], "74483": ["<p>Hello,<br>\nSorry in advance if this is the wrong place to ask this question. I\u2019ve been looking online, and it doesn\u2019t seem like others are having this issue.</p>\n<p>I\u2019m trying to utilise MosaicJ as a plug-in for ImageJ (following this page as a guide on how to do so: <a href=\"http://bigwww.epfl.ch/thevenaz/mosaicj/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">MosaicJ</a> ), and I\u2019ve managed to install it as intended. However, when i open the plugin, no menu bar is shown (see screenshot). Does anyone know any ways to trouble shoot this? I\u2019m using windows on my laptop if that helps.</p>\n<p>Thank you, G. Gibson.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a98637ec2971236e0b946f2ca73bfa1c6fb4e7d2.png\" data-download-href=\"/uploads/short-url/obGfL9GUfiFXKW4yHR59T0zXuMy.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a98637ec2971236e0b946f2ca73bfa1c6fb4e7d2_2_690x370.png\" alt=\"image\" data-base62-sha1=\"obGfL9GUfiFXKW4yHR59T0zXuMy\" width=\"690\" height=\"370\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a98637ec2971236e0b946f2ca73bfa1c6fb4e7d2_2_690x370.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a98637ec2971236e0b946f2ca73bfa1c6fb4e7d2_2_1035x555.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a98637ec2971236e0b946f2ca73bfa1c6fb4e7d2_2_1380x740.png 2x\" data-dominant-color=\"EBEBEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1916\u00d71029 58.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello,</p>\n<p>I have exactly the same issue. I am pretty sure that\u2019s because of the <a href=\"https://forum.image.sc/t/fiji-2-7-0-released/71652/9\">major fiji update</a> that occured on september.<br>\nMaybe the BIG-EPFL team is currently working on repairing the plugin.<br>\n<a class=\"mention\" href=\"/u/daniel.sage\">@daniel.sage</a> Do you have some updates on that ?</p>\n<p>Thanks,<br>\nBest,</p>\n<p>R\u00e9my.</p>", "<p>Hello <a class=\"mention\" href=\"/u/gabriel_gibson\">@Gabriel_Gibson</a>,</p>\n<p>I have good news. Thanks to <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>, we found out that <code>MosaicJ</code> plugin is still working with the old Fiji layout called <code>Metal</code> but breaks with the new <code>FlatLaf</code> layout.</p>\n<p>As a temporary fix, you can open your Fiji and go on <code>Edit -&gt; Options -&gt; Look and feel...</code> and select <code>Metal</code> in the drop-down menu. Then, close Fiji and open it again. MosaicJ should work as expected.</p>\n<p>I open an <a href=\"https://github.com/fiji-BIG/MosaicJ/pull/1\" rel=\"noopener nofollow ugc\">Pull Request</a> on Github to make the code working on FlatLaf layout as well. I hope the new working version will be released soon.</p>\n<p>R\u00e9my.</p>"], "78581": ["<p>Hi everyone thank you for your help with this issues.</p>\n<p>After installing OMERO.fpbioimage app the Omero web client seem to be stuck in a reload loop after the users login. This continuous reload stops by itself after a minute or two, no idea what\u2019s causing it, the omero web log files say nothing about it.</p>\n<p>We then proceeded to upload two .ndpi images,</p>\n<p>For one of them omero doesn\u2019t generate a thumbnail, you can see in the screenshot what omero web displays.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724.png\" data-download-href=\"/uploads/short-url/2BEDBK3qBjVzpLEZOoHvhoUOsVS.png?dl=1\" title=\"Screen Shot 2023-03-09 at 9.59.21 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724_2_690x256.png\" alt=\"Screen Shot 2023-03-09 at 9.59.21 AM\" data-base62-sha1=\"2BEDBK3qBjVzpLEZOoHvhoUOsVS\" width=\"690\" height=\"256\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724_2_690x256.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724_2_1035x384.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1246085f7cdc92b8868381d2e387db0df16a3724.png 2x\" data-dominant-color=\"E0E2E5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-09 at 9.59.21 AM</span><span class=\"informations\">1338\u00d7498 85.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The other one seems ok but when you zoom in it gets pixelated, which could mean that there was a problem with the piramidal transformation?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab.jpeg\" data-download-href=\"/uploads/short-url/tpM64vErEz2S3GuazT3kM2IscbN.jpeg?dl=1\" title=\"Screen Shot 2023-03-09 at 10.00.34 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_690x444.jpeg\" alt=\"Screen Shot 2023-03-09 at 10.00.34 AM\" data-base62-sha1=\"tpM64vErEz2S3GuazT3kM2IscbN\" width=\"690\" height=\"444\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_690x444.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_1035x666.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce28f3af88c41d6f091703bf2e93a740250b07ab_2_1380x888.jpeg 2x\" data-dominant-color=\"CACFCE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-09 at 10.00.34 AM</span><span class=\"informations\">1556\u00d71002 171 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8.jpeg\" data-download-href=\"/uploads/short-url/jV2N1GRNYF0lHqvVh2NitKpQf56.jpeg?dl=1\" title=\"Screen Shot 2023-03-09 at 10.00.46 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_690x427.jpeg\" alt=\"Screen Shot 2023-03-09 at 10.00.46 AM\" data-base62-sha1=\"jV2N1GRNYF0lHqvVh2NitKpQf56\" width=\"690\" height=\"427\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_690x427.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_1035x640.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b9c172b816cf046920793d1a7fbb775d556cfe8_2_1380x854.jpeg 2x\" data-dominant-color=\"CBCECD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-09 at 10.00.46 AM</span><span class=\"informations\">1748\u00d71082 55.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Again, thank you for any help,<br>\nSaludos,</p>", "<p>Hi,</p>\n<p>Did you change anything else about your OMERO.web installation when you installed <code>fpbioimage</code>?<br>\nDoes it do a \u201creload loop\u201d on every login now? What about if you logout first?</p>\n<p>If you try to login from a new browser (or use \u201cincognito\u201d mode) does that help?</p>\n<p>What size is the image which fails to generate a thumbnail?<br>\nIt might simply be taking a long time to generate a pyramid. You can check the Processor log.</p>\n<p>We generally recommend that if you\u2019re having pyramid generation issues, it might help to build a pyramid image offline first, then import that into OMERO - Either ome.tiff of OME-NGFF.<br>\nA UI tool for doing this conversion is <a href=\"https://www.glencoesoftware.com/products/ngff-converter/\" class=\"inline-onebox\">NGFF-Converter | Glencoe Software, Inc.</a><br>\nThis would also allow you to check the pyramid image before importing.</p>\n<p>Or if you would like to share the image, we can test\u2026?</p>", "<p>We installed all the Omero Web appsthe same day, we think the problem is fpbioimage because the reload stopped after we uninstalled this specific app and started again after we reinstall  it (we tried uninstalling and reinstalling every app).</p>\n<p>The continuous reload happen after login.  Once the reload loop stops, you can refresh the page and there is no reload loop. If you logout and login, the reload loop happens again. I haven\u2019t tried the incognito mode, ill do it as soon as i can.</p>\n<p>The image that doesn\u2019t generate the thumbnail is 5gb, but the other one that does generate the thumbnail but doesn\u2019t do the pyramidal conversion is 10gb.</p>\n<p>Here are the dropbox link to the images.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12643b243ca3bc4e2fa6489fc836c2f58e96d9a5.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi\" target=\"_blank\" rel=\"noopener nofollow ugc\">Dropbox</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png\" class=\"thumbnail onebox-avatar\" width=\"160\" height=\"160\">\n\n<h3><a href=\"https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi\" target=\"_blank\" rel=\"noopener nofollow ugc\">Llano_de_la_cruz_R15 - 2023-02-01 09.45.40.ndpi</a></h3>\n\n  <p>Shared with Dropbox</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.dropbox.com/s/qbkszwh5kxa9rpk/STRI_Fm_Ferreira_IIES-PALY-7822%20-%202023-01-23%2014.10.35.ndpi\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12643b243ca3bc4e2fa6489fc836c2f58e96d9a5.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.dropbox.com/s/qbkszwh5kxa9rpk/STRI_Fm_Ferreira_IIES-PALY-7822%20-%202023-01-23%2014.10.35.ndpi\" target=\"_blank\" rel=\"noopener nofollow ugc\">Dropbox</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/587edba7ad1dde9f24611003b51f0ec4f3ba211c.png\" class=\"thumbnail onebox-avatar\" width=\"160\" height=\"160\">\n\n<h3><a href=\"https://www.dropbox.com/s/qbkszwh5kxa9rpk/STRI_Fm_Ferreira_IIES-PALY-7822%20-%202023-01-23%2014.10.35.ndpi\" target=\"_blank\" rel=\"noopener nofollow ugc\">STRI_Fm_Ferreira_IIES-PALY-7822 - 2023-01-23 14.10.35.ndpi</a></h3>\n\n  <p>Shared with Dropbox</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>thanks,</p>", "<p>Ok. good news, we fixed the reload loop issue, the fpbioimage folder on omero-web static/ didn\u2019t have the proper permission, (don\u2019t know why, we follow the standard installation instructions).</p>\n<p>The other problems persist.</p>", "<p>We are still working on resolving this issues.</p>\n<p>We tried reuploading the image that doesn\u2019t generate a thumbnail and the Blitz log file throws this error;</p>\n<pre><code class=\"lang-auto\">2023-03-15 11:03:58,069 ERROR [        ome.services.util.ServiceHandler] (Server-161) java.lang.Error:  Wrapped Exception: (java.lang.UnsatisfiedLinkError):\n'void org.libjpegturbo.turbojpeg.TJDecompressor.init()'\njava.lang.UnsatisfiedLinkError: 'void org.libjpegturbo.turbojpeg.TJDecompressor.init()'\n        at org.libjpegturbo.turbojpeg.TJDecompressor.init(Native Method)\n        at org.libjpegturbo.turbojpeg.TJDecompressor.&lt;init&gt;(TJDecompressor.java:81)\n        at loci.formats.services.JPEGTurboServiceImpl.getTile(JPEGTurboServiceImpl.java:349)\n        at loci.formats.services.JPEGTurboServiceImpl.getTile(JPEGTurboServiceImpl.java:251)\n        at loci.formats.in.NDPIReader.openBytes(NDPIReader.java:217)\n        at loci.formats.ImageReader.openBytes(ImageReader.java:465)\n        at loci.formats.ChannelFiller.openBytes(ChannelFiller.java:167)\n        at loci.formats.ChannelFiller.openBytes(ChannelFiller.java:159)\n        at loci.formats.ChannelSeparator.openBytes(ChannelSeparator.java:200)\n        at loci.formats.ChannelSeparator.openBytes(ChannelSeparator.java:151)\n        at loci.formats.ReaderWrapper.openBytes(ReaderWrapper.java:341)\n        at ome.io.bioformats.BfPixelsWrapper.getWholePlane(BfPixelsWrapper.java:366)\n        at ome.io.bioformats.BfPixelsWrapper.getPlane(BfPixelsWrapper.java:263)\n        at ome.io.bioformats.BfPixelBuffer.getPlane(BfPixelBuffer.java:209)\n        at omeis.providers.re.data.PlaneFactory.createPlane(PlaneFactory.java:208)\n        at omeis.providers.re.HSBStrategy.getWavelengthData(HSBStrategy.java:100)\n        at omeis.providers.re.HSBStrategy.makeRenderingTasks(HSBStrategy.java:231)\n        at omeis.providers.re.HSBStrategy.render(HSBStrategy.java:328)\n        at omeis.providers.re.HSBStrategy.renderAsPackedInt(HSBStrategy.java:292)\n        at omeis.providers.re.Renderer.renderAsPackedInt(Renderer.java:558)\n        at ome.services.ThumbnailBean.createScaledImage(ThumbnailBean.java:648)\n        at ome.services.ThumbnailBean.retrieveThumbnail(ThumbnailBean.java:1255)\n        at ome.services.ThumbnailBean.getThumbnailWithoutDefault(ThumbnailBean.java:1127)\nat jdk.internal.reflect.GeneratedMethodAccessor1622.invoke(Unknown Source)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\n        at ome.security.basic.EventHandler.invoke(EventHandler.java:154)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at ome.tools.hibernate.SessionHandler.doStateful(SessionHandler.java:216)\n        at ome.tools.hibernate.SessionHandler.invoke(SessionHandler.java:200)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99)\n        at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:282)\n        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at ome.tools.hibernate.ProxyCleanupFilter$Interceptor.invoke(ProxyCleanupFilter.java:249)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at ome.services.util.ServiceHandler.invoke(ServiceHandler.java:121)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)\n        at com.sun.proxy.$Proxy110.getThumbnailWithoutDefault(Unknown Source)\n        at jdk.internal.reflect.GeneratedMethodAccessor1622.invoke(Unknown Source)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\n        at ome.security.basic.BasicSecurityWiring.invoke(BasicSecurityWiring.java:93)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at ome.services.blitz.fire.AopContextInitializer.invoke(AopContextInitializer.java:43)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)\n        at com.sun.proxy.$Proxy110.getThumbnailWithoutDefault(Unknown Source)\n        at jdk.internal.reflect.GeneratedMethodAccessor1625.invoke(Unknown Source)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at ome.services.blitz.util.IceMethodInvoker.invoke(IceMethodInvoker.java:172)\n        at ome.services.throttling.Callback.run(Callback.java:56)\n        at ome.services.throttling.InThreadThrottlingStrategy.callInvokerOnRawArgs(InThreadThrottlingStrategy.java:56)\n        at ome.services.blitz.impl.AbstractAmdServant.callInvokerOnRawArgs(AbstractAmdServant.java:140)\n        at ome.services.blitz.impl.ThumbnailStoreI.getThumbnailWithoutDefault_async(ThumbnailStoreI.java:132)\n        at jdk.internal.reflect.GeneratedMethodAccessor1624.invoke(Unknown Source)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\n        at omero.cmd.CallContext.invoke(CallContext.java:85)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\n        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)\n        at com.sun.proxy.$Proxy111.getThumbnailWithoutDefault_async(Unknown Source)\n        at omero.api._ThumbnailStoreTie.getThumbnailWithoutDefault_async(_ThumbnailStoreTie.java:162)\n        at omero.api._ThumbnailStoreDisp.___getThumbnailWithoutDefault(_ThumbnailStoreDisp.java:689)\n        at omero.api._ThumbnailStoreDisp.__dispatch(_ThumbnailStoreDisp.java:1108)\n        at IceInternal.Incoming.invoke(Incoming.java:221)\n        at Ice.ConnectionI.invokeAll(ConnectionI.java:2536)\n        at Ice.ConnectionI.dispatch(ConnectionI.java:1145)\n        at Ice.ConnectionI.message(ConnectionI.java:1056)\n        at IceInternal.ThreadPool.run(ThreadPool.java:395)\n        at IceInternal.ThreadPool.access$300(ThreadPool.java:12)\n        at IceInternal.ThreadPool$EventHandlerThread.run(ThreadPool.java:832)\n        at java.base/java.lang.Thread.run(Thread.java:829)\n2023-03-15 11:04:00,187 INFO  [        ome.services.util.ServiceHandler] (Server-162)  Excp:    java.lang.UnsatisfiedLinkError: 'void org.libjpegturbo.turbojpeg.TJDecompressor.init()'\n</code></pre>\n<p>Seems to be related to the thumbnail not been generated?, but only for a specific file? [ <a href=\"https://www.dropbox.com/s/9kvb51j0o3fkju6/Llano_de_la_cruz_R15%20-%202023-02-01%2009.45.40.ndpi\" rel=\"noopener nofollow ugc\">Llano_de_la_cruz_R15 - 2023-02-01 09.45.40.ndpi</a>]</p>\n<p>Because as you can see in this screenshot, the other image thumbnail gets generated, even thou there is no pyramid \u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3.png\" data-download-href=\"/uploads/short-url/AiYqJ7NjIVokO6zWCwapAug4bFF.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_690x388.png\" alt=\"image\" data-base62-sha1=\"AiYqJ7NjIVokO6zWCwapAug4bFF\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/fe735bacc47a5f47f7baf8924ee01a31e557bee3_2_1380x776.png 2x\" data-dominant-color=\"DFE0E2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71080 167 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Searching for <code>libjpegturbo.turbojpeg.TJDecompressor</code> and bioformats found this issue: <a href=\"https://github.com/ome/bioformats/issues/3756\" class=\"inline-onebox\">Libjpeg-turbo exception when converting/opening .ndpi file. \u00b7 Issue #3756 \u00b7 ome/bioformats \u00b7 GitHub</a> which looks like what you\u2019re seeing.</p>"], "78585": ["<p>Hey!</p>\n<p>I am working on a project and have become a little stuck.</p>\n<p>I am looking to use the output of a pixel classifier in QuPath to analyse specific subsections of a whole slide image of a lung slice. The subsection must contain annotations for three different classifications in close proximity. (eg: a 500um x 500um region containing: a tumour, lung vasculature and parenchyma.)</p>\n<p>So far I have:<br>\na. Created a pixel classifier in QuPath for the images.<br>\nb. Worked out how to filter the generated annotations from the pixel classifier by size.</p>\n<p>My problem is what to do next.</p>\n<ol>\n<li>I want to automate the analysis so that (for example) we loop through all of the tumour annotations and check if there are both lung vasculature and parenchyma annotations within +/- 250 pixels of the tumour annotation edge.</li>\n<li>I then would create 100um x 100um tiles from the parenchyma annotation and export these regions for analysis.</li>\n<li>I will then run the chosen 100um x 100um tiles through a texture analysis program.</li>\n</ol>\n<p>I plan to export the annotations (un-split) to ROIs and then perform steps 1 and 2 of the above plan in a separate python file and send the 100um x 100um tiles back into QuPath as new ROIs.</p>\n<p>I am now reaching a bit of a roadblock though, hopefully, some kind people can help answer a few of my questions:</p>\n<p>Firstly, if all of the above analysis can be done exclusively in QuPath, please let me know and I will focus on keeping the analysis within the one program.</p>\n<p>Secondly, does anyone know if it is possible to perform the \u201csplit\u201d function on ROIs (available in ImageJ) within Python?</p>\n<p>I have tried opening the ROIs using roifile in python after splitting the larger annotation in QuPath first. The classifier name is not exported in the metadata for the ROIs when split in QuPath, which makes my analysis rather tricky. Can this characteristic also be exported with the ROIs?</p>", "<aside class=\"quote no-group\" data-username=\"RyanC\" data-post=\"1\" data-topic=\"78585\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/r/a4c791/40.png\" class=\"avatar\"> RyanC:</div>\n<blockquote>\n<p>Secondly, does anyone know if it is possible to perform the \u201csplit\u201d function on ROIs (available in ImageJ) within Python?</p>\n</blockquote>\n</aside>\n<p>I imagine using PyImageJ it should be. Don\u2019t have much experience with it though. Not really clear to me what you mean by splitting the annotations though. If you want the sections of each class that exist within the particular 100x100 tile, you could do that with a second mask file. Then you don\u2019t need to deal with ROIs at all, you have the exact pixel masks, which can be created in QuPath using the labeledImageServer.</p>", "<aside class=\"quote no-group\" data-username=\"RyanC\" data-post=\"1\" data-topic=\"78585\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/r/a4c791/40.png\" class=\"avatar\"> RyanC:</div>\n<blockquote>\n<p>if it is possible to perform the \u201csplit\u201d function on ROIs (available in ImageJ) within Python?</p>\n</blockquote>\n</aside>\n<p>Try <code>RoiFile.coordinates(True)</code> to split \u201cmulti_coordinates\u201d into separate coordinates. There is currently no convenience function to split a ROI with multi_coordinates into separate ROI instances but that could be added.</p>", "<p>Exactly what I was looking for!<br>\nThank you <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78586": ["<p>Hi,</p>\n<p>I\u2019m currently making some tools that will likely have to parse and modify cellprofiler pipelines. It looks like there\u2019s work towards <a href=\"https://github.com/CellProfiler/core/pull/89\" rel=\"noopener nofollow ugc\">JSON support</a>, so I\u2019m hoping I don\u2019t have to write my own parser, but I can\u2019t seem to find how to export a pipeline as JSON format in 4.2.5.</p>\n<p>Is this still work in progress, or have a missed a big obvious button somewhere?</p>"], "78587": ["<p>I\u2019m trying to train a new cellpose model in GUI. I have green cytoplasm and red nuclei in RGB images. Segmentation with pretrained models looks ok.<br>\nWhen I train a new model, I receive this error in the terminal:</p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/gui/gui.py\", line 1631, in new_model\n    self.train_model()\n  File \"/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/gui/gui.py\", line 1655, in train_model\n    self.new_model_path = self.model.train(self.train_data, self.train_labels, \n  File \"/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/models.py\", line 764, in train\n    train_data, train_labels, test_data, test_labels, run_test = transforms.reshape_train_test(train_data, train_labels,\n  File \"/Users/bocan/opt/anaconda3/envs/data_analysis_env/lib/python3.10/site-packages/cellpose/transforms.py\", line 422, in reshape_train_test\n    if train_labels[0].ndim &lt; 2 or train_data[0].ndim &lt; 2:\nIndexError: list index out of range\n</code></pre>\n<p>It seems similar to <a href=\"https://github.com/MouseLand/cellpose/issues/571#:~:text=422%20if%20train_labels%5B0%5D.ndim%20%3C%202%20or%20train_data%5B0%5D.ndim%20%3C%202%3A\" rel=\"noopener nofollow ugc\">this previously reported issue</a> on GitHub.<br>\nReinstalling cellpose did not help. Same problem occurs irrespective to initial model I choose for training, also irrespective to channel settings.<br>\nAny ideas? Could it be a bug?<br>\nThanks a lot.</p>", "<p>I believe it must be a bug, because moving the images used for training to another folder helped. I moved them to a folder where I trained previous models.</p>"], "78588": ["<p>Hi,<br>\nI\u2019m new to cellfinder, and I was able to run the tutorial smoothly (looks great - good job!).<br>\nIn the documentation you indicate I need to use a fully reconstructed brain, but I was wondering if I could only use the cell detection part on a smaller stack, without registration to the atlas.<br>\nIf not, what would you recommend instead of cellfinder?<br>\nThank you very much,<br>\nAmit<br>\nHere are two examples (two channels of the same section):<br>\n<a href=\"https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/ERyg9NTENd9Cg0jqK4suH64BhjGX6cFrntpTu2SfyQGtLQ?e=JbgtD9\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/ERyg9NTENd9Cg0jqK4suH64BhjGX6cFrntpTu2SfyQGtLQ?e=JbgtD9</a></p>\n<p><a href=\"https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/EdVprenLy8tJhevWYKdibiUBhL1XM2uduZlkL2VdT6SsHw?e=zGhYdQ\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://emckclac-my.sharepoint.com/:u:/g/personal/k2145690_kcl_ac_uk/EdVprenLy8tJhevWYKdibiUBhL1XM2uduZlkL2VdT6SsHw?e=zGhYdQ</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/amitbenben\">@Amitbenben</a>,</p>\n<p>You can run the cell detection algorithm on any 3D image by using the napari plugin. The instructions are here:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://docs.brainglobe.info/cellfinder-napari/introduction\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/9995eaa313ce50caffc90f5047ed4758d1c96a3e.png\" class=\"site-icon\" width=\"256\" height=\"256\">\n\n      <a href=\"https://docs.brainglobe.info/cellfinder-napari/introduction\" target=\"_blank\" rel=\"noopener\">docs.brainglobe.info</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/362;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da_2_690x362.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da_2_1035x543.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/5/d59bbfdedea7e137666322985e80e9e1857986da.png 2x\" data-dominant-color=\"F8FAFC\"></div>\n\n<h3><a href=\"https://docs.brainglobe.info/cellfinder-napari/introduction\" target=\"_blank\" rel=\"noopener\">Introduction</a></h3>\n\n  <p>About the cellfinder napari plugin</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>However, as your images are fairly small, you may want to use a cell segmentation (rather than just soma detection) tool instead, something like <a href=\"https://www.cellpose.org/\">cellpose</a>.</p>\n<p>Adam</p>"], "78589": ["<ol>\n<li>Is there a way to rename the bodyparts that I already labeled?</li>\n<li>Is there a way to select a subset of labeled bodyparts (say I have 31 body parts defined and labeled but I only want to train with 21 of them for some reason), and create training set?</li>\n</ol>", "<ol>\n<li>Rename them in the config and all the <code>CollectedData_name.h5</code> files (copy them first just to be safe)</li>\n<li>Remove the names you don\u2019t want to use from the config and CollectedData files like above</li>\n</ol>"], "78591": ["<p>Hello!</p>\n<p>My research focuses on treating S. aureus biofilms with a chemical that disrupts their biofilm formation. Because our flow cell was made in-house, it is too thick for most objective lenses on our confocal and I can only use a 10x objective.</p>\n<p>I am not seeing formation of clear microcolonies (see example image) and I am not sure if this is due to S. aureus not forming microcolonies (literature is not consistent on if S. aureus does or not) or if this is a product of low magnification. Either way, I have essentially been treating each z-stack as a full biofilm. Does this seem viable?</p>\n<p>I am also curious about the strength of my single object parameters considering the magnification. Is it better to focus on global biofilm parameters?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01.png\" data-download-href=\"/uploads/short-url/wtuqT7lrxJDNEWs3PDxK3gsBxWp.png?dl=1\" title=\"example_4EB_biofilm\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01_2_504x500.png\" alt=\"example_4EB_biofilm\" data-base62-sha1=\"wtuqT7lrxJDNEWs3PDxK3gsBxWp\" width=\"504\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01_2_504x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e39afb7396c8821b744ea604056a8c2ef8bdac01.png 2x\" data-dominant-color=\"626261\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">example_4EB_biofilm</span><span class=\"informations\">531\u00d7526 295 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Alex,</p>\n<p>given the structure of your biofilms as shown in the image it seems to me that it does indeed make sense to analyze the entire image and extract its parameters as if it were a single biofilm. Whether this makes sense in relation to your research question or not depends strongly on the question as well as the properties you wish to quantify. In my experience, images such as yours can be perfectly suitable to address a variety of research questions. Single cell resolution is not always needed.</p>\n<p>In the current version of BiofilmQ, all properties are suitable to be calculated based on cubes, which means that they are useful to characterize low-resolution images. You don\u00b4t need to focus on only global biofilm properties, but can take advantage the full breadth of available properties.</p>\n<p>One thing that you should be aware of in your context is that the spatial properties calculated by the \u201cdistance to center biofilm\u201d parameter in the parameter calculation tab will in your case likely not be very useful, because they will mainly represent the distance to the center of the image. When using spatial properties, e.g. for visualizing localization of fluorescence etc. I would recommend to either use distanceToSubstrate or distanceToSurface.</p>\n<p>Let me know if you would like more explanation on one of the things mentioned above or have any other questions.</p>\n<p>Best,</p>\n<p>Hannah</p>", "<p>Hi Hannah,</p>\n<p>Thank you so much for the detailed response! My next question was going to be about the \u201cdistance to center biofilm\u201d parameter but you answered it for me!</p>"], "78594": ["<p>Hi,</p>\n<p>I\u2019m a former ImageJ user from over 20 years ago, where I used ImageJ to prototype processing pipelines for real-time neutron and X-Ray radiography systems, then subsequently to develop color management/recovery algorithms for a 100K fps camera and other industrial/scientific imaging systems, after which my career moved away from imaging. I\u2019m now semi-retired and am looking to refresh those ancient skills.</p>\n<p>My initial target is my forearm and my new (and first) tattoo, where I hope to construct a flattened image suitable for printing or as a desktop/phone background. While this could be trivial to do using Photoshop or photogrammetry tools, I\u2019d prefer to use ImageJ.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd.jpeg\" data-download-href=\"/uploads/short-url/8NLDFs1lemsp21hrfcO9reVo2v3.jpeg?dl=1\" title=\"My Tattoo 5 Days Later\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_374x500.jpeg\" alt=\"My Tattoo 5 Days Later\" data-base62-sha1=\"8NLDFs1lemsp21hrfcO9reVo2v3\" width=\"374\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_374x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_561x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3db19a8a6f0bdcd206e8ef8c7394b73f2d078edd_2_748x1000.jpeg 2x\" data-dominant-color=\"898280\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">My Tattoo 5 Days Later</span><span class=\"informations\">1666\u00d72222 310 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Below is the artwork the tattoo artist used for inspiration:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7.jpeg\" data-download-href=\"/uploads/short-url/k6bNopOXsfpArsM3j5yYF20z2bt.jpeg?dl=1\" title=\"Bee Art\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7_2_500x500.jpeg\" alt=\"Bee Art\" data-base62-sha1=\"k6bNopOXsfpArsM3j5yYF20z2bt\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8cde9d7c126440dc51c20198b5f69a07c879d8b7.jpeg 2x\" data-dominant-color=\"B4BAB4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Bee Art</span><span class=\"informations\">768\u00d7768 124 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I installed Fiji and was instantly gratified to see that <em><strong>very</strong></em> familiar UI, seemingly unchanged! Digging into the menus revealed extensive growth and evolution, leaving me feeling a bit lost.</p>\n<p>I know I need to start with capturing a suitable image set.</p>\n<ul>\n<li>What constitutes a \u201csuitable\u201d image set? I\u2019m guessing at least 10 images taken around my forearm at a distance great enough to minimize lens distortion. I know I\u2019ve previously computed optimal acquisition metrics for similar problems in the 2D and 3D industrial inspection domains, but the specific memories elude me.</li>\n<li>At a low level, I know I\u2019ll need to import, resize and rotate the images to get closer to a common image geometry. So, it may be good if each image contained a common central feature, such as the centerline of the bee.</li>\n<li>I could take images with my arm pressed against a pane of glass, providing a physically flattened local geometry. Though I may need to account for skin elasticity\u2026</li>\n<li>I can also shoot 4K video, though compression artifacts may be an issue.</li>\n</ul>\n<p>From there, I see several paths to pursue:</p>\n<ul>\n<li>My forearm is roughly conic: Map the topology, unwarp each image, then identify and combine the \u201cbest center strip\u201d from each image.</li>\n<li>The image contains (hopefully) uniformly sized tiled hexagons: Extract the edges, identify the vertices, then warp each to an ideal hexagon.</li>\n<li>Use the original art as a template.</li>\n</ul>\n<p>Once results are obtained, the flattened skin image could be perceived as a harvested human hide, triggering intense ick incidents, so additional processing may be warranted. It may be fun to try to subtract the skin texture and color (as pre- and/or post-processing) to create a drawing-like image, then process that into directional color gradients and lines to create an idealized vector (SVG) representation.</p>\n<p>As I wandered through the menus I stumbled upon the <a href=\"http://bigwww.epfl.ch/thevenaz/UnwarpJ/\" rel=\"noopener nofollow ugc\"><code>bUnwarpJ</code></a> plugin, triggering \u201c<em>One &amp; Done</em>\u201d fantasies. Which were not to be realized, as only the central bee was flattened, not the entire tattoo. I suspect the differences in background grid size confused the plugin.</p>\n<p>Clearly, a pipeline is needed. Rather than dive into the deep end right away, I\u2019m looking for ImageJ \u201chints\u201d as starting points, a list of modern features or plugins I should initially investigate.</p>\n<p>I\u2019m also thinking it may be fun to try a few different approaches, then compare/contrast them, which would inspire the creation of metrics for a comparison/evaluation/grading pipeline (noise content, color fidelity, geometry accuracy, compute cost, etc.). I see the potential for some really crazy approaches, including ones in the Rube Goldberg tradition. If anyone would like to play along, I\u2019ll post my image sets as I acquire them.</p>\n<p>Ideally, I\u2019d like to record progress in this thread, then contribute a document covering the results back to the community as a (possibly bizarre) tutorial. I like to leave tutorials or blog posts behind whenever I enter a new domain: We\u2019re only beginners once! (Well, twice for me, I suppose.)</p>\n<p>While I\u2019m certainly willing to proceed on my own, I was wondering if anyone here would wish to mentor me on this journey? All contributions are welcome!</p>\n<p>Thanks,</p>\n<p>-BobC</p>"], "78596": ["<p>Hello everyone,</p>\n<p>I am having an issue I have not dealt with before where cells are being counted in a peculiar manner. Specifically, the cells are counted in a square box at the center and also on the edges of the image rather than the whole area. This seems to be dependent on the pixel size of the stardist script provided. Please let me know what this may be caused by.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a.jpeg\" data-download-href=\"/uploads/short-url/sk8iwamMKF659wTiFnGVyOQV5X4.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_690x349.jpeg\" alt=\"image\" data-base62-sha1=\"sk8iwamMKF659wTiFnGVyOQV5X4\" width=\"690\" height=\"349\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_690x349.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_1035x523.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/6/c68382a3bdace66687d84729a89ffab14c52639a_2_1380x698.jpeg 2x\" data-dominant-color=\"908F7A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7973 277 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e.jpeg\" data-download-href=\"/uploads/short-url/ulLm1R1k8H1gBUK9EBh1xiuUbU2.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_690x328.jpeg\" alt=\"image\" data-base62-sha1=\"ulLm1R1k8H1gBUK9EBh1xiuUbU2\" width=\"690\" height=\"328\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_690x328.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_1035x492.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4b706adbc779914ecf913d761e725ed15e4975e_2_1380x656.jpeg 2x\" data-dominant-color=\"959385\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7913 218 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Background subtraction for the tile. The black is probably throwing it off. If you change the pixel size that changes the tile size, so you get different results.</p><aside class=\"quote quote-modified\" data-post=\"6\" data-topic=\"77196\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/questions-and-problems-when-using-qupath-cellpose/77196/6\">Questions and problems when using Qupath Cellpose</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Most likely due to how background is handled. This is not new or specific to CellPose. \n\n\netc.\n  </blockquote>\n</aside>\n<p>\nNot a new problem, and not specific to StarDist, sadly.</p>\n<p>Have you tried making your annotation only cover the area where the cells are, and no black space?</p>", "<p>Sorry for the late reply,</p>\n<p>This solution worked great! Thank you.</p>"], "772": ["<p>I\u2019ve been at this for a while and I\u2019m admitting defeat. I know it is possible, I just can\u2019t get it to work. See attached Electron Microscope image ( a small section to start, keep it simple, images are much larger and more complicated)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/oY8aED88VWe6Jbgi80LgN3Kr8rz.zip\">Line Detection Test.zip</a> (870.5 KB)</p>\n<p>The image is a metal fracture surface. Due to the stress placed on the material, horizontal cracks are formed, which need to be quantified (once in binary form i have a macro down for it).  The areas around in are dimpled, often described as \u201corange peel\u201d in classrooms.</p>\n<p>Need to:</p>\n<ol>\n<li>adjust Brightness/Contrast for maximum sharpness of the cracks (auto adjust seems to sharpen the dimples more)</li>\n<li>separate line segments from \u201cdimples\u201d (i.e. discard/threshold out dimples, or anything circular)</li>\n<li>discard any line segments that are not (roughly) horizontal. Reduce any shapes to a single line along the major axis (Draw out the Feret Diameter?).</li>\n</ol>", "<p>Hi Ajay</p>\n<p>I took a quick (5 minute) try at processing your image.  I can\u2019t say I achieved 100% success and perhaps you allready tried all this\u2026</p>\n<ul>\n<li>Apply a smoothing filter (Gaussian Blur)</li>\n<li>Apply a horizontal edge filter-<br>\nI achieved this by running process-&gt;filters-&gt;convolve with the following kernel</li>\n</ul>\n<p>-1 -2 -1<br>\n0 0 0<br>\n1 2 1</p>\n<ul>\n<li>Apply Otsu automatic threshold</li>\n<li>Analyze particles.</li>\n</ul>\n<p>\u2026 maybe other people will have better ideas\u2026 I think the key is finding a combination of smoothing and edge filter that will bring out the horizontal cracks, then optimizing the particle filter\u2026</p>\n<p>below is the beanshell script that was recorded</p>\n<pre><code class=\"lang-auto\">IJ.run(imp, \"Gaussian Blur...\", \"sigma=2\");\nIJ.run(imp, \"Convolve...\", \"text1=[-1 -2 -1\\n0 0 0 \\n1 2 1 \\n] normalize\");\nIJ.run(imp, \"Auto Threshold\", \"method=Otsu white\");\nIJ.run(imp, \"Analyze Particles...\", \"size=50-Infinity circularity=0.00-0.50 show=Masks display add\");\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a></p>\n<p>I tried your suggestions, my results lead to excessive fragmentation. However, I do want to say that your are on the right track, which is FAR further than I would have ever gotten on my own.</p>\n<p>For giggles, I applied a <em>vertical</em> Sobel filter on my whole image when the horizontal filter wasn\u2019t getting me where I needed to go, and sure enough it highlighted a definite alignment at around 70\u00b0. I tried using a Fourier transform via FFT (and bandpass filters) but I can\u2019t seem to get end results, even though immediately after passing through a vertical bandpass filter the vertical component looks much cleaner.</p>\n<p>(That is of course, if I understand how or why or when the $#@%&amp;* works. Sometimes I get an output, sometimes I get just an FFT which can be re-transformed, and sometimes I get nothing after a filter image. But then again, that seems to also be the case when I go Analyze&gt;Filter&gt;Convolve where Preview and Filtering mysteriously stops working\u2026 weird)</p>\n<p>This leads me to suspect a Gabor filter (or variant) may be where I need to go, but I am having troubles following the Gabor example macro (and there is no plugin).</p>\n<p>I haven\u2019t posted a full image in public, as I cannot, but I can privately send people the full image, if it helps.</p>", "<aside class=\"quote\" data-post=\"3\" data-topic=\"772\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"//forum.image.sc/user_avatar/forum.image.sc/ajay.sood/40/344_1.png\" class=\"avatar\"> Ajay.Sood:</div>\n<blockquote>\n<p>This leads me to suspect a Gabor filter (or variant) may be where I need to go, but I am having troubles following the Gabor example macro (and there is no plugin).</p>\n</blockquote>\n</aside>\n<p>Do you mean this <a href=\"http://imagej.net/Gabor_Filter_script\">Gabor filter example</a>??  Or a different macro?</p>\n<p>I tried running the above Macro.  I couldn\u2019t see the actual link to download the beanshell script so I had to copy and paste into fiji script editor then explicitly save it as beanshell before it would work.</p>\n<p>Did you manage to get the Gabor filter working on your image??  If so, and results were not good, maybe tweaking below parameters will get you better results?  Or maybe you tried tweaking parameters and ran into problems?</p>\n<pre><code class=\"lang-java\">// Sigma defining the size of the Gaussian envelope\nsigma = 8.0;\n// Aspect ratio of the Gaussian curves\ngamma = 0.25;\n// Phase\npsi = Math.PI / 4.0 * 0;\n// Frequency of the sinusoidal componentF\nx = 3.0;\n// Number of diferent orientation angles to use\nnAngles = 5;\n</code></pre>", "<p>Yes, that\u2019s the only example I could find. I had troubles following the example code logic to implement it. It has a lot to do with the fact that I don\u2019t have a good grasp of how to go from an explanation of image processing in the frequency space to modifying the equation appropriately to get it to do what I need it to do. If it was a plugin already, I could try altering the parameters experimentally until it started to make sense in my head. I\u2019m stuck troubleshooting code before I get to trying it out.</p>\n<p>PS From a purely Image Analysis perspective (without getting into ImageJ/Fiji), there are many options if I can wrap my head around the math. By variant I meant stuff like <a href=\"https://en.wikipedia.org/wiki/Log_Gabor_filter\" rel=\"nofollow noopener\">Log Gabor filter - Wikipedia</a>. But the OCR on the Chinese character in the article <a href=\"https://en.wikipedia.org/wiki/Gabor_filter\" rel=\"nofollow noopener\">Gabor Filter - Wikipedia </a> Illustrates what I hope the Gabor filter can do for me.</p>", "<p>Hello Ajay,</p>\n<p>Using the Gabor filter script you can try to enhance the horizontal structures in your image. You just need a bit of patience to find the right parameters. Have a look at the following results I obtained from your image using <code>sigma</code> = 9, <code>gamma</code> = 0.25, <code>psi</code> = 0, <code>Fx</code> = 2.0, <code>nAngles</code> = 10 and a filter size of 51x51:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/1X/e6df3d1a33fea9fc37abdcef5cad1a03f248ce43.png\" data-download-href=\"/uploads/short-url/wWo6v7QPJp6z0B49HtZ76p6AWpJ.png?dl=1\" title=\"\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/1X/e6df3d1a33fea9fc37abdcef5cad1a03f248ce43_2_690x371.png\" alt data-base62-sha1=\"wWo6v7QPJp6z0B49HtZ76p6AWpJ\" width=\"690\" height=\"371\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/1X/e6df3d1a33fea9fc37abdcef5cad1a03f248ce43_2_690x371.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/1X/e6df3d1a33fea9fc37abdcef5cad1a03f248ce43_2_1035x556.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/1X/e6df3d1a33fea9fc37abdcef5cad1a03f248ce43_2_1380x742.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/1X/e6df3d1a33fea9fc37abdcef5cad1a03f248ce43_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\"></span><span class=\"informations\">1524\u00d7820 612 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here you are the exact script I used, which is a faster version than the one on the site since it uses FFT:</p>\n<pre><code class=\"lang-auto\">import net.imglib2.img.ImagePlusAdapter;\nimport net.imglib2.img.Img;\nimport net.imglib2.img.display.imagej.ImageJFunctions;\nimport net.imglib2.type.numeric.real.FloatType;\nimport net.imglib2.algorithm.fft2.FFTConvolution;\n\nimport ij.*;\nimport ij.process.*;\nimport ij.plugin.filter.*;\nimport ij.plugin.ContrastEnhancer;\nimport ij.plugin.ZProjector;\n\n/**\n* This script calculates a set of Gabor filters over the selected image.\n*\n* Parameters: sigma, gamma, psi, Fx, nAngles\n*/\n\nstart = System.currentTimeMillis();\n\n// Sigma defining the size of the Gaussian envelope\nsigma = 9;\n// Aspect ratio of the Gaussian curves\ngamma = 0.25;\n// Phase\npsi = Math.PI / 4.0 * 0;\n// Frequency of the sinusoidal component\nFx = 2.0;\n\n// Number of diferent orientation angles to use\nnAngles = 10;\n\n// copy original image and transform it to 32 bit \noriginalImage = IJ.getImage();\noriginalImage = new ImagePlus(originalImage.getTitle(), originalImage.getProcessor().convertToFloat());\nwidth = originalImage.getWidth();\nheight = originalImage.getHeight();\n\n// Apply aspect ratio to the Gaussian curves\nsigma_x = sigma;\nsigma_y = sigma / gamma;\n\n// Decide size of the filters based on the sigma\nlargerSigma = (sigma_x &gt; sigma_y) ? (int) sigma_x : (int) sigma_y;\nif(largerSigma &lt; 1)\n    largerSigma = 1;\n    \nip = originalImage.getProcessor().duplicate();\n\nsigma_x2 = sigma_x * sigma_x;\nsigma_y2 = sigma_y * sigma_y;\n\n// Create set of filters\n\nfilterSizeX = 51; //6 * largerSigma + 1;\nfilterSizeY = 51; //6 * largerSigma + 1;\n\n\nmiddleX = (int) Math.round(filterSizeX / 2);\nmiddleY = (int) Math.round(filterSizeY / 2);\n\nis = new ImageStack(width, height);\nkernels = new ImageStack(filterSizeX, filterSizeY);\n\nrotationAngle = Math.PI/(double)nAngles;\n// Rotate kernel from 0 to 180 degrees\nfor (i=0; i&lt;nAngles; i++)\n{    \n    theta = rotationAngle * i;\n    filter = new FloatProcessor(filterSizeX, filterSizeY);    \n    for (int x=-middleX; x&lt;=middleX; x++)\n    {\n        for (int y=-middleY; y&lt;=middleY; y++)\n        {            \n            xPrime = (double)x * Math.cos(theta) + (double)y * Math.sin(theta);\n            yPrime = (double)y * Math.cos(theta) - (double)x * Math.sin(theta);\n                \n            a = 1.0 / ( 2.0 * Math.PI * sigma_x * sigma_y ) * Math.exp(-0.5 * (xPrime*xPrime / sigma_x2 + yPrime*yPrime / sigma_y2) );\n            c = Math.cos( 2.0 * Math.PI * (Fx * xPrime) / filterSizeX + psi); \n            \n            filter.setf(x+middleX, y+middleY, (float)(a*c) );\n        }\n    }\n    kernels.addSlice(\"kernel angle = \" + theta, filter);\n}\n\n// Show kernels\nip_kernels = new ImagePlus(\"kernels\", kernels);\nip_kernels.show();\n\n// Apply kernels\nfor (i=0; i&lt;nAngles; i++)\n{\n    ip2 = originalImage.duplicate();\n    \n    kernel = ImagePlusAdapter.wrap( new ImagePlus(\"\", kernels.getProcessor( i+1 )) );\n    image2 = ImagePlusAdapter.wrap( ip2 );\n    \n\n    c = new FFTConvolution( image2, kernel );\n    c.convolve();\n                        \n    ip2 = ImageJFunctions.wrap( image2, \"\" );\n    \n    is.addSlice(\"gabor angle = \" + (180.0/nAngles*i), ip2.getProcessor() );    \n}\n\n// Normalize filtered stack (it seems necessary to have proper results)\nc = new ContrastEnhancer();\nfor(int i=1 ; i &lt;= is.getSize(); i++)\n{\n    c.stretchHistogram(is.getProcessor(i), 0.4);\n}\n\n\nprojectStack = new ImagePlus(\"filtered stack\",is);\nIJ.run(projectStack, \"Enhance Contrast\", \"saturated=0.4 normalize normalize_all\");\n                \nresultStack = new ImageStack(width, height);\n                \nzp = new ZProjector(projectStack);\nzp.setStopSlice(is.getSize());\nfor (int i=0;i&lt;=5; i++)\n{\n    zp.setMethod(i);\n    zp.doProjection();\n    resultStack.addSlice(\"Gabor_\" + i \n            +\"_\"+sigma+\"_\" + gamma + \"_\"+ (int) (psi / (Math.PI/4) ) +\"_\"+Fx, \n            zp.getProjection().getChannelProcessor());\n}\n\n// Display filtered images\n(new ImagePlus(\"Gabor, sigma=\"+sigma+\" gamma=\"+gamma+ \" psi=\"+psi, is)).show();\n\nresult= new ImagePlus (\"Gabor stack projections\", resultStack) ;\nIJ.run(result, \"Enhance Contrast\", \"saturated=0.4 normalize normalize_all\");\nresult.show();\n\nend = System.currentTimeMillis();\nIJ.log( \"Gabor filter took \" + (end-start) + \"ms\" );\n</code></pre>", "<aside class=\"quote no-group\" data-post=\"6\" data-topic=\"772\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/iarganda/40/61_2.png\" class=\"avatar\"> iarganda:</div>\n<blockquote>\n<p>You just need a bit of patience to find the right parameters. Have a look at the following results I obtained from your image using sigma = 9, gamma = 0.25, psi = 0, Fx = 2.0, nAngles = 10 and a filter size of 51x51:</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/iarganda\">@iarganda</a> I wonder if a size of 51 isn\u2019t big enough to capture the entire filter.  Inside the convolve the filter will be padded (with 0 I think) and there will be a discontinuity at the edge, and the properties of the filter are different.</p>\n<p>Your result actually isn\u2019t bad\u2026 when I use 255 as the size the result is much blurrier</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/1X/d9b5d62a70af659ff6be64f362ba08b72a0a5a1e.png\" data-download-href=\"/uploads/short-url/v3Xc2p9ZZLk0Np1pXucYBzlf6dE.png?dl=1\" title=\"\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/1X/d9b5d62a70af659ff6be64f362ba08b72a0a5a1e.png\" alt data-base62-sha1=\"v3Xc2p9ZZLk0Np1pXucYBzlf6dE\" width=\"690\" height=\"370\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/1X/d9b5d62a70af659ff6be64f362ba08b72a0a5a1e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\"></span><span class=\"informations\">763\u00d7410 108 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p><a class=\"mention\" href=\"/u/ajay.sood\">@Ajay.Sood</a> If I start with the image that <a class=\"mention\" href=\"/u/iarganda\">@iarganda</a> created, apply the horizontal edge filter, then take only positive values, then apply an automatic threshold, then count particles with min size=1000 I get an OK result  (maybe this is getting a bit hacktastic\u2026 but it seems to be heading in the right direction at least\u2026)<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/1X/ce555a27570559f4435d2d8956d79369164bb1f8.png\" alt data-base62-sha1=\"trjdXcmxgM3M0WHjhjiWj3YDP5e\" width=\"377\" height=\"378\"></p>\n<p>The script I used is below (it uses concepts from IJ2 and ops and is based on scripts <a href=\"https://imagej.github.io/presentations/2015-09-04-imagej2-scripting/\" rel=\"nofollow noopener\">presented here</a></p>\n<pre><code class=\"lang-python\"># @OpService ops\n# @UIService ui\n# @Dataset inputData\n\nfrom net.imglib2.img.display.imagej import ImageJFunctions\nfrom ij import IJ\nfrom ij.plugin import Duplicator\n\nminParticleSize=1000\n\n# create horizontal edge filter ###################################3333\nkernel=ops.create().img([3,3])\n\nkernelList=[-1,-2,-1,0,0,0,1,2,1]\nkernelCursor=kernel.cursor()\n\ni=0\nfor t in kernel:\n    t.setReal(kernelList[i])\n    i=i+1\n# end create horizontal edge filter ###################################3333\n\n# convolve with edge filter\nfiltered=ops.filter().convolve(inputData, kernel)\n\n# get rid of negative values (auto threshold will work better)\nfor p in filtered:\n    if (p.getRealFloat()&lt;0): p.setReal(0)\n\nui.show(filtered)\n\n# threshold\nthresholded = ops.threshold().ij1(filtered)\n\n# convert to imagej1 imageplus so we can run analyze particles\nimpThresholded=ImageJFunctions.wrap(thresholded, \"wrapped\")\nui.show(\"thresholded\", impThresholded)\n\n# convert to mask and analyze particles\nIJ.run(impThresholded, \"Convert to Mask\", \"\")\nIJ.run(impThresholded, \"Analyze Particles...\", \"size=\"+str(minParticleSize)+\"-Infinity show=[Overlay Masks]\");\nimpThresholded.updateAndDraw()\n</code></pre>", "<p>Hello <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a> and <a class=\"mention\" href=\"/u/iarganda\">@iarganda</a>,</p>\n<p>One of the curses of my industries, being pulled off to fight another fire, has left me unable to try your suggested solutions right now. However, I appreciate the responses and wanted to take the time to thank you for looking into it. I will try it as soon as I can manage it and get back to you.</p>\n<p>Just a reminder, the image I posted in general was a very small picture, the originals are about 8300x3200 pixels. I will have to play with the numbers to adjust for the more challenging areas. If you guys are curious, I can privately send you the full image.</p>\n<p>In some of the images,  I will have to figure out how to separate the cracks from extrusion marks and/or polishing marks, which are frustratingly in the same orientation as the cracks (all hail Murphy\u2019s Law!). The good news is we\u2019re also getting slightly better images because we\u2019ve adjusted the electron microscope settings (increased kV) to get more depth information.</p>", "<p>Hi <a class=\"mention\" href=\"/u/iarganda\">@iarganda</a>,</p>\n<p>Thanks for writing this FFT-Gabor filter script. My Beanshell skills are zero, but I have found it useful by manually adjusting values. Are there plans for a Gabor (and LogGabor) filter plugin for ImageJ? It could be useful for for machine learning-based segmentation if a systematically adjusted range of sigma, gamma, psi, frequency and orientation values could be entered using a macro. There seems to be a plugin as part of the Weka 2D plugin, but only gamma is adjustable.</p>\n<p>Guy</p>", "<p>I\u2019m glad it was useful!</p>\n<p>To be honest, there are so many parameters that I didn\u2019t implement more options in Trainable Weka Segmentation to avoid creating too many image features\u2026</p>", "<p>Hi <a class=\"mention\" href=\"/u/iarganda\">@iarganda</a>,</p>\n<p>In the example above, when the orientation of the structure to the segment is known, wouldn\u2019t it be possible to only use that orientation (<code>nAngles = 1</code> and somehow Gabor angle = 90)?</p>\n<p>Thanks!</p>"], "78599": ["<p>I am lucky enough to have access to an NVidia A40 GPU.  However, it is not obvious to me how to get Startdist (and CSBDeep) to use the GPU.  It looks like the minimum Cuda version for this GPU is 11.0, and that the minimum Tensorflow version for Cuda 11is 2.4. Am I out of luck or is there a way to get this to work?</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicost\">@nicost</a></p>\n<p>There should actually not be any problem - tensorflow/csbdeep/stardist should be fully compatible with your GPU (and cuda 11 or 12). What kind of issue do you currently face?</p>\n<p>Cheers,<br>\nMartin</p>", "<p>Cool! I was trying to run it under Fiji, and that lists only Tensorflow 1.  In the mean time switched gears to Python.  Is there a way to use Tensorflow 2 with the CSBDeep plugin in Fiji?</p>", "<p>Yes, the tensorflow plugin in fiji (which csbdeep/stardist relies on) is still tied to 1.x, so indeed sticking to python is preferable</p>"], "78601": ["<p>Hello,</p>\n<p>I have trained a variation of the YOLOX object detection neural network for an ecological application that I am attempting to export for use in DeepImageJ. This type of network is different from many of the existing models in the bioimage model zoo because instead of returning a segmentation mask, it returns a list of predictions in the from [(x1, y1, x2, y2, confidence, class label), \u2026] which represent predicted bounding boxes. I would like these predictions to be returned as a table in Fiji, similar to <a href=\"https://bioimage.io/#/?tags=deepimagej%2Fskinlesionclassification&amp;id=deepimagej%2Fskinlesionclassification\" rel=\"noopener nofollow ugc\">this example</a></p>\n<p>I have exported my model in torchscript format, and I can view the model in Fiji, but when I try to run the model on the example image, Fiji crashes. I assume that this is due to an issue with the rdf.yml configuration file I am using, or the format of the outputs returned by the model. I will provide this configuration file below, and I would be grateful for any suggestions on how to modify it to correctly return the predictions to deepImageJ, or really any more documentation on how to format the file. I was originally attempting to create the file using the <a href=\"https://github.com/bioimage-io/core-bioimage-io-python\" rel=\"noopener nofollow ugc\">bioimage.io python tool</a> but I could not get it to run without errors and switched to trying to create the configuration manually.</p>\n<p>I can provide the torchscript model on request as well if that would be helpful.</p>\n<p>rdf.yml:</p>\n<pre><code class=\"lang-auto\">attachments:\n  files: [results.csv, test.jpg]\nauthors:\n- {name: Jemison}\ncite:\n- {text: Jemison et al., url: tbd}\nconfig:\n  deepimagej:\n    allow_tiling: true\n    model_keys: null\n    prediction:\n      prediction:\n            preprocess:\n              - {spec: null}\n            postprocess:\n              - {spec: null}\n    pyramidal_model: false\n    test_information:\n      inputs:\n      - name: test.jpg\n        pixel_size: {x: 1.0, y: 1.0, z: 1.0}\n        size: 896 x 684 x 3 x 1\n      memory_peak: null\n      outputs:\n      - {name: results.csv, size: 100x6, type: ResultsTable}\n      runtime: null\ncovers: [cover.png]\ndescription: kelp sporophyte object detection\ndocumentation: doc.md\nformat_version: 0.4.8\ninputs:\n  - name: input0\n    axes: bcyx\n    data_type: float32\n    data_range: [-inf, inf]\n    shape:\n        min: [1, 1, 3, 1]\n        step: [0, 0, 1, 1]\nlicense: CC-BY-4.0\nlinks: [deepimagej/deepimagej]\nname: YOLOX-kelp\noutputs:\n- axes: bcyx\n  data_range: [-.inf, .inf]\n  data_type: float32\n  name: output0\n  postprocessing:\n  - {spec: null}\n  shape: [100, 6]\nsample_inputs: [test.jpg]\nsample_outputs: [results.csv]\ntags: [kelp-object-detection]\ntest_inputs: [test.jpg]\ntest_outputs: [results.csv]\ntimestamp: '2023-02-17T11:56:47.272556'\ntype: model\nweights:\n  torchscript: {sha256: 0b65224af72b746b8c5c431eed9d1ea5b74a40883ee8b7688e41505cb738fc51,\n    source: yolox_kelp_l_combined.pt}\n</code></pre>", "<p>Hello <a class=\"mention\" href=\"/u/liam\">@liam</a> , it seems that the error is happening on the yaml file as you suggest.<br>\nAs you mention too, the python tool is mostly suggested for image2image models,<br>\nFor your model, the yaml file should be slightly different. If you want to return a table, the axis will have to be different. It seems that the output has only two dims (shape: [100, 6]) whereas the axes specified show 4 dimensions (bcyx).<br>\nIf you could provide the torchscript model, I will be able to give you a more detailed solution.<br>\nREgards,<br>\nCarlos</p>", "<p>Hello <a class=\"mention\" href=\"/u/carlosuc3m\">@carlosuc3m</a>, thank you for the quick reply. The torchscript file is too large to include in this post but I will provide a link that you should be able to use to download it.</p>\n<p>Thanks,</p>\n<p>Liam</p>\n<p><a href=\"https://panthers-my.sharepoint.com/:u:/g/personal/ljemison_uwm_edu/EUbtQmE03ntAg9OT6-MCH2QBxwGEZxkod3jcpOOOb0zD5w?e=EK8CB6\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://panthers-my.sharepoint.com/:u:/g/personal/ljemison_uwm_edu/EUbtQmE03ntAg9OT6-MCH2QBxwGEZxkod3jcpOOOb0zD5w?e=EK8CB6</a></p>", "<p>HEllo <a class=\"mention\" href=\"/u/liam\">@liam</a> , could you please a short pytorch script to load and run this torchscript model in colab on a random tensor?</p>"], "78603": ["<p>I am using the MOSAIC plugin to track the particles and in trajectory visualization one particle track is not visible ( I think it\u2019s in black color and my background is also black)</p>\n<p>How can I change the color or the tracked trajectories? or Can I make it all with a single color? (i.e. blue/green/etc. for all particles)</p>\n<p>attached two images for reference (trajectory of the particle in middle is not visible):</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5.png\" data-download-href=\"/uploads/short-url/vOYeNUP0hN6zzSY0gyjK3Xzx8Cp.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5_2_486x500.png\" alt=\"image\" data-base62-sha1=\"vOYeNUP0hN6zzSY0gyjK3Xzx8Cp\" width=\"486\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5_2_486x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5_2_729x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df066c7cbebb613815ab81add4e3cd423e1bcaf5.png 2x\" data-dominant-color=\"040404\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">800\u00d7822 23 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c0eaf055b92a599d07df318055949e187916314.png\" data-download-href=\"/uploads/short-url/d8nnZhIJeblFw71rKlHjjt0ZtVq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c0eaf055b92a599d07df318055949e187916314_2_465x500.png\" alt=\"image\" data-base62-sha1=\"d8nnZhIJeblFw71rKlHjjt0ZtVq\" width=\"465\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c0eaf055b92a599d07df318055949e187916314_2_465x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c0eaf055b92a599d07df318055949e187916314_2_697x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c0eaf055b92a599d07df318055949e187916314.png 2x\" data-dominant-color=\"060608\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">752\u00d7808 21.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"ravikumar\" data-post=\"1\" data-topic=\"78603\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/r/ed8c4c/40.png\" class=\"avatar\"> Ravi Kumar:</div>\n<blockquote>\n<p>How can I change the color or the tracked trajectories? or Can I make it all with a single color? (i.e. blue/green/etc. for all particles)</p>\n</blockquote>\n</aside>\n<p>Unfortunately it is not possible. Visualization of trajectories is just a helper tool and hence it is kept as simple as possible.</p>\n<p>Could you please check if this middle particle is a part of any trajectory?<br>\nMaybe you cannot see any trajectory since there is no one\u2026 You can just open \u201cAll Trajectories to Table\u201d and check if there is trajectory with coordinates of your  particle in the middle.</p>", "<p>Actually one more thing to be sure - colors of trajectories are generated randomly, so could you pleas track and visualize trajectories once again and see if it helps (or confirm that there are no trajectory detected for middle particle)?</p>", "<p>Yes, the trajectory of the middle particle is there for all the frames. When I used more frames, the trajectories overlapped, and I could see the middle particle was black, so it was not visible on the black background.</p>\n<p>I have tried doing it multiple times. black color trajectory is always there; just the particle changes.</p>", "<p>Could you please provide me with your input image/sequence and parameters that you are using? I cannot reproduce it on images I have.</p>"], "72461": ["<p>Hello all!  I\u2019m having an issue adding new videos to my project that I haven\u2019t found addressed anywhere online.</p>\n<p>I use the GUI (launched from Anaconda environment with admin privileges) and after loading my config file I use \u201cLoad Videos\u201d to select the videos I want, and then press \u201cAdd New Videos.\u201d  Each time I get this error, even if I\u2019ve gone through and deleted the replicates beforehand:</p>\n<p>SameFileError: \u2018D:\\Innovation Videos\\Camera 1\\11-17\\GH010624.MP4\u2019 and \u2018C:\\Users\\rmtra\\Documents\\1-Research\\Expl+Solve\\DLC\\PuzzleBox_TopView-Becca-2022-09-22\\videos\\GH010624.MP4\u2019 are the same file</p>\n<p>Further, the functioning .symlink files for my added videos appear in the \u201cvideos\u201d folder, and folders for each video appear in the \u201clabeled-data\u201d folder.  The video paths are not added to the config file, however.  I assume it is for this reason that any attempts to extract frames are unsuccessful.</p>\n<p>Is there a way to fix this?  Or do I need to manually add the video file paths?</p>\n<p>Any help is appreciated!</p>", "<p>Well the error is correct, you are trying to add the same file (it has exactly the same name). Is your config.yaml <code>video_sets</code> part empty?</p>", "<p>It has the original batch of videos I trained.  Notably, GH010624 is not on the video set list.  I also deleted that video from the \u201cvideos\u201d folder before retrying, and it still comes up with that error.</p>", "<p>Could you post the inside of your <code>config.yaml file?</code></p>", "<p>Sorry for the delay!  Here\u2019s the contents of the config file.</p>\n<pre><code># Project definitions (do not edit)\n</code></pre>\n<p>Task: PuzzleBox_TopView<br>\nscorer: Becca<br>\ndate: Sep22<br>\nmultianimalproject: false<br>\nidentity:</p>\n<pre><code># Project path (change when moving around)\n</code></pre>\n<p>project_path: C:\\Users\\rmtra\\Documents\\1-Research\\Expl+Solve\\DLC\\PuzzleBox_TopView-Becca-2022-09-22</p>\n<pre><code># Annotation data set configuration (and individual video cropping parameters)\n</code></pre>\n<p>video_sets:<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH010614.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH010615.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH010616.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH010617.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH010618.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH010619.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH020614.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH020615.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH020616.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH020617.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH020618.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH020619.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH030614.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH030615.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH030616.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH030617.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH030618.MP4:<br>\ncrop: 0, 1920, 0, 1080<br>\nD:\\Innovation Videos\\Camera 1\\1-6\\GH030619.MP4:<br>\ncrop: 0, 1920, 0, 1080</p>\n<p>bodyparts:</p>\n<ul>\n<li>\n<p>BeakTip</p>\n</li>\n<li>\n<p>BeakBridge</p>\n</li>\n<li>\n<p>LeftNare</p>\n</li>\n<li>\n<p>RightNare</p>\n</li>\n<li>\n<p>LeftGapeline</p>\n</li>\n<li>\n<p>RightGapeline</p>\n</li>\n<li>\n<p>Chin</p>\n</li>\n<li>\n<p>LeftEye</p>\n</li>\n<li>\n<p>RightEye</p>\n</li>\n<li>\n<p>LeftShoulder</p>\n</li>\n<li>\n<p>RightShoulder</p>\n</li>\n<li>\n<p>TailTip</p>\n</li>\n<li>\n<p>LeftAnklejoint</p>\n</li>\n<li>\n<p>RightAnklejoint</p>\n</li>\n<li>\n<p>Front</p>\n</li>\n<li>\n<p>Back</p>\n</li>\n<li>\n<p>Left</p>\n</li>\n<li>\n<p>Right</p>\n<h1>\n<a name=\"fraction-of-video-to-startstop-when-extracting-frames-for-labelingrefinement-1\" class=\"anchor\" href=\"#fraction-of-video-to-startstop-when-extracting-frames-for-labelingrefinement-1\"></a>Fraction of video to start/stop when extracting frames for labeling/refinement</h1>\n<h1>\n<a name=\"fraction-of-video-to-startstop-when-extracting-frames-for-labelingrefinement-2\" class=\"anchor\" href=\"#fraction-of-video-to-startstop-when-extracting-frames-for-labelingrefinement-2\"></a>Fraction of video to start/stop when extracting frames for labeling/refinement</h1>\n</li>\n</ul>\n<p>start: 0<br>\nstop: 1<br>\nnumframes2pick: 20</p>\n<pre><code># Plotting configuration\n</code></pre>\n<p>skeleton:</p>\n<ul>\n<li>\n<ul>\n<li>BeakBridge</li>\n<li>BeakTip</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>BeakTip</li>\n<li>Front</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>\n<p>BeakTip</p>\n</li>\n<li>\n<p>Back<br>\nskeleton_color: black<br>\npcutoff: 0.6<br>\ndotsize: 12<br>\nalphavalue: 0.7<br>\ncolormap: rainbow</p>\n<h1>\n<a name=\"trainingevaluation-and-analysis-configuration-3\" class=\"anchor\" href=\"#trainingevaluation-and-analysis-configuration-3\"></a>Training,Evaluation and Analysis configuration</h1>\n</li>\n</ul>\n</li>\n</ul>\n<p>TrainingFraction:</p>\n<ul>\n<li>\n<p>0.95<br>\niteration: 0<br>\ndefault_net_type: resnet_50<br>\ndefault_augmenter: default<br>\nsnapshotindex: -1<br>\nbatch_size: 8</p>\n<h1>\n<a name=\"cropping-parameters-for-analysis-and-outlier-frame-detection-4\" class=\"anchor\" href=\"#cropping-parameters-for-analysis-and-outlier-frame-detection-4\"></a>Cropping Parameters (for analysis and outlier frame detection)</h1>\n</li>\n</ul>\n<p>cropping: false<br>\n<a class=\"hashtag\" href=\"/tag/if\">#<span>if</span></a> cropping is true for analysis, then set the values here:<br>\nx1: 0<br>\nx2: 640<br>\ny1: 277<br>\ny2: 624</p>\n<pre><code># Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)\n</code></pre>\n<p>corner2move2:</p>\n<ul>\n<li>50</li>\n<li>50<br>\nmove2corner: true</li>\n</ul>", "<p>So if there is no folder in labeled-data for this video, no symlink/copied file in videos and it\u2019s not in the config this should not have happened. You can just add all of this manually and extraction should work then.</p>\n<p>Just for the sake of solving the issue though, if you could post the whole traceback of the error maybe it would help finding where exactly it\u2019s happening. Sorry I should\u2019ve asked for it earlier</p>", "<p>Here it is:</p>\n<h2>\n<a name=\"adding-new-videos-to-be-able-to-label-attempting-to-create-a-symbolic-link-of-the-video-the-syntax-of-the-command-is-incorrect-symlink-creation-impossible-exfat-architecture-cuttingpasting-the-video-instead-1\" class=\"anchor\" href=\"#adding-new-videos-to-be-able-to-label-attempting-to-create-a-symbolic-link-of-the-video-the-syntax-of-the-command-is-incorrect-symlink-creation-impossible-exfat-architecture-cuttingpasting-the-video-instead-1\"></a>adding new videos to be able to label \u2026<br>\nAttempting to create a symbolic link of the video \u2026<br>\nThe syntax of the command is incorrect.<br>\nSymlink creation impossible (exFat architecture?): cutting/pasting the video instead.</h2>\n<p>FileExistsError                           Traceback (most recent call last)<br>\nFile ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\create_project\\add.py:87, in add_new_videos(config, videos, copy_videos, coords, extract_frames)<br>\n86 dst = str(dst)<br>\n\u2014&gt; 87 os.symlink(src, dst)<br>\n88 print(\u201cCreated the symlink of {} to {}\u201d.format(src, dst))</p>\n<p>FileExistsError: [WinError 183] Cannot create a file when that file already exists: \u2018D:\\Innovation Videos\\Camera 1\\11-17\\GH010624.MP4\u2019 \u2192 \u2018C:\\Users\\rmtra\\Documents\\1-Research\\Expl+Solve\\DLC\\PuzzleBox_TopView-Becca-2022-09-22\\videos\\GH010624.MP4\u2019</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>CalledProcessError                        Traceback (most recent call last)<br>\nFile ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\create_project\\add.py:93, in add_new_videos(config, videos, copy_videos, coords, extract_frames)<br>\n91     import subprocess<br>\n\u2014&gt; 93     subprocess.check_call(\u201cmklink %s %s\u201d % (dst, src), shell=True)<br>\n94 except (OSError, subprocess.CalledProcessError):</p>\n<p>File ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\subprocess.py:364, in check_call(*popenargs, **kwargs)<br>\n363         cmd = popenargs[0]<br>\n \u2192 364     raise CalledProcessError(retcode, cmd)<br>\n365 return 0</p>\n<p>CalledProcessError: Command \u2018mklink C:\\Users\\rmtra\\Documents\\1-Research\\Expl+Solve\\DLC\\PuzzleBox_TopView-Becca-2022-09-22\\videos\\GH010624.MP4 D:\\Innovation Videos\\Camera 1\\11-17\\GH010624.MP4\u2019 returned non-zero exit status 1.</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>OSError                                   Traceback (most recent call last)<br>\nFile ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\shutil.py:791, in move(src, dst, copy_function)<br>\n790 try:<br>\n \u2192 791     os.rename(src, real_dst)<br>\n792 except OSError:</p>\n<p>OSError: [WinError 17] The system cannot move the file to a different disk drive: \u2018D:\\Innovation Videos\\Camera 1\\11-17\\GH010624.MP4\u2019 \u2192 \u2018C:\\Users\\rmtra\\Documents\\1-Research\\Expl+Solve\\DLC\\PuzzleBox_TopView-Becca-2022-09-22\\videos\\GH010624.MP4\u2019</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>SameFileError                             Traceback (most recent call last)<br>\nFile ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\create_new_project.py:440, in Create_new_project.add_videos(self, event)<br>\n438 if len(self.filelistnew) &gt; 0:<br>\n439     self.filelistnew = self.filelistnew + self.addvids<br>\n \u2192 440     add_new_videos(self.cfg, self.filelistnew)<br>\n441 else:<br>\n442     print(\u201cPlease select videos to add first. Click \u2018Load New Videos\u2019\u2026\u201d)</p>\n<p>File ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\create_project\\add.py:99, in add_new_videos(config, videos, copy_videos, coords, extract_frames)<br>\n94     except (OSError, subprocess.CalledProcessError):<br>\n95         print(<br>\n96             \u201cSymlink creation impossible (exFat architecture?): \"<br>\n97             \u201ccutting/pasting the video instead.\u201d<br>\n98         )<br>\n\u2014&gt; 99         shutil.move(os.fspath(src), os.fspath(dst))<br>\n100         print(\u201d{} moved to {}\".format(src, dst))<br>\n101 videos = destinations</p>\n<p>File ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\shutil.py:811, in move(src, dst, copy_function)<br>\n809         rmtree(src)<br>\n810     else:<br>\n \u2192 811         copy_function(src, real_dst)<br>\n812         os.unlink(src)<br>\n813 return real_dst</p>\n<p>File ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\shutil.py:435, in copy2(src, dst, follow_symlinks)<br>\n433 if os.path.isdir(dst):<br>\n434     dst = os.path.join(dst, os.path.basename(src))<br>\n \u2192 435 copyfile(src, dst, follow_symlinks=follow_symlinks)<br>\n436 copystat(src, dst, follow_symlinks=follow_symlinks)<br>\n437 return dst</p>\n<p>File ~\\anaconda3\\envs\\DEEPLABCUT\\lib\\shutil.py:244, in copyfile(src, dst, follow_symlinks)<br>\n241 sys.audit(\u201cshutil.copyfile\u201d, src, dst)<br>\n243 if _samefile(src, dst):<br>\n \u2192 244     raise SameFileError(\u201c{!r} and {!r} are the same file\u201d.format(src, dst))<br>\n246 file_size = 0<br>\n247 for i, fn in enumerate([src, dst]):</p>\n<p>SameFileError: \u2018D:\\Innovation Videos\\Camera 1\\11-17\\GH010624.MP4\u2019 and \u2018C:\\Users\\rmtra\\Documents\\1-Research\\Expl+Solve\\DLC\\PuzzleBox_TopView-Becca-2022-09-22\\videos\\GH010624.MP4\u2019 are the same file</p>", "<p>I wonder what would happen if you changed the name of the video file you\u2019re trying to add to the project. Symlink creation fail is also interesting. Is your D drive an SD card or a USB stick? This would explain it - not the same file error though.</p>", "<p>I\u2019ve still been messing with this.  Tried again from scratch, but am running into the same issue.  I think it might be because when extracting frames, I ran out of space on my computer?  I have space now, but could the interruption have messed with the process?</p>", "<p>Shouldn\u2019t matter when you restart the extraction. The paths not being added to the config are bugging me. Does the same thing happen if you try with <code>deeplabcut.add_new_videos</code> in ipython?</p>", "<p>I\u2019m having this same exact issue. Did you guys figure this out?</p>", "<p>In 2.2.3 adding new videos through the GUI doesn\u2019t work. You have to do it through ipython</p>"], "78606": ["<p><strong>University of Birmingham - Institute of Clinical Sciences, College of Medical and Dental Sciences</strong><br>\n<strong>Research Fellow in 3D Bioimaging of Vasculature</strong></p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.jobs.ac.uk/job/CYD110/research-fellow-in-3d-bioimaging-of-vasculature\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/662736cf8236ea9a6f73f45cf77c0cd85b341026.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://www.jobs.ac.uk/job/CYD110/research-fellow-in-3d-bioimaging-of-vasculature\" target=\"_blank\" rel=\"noopener nofollow ugc\">Jobs.ac.uk</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/313;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0_2_690x313.jpeg\" class=\"thumbnail\" width=\"690\" height=\"313\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0_2_690x313.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/9/49ffc24cdaac2ded5a33a2c52f080bc99ebb8df0.jpeg 2x\" data-dominant-color=\"AEC2DA\"></div>\n\n<h3><a href=\"https://www.jobs.ac.uk/job/CYD110/research-fellow-in-3d-bioimaging-of-vasculature\" target=\"_blank\" rel=\"noopener nofollow ugc\">Research Fellow in 3D Bioimaging of Vasculature at University of Birmingham</a></h3>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p><strong>Closing date 20th March 2023</strong><br>\nSalary \u00a333,348 to \u00a343,155 with potential progression once in post to \u00a345,737</p>\n<p>Research Fellow position available for Bioimaging, image analysis and machine learning PhD\u2019s interested to work on \u201cplacental of tissue and vasculature\u201d in a cross-disciplinary project involving University of Birmingham, University of Manchester, Rosalind Franklin Institute, Diamond Light Source, Swansea University and University of Auckland. The position is fixed term until Sept 2023 to image and analyse human placenta ex vivo. The position is suitable for a candidate with a PhD (or near to completion) in Bioimaging, Biomaterials or Date Science and experience relevant to research areas in tomography, image processing and analysis.</p>\n<p>For further details please contact Dr Gowsihan Poologasundarampillai (<a href=\"mailto:g.poologasundarampillai@bham.ac.uk\">g.poologasundarampillai@bham.ac.uk</a>).</p>"], "78608": ["<p>I am trying to use a pixel classifier I trained and apply it to selected smaller annotations on a slide that has other larger annotations and then run in batch. When I run the pixel classifier on a slide through the GUI with my annotations selected (ie load pixel classifier, region: any annotation ROI, measure: current selection) it seems to work fine and fairly quickly (it takes ~1 minute). However, I tried scripting it using the following script:</p>\n<pre><code class=\"lang-auto\">def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}\n\nfor (annotation in annotations) {\n\ngetCurrentHierarchy().getSelectionModel().setSelectedObjects(annotations, null)\n\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n}\n\n</code></pre>\n<p>It seems that this runs very slowly. Is there something wrong with my code leading to this? I\u2019m using version 0.4.2 and my memory is not maxing out. I found the following thread (<a href=\"https://forum.image.sc/t/qupath-measure-pixel-classifier-area-per-cell-detection-for-wsis/72701/7\" class=\"inline-onebox\">Qupath measure pixel classifier area per cell detection for WSIs - #7 by petebankhead</a>) but still a bit confused on how I should amend my script since I\u2019m not detecting cells.</p>\n<p>I did also try pre-requesting tiles using this script:</p>\n<pre><code class=\"lang-auto\">def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}\n\ngetCurrentHierarchy().getSelectionModel().setSelectedObjects(annotations, null)\n\ndef imageData = getCurrentImageData()\ndef classifier = loadPixelClassifier('PGM1')\ndef classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)\n\nclassifierServer.getTileRequestManager().getAllTileRequests()\n    .parallelStream()\n    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }\n    \n    addPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n</code></pre>\n<p>However, that didn\u2019t seem to help.</p>\n<p>In addition, how can I keep the pixel classification overlay visible after adding measurements? In other words, after I run the script I want to be able to QC the areas that were classified? I don\u2019t want to export the images but have everything stay in QuPath.</p>\n<p>Would I just add in something like the following to the script?</p>\n<pre><code class=\"lang-auto\">createAnnotationsFromPixelClassifier(\"PGM1\", 0.0, 0.0)\n\n</code></pre>\n<p>My images look like this (they are .svs files that are less than 1GB each) and I am only interested in getting information from the rectangular boxes:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg\" data-download-href=\"/uploads/short-url/zA4foiBfkVdcs62ejjYW44cbxsq.jpeg?dl=1\" title=\"Screen Shot 2023-03-15 at 2.41.35 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg\" alt=\"Screen Shot 2023-03-15 at 2.41.35 AM\" data-base62-sha1=\"zA4foiBfkVdcs62ejjYW44cbxsq\" width=\"646\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_969x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg 2x\" data-dominant-color=\"D9D6D9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-15 at 2.41.35 AM</span><span class=\"informations\">1270\u00d7982 113 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>I think this post is more related <a href=\"https://forum.image.sc/t/batch-processing-unusually-slow/49633/28\" class=\"inline-onebox\">Batch processing unusually slow? - #28 by petebankhead</a></p>\n<aside class=\"quote no-group\" data-username=\"ADW123\" data-post=\"1\" data-topic=\"78608\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png\" class=\"avatar\"> ADW123:</div>\n<blockquote>\n<p>In addition, how can I keep the pixel classification overlay visible after adding measurements?</p>\n</blockquote>\n</aside>\n<p>The measurements shouldn\u2019t turn your pixel classifier on or off. You can leave it on while generating measurements, or re-load it to see it.  If you want to script adding areas, you can, but that will take even longer.</p>", "<p>I did see that link as well. I haven\u2019t tried running in batch yet as I can\u2019t get things to run efficiently via script with one image. Even after troubleshooting, the image was still processing getting the measurements via script after 10 minutes.</p>", "<aside class=\"quote no-group\" data-username=\"ADW123\" data-post=\"1\" data-topic=\"78608\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png\" class=\"avatar\"> ADW123:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">def annotations = getAnnotationObjects().findAll() {it.getROI().getNumPoints() == 4}\n\nfor (annotation in annotations) {\n\ngetCurrentHierarchy().getSelectionModel().setSelectedObjects(annotations, null)\n\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n}\n</code></pre>\n</blockquote>\n</aside>\n<p>Is it just me or is it running slowly because you\u2019re running the pixel classifier on all your annotations each time you go through the loop.</p>\n<p>The whole point of selecting the objects is that you do not have to do it in a loop <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>you could also make it more explanable by using <code>selectObjectsByClassification()</code> instead of using <code>findAll()</code></p>\n<pre><code class=\"lang-auto\">selectObjectsByClassification(null) // or a string with the classification of your boxed if they have one\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n</code></pre>", "<p>Thanks <a class=\"mention\" href=\"/u/oburri\">@oburri</a>! I\u2019ve updated to the following code:</p>\n<pre><code class=\"lang-auto\">selectObjectsByClassification('ACST*','DC*','LCST*')\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n</code></pre>\n<p>However, that seems to also be quite time-intensive in contrast to the GUI.</p>\n<p>I then tried the following to no avail:</p>\n<pre><code class=\"lang-auto\">def tempRequest = RegionRequest.createInstance(getCurrentServer().getPath(), 1.0, 0, 0, 10, 10)\ndef tempImg = getCurrentServer().readBufferedImage(tempRequest)\n\ndef imageData = getCurrentImageData()\ndef classifier = loadPixelClassifier('PGM1')\ndef classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)\n\nclassifierServer.getTileRequestManager().getAllTileRequests()\n    .parallelStream()\n    .forEach { PixelClassificationImageServer.readRegion(it.getRegionRequest()) }\n\n</code></pre>\n<p>Not sure if I\u2019m going wrong somewhere still?</p>", "<aside class=\"quote no-group\" data-username=\"oburri\" data-post=\"4\" data-topic=\"78608\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/oburri/40/3464_2.png\" class=\"avatar\"> Olivier Burri:</div>\n<blockquote>\n<p>Is it just me or is it running slowly because you\u2019re running the pixel classifier on all your annotations each time you go through the loop.</p>\n</blockquote>\n</aside>\n<p>Well-spotted! I didn\u2019t notice this when I looked at the thread this morning\u2026</p>\n<p>Another two-line version of the script would be:</p>\n<pre><code class=\"lang-groovy\">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n</code></pre>\n<aside class=\"quote no-group\" data-username=\"ADW123\" data-post=\"1\" data-topic=\"78608\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png\" class=\"avatar\"> ADW123:</div>\n<blockquote>\n<p>I did also try pre-requesting tiles using this script:</p>\n</blockquote>\n</aside>\n<p>That will probably make it much slower, since it\u2019s requesting classifications for every pixel in the image \u2013 even outside the annotations.</p>\n<p>You\u2019d need something more selected. This might work (although I haven\u2019t properly tested it to check):</p>\n<pre><code class=\"lang-groovy\">def manager = classifierServer.getTileRequestManager()\ndef tileRequests = new HashSet&lt;&gt;()\nfor (def annotation in getSelectedObjects()) {\n    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())\n    tileRequests.addAll(manager.getTileRequests(request))\n}\ntileRequests.parallelStream()\n    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }\n    \n</code></pre>", "<p>Thanks, <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>! I have tried the more targeted script:</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\ndef classifier = loadPixelClassifier('PGM1')\ndef classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)\ndef manager = classifierServer.getTileRequestManager()\ndef tileRequests = new HashSet&lt;&gt;()\nfor (def annotation in getSelectedObjects()) {\n    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())\n    tileRequests.addAll(manager.getTileRequests(request))\n}\ntileRequests.parallelStream()\n    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }\n    \nselectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n</code></pre>\n<p>This still seems to have the same time-related concern as before (ie over 10 minutes on one slide). Given that the tissue itself has been annotated (image attached below) would the new script still be requesting classifications for every pixel in the image?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg\" data-download-href=\"/uploads/short-url/zA4foiBfkVdcs62ejjYW44cbxsq.jpeg?dl=1\" title=\"Screen Shot 2023-03-15 at 2.41.35 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg\" alt=\"Screen Shot 2023-03-15 at 2.41.35 AM\" data-base62-sha1=\"zA4foiBfkVdcs62ejjYW44cbxsq\" width=\"646\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_646x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e_2_969x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f95fd9a6fa10f286d7dc99d381d94c83a5fd7d4e.jpeg 2x\" data-dominant-color=\"D9D6D9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-15 at 2.41.35 AM</span><span class=\"informations\">1270\u00d7982 113 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I tried adjusting the script to the following but it didn\u2019t change anything:</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\ndef classifier = loadPixelClassifier('PGM1')\ndef classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)\ndef manager = classifierServer.getTileRequestManager()\ndef tileRequests = new HashSet&lt;&gt;()\nfor (def annotation in getSelectedObjects()) {\n    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI(){it.getROI().getNumPoints() == 4})\n    tileRequests.addAll(manager.getTileRequests(request))\n}\ntileRequests.parallelStream()\n    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }\n    \nselectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)\naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"ADW123\" data-post=\"7\" data-topic=\"78608\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png\" class=\"avatar\"> ADW123:</div>\n<blockquote>\n<p>Given that the tissue itself has been annotated (image attached below) would the new script still be requesting classifications for every pixel in the image?</p>\n</blockquote>\n</aside>\n<p>Hmmm, it <em>shouldn\u2019t</em> be.</p>\n<p>But you ought to put the <code>selectObjects</code> line up at the top of the script in your last post \u2013 since <code>getSelectedObjects()</code> is used later. So in that version, it\u2019s likely that the \u2018requesting tiles in parallel\u2019 isn\u2019t actually doing anything, and the performance is the same as if the 2-lines at the end were run only.</p>", "<p>Thanks, <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>!</p>\n<p>So I made the following changes:</p>\n<pre><code class=\"lang-auto\">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)\n\ndef imageData = getCurrentImageData()\ndef classifier = loadPixelClassifier('PGM1')\ndef classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)\ndef manager = classifierServer.getTileRequestManager()\ndef tileRequests = new HashSet&lt;&gt;()\nfor (def annotation in getSelectedObjects()) {\n    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())\n    tileRequests.addAll(manager.getTileRequests(request))\n}\ntileRequests.parallelStream()\n    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }\n    \naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n</code></pre>\n<p>It seems to run in half the time. I then ran the following which seems to run faster:</p>\n<pre><code class=\"lang-auto\">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)\n\ndef tempRequest = RegionRequest.createInstance(getCurrentServer().getPath(), 1.0, 0, 0, 10, 10)\ndef tempImg = getCurrentServer().readBufferedImage(tempRequest)\n\ndef imageData = getCurrentImageData()\ndef classifier = loadPixelClassifier('PGM1')\ndef classifierServer = PixelClassifierTools.createPixelClassificationServer(imageData, classifier)\ndef manager = classifierServer.getTileRequestManager()\ndef tileRequests = new HashSet&lt;&gt;()\nfor (def annotation in getSelectedObjects()) {\n    def request = RegionRequest.createInstance(classifierServer.getPath(), 1.0, annotation.getROI())\n    tileRequests.addAll(manager.getTileRequests(request))\n}\ntileRequests.parallelStream()\n    .forEach { classifierServer.readBufferedImage(it.getRegionRequest()) }\n    \naddPixelClassifierMeasurements(\"PGM1\", \"PGM1\")\n\n</code></pre>\n<p>Anything else you think I should adjust?</p>", "<p>Progress! Did you restart QuPath in between though, or  use <em>View \u2192 Show memory monitor</em> to clear the tile cache? If not, then the improved performance could simply be that the pixels are already available when you run the second script \u2013 so the slow bit of the task if already complete.</p>\n<p>I don\u2019t have any ideas right now how to improve things further. If you really want to dig into it, then <a href=\"https://visualvm.github.io\">VisualVM</a> with CPU sampling can help identify precisely where the slow bits are\u2026  which can be useful, but doesn\u2019t mean we\u2019ll necessarily be able to find a way to speed them up anyway.</p>\n<p>It seems we really need to look deeper into optimizing the performance within QuPath itself.</p>", "<aside class=\"quote no-group\" data-username=\"ADW123\" data-post=\"5\" data-topic=\"78608\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png\" class=\"avatar\"> ADW123:</div>\n<blockquote>\n<p><code>selectObjectsByClassification('ACST*','DC*','LCST*')</code></p>\n</blockquote>\n</aside>\n<p>Just to clear the confusion for me, these regions are classifications of your rectangles? If it\u2019s the classes of the larger annotations, then it makes sense again that it\u2019s slow. I see that you are using<br>\n<code>selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)</code> in the latest script, so I know that the point is somewhat moot, but I want to be sure.</p>", "<p>Yes, these are the rectangles <img src=\"https://emoji.discourse-cdn.com/twitter/smile.png?v=12\" title=\":smile:\" class=\"emoji\" alt=\":smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>, everything worked well for my first project running in batch! I then tried for another project and it seems the script stalls in batch mode. The second project has the same annotation hierarchy, image sizes, and number of images. I did notice that I included an image that had no annotations when I ran in batch mode and it stalled there as it looks like it tried to classify the whole slide. Is there a way for the script to skip these images? I know I can not include them in the batch run list but I don\u2019t have a working list of the images with no annotations at the moment.</p>", "<p>You can use something like this at the start:</p>\n<pre><code class=\"lang-groovy\">selectObjects(p -&gt; p.isAnnotation() &amp;&amp; p.getROI().getNumPoints() == 4)\nif (getSelectedObjects().isEmpty())\n    return;\n</code></pre>", "<p>Thank you! That works great!</p>"], "78612": ["<p>Hi everyone,</p>\n<p>Thank you in advance for any help. I have a quick question about relate objects and colocalization. I noticed today that, unlike other times I\u2019ve used relate objects for fos and another stain, I was getting a colocalization count higher than possible if it was 1-to-1. Does the order of the relate objects for parent and child matter such that the parent should always be the stain with the smaller number of objects? Or is there another step I can take to ensure there\u2019s only 1 fos object per stained cell? Thank you!!</p>\n<p>Best,<br>\nBrandon</p>"], "78620": ["<p>Dear all,</p>\n<p>I\u2019m happy to announce Fast4DReg, a new tool for correcting drift in 4D microscopy data. This tool was specifically built to remove lateral and axial drift in fluorescent 3D live cell imaging data. Fast4DReg does so by creating intensity projections of each time point and using cross-correlation to estimate the amount of drift. This estimated drift can be directly used to correct the drift in the same video, or it can correct drift in another video, for example, another channel. Using the intensity projections for drift estimation makes this tool especially fast. Additionally, Fast4DReg can correct misaligned channels, for example in the case where multiple cameras are used for multichannel image acquisition. The drift correction is performed the same way as for 3D videos, but prior to drift estimation, the channels are converted into time points (and back to the channel after correction).</p>\n<p>Availability:</p>\n<ul>\n<li>\n<p>Fast4DReg is available through Fiji update sites.</p>\n</li>\n<li>\n<p>In <a href=\"https://github.com/guijacquemet/Fast4DReg\" rel=\"noopener nofollow ugc\">Github</a> we provide detailed step-by-step instructions on how to use the tool.</p>\n</li>\n<li>\n<p>In <a href=\"https://zenodo.org/record/7514913#.ZAc-W3ZBwQ8\" rel=\"noopener nofollow ugc\">Zenodo</a>, we share all data used for testing the tool.</p>\n</li>\n<li>\n<p>The publication can be found here: <a href=\"https://doi.org/10.1242/jcs.260728\" rel=\"noopener nofollow ugc\">Journal of Cell Science: Fast4DReg \u2013 fast registration of 4D microscopy datasets</a>.</p>\n</li>\n<li>\n<p>Read more in my <a href=\"https://focalplane.biologists.com/2023/03/10/fast4dreg-to-the-rescue-of-your-drifty-microscopy-data/\" rel=\"noopener nofollow ugc\">Focal Plane Blog post.</a></p>\n</li>\n</ul>\n<p>Please don\u2019t hesitate to reach out if you need help getting started!</p>\n<p>Best regards,<br>\nJoanna Pylv\u00e4n\u00e4inen and <a class=\"mention\" href=\"/u/guillaume_jacquemet\">@Guillaume_Jacquemet</a><br>\nemail: <a href=\"mailto:joanna.pylvanainen@abo.fi\">joanna.pylvanainen@abo.fi</a><br>\nweb: <a href=\"https://cellmig.org/\" rel=\"noopener nofollow ugc\">https://cellmig.org/</a><br>\nTwitter: <a href=\"https://twitter.com/JwPylvanainen\" rel=\"noopener nofollow ugc\">https://twitter.com/JwPylvanainen</a></p>"], "51997": ["<p>Hi,</p>\n<p>I am trying to control a Nikon eclipse Ti2 microscope via micromanager- I have installed the suggested Nikon Ti2 control application and followed the instructions but when I try to add the Nikon Ti2 devices via micro-manager hardware configuration wizard the micromanager crashes. Does anyone have any suggestions? The version of micro manager I have is 2.0 gamma.</p>\n<p>Thanks!</p>", "<p>Hi,</p>\n<p>The TI2 device adapter currently requires that a specific older version of the TI2 SDK DLL file be copied to the Micro-Manager installation folder. Please check this for more information: <a href=\"https://micro-manager.org/wiki/NikonTi2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">NikonTi2 - Micro-Manager</a></p>\n<p>Hopefully this will be updated in the future, but for now just stick with the version recommended online.</p>\n<p>\u2013Nick</p>", "<p>Thanks for the quick reply! I have downloaded the version recommended (Ver. 1.2.0.55.) and copied this file to the Micro-Manager installation folder yet micro-manager still crashes whenever any Ti2 devices are installed. I may just try uninstalling and redownloading just in case :\u2019) but any other suggestions would be much appreciated.</p>", "<p>I uninstalled everything to do with the Nikon Ti2 and deleted it from micromanager and then reinstalled it and it seemed to work so thank you!! <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"></p>", "<aside class=\"quote no-group\" data-username=\"mdj97\" data-post=\"3\" data-topic=\"51997\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mdj97/40/43605_2.png\" class=\"avatar\"> mdj97:</div>\n<blockquote>\n<p>still crashes whenever any Ti2 devices are installed</p>\n</blockquote>\n</aside>\n<p>Hi mdj97,<br>\nWe have the same issue that MM2.0 still crashes whenever any Ti2 devices are installed.<br>\nHave you been able to solve this issue? How? Would yo have any recomendation, nightly build, etc?<br>\nWe would appreciate this very much!<br>\nMany thanks in advance.<br>\nGerm\u00e1n</p>", "<p>Which version of the Nikon Ti2 SDK DLL do you have installed?</p>", "<p>Hi Nico,<br>\nWe have version 1.2.0 installed.<br>\nBest,<br>\nGerm\u00e1n</p>", "<p>apologies in reviving this topic <a class=\"mention\" href=\"/u/nicost\">@nicost</a> <a class=\"mention\" href=\"/u/nanthony21\">@nanthony21</a> <a class=\"mention\" href=\"/u/mdj97\">@mdj97</a> <a class=\"mention\" href=\"/u/german.camargo\">@german.camargo</a></p>\n<p>I followed the advice for <a href=\"https://micro-manager.org/NikonTi2\" rel=\"noopener nofollow ugc\">NikonTi2 - Micromanager</a> with MM2nightly <a href=\"https://download.micro-manager.org/nightly/2.0/Windows/MMSetup_64bit_2.0.1_20230321.exe\" rel=\"noopener nofollow ugc\">MMSetup_64bit_2.0.1_20230321.exe</a> and Ti2 Control 2.70.101.</p>\n<p>However, when starting the device configuration wizard, it says NikonTi2 (not available)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a.png\" data-download-href=\"/uploads/short-url/eGhDw9nAIPW6HwkIzCnsWTdKesW.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_690x354.png\" alt=\"image\" data-base62-sha1=\"eGhDw9nAIPW6HwkIzCnsWTdKesW\" width=\"690\" height=\"354\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_690x354.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_1035x531.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/6/66e61370441e7de2b7e2b312bd07c236c357573a_2_1380x708.png 2x\" data-dominant-color=\"818080\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1567\u00d7804 51 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Would one of you maybe have a pointer for me how to continue?</p>\n<p>Thank you for your support!</p>"], "60189": ["<p>Hello! I have a set of thermal images that I would like to measure temperature from the pixel intensity of a manually selected ROI in ImageJ. Each image has a scale bar with the high and low temperature and the RGB color scale. Is it possible to use this scale to assign a temperature given a pixel intensity? Thanks!</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/0754f60d3c1ecfcdb036aea89ba6bff3ebdd91d3.jpeg\" alt=\"reds\" data-base62-sha1=\"12RmYWYsjePG1wSlwGyQLlQ0fhp\" width=\"640\" height=\"480\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/larsonjimmy\">@LarsonJimmy</a><br>\nWelcome to the forum !</p>\n<p>Ideally, your thermal image is a single channel grayscale image displayed with a color LUT.</p>\n<p>In your case with the color image at hand, mapping the pixel information to the heat value is more complicated.<br>\nThe mapping is in principle possible if the image contains a defined/limited number of intensities (i.e. a defined/limited number of colors).</p>\n<p>In your case the jpeg image contains 107739 different colors.<br>\nThus a definite mapping is not possible with your example image.</p>", "<p>Hi <a class=\"mention\" href=\"/u/larsonjimmy\">@LarsonJimmy</a> ,</p>\n<p>I agree with <a class=\"mention\" href=\"/u/phaub\">@phaub</a> that you should ideally use a single channel grayscale image for that.</p>\n<p>If you have access to the source, I believe those cameras must also provide grayscale images somehow. You should look for those raw images for quantification and avoid jpeg whenever possible.</p>", "<p>As the other ones here pointed out, it is better to retrieve the original greyscale image; the image you show here is such a greyscale image where a temperature LUT has been applied for visual display, so now the original values are lost.</p>\n<p>As an approximation to recreate a calibrated greyscale image from this image, you can try the below macro. It is not perfect but gives an approximation within a degree or two. Hover the cursor at any point to read the temperature in the ImageJ status bar.</p>\n<pre><code class=\"lang-auto\">run(\"Lab Stack\");\nsetSlice(1);\nrun(\"Duplicate...\", \"title=[Temperature Image]\");\nrun(\"8-bit\");\nrun(\"Specify...\", \"width=6 height=380 x=627 y=51 slice=1\");\nrun(\"Coordinates...\", \"left=627 right=633 top=36.8 bottom=23.1\");\nStack.setYUnit(\"temp\");\nsetKeyDown(\"alt\"); run(\"Plot Profile\");\nPlot.getValues(tempValue, greyValue);\nfor (i=0; i &lt; tempValue.length; i++)\n      tempValue[i] = 36.8 - tempValue[i];\nArray.show(greyValue, tempValue);\nxValues = String.join(greyValue, \"\\n\");\nyValues = String.join(tempValue, \"\\n\");\ncalibOptions = \"function=[Rodbard] unit=Temperature show \";\ncalibOptions += \"text1=[\" + xValues + \"]\";\ncalibOptions += \"text2=[\" + yValues + \"]\";\nselectWindow(\"Temperature Image\");\nrun(\"Calibrate...\", calibOptions);\nselectWindow(\"Temperature Image\");\nrun(\"Select None\");\nrun(\"Fire\");\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"steinr\" data-post=\"4\" data-topic=\"60189\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/aca169/40.png\" class=\"avatar\"> steinr:</div>\n<blockquote>\n<p>approximation to recreate a calibrated greyscale image</p>\n</blockquote>\n</aside>\n<p>Perfect !<br>\nReally nice solution. <img src=\"https://emoji.discourse-cdn.com/twitter/heart.png?v=10\" title=\":heart:\" class=\"emoji\" alt=\":heart:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/green_heart.png?v=10\" title=\":green_heart:\" class=\"emoji\" alt=\":green_heart:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/blue_heart.png?v=10\" title=\":blue_heart:\" class=\"emoji\" alt=\":blue_heart:\"> &gt; <img src=\"https://emoji.discourse-cdn.com/twitter/black_heart.png?v=10\" title=\":black_heart:\" class=\"emoji\" alt=\":black_heart:\"></p>", "<p>I was in a bit of a haste when I replied yesterday so I did not have time to write comments in the macro. Here is a version of the same macro, explaining how it works:</p>\n<pre><code class=\"lang-auto\">//convert the image to LAB color space\nrun(\"Lab Stack\");\nsetSlice(1);\n//extract the L image and convert to greyscale\nrun(\"Duplicate...\", \"title=[Temperature Image]\");\nrun(\"8-bit\");\n//create a ROI on the scalebar\nrun(\"Specify...\", \"width=6 height=380 x=627 y=51 slice=1\");\n//calibrate the image so that the Y direction is the temperature\nrun(\"Coordinates...\", \"left=627 right=633 top=36.8 bottom=23.1\");\nStack.setYUnit(\"temp\");\n//do a profile plot\nsetKeyDown(\"alt\"); run(\"Plot Profile\");\n//extract the plot values\nPlot.getValues(tempValue, greyValue);\n//the plot starts at 0 and increases downwards,\n//so add the max temperature and subtract the values\nfor (i=0; i &lt; tempValue.length; i++)\n      tempValue[i] = 36.8 - tempValue[i];\n//show the arrays, just for information\nArray.show(greyValue, tempValue);\n//merge the columns to a row of values\nxValues = String.join(greyValue, \"\\n\");\nyValues = String.join(tempValue, \"\\n\");\n//create a density calibration where X is the greyvalue and Y is the temperature\ncalibOptions = \"function=[Rodbard] unit=Temperature show \";\ncalibOptions += \"text1=[\" + xValues + \"]\";\ncalibOptions += \"text2=[\" + yValues + \"]\";\n//apply the calibration\nselectWindow(\"Temperature Image\");\nrun(\"Calibrate...\", calibOptions);\nselectWindow(\"Temperature Image\");\nrun(\"Select None\");\n//add a LUT similar to the existing LUT\nrun(\"Fire\");\n//optional: if we convert to 32-bit, the temperature vs greyscale \n//relationship will be linear; this looks better with a colored LUT\nrun(\"32-bit\");\n\n</code></pre>", "<p><a class=\"mention\" href=\"/u/steinr\">@steinr</a> When I try to run this code on a smaller image, compensating the values. I get a final temperature image with full black and full white values. what could be the issue here?</p>\n<p>Original image<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/4791aa1b6ce4f106f631b3aba235761afcd43c4c.jpeg\" alt=\"image\" data-base62-sha1=\"ad7Z4M1yidSPLbYUbuegTsJyl40\" width=\"412\" height=\"364\"></p>\n<p>B/W convert<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/b/9b5a611a7743f74b6fab55e6caee8bed2d441024.png\" alt=\"image\" data-base62-sha1=\"majDYdGv70rBQeqd4rkvNNQju5K\" width=\"412\" height=\"389\"></p>\n<p>Temperature image<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/89d23199b792c124af1092997989383f121e0608.png\" data-download-href=\"/uploads/short-url/jFdKAIA1LBI85RHsxWgGq5J6SE0.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/89d23199b792c124af1092997989383f121e0608_2_598x500.png\" alt=\"image\" data-base62-sha1=\"jFdKAIA1LBI85RHsxWgGq5J6SE0\" width=\"598\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/89d23199b792c124af1092997989383f121e0608_2_598x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/89d23199b792c124af1092997989383f121e0608.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/89d23199b792c124af1092997989383f121e0608.png 2x\" data-dominant-color=\"D3D3D3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">610\u00d7510 33.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Sorry for the late reply;</p>\n<p>The example macro I posted was meant to work with the \u201cFire\u201d LUT as used in the previously posted image; in which the L channel has a steady rise in value across the LUT. The image you posted has a different LUT, so you need a different type of color \u201cdecoding\u201d to get it to work.</p>\n<p>But I am not sure if it is worth the effort to try to analyze the image as is, as it is a JPEG image so the translation between colors and values are already lost in the compression. If you zoom in on the color bar there is no consistency in colors across the width of the bar, so any attempt on translating colors to physical values will not work well as you do not have any proper reference. Try to get hold of the original data instead.</p>\n<p>Stein</p>"], "78624": ["<p>I was wondering if cellprofiler offers Java APIs so I can use the particular module (i.e feature extraction). I found on the GitHub it is possible on Python but no info on Java.</p>\n<p>Thanks everyone in advance.</p>"], "70434": ["<p>Hello,</p>\n<p>When generating a video from the \u201cCreate videos\u201d tab of the GUI, I am returned with a video without labels identical to the original video. The strange thing is that the video for checking detections (created from \u201cAnalyze videos\u201d tab) looks fine and the CVS file is populated. I believe I found the underlying problem, but I don\u2019t know how to resolve it. When analyzing videos with 3 subjects, I can generate videos with animal ID colored. The problem arises when analyzing videos with 2 subjects, where I specify in the \u201cAnalyze videos\u201d tab that the number of animals to look for is 2. Interestingly, when I specify that the number of animals to look for is 3 in a video where there are only 2 animals, the video with ID colored is generated. Of course, the resulting video is of no use, since 3 identities are labeled in 2 animals. How could I resolve this?</p>\n<p>Here is some information on the network I trained:</p>\n<ul>\n<li>dlcrnetms5</li>\n<li>Multi-animal project\n<ul>\n<li>Three subjects max (rats)</li>\n</ul>\n</li>\n<li>Trained for 100k interactions</li>\n<li>Trained with over 1k labeled frames of 3 or 2 closely interacting</li>\n</ul>\n<p>I ran some tests and verified settings to eliminate possibilities:</p>\n<ul>\n<li>Reducing the pcutoff from default to 0.1 did not change the outcome</li>\n<li>Creating a video with animal ID colored or not colored did not change the outcome</li>\n<li>The tracker type (default nor transformer) did not change the outcome</li>\n<li>Dotsize was adequate (size=6)</li>\n</ul>", "<p>I found a \u201cfix\u201d for the second problem. In my config file, I specified that there are 3 individuals, but if I modify this to 2 individuals, the video is created. I know this solution is not optimal.</p>", "<p>I have an update on this issue. The name of individuals in the config file was originally changed to \u201cRat1\u201d,  \u201cRat2\u201d, and \u201cRat3\u201d from the default \u201cind1\u201d, \u201cind2\u201d and \u201cind3\u201d. When Analyzing videos with 2 subjects in a network trained for 3 individuals, the names of individuals are changed to the default \u201cind#\u201d labels. Therefore, individuals are reported in the CSV output file as \u201cind1\u201d and \u201cind2\u201d, instead of \u201cRat1\u201d and \u201cRat2\u201d. The video for checking labels is made, but not the Final video with the ID colors. When I change the cofig file by removing \u201cRat3\u201d, then the output CVS file rightly labels the individuals as \u201cRat1\u201d and \u201cRat2\u201d. Evenmore, the final video is made.</p>", "<p>I\u2019ll take a look at the code. Maybe the situation with <code>n_tracks</code> changing isn\u2019t handled well down the line when labeled video is being created.</p>"], "78627": ["<p>Hi everyone,</p>\n<p>I am relatively new to ImageJ and to coding in general, so I do not have a firm background in the inner workings of Java or FiJi. For a project I want to mark cells in a culture as either belonging to one group or another. I am using the ROI-group feature in FiJi to manually sort them into either group 0 or group 1. When I then try to save these ROIs either manually in the ROI manager or via the <em>rioManager(\u201csave\u201d)</em><br>\ncommand in a macro it sometimes happens that the command moves all the ROIs to group 1 before saving. When I then try it again after opening the macro in the editor, changing any line of code (not necessarily in the vicinity of the <em>save</em>-command like adding a <em>print</em>-statement) and then saving the macro, the command works as intended. What might be the source of this error and how can I prevent it in the future?</p>\n<p>Thanks in advance!</p>\n<p>Lucas</p>", "<p>I was able to resolve the problem myself. I turns out the <em>roiManager(\u201csave\u201d)</em> command automatically saves all ROIs in the Group selected as default except when the default group is 0.<br>\nSo I was able to circumvent the problem by adding <em>Roi.setDefaultGroup(0)</em> in the beginning of my macro. The previous success with adding a line of code was only an accident because to test the system I marked ROIs randomly to see if it works and by chance I ended with the default group = 0 after editing the macro.</p>"], "78628": ["<p>In DLC-live-ma I am tracking 18 body parts in <strong>10 animals</strong>, so the output from DLC is a numpy array of shape <strong>10</strong>, 18, 4.</p>\n<p>Then I feed a video with only <strong>5</strong> animals and all are detected. So the array\u2019s first dimension gets populated until position 5, and from 5 onward it\u2019s al <em>nan</em>.</p>\n<p>Every time DLC loses track of an animal for a few frames and then re-aquires it, it gives this animal a new ID. So the next position in the array gets populated.</p>\n<p>The problem is that after it populates the last position (<strong>10</strong>), it stops tracking any \u201cnew\u201d animals. Since every once in a while tracking of an animal is lost, in a few minutes the entire array is nan and no new detections are made.</p>\n<p>To get around that I set the number of animals as high as possible, such as 5000 animals. But even then, after a few minutes the entire array is used and I start getting all <em>nan</em>.</p>\n<p>Is there a setting to ignore animal ID, or how can I make DLC re-use animal IDs (for example if the animal has not been detected after 5 or 10 frames)?</p>", "<p>Since live multianimal isn\u2019t fully ready yet and it is possible this might be a bug, consider making an issue of this with steps to reproduce on the DLC-live github</p>"], "78629": ["<p>I update the old version (0.3.2) with the related version (v.0.4.3) by overwriting all files. Then this  main program can not be opened as following notes:</p>\n<blockquote>\n<p>Exception in thread \u201cmain\u201d java.lang.NoSuchMethodError: \u2018java.lang.String ch.qos.logback.core.util.EnvUtil.logbackVersion()\u2019<br>\nat ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:81)<br>\nat ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:77)<br>\nat ch.qos.logback.classic.spi.LogbackServiceProvider.initializeLoggerContext(LogbackServiceProvider.java:50)<br>\nat ch.qos.logback.classic.spi.LogbackServiceProvider.initialize(LogbackServiceProvider.java:41)<br>\nat org.slf4j.LoggerFactory.bind(LoggerFactory.java:167)<br>\nat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:154)<br>\nat org.slf4j.LoggerFactory.getProvider(LoggerFactory.java:437)<br>\nat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:423)<br>\nat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:372)<br>\nat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:398)<br>\nat qupath.QuPath.(QuPath.java:90)</p>\n</blockquote>\n<p><strong>Desktop (please complete the following information):</strong></p>\n<ul>\n<li>OS:  Ubuntu 20.04</li>\n<li>QuPath Version:  0.4.3</li>\n</ul>", "<p>Have you tried deleting and reinstalling? If a mixture of old and new files exist in the folder then I\u2019d expect things to fail.</p>\n<p>Although in Ubtunu there isn\u2019t really any installation involved \u2013 just extracting the compressed files, setting the permissions, and running: <a href=\"https://qupath.readthedocs.io/en/0.4/docs/intro/installation.html\" class=\"inline-onebox\">Installation \u2014 QuPath 0.4.3 documentation</a></p>", "<p>Thank you very much! It works!</p>"], "78626": ["<p>Hello everyone,</p>\n<p>I have trained a Classifier to detect objects in order to create Annotations.<br>\nThe next step would be to detect the Cells, while being able to tell in which Annotation they can be found.</p>\n<p>Problem: After I have run the cell detection, those Annotations have been deleted. Only the annotations, that I have made myself (without the Classifier) are left.</p>\n<p>What I have already tried:</p>\n<ul>\n<li>run the Cell detection before training the Classifier: when I create the Superpixels for the Classifier, the Cells get deleted and vice versa</li>\n<li>locking the Annotations</li>\n</ul>\n<p>Thank you so much in advance, I really appreciate your help.</p>\n<p>Regards,<br>\nAlexandra <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78631": ["<p>Dear all,</p>\n<p>I would like to get DeepLabCut (DLC) set up on Amazon Web Service EC2 for the training and analyzing part of the pipeline.  I would like to use one of their instances designed for deep learning. For those who already managed to run DLC using AWS could you please share with the community the <strong>AWS configuration you use? For instance, which Amazon Machine Image (AMI) you chose?</strong> It would be great if you share your experience or give any other recommendation here since there is not much information online about this.</p>\n<p>Thank you in advance.</p>\n<p>Best,</p>"], "78633": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg\" data-download-href=\"/uploads/short-url/hrsyEwW0fgLIWgVqLo2PLA1jWnl.jpeg?dl=1\" title=\"Angle between annotations\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg\" alt=\"Angle between annotations\" data-base62-sha1=\"hrsyEwW0fgLIWgVqLo2PLA1jWnl\" width=\"570\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 2x\" data-dominant-color=\"D7D4D5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Angle between annotations</span><span class=\"informations\">838\u00d7735 182 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>I am doing a study on invasive fungal infections in which I need to analyze the fungal elements within tissue samples. In the picture, it is a fungal infection in a human lung, stained with Grocott methamine silver.</p>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<p>One of the factors I want to evaluate is the angle with which the fungal hyphae branch (yellow on the picture). Is it possible to measure the angle between two linear annotations in QuPath?</p>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<p>I am a fairly new user of QuPath and are only familiar with the fundamentals. I have tried to look through previous questions in this forum as well as available tutorials, without any luck. Hopefully some of you out there can help me! <img src=\"https://emoji.discourse-cdn.com/twitter/pray.png?v=12\" title=\":pray:\" class=\"emoji\" alt=\":pray:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg\" data-download-href=\"/uploads/short-url/hrsyEwW0fgLIWgVqLo2PLA1jWnl.jpeg?dl=1\" title=\"Angle between annotations\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg\" alt=\"Angle between annotations\" data-base62-sha1=\"hrsyEwW0fgLIWgVqLo2PLA1jWnl\" width=\"570\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7_2_570x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/a/7a3f9eff21c00443ebbead0bf361b4e2580d08c7.jpeg 2x\" data-dominant-color=\"D7D4D5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Angle between annotations</span><span class=\"informations\">838\u00d7735 182 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>There are a decent number of posts for calculating Feret angles if you have an object that can be used for that function.</p>\n<p>Regardless, I think in your case you would need some way to pair the two lines (proximity of points?), and then it would be easiest to script the calculation of the angle directly from the XY coordinates of the points, assuming you had 3 points and knew which of the points was the connection.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/LineROI.html#getAllPoints()\">\n  <header class=\"source\">\n\n      <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/LineROI.html#getAllPoints()\" target=\"_blank\" rel=\"noopener\">qupath.github.io</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/LineROI.html#getAllPoints()\" target=\"_blank\" rel=\"noopener\">LineROI (QuPath 0.4.0)</a></h3>\n\n  <p>declaration: package: qupath.lib.roi, class: LineROI</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thank you for the reply <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>In the meantime I also found another way to do it. I found out that I could simply just create an annotation over an area with a high fungal load and export my ROI to ImageJ, in which there is a built-in angle-tool. I know, that the measurements are then not done in QuPath per se, but I works very smooth and easy for this particular problem. <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<ul>\n<li>Maybe others who have run into a similar problem could also benefit from this simplified and manual version of a solution <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">\n</li>\n</ul>"], "78635": ["<p>Hey,</p>\n<p>I am writing a short script to import a (log)file as OriginalFile.</p>\n<p>This seems to work (seeing the script logs).<br>\nBut the script does not allow the user to download even though this is described <a href=\"https://omero.readthedocs.io/en/stable/developers/scripts/style-guide.html#script-outputs\" rel=\"noopener nofollow ugc\">here</a> and generally works for <em>File_Annotations</em> (that are also described there).</p>\n<p>Script code snippet:</p>\n<pre><code class=\"lang-auto\">                mimetype = 'text/plain'\n                obj = client.upload(export_file, type=mimetype)\n                obj_id = obj.id.val\n                print(f\"Uploaded object {obj_id}/{type(obj)}:{obj}\")\n                \n                message += output_display_name\n                client.setOutput(\"Original_File\", robject(obj))  # Original_File\n</code></pre>\n<p>I don\u2019t have a particular file or dataset to attach this file to, so I\u2019m just using Original File only.</p>\n<p>How can I make this work so that you can download the file from response message?</p>\n<p>Thanks in advance,</p>\n<p>Torec</p>", "<p>I circumvent it by attaching it to a project now and returning the file annotation, that works fine.</p>\n<p>But that doesn\u2019t really answer my original question, and it requires the user providing an extra input (the project) giving more clutter to the screen =)</p>\n<p>Regards, Torec</p>", "<p>Hi,</p>\n<p>There\u2019s a URL that you can use to download original files from the webclient, based on ID:</p>\n<p><code>webclient/download_original_file/ID/</code>.</p>\n<p>If your script knows the URL where your webclient is deployed, you could generate that link and include that in the script response \u201cmessage\u201d.</p>\n<pre><code class=\"lang-auto\">from omero.rtypes import wrap\n\n...\n        orig_file_id = 123\n        webclient = \"https://my-server/webclient/\"\n        url = f\"{webclient}download_original_file/{orig_file_id}/\"\n        client.setOutput(\"URL\", wrap({\"type\": \"URL\", \"href\": url}))\n        client.setOutput(\"Message\", wrap(\"Click the button to download\"))\n</code></pre>\n<p>This will generate a button to open the URL (and download the file) in a new window:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/c/fc3ef3fce60f815965f5e9299e82e4b83d69147d.png\" alt=\"Screenshot 2023-03-17 at 12.32.30\" data-base62-sha1=\"zZtckzrR6BQKMsuZK0Tgl7k0t1H\" width=\"406\" height=\"102\"></p>", "<p>Oh thanks <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>, that\u2019s a pretty neat solution.</p>"], "78636": ["<p><a class=\"attachment\" href=\"/uploads/short-url/lhe9ef8xSjl6wdR35ie39GurX9E.cpproj\">David_roh.cpproj</a> (702.3 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/amZgRzf971WPs5qtgdIMgVGP2uS.cpproj\">gro\u00df2_AlexaFluor150.cpproj</a> (702.3 KB)</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f.jpeg\" data-download-href=\"/uploads/short-url/znCbf4xVCsE36LLE3pnFx0L70UT.jpeg?dl=1\" title=\"David_roh_automatisch_belichtet\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_667x500.jpeg\" alt=\"David_roh_automatisch_belichtet\" data-base62-sha1=\"znCbf4xVCsE36LLE3pnFx0L70UT\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_1334x1000.jpeg 2x\" data-dominant-color=\"1A1525\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">David_roh_automatisch_belichtet</span><span class=\"informations\">1388\u00d71040 56.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4.jpeg\" data-download-href=\"/uploads/short-url/akcuJVgazKVFhpnNT6dCwAkmUF6.jpeg?dl=1\" title=\"David_roh\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_667x500.jpeg\" alt=\"David_roh\" data-base62-sha1=\"akcuJVgazKVFhpnNT6dCwAkmUF6\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_1334x1000.jpeg 2x\" data-dominant-color=\"1A0D2E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">David_roh</span><span class=\"informations\">1388\u00d71040 56.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd.jpeg\" data-download-href=\"/uploads/short-url/1yHp9zI2ePDOmT4OJUF33FIuA9L.jpeg?dl=1\" title=\"gro\u00df_automatischeBelichtung\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_667x500.jpeg\" alt=\"gro\u00df_automatischeBelichtung\" data-base62-sha1=\"1yHp9zI2ePDOmT4OJUF33FIuA9L\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_1334x1000.jpeg 2x\" data-dominant-color=\"170B1F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">gro\u00df_automatischeBelichtung</span><span class=\"informations\">1388\u00d71040 53.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8.jpeg\" data-download-href=\"/uploads/short-url/tObNxTSMUFtZnW9rdiVeq3mU9VS.jpeg?dl=1\" title=\"gro\u00df2_AlexaFluor150\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_667x500.jpeg\" alt=\"gro\u00df2_AlexaFluor150\" data-base62-sha1=\"tObNxTSMUFtZnW9rdiVeq3mU9VS\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_1334x1000.jpeg 2x\" data-dominant-color=\"32181F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">gro\u00df2_AlexaFluor150</span><span class=\"informations\">1388\u00d71040 84.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017.jpeg\" data-download-href=\"/uploads/short-url/sQC0JhGxwgcvfVu4zJpwyeno7PN.jpeg?dl=1\" title=\"Test5_AlexaFluor370\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_667x500.jpeg\" alt=\"Test5_AlexaFluor370\" data-base62-sha1=\"sQC0JhGxwgcvfVu4zJpwyeno7PN\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_1334x1000.jpeg 2x\" data-dominant-color=\"3B1D48\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test5_AlexaFluor370</span><span class=\"informations\">1388\u00d71040 118 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hello everybody,<br>\nI am trying to determine the cell boundaries on the images in the attachment. Unfortunately, my results are not accurate enough so far. Does anyone of you maybe know a way how I can best determine the cell borders?<br>\nI have attached two pipelines of mine. Maybe one of you has an idea how I should improve them for more accurate results. Furthermore, I have attached some more sample images so that you know how the images look like with which a pipeline should work.</p>\n<p>I would really appreciate your help.<br>\nWith kind regards,<br>\nJustine</p>", "<p>Here I have four more pictures so you can get an impression.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad.jpeg\" data-download-href=\"/uploads/short-url/f6pqeiHv10NkXTbnJf3GcFd7rEp.jpeg?dl=1\" title=\"David_roh_2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_667x500.jpeg\" alt=\"David_roh_2\" data-base62-sha1=\"f6pqeiHv10NkXTbnJf3GcFd7rEp\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_1334x1000.jpeg 2x\" data-dominant-color=\"3E1F29\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">David_roh_2</span><span class=\"informations\">1388\u00d71040 101 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd.jpeg\" data-download-href=\"/uploads/short-url/2PF6lBAfqnzvNgheiq68i8SUuhn.jpeg?dl=1\" title=\"Test4_automatischeBelichtung\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_667x500.jpeg\" alt=\"Test4_automatischeBelichtung\" data-base62-sha1=\"2PF6lBAfqnzvNgheiq68i8SUuhn\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_1334x1000.jpeg 2x\" data-dominant-color=\"1E0F26\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test4_automatischeBelichtung</span><span class=\"informations\">1388\u00d71040 72.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e.jpeg\" data-download-href=\"/uploads/short-url/cwc2CemlbbZzwOa5ku77OlLnHNc.jpeg?dl=1\" title=\"Test3_AlexaFluor250\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_667x500.jpeg\" alt=\"Test3_AlexaFluor250\" data-base62-sha1=\"cwc2CemlbbZzwOa5ku77OlLnHNc\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_1334x1000.jpeg 2x\" data-dominant-color=\"28141F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test3_AlexaFluor250</span><span class=\"informations\">1388\u00d71040 87.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428.jpeg\" data-download-href=\"/uploads/short-url/l3ffxwNBcvPkRyQa592OUQ0wpKw.jpeg?dl=1\" title=\"gro\u00df2_automatischeBelichtung\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_667x500.jpeg\" alt=\"gro\u00df2_automatischeBelichtung\" data-base62-sha1=\"l3ffxwNBcvPkRyQa592OUQ0wpKw\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_1334x1000.jpeg 2x\" data-dominant-color=\"160B20\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">gro\u00df2_automatischeBelichtung</span><span class=\"informations\">1388\u00d71040 48.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Not familiar with cellprofiler, but if it has edge detection, running that in the membrane channel might generate a good binary image. Possibly through ImageJ integration if nothing else.</p>", "<p>An other option would be to install Cellpose plugin for Cellprofiler.<br>\nHowever that plugin doesn\u2019t work with the standalone Cellprofiler. You would need to install Cellprofiler from github repository.<br>\n(Check here: <a href=\"https://forum.image.sc/t/new-cellprofiler-4-plugin-runcellpose/56858\" class=\"inline-onebox\">New CellProfiler 4 Plugin: RunCellpose</a>)</p>\n<p>If all your images are like those you shared here <img src=\"https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12\" title=\":star_struck:\" class=\"emoji\" alt=\":star_struck:\" loading=\"lazy\" width=\"20\" height=\"20\"> It should give very very accurate segmentation.</p>"], "78649": ["<p>I have uninstalled and reinstalled fiji (imagej) twice, ensured imagej and the bio-formats plug-in are up to date, and I am still unable to open mvd2 files (obtained using volocity). I verified in the bio-formats configuration menu that volocity file extensions were enabled. I am getting the following message:</p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 138MB of 24287MB (&lt;1%)</p>\n<p>java.lang.IllegalStateException: Too early in import process: current step is FILE, but must be after STACK<br>\nat loci.plugins.in.ImportProcess.assertStep(ImportProcess.java:778)<br>\nat loci.plugins.in.ImportProcess.getReader(ImportProcess.java:306)<br>\nat loci.plugins.in.Importer.run(Importer.java:99)<br>\nat loci.plugins.LociImporter.run(LociImporter.java:78)<br>\nat ij.IJ.runUserPlugIn(IJ.java:237)<br>\nat ij.IJ.runPlugIn(IJ.java:203)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.lang.Thread.run(Thread.java:750)</p>", "<p>Hi <a class=\"mention\" href=\"/u/khinton\">@Khinton</a>, do you have a link to a sample file that I can test? If you need a suitable upload location then we recommend <a href=\"https://zenodo.org/\">Zenodo</a></p>"], "78650": ["<p>Hi,</p>\n<p>I stained sections with picrosirius red and took images with a polarised light microscope. I\u2019m trying to use qupath to differentiate between red, green and yellow. I tried it by setting the image type to brightfieqld and then using colour deconvolution to separate the three. However, when I try to use them as a threshold nothing is selected, no matter what values I use. They also don\u2019t appear in brightness and contrast menu so I\u2019m not sure if I applied it correctly. I would use a normal threshold using RGB but since yellow is red+green that won\u2019t work. Does anyone have experience analysing PSR with qupath, this is my first time using this stain.</p>\n<p>thanks in advance!</p>\n<p>The command I\u2019ve been using</p>\n<p>setColorDeconvolutionStains(\u2018{\u201cName\u201d : \u201cCollagen Types\u201d, \u201cStain 1\u201d : \u201cRed Collagen\u201d, \u201cValues 1\u201d : \u201c0.06515 0.65628 0.66028\u201d, \u201cStain 2\u201d : \u201cYellow Collagen\u201d, \u201cValues 2\u201d : \u201c0.35717 0.56026 0.74735\u201d, \u201cStain 3\u201d : \u201cGreen Collagen\u201d, \u201cValues 3\u201d : \u201c0.47394 0.55093 0.68692\u201d, \u201cBackground\u201d : \" 255 255 255\"}\u2019);</p>\n<p>Example image</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71272c07e9c64f341bbbde6edbc71669413ff8bd.jpeg\" data-download-href=\"/uploads/short-url/g8ZSxQUBFWe2FO81dQAQ9NXTzWd.jpeg?dl=1\" title=\"Screenshot 2023-03-15 at 15.09.59\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71272c07e9c64f341bbbde6edbc71669413ff8bd.jpeg\" alt=\"Screenshot 2023-03-15 at 15.09.59\" data-base62-sha1=\"g8ZSxQUBFWe2FO81dQAQ9NXTzWd\" width=\"531\" height=\"500\" data-dominant-color=\"080D07\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-15 at 15.09.59</span><span class=\"informations\">778\u00d7732 30.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Using brightfield methods on a black background image will likely be problematic, I suspect you will want to treat the image as a fluorescence image. That lets you perform thresholding on red, on green, and then checking overlap for yellow (unfortunately).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888.jpeg\" data-download-href=\"/uploads/short-url/n3oZ33agrif0yyV9l0DfAGbhtb2.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_455x375.jpeg\" alt=\"image\" data-base62-sha1=\"n3oZ33agrif0yyV9l0DfAGbhtb2\" width=\"455\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_455x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_682x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a1947a48042496dc45a636ee6f8175e25d68c888_2_910x750.jpeg 2x\" data-dominant-color=\"302D2A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1276\u00d71050 70 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f.jpeg\" data-download-href=\"/uploads/short-url/gnf4NobtI0gEkOl0vSDVM4Z1Fld.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_471x375.jpeg\" alt=\"image\" data-base62-sha1=\"gnf4NobtI0gEkOl0vSDVM4Z1Fld\" width=\"471\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_471x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_706x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72c366c73135de03519bb7695ca73f4f24cda18f_2_942x750.jpeg 2x\" data-dominant-color=\"2A2E29\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1407\u00d71118 80 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Also useful for this <a href=\"https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579\" class=\"inline-onebox\">Script for generating double threshold classifier</a></p>"], "78651": ["<p>dear brainreg gurus,<br>\nping <a class=\"mention\" href=\"/u/adamltyson\">@adamltyson</a> <a class=\"mention\" href=\"/u/alessandrofelder\">@alessandrofelder</a> <a class=\"mention\" href=\"/u/paddyroddy\">@paddyroddy</a></p>\n<p>we have acquired a full mouse brain on a zeiss lightsheet 7 (iDisco clearing) but struggle with the alignment to the Allen atlas, see command and screenshots of the output below.</p>\n<p><code>brainreg ./C0_ds ./brainreg_out10 --atlas allen_mouse_25um --orientation sal -v 17.5011 3.7279 3.7279 --save-original-orientation</code></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5.png\" data-download-href=\"/uploads/short-url/6rvUYHmXMsAr5JKmcDfg2KkqYjX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5_2_690x434.png\" alt=\"image\" data-base62-sha1=\"6rvUYHmXMsAr5JKmcDfg2KkqYjX\" width=\"690\" height=\"434\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5_2_690x434.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5_2_1035x651.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2d29469652720ba0d5903eb3beea1e915848a3f5.png 2x\" data-dominant-color=\"555759\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1098\u00d7691 158 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8.png\" data-download-href=\"/uploads/short-url/vnRa9SAF2V5vPTc8E2kPqdENZvG.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8_2_690x430.png\" alt=\"image\" data-base62-sha1=\"vnRa9SAF2V5vPTc8E2kPqdENZvG\" width=\"690\" height=\"430\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8_2_690x430.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8_2_1035x645.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/dbf5c9aa2fbc07e17759d10b28c714fa53e0fef8.png 2x\" data-dominant-color=\"535556\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1102\u00d7687 160 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I also tried reducing the <code>--bending-energy-weight </code> but the result did not improve much.</p>\n<p>Would you have a pointer for me on how the registration could be improved?<br>\nI have uploaded the (downsampled) raw data as well as some of my trials in case you\u2019d like to have a look or give it a trial:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://drive.switch.ch/index.php/s/k2CqpRT6FKR88Zp\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/c/2cbb91e9d80628fed03b013cee5d9fe41a5aea94.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://drive.switch.ch/index.php/s/k2CqpRT6FKR88Zp\" target=\"_blank\" rel=\"noopener nofollow ugc\">SWITCHdrive</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/0/30b920a598e0f67f5231643db766096701cb7c7e.png\" class=\"thumbnail onebox-avatar\" width=\"194\" height=\"194\">\n\n<h3><a href=\"https://drive.switch.ch/index.php/s/k2CqpRT6FKR88Zp\" target=\"_blank\" rel=\"noopener nofollow ugc\">SWITCHdrive -</a></h3>\n\n  <p>brainreg is publicly shared</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>If not, is my raw data maybe not of sufficient quality? If so, what should be better in the ideal case?</p>\n<p>Thank you for your help!</p>", "<p>Do you only have that one channel? brainreg was designed to work with autofluorescence channels with no labelling. I think there are two things limiting the registration:</p>\n<ul>\n<li>Bright labelling in the channel used for registration</li>\n<li>Limited contrast between brain structures</li>\n</ul>", "<p>Hi <a class=\"mention\" href=\"/u/adamltyson\">@adamltyson</a> ,</p>\n<p>thanks for your reply! I indeed have only this one channel.</p>\n<p>We used iDisco clearing with ECi, and in general the autofluorescence in the sample was very weak. While thats usually a good thing, I think thats likely the issue in this case.</p>\n<p>It sound like an odd question, but how would you recommend to clear a whole mouse brain sample so that enough autofluorescence remains?</p>", "<p>Hi <a class=\"mention\" href=\"/u/cellkai\">@CellKai</a>,</p>\n<blockquote>\n<p>It sound like an odd question, but how would you recommend to clear a whole mouse brain sample so that enough autofluorescence remains?</p>\n</blockquote>\n<p>No idea actually. most of the clearing I\u2019ve done has been with CLARITY, in which I always found lots of autofluroescence.</p>\n<p>I forgot one pretty important suggestion though - you could try another BrainGlobe atlas. The original Allen mouse atlases have a serial two-photon reference image that doesn\u2019t look very similar to iDISCO brains. There are two others you could use that have an iDISCO lightsheet reference image:</p>\n<ul>\n<li>\n<code>perens_lsfm_mouse_20um</code> - This is a 20um resolution atlas from <a href=\"https://link.springer.com/article/10.1007/s12021-020-09490-8\">Perens et al. 2020</a>. It has the same annotations as the Allen, but with an iDISCO reference image. The downside is that it\u2019s in a different coordinate space as the Allen, due to clearing-induced tissue deformation. This should improve registration, but it will mean that any features in your images would need to be transformed to the Allen CCF if you wanted to analyse them alongside other data.</li>\n<li>\n<code>kim_dev_mouse_idisco_10um</code> - This is an as-yet <a href=\"https://data.mendeley.com/datasets/2svx788ddf/1\">upublished 10um resolution atlas from the Kim lab</a>. It has an iDISCO template, but in the same coordinate space as the Allen. It has different annotations as the Allen, but because it\u2019s in the same coordinate space, converting between the two shouldn\u2019t be hard.</li>\n</ul>\n<p>We haven\u2019t tested these atlases much for iDISCO registration, but theoretically they should perform better than the original Allen mouse atlas.</p>"], "78653": ["<p>Hi all, I am launching a new online course for bioimage analysis scientist about pathology called \u201cPathology 101 for tissue image analysis\u201d. This is not the place to promote products, so just wanted to let you know about the launch and if you would like to learn more, comment below or send me a message, and I let you know more about it <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78654": ["<p>Hey there,<br>\nwe have two issues usng the acquisition engine in pycromanager:</p>\n<ol>\n<li>When we take a multidimensional acquisition (MDA) using Acquisition.acquire(events) we get a resource warning from python (ResourceWarning: unclosed file &lt;_io.BufferedReader name=\u2018C:\\temp\\scan_5\\scan_NDTiffStack.tif\u2019&gt;). The acquisition works fine, however we are wondering if that is a big issue or if we can ignore the warning. Especially, if this slows down our code?!</li>\n<li>Gerneral question: If we initialise xyz-events for a z-scan, the events give us a the correct z position (eg. 0.25um). However this seems not to be stored in the metadata (eg. when opening in ImageJ), it always shows 1um as z-step. Do we have to write this separately to the metadata or are we doing something wrong with intialising the xyz-events. Similar issue, if we initialise t-events it is still saved as a z-stack with an 1um z-step.</li>\n</ol>\n<p>Any help is greatly appreciated.</p>\n<p><em>ev = multi_d_acquisition_events(xyz_positions=xyz)<br>\nwith Acquisition(directory=save_directory, name=save_name, pre_hardware_hook_fn=preHardwareFn,<br>\npost_camera_hook_fn=cameraHookFn, show_display=False, saving_queue_size=5000) as acq:<br>\nacq.acquire(ev)</em></p>", "<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"1\" data-topic=\"78654\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>When we take a multidimensional acquisition (MDA) using Acquisition.acquire(events) we get a resource warning from python (ResourceWarning: unclosed file &lt;_io.BufferedReader name=\u2018C:\\temp\\scan_5\\scan_NDTiffStack.tif\u2019&gt;). The acquisition works fine, however we are wondering if that is a big issue or if we can ignore the warning. Especially, if this slows down our code?!</p>\n</blockquote>\n</aside>\n<p>Can you post the full output? I wouldn\u2019t think there\u2019s any reason that this is slowing things down. Do you see otherwise?</p>\n<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"1\" data-topic=\"78654\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>Gerneral question: If we initialise xyz-events for a z-scan, the events give us a the correct z position (eg. 0.25um). However this seems not to be stored in the metadata (eg. when opening in ImageJ), it always shows 1um as z-step. Do we have to write this separately to the metadata or are we doing something wrong with intialising the xyz-events. Similar issue, if we initialise t-events it is still saved as a z-stack with an 1um z-step.</p>\n</blockquote>\n</aside>\n<p>ImageJ or FIJI? ImageJ has its own weird way of storing this metadata, which the default format of pycromanager does not write. The metadata is there\u2013you can access it through python, for example</p>", "<p>Well, it kind of is the full output. For our original script the full output lines are:</p>\n<p><em>1. start of acq<br>\n2. end of acq<br>\n3. Acquisition finished!<br>\n4. D:\\WorkStuff\\labA3Marc\\marcPy\\softwareHelpers\\uManagerInt.py:98: ResourceWarning: unclosed file &lt;_io.BufferedReader name=\u2018C:\\temp\\scan_30\\scan_NDTiffStack.tif\u2019&gt;<br>\n5. uManagerMDA(prog)<br>\n6. ResourceWarning: Enable tracemalloc to get the object allocation traceback</em></p>\n<p>Unfortunately, I could not reproduce the issue in a minimal example. It only appears in our original script. However the different output lines give a hint about the origin.<br>\n<em>start of acq</em> and <em>end of acq</em> is around the acquisition statement (see origninal script below). <em>Acquisition finished!</em> is printed out when the thread, where the acquisition runs in, stops (I tired it without the thread and the warning still apears). Any idea what causes this or how I can approach the issue?</p>\n<p>original script:</p>\n<pre><code># some stuff before this\nprint(\"start of acq\")\nif mode == 'uManager' or notZstack():\n    with Acquisition(directory=save_directory, name=save_name, pre_hardware_hook_fn=preHardwareFn,\n                     show_display=False, saving_queue_size=5000) as acq:\n        acq.acquire(events)\nelif mode == 'uManagerAO':\n    setAOTask(ni.setUpSingleChAO(\"Dev1/ao0\"))  # initialise task (AO output)\n    with Acquisition(directory=save_directory, name=save_name, pre_hardware_hook_fn=preHardwareFn,\n                     post_camera_hook_fn=cameraHookFn, show_display=False, saving_queue_size=5000) as acq:\n        acq.acquire(events)               # do acquisition\n    ni.setSingleValueChAO(getAOTask(), 0)\n    ni.closeTask(taskAO)\nelse:\n    print(\"no such uManager moder registered\")\nprint(\"end of acq\")\n# some stuff after this\n</code></pre>", "<p>It sounds like you\u2019re having some problem with a file pointer to the acquired data remaining open. Hard to say why this is without more information and/or the ability to reproduce. It could be related to the underlying hard drive somehow. It\u2019s quite possible that no real problems will result from this.</p>\n<p>You could try doing as suggested and put at the top of your code:</p>\n<pre><code class=\"lang-auto\">import tracemalloc\ntracemalloc.start()\n</code></pre>", "<p>Below the additional statement. Does that make any sens to you?</p>\n<p><em>Object allocated at (most recent call last):<br>\nFile \u201cD:\\WorkStuff\\labA3Marc\\marcPy\\venv\\Lib\\site-packages\\ndtiff\\nd_tiff_current.py\u201d, lineno 56<br>\nself.file = open(tiff_path, \u201crb\u201d)</em></p>\n<p>Could I save it as something different than an NDTIFF (how would I do this)?</p>"], "78656": ["<p>Hi <a href=\"https://forum.image.sc/groups/team\">@team</a>!</p>\n<p>We would like to join as a community partner with BioImage Model Zoo: <a href=\"https://bioimage.io/#/\" rel=\"noopener nofollow ugc\">bioimage.io</a>. Regarding the requirements:</p>\n<ul>\n<li>The source code of <a href=\"http://bioimage.io\" rel=\"noopener nofollow ugc\">bioimage.io</a> platform is released under MIT license. Available here: <a href=\"https://github.com/bioimage-io/bioimage.io/blob/8a1bd04b86da51d24eadb80f8c71dc14e8f80aca/LICENSE\" rel=\"noopener nofollow ugc\">bioimage.io MIT License</a>\n</li>\n<li>We already reference the forum as primary point of contact in our <a href=\"https://github.com/bioimage-io/bioimage.io/blob/8a1bd04b86da51d24eadb80f8c71dc14e8f80aca/README.md\" rel=\"noopener nofollow ugc\">readme.md in github</a> and in the about page of our website.</li>\n<li>I could serve as community representative.</li>\n<li>And here\u2019s our logo (transparent background, so should work for both light and dark themes):</li>\n</ul>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/1/c18b6788250f7b1a8273a7676902b52e4ec70efa.svg\" alt=\"bioimage-io-icon\" data-base62-sha1=\"rCaP6XAcln76LHRccH5M5PU61QK\" width=\"500\" height=\"500\"></p>\n<p>Thanks a lot!</p>"], "78657": ["<p>Dera QuPath developper,</p>\n<p>Is there a way to load a json file containing the coordinate of spots for spatial transcriptiomics (10x Visum) in QuPath. It seems than TMA is close to spatial trans in term of format even if I don\u2019t know if spatial trans spots have similar shape ?<br>\nAn evolution of the software in this direction could be really interesting\u2026Thanks,</p>", "<p>Have you tried <a href=\"https://forum.image.sc/t/qupath-and-visium-json/59908/7\" class=\"inline-onebox\">QuPath and Visium JSON - #7 by petebankhead</a></p>", "<p>Here\u2019s a video in which I discuss some of the limitations of using QuPath (at least in it\u2019s current state) for analyzing Visium-processed samples:</p><div class=\"onebox lazyYT lazyYT-container\" data-youtube-id=\"o7jv4jUhK_8\" data-youtube-title=\"Discussion of Spatial Transcriptomics in QuPath\" data-parameters=\"feature=oembed&amp;wmode=opaque\">\n  <a href=\"https://www.youtube.com/watch?v=o7jv4jUhK_8\" target=\"_blank\" rel=\"noopener\">\n    <img class=\"ytp-thumbnail-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/098469283fe74e71b45f3101119cb070407b9ae3.jpeg\" title=\"Discussion of Spatial Transcriptomics in QuPath\" width=\"480\" height=\"360\">\n  </a>\n</div>\n<p>\nIs there a specific analysis you intend to conduct on these sections? There are several other packages that offer extensive support such as <a href=\"https://squidpy.readthedocs.io/en/stable/\">Squidpy</a> if you have Python experience, or <a href=\"https://themilolab.github.io/SPATA2/\">SPATA2</a> if you feel more comfortable in R.</p>", "<p>Great thanks very much !</p>"], "78658": ["<p>Hi,</p>\n<p>So I am using a script in python that I run headlessly where I call the plugin Linear Stack Alignment with SIFT. Here is my code.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/rwbhoLdtqNSPYjUIPrnyidSYixh.txt\">script.txt</a> (471 Bytes)<br>\nWhat is strange is that after reading the documentation when I do IJ.run the imp variable should be override and contain the registered stack. However, after careful examination the saved file is exactly as my source file. I confirm that the paths are correct, that the stack is well read and I believe that the plugin is working as the prompt is showing for each slice that features are extracted with SIFT (exactly as if I was using Fiji from the interface).</p>\n<p>Does anyone have an idea why is this happening?</p>\n<p>Thank you.</p>", "<p>Hi <a class=\"mention\" href=\"/u/mzugravu\">@mzugravu</a>,</p>\n<p>Let me first put your code here so that it is easier for readers to follow:</p>\n<pre><code class=\"lang-auto\">def align_stack(source_file, output_file):\n\n    imp = IJ.openImage(source_file)\n    IJ.run(imp, \"Linear Stack Alignment with SIFT\", \"initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate\")\n    IJ.save(imp, output_file)\n</code></pre>\n<p>The problem is, that the plugin will not modify the input image, but create a separate output image. That is why imp is unmodified. You can just get the active image with <code> IJ.getImage()</code> after the SIFT and save that one.</p>\n<p>Also be careful the plugin is not a plugin-filter, it takes the active image not the imp you pass. When I use the macro recorder I get</p>\n<pre><code class=\"lang-auto\">IJ.run(\"Linear Stack Alignment with SIFT\", \"initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 expected_transformation=Rigid interpolate\");\n</code></pre>\n<p>Best,<br>\nVolker</p>", "<p>Hi <a class=\"mention\" href=\"/u/volker\">@volker</a>,</p>\n<p>Thank you for replying.</p>\n<p>I\u2019ve tried to add this line as you suggested <code>output = IJ.getImage()</code> after I run the plugin but I get this  <em>There are no images open</em>.</p>\n<p>I am not sure I get the part where you say the plugin is not a plugin-filter\u2026 I believe I need to open the image like that and then pass it to the plugin because if I don\u2019t do it I get again <em>There are no images open</em>.</p>", "<p>Hi <a class=\"mention\" href=\"/u/mzugravu\">@mzugravu</a>,<br>\nyes, it should be like that, opening the image without displaying and passing it to the plugin. However in this case it\u2019s not possible, because this plugin does not accept an image as input, it grabs the active image from ImageJ. Therefore you need to display the image, so that the plugin can get it. To do this either replace <code>openImage</code> with <code>open</code> or call <code>show</code> on the image after opening it.<br>\nFor example:</p>\n<pre><code class=\"lang-auto\">from ij import IJ\ndef align_stack(source_file, output_file):\n    inputImage = IJ.openImage(source_file)\n    inputImage.show()\n    IJ.run(\"Linear Stack Alignment with SIFT\", \"initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate\")\n    image = IJ.getImage()\n    IJ.save(image, output_file)\n    inputImage.close()\n    image.close()\n    \nalign_stack(\"/home/baecker/in.tif\", \"/home/baecker/out.tif\")\n</code></pre>\n<p>Best,<br>\nVolker</p>", "<p>Thank you so much. Now it works. Actually I don\u2019t think I need to display it, as long if <code>IJ.open</code> does not display it. Even before when using <code>IJ.openImage</code> it did run the SIFT. I just simply failed to retrieve the output\u2026 I could not run <code>inputImage.show()</code> as I am running headlessly. I run my script on a cluster so no GUI available.</p>\n<p>This is my script updated.</p>\n<pre><code class=\"lang-auto\">def align_stack(source_file, output_file):\n    input_image = IJ.open(source_file)\n    IJ.run(input_image,\"Linear Stack Alignment with SIFT\", \"initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate\")\n    image = IJ.getImage()\n    IJ.save(image, output_file)\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/mzugravu\">@mzugravu</a> ,<br>\nhappy that it works for you.</p>\n<p>You don\u2019t actually need to display it, but it has to become the active image, which you can also achieve when displaying it in batch mode.</p>\n<p>The <code>IJ.open</code> does not return anything (input_image will be None) and the SIFT plugin does not take an image as parameter. It works because the <code>open</code>opens and displays the image which becomes the active image that is used by the SIFT plugin.<br>\nSo to avoid confusion, you should better write:</p>\n<pre><code class=\"lang-auto\">def align_stack(source_file, output_file):\n    IJ.open(source_file)\n    IJ.run(\"Linear Stack Alignment with SIFT\", \"initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=4 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 filter maximal_alignment_error=25 minimal_inlier_ratio=0.05 expected_transformation=Rigid output=interpolate\")\n    image = IJ.getImage()\n    IJ.save(image, output_file)\n</code></pre>\n<p>Best,<br>\nVolker</p>"], "78659": ["<p>Dear Community,<br>\nI ran into a problem when handling shapes in Napari.<br>\nHere\u2019s an examplary code snippet to reproduce the error:</p>\n<pre><code class=\"lang-auto\">import napari\nimport numpy as np\n\nfrom skimage.data import astronaut\n\n# set up viewer\nviewer = napari.Viewer()\n# add an image\nviewer.add_image(astronaut())\n# add a shapes layer\nellipse = np.array([[59, 222], [110, 289], [170, 243], [119, 176]])\nviewer.add_shapes(ellipse,\n                  shape_type='ellipse',\n                  edge_width=5,\n                  edge_color='coral',\n                  face_color='purple',\n                  )\n# run viewer\nnapari.run()\n</code></pre>\n<p>The problem arises when I select the ellipse shape and try to rotate it (by clicking on the extra vertice that appears a bit farther out from the displayed bounding box). This seems to crash Napari and results in the following error message:</p>\n<pre><code class=\"lang-auto\">TypeError                                 Traceback (most recent call last)\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\vispy\\app\\backends\\_qt.py:532, in QtBaseCanvasBackend.mouseMoveEvent(self=&lt;vispy.app.backends._qt.CanvasBackendDesktop object&gt;, ev=&lt;PyQt5.QtGui.QMouseEvent object&gt;)\n    530 if self._vispy_canvas is None:\n    531     return\n--&gt; 532 self._vispy_mouse_move(\n        self = &lt;vispy.app.backends._qt.CanvasBackendDesktop object at 0x000001D96CE223A0&gt;\n        ev = &lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt;\n    533     native=ev,\n    534     pos=_get_event_xy(ev),\n    535     modifiers=self._modifiers(ev),\n    536 )\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\vispy\\app\\base.py:216, in BaseCanvasBackend._vispy_mouse_move(self=&lt;vispy.app.backends._qt.CanvasBackendDesktop object&gt;, **kwargs={'button': 1, 'buttons': [1], 'last_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_mouse_press': None, 'modifiers': (), 'native': &lt;PyQt5.QtGui.QMouseEvent object&gt;, 'pos': (693, 194), 'press_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;})\n    213 else:\n    214     kwargs['button'] = self._vispy_mouse_data['press_event'].button\n--&gt; 216 ev = self._vispy_canvas.events.mouse_move(**kwargs)\n        self = &lt;vispy.app.backends._qt.CanvasBackendDesktop object at 0x000001D96CE223A0&gt;\n        self._vispy_canvas.events.mouse_move = &lt;vispy.util.event.EventEmitter object at 0x000001D96CE23550&gt;\n        kwargs = {'native': &lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt;, 'pos': (693, 194), 'modifiers': (), 'buttons': [1], 'press_event': &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=False last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[694 194] press_event=None source=None sources=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_event': &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=False last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[694 194] press_event=None source=None sources=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_mouse_press': None, 'button': 1}\n        self._vispy_canvas.events = &lt;vispy.util.event.EmitterGroup object at 0x000001D96CE23400&gt;\n        self._vispy_canvas = &lt;VispyCanvas (PyQt5) at 0x1d971739130&gt;\n    217 self._vispy_mouse_data['last_event'] = ev\n    218 return ev\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\vispy\\util\\event.py:453, in EventEmitter.__call__(self=&lt;vispy.util.event.EventEmitter object&gt;, *args=(), **kwargs={'button': 1, 'buttons': [1], 'last_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;, 'last_mouse_press': None, 'modifiers': (), 'native': &lt;PyQt5.QtGui.QMouseEvent object&gt;, 'pos': (693, 194), 'press_event': &lt;MouseEvent blocked=False button=1 buttons=[1] d...rces=[] time=1678893048.6828024 type=mouse_press&gt;})\n    450 if self._emitting &gt; 1:\n    451     raise RuntimeError('EventEmitter loop detected!')\n--&gt; 453 self._invoke_callback(cb, event)\n        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;\n        self = &lt;vispy.util.event.EventEmitter object at 0x000001D96CE23550&gt;\n        cb = &lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;\n    454 if event.blocked:\n    455     break\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\vispy\\util\\event.py:471, in EventEmitter._invoke_callback(self=&lt;vispy.util.event.EventEmitter object&gt;, cb=&lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object&gt;&gt;, event=&lt;MouseEvent blocked=False button=1 buttons=[1] d...urces=[] time=1678893048.7535458 type=mouse_move&gt;)\n    469     cb(event)\n    470 except Exception:\n--&gt; 471     _handle_exception(self.ignore_callback_errors,\n        self = &lt;vispy.util.event.EventEmitter object at 0x000001D96CE23550&gt;\n        cb = &lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;\n        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;\n        (cb, event) = (&lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;, &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;)\n    472                       self.print_callback_errors,\n    473                       self, cb_event=(cb, event))\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\vispy\\util\\event.py:469, in EventEmitter._invoke_callback(self=&lt;vispy.util.event.EventEmitter object&gt;, cb=&lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object&gt;&gt;, event=&lt;MouseEvent blocked=False button=1 buttons=[1] d...urces=[] time=1678893048.7535458 type=mouse_move&gt;)\n    467 def _invoke_callback(self, cb, event):\n    468     try:\n--&gt; 469         cb(event)\n        cb = &lt;bound method QtViewer.on_mouse_move of &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;&gt;\n        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;\n    470     except Exception:\n    471         _handle_exception(self.ignore_callback_errors,\n    472                           self.print_callback_errors,\n    473                           self, cb_event=(cb, event))\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\napari\\_qt\\qt_viewer.py:1077, in QtViewer.on_mouse_move(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, event=&lt;MouseEvent blocked=False button=1 buttons=[1] d...urces=[] time=1678893048.7535458 type=mouse_move&gt;)\n   1069 def on_mouse_move(self, event):\n   1070     \"\"\"Called whenever mouse moves over canvas.\n   1071 \n   1072     Parameters\n   (...)\n   1075         The vispy event that triggered this method.\n   1076     \"\"\"\n-&gt; 1077     self._process_mouse_event(mouse_move_callbacks, event)\n        event = &lt;MouseEvent blocked=False button=1 buttons=[1] delta=[0. 0.] handled=False is_dragging=True last_event=MouseEvent modifiers=() native=&lt;PyQt5.QtGui.QMouseEvent object at 0x000001D97E660040&gt; pos=[693 194] press_event=MouseEvent source=None sources=[] time=1678893048.7535458 type=mouse_move&gt;\n        self = &lt;napari._qt.qt_viewer.QtViewer object at 0x000001D971723C10&gt;\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\napari\\_qt\\qt_viewer.py:1026, in QtViewer._process_mouse_event(self=&lt;napari._qt.qt_viewer.QtViewer object&gt;, mouse_callbacks=&lt;function mouse_move_callbacks&gt;, event=&lt;ReadOnlyWrapper at 0x000001D97E64A5C0 for MouseEvent&gt;)\n   1024 layer = self.viewer.layers.selection.active\n   1025 if layer is not None:\n-&gt; 1026     mouse_callbacks(layer, event)\n        event = &lt;ReadOnlyWrapper at 0x000001D97E64A5C0 for MouseEvent at 0x000001D97B124AC0&gt;\n        layer = &lt;Shapes layer 'ellipse' at 0x1d9730a4460&gt;\n        mouse_callbacks = &lt;function mouse_move_callbacks at 0x000001D96E445280&gt;\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\napari\\utils\\interactions.py:172, in mouse_move_callbacks(obj=&lt;Shapes layer 'ellipse'&gt;, event=&lt;ReadOnlyWrapper at 0x000001D97E64A5C0 for MouseEvent&gt;)\n    169 obj._persisted_mouse_event[gen].__wrapped__ = event\n    170 try:\n    171     # try to advance the generator\n--&gt; 172     next(gen)\n        gen = &lt;generator object select at 0x000001D97E3BD5F0&gt;\n    173 except StopIteration:\n    174     # If done deleted the generator and stored event\n    175     del obj._mouse_drag_gen[func]\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\napari\\layers\\shapes\\_shapes_mouse_bindings.py:63, in select(layer=&lt;Shapes layer 'ellipse'&gt;, event=&lt;ReadOnlyWrapper at 0x000001D97E651080 for ReadOnlyWrapper&gt;)\n     61     _drag_selection_box(layer, coordinates)\n     62 else:\n---&gt; 63     _move(layer, coordinates)\n        layer = &lt;Shapes layer 'ellipse' at 0x1d9730a4460&gt;\n        coordinates = (82.56905945524973, 190.80641155985776)\n     65 # if a shape is being moved, update the thumbnail\n     66 if layer._is_moving:\n\nFile ~\\MiniConda\\envs\\napari-env\\lib\\site-packages\\napari\\layers\\shapes\\_shapes_mouse_bindings.py:492, in _move(layer=&lt;Shapes layer 'ellipse'&gt;, coordinates=(82.56905945524973, 190.80641155985776))\n    487     offset = handle - layer._fixed_vertex\n    488     layer._drag_start = -np.degrees(\n    489         np.arctan2(offset[0], -offset[1])\n    490     )\n--&gt; 492 new_offset = coord - layer._fixed_vertex\n        coord = [82.56905945524973, 190.80641155985776]\n        layer = &lt;Shapes layer 'ellipse' at 0x1d9730a4460&gt;\n        layer._fixed_vertex = None\n    493 new_angle = -np.degrees(np.arctan2(new_offset[0], -new_offset[1]))\n    494 fixed_offset = handle - layer._fixed_vertex\n\nTypeError: unsupported operand type(s) for -: 'list' and 'NoneType'\n</code></pre>\n<p>Meanwhile, moving or resizing shapes works just fine.</p>\n<p>Here some additional information on my Napari installation:</p>\n<pre><code class=\"lang-auto\">\n(base) C:\\&gt;conda activate napari-env\n\n(napari-env) C:\\&gt;napari --info\nnapari: 0.4.17\nPlatform: Windows-10-10.0.19045-SP0\nPython: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]\nQt: 5.15.2\nPyQt5: 5.15.7\nNumPy: 1.24.1\nSciPy: 1.9.3\nDask: 2022.12.1\nVisPy: 0.11.0\nmagicgui: 0.6.1\nsuperqt: 0.4.0\nin-n-out: 0.1.6\napp-model: 0.1.1\nnpe2: 0.6.1\n\nOpenGL:\n  - GL version:  4.6.0 NVIDIA 411.31\n  - MAX_TEXTURE_SIZE: 16384\n\nScreens:\n  - screen 1: resolution 1920x1200, scale 1.0\n  - screen 2: resolution 1920x1200, scale 1.0\n\nPlugins:\n  - Partial-Aligner: 0.0.1 (2 contributions)\n  - napari: 0.4.17 (77 contributions)\n  - napari-console: 0.0.6 (0 contributions)\n  - napari-crop: 0.1.7 (2 contributions)\n  - napari-sift-registration: 0.1.2 (2 contributions)\n  - napari-svg: 0.1.6 (2 contributions)\n  - napari-tools-menu: 0.1.19 (0 contributions)\n\n(napari-env) C:\\&gt;\n</code></pre>\n<p>Does anybody have an idea what could cause this behavior and how to fix it?</p>\n<p>Best,<br>\nAndreas</p>", "<p>Hi <a class=\"mention\" href=\"/u/andreas.m.arnold\">@andreas.m.arnold</a>, this bug was recently fixed in <a href=\"http://github.com/napari/napari/pull/5449\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Bugfix: Ensure layer._fixed_vertex is set when rotating by psobolewskiPhD \u00b7 Pull Request #5449 \u00b7 napari/napari \u00b7 GitHub</a>; unfortunately, it\u2019s not yet released.</p>\n<p>I don\u2019t know if there\u2019s a workaround other than using the dev version. <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a> ?</p>", "<p>Um, it\u2019s not a big code change so could be patched locally? but that seems less than ideal.<br>\nIt is a pretty annoying bug though, which all the more emphasizes that we need a release if only to get some of the bugfixes in the wild.</p>\n<p>Regarding running dev version, we do have nightlies on conda, so this works pretty well without requiring anything special:<br>\n<code>conda create -n napari-main -c \"napari/label/nightly\" -c conda-forge napari python=3.10</code><br>\n(I use mamba, but whatever)<br>\nOther option is to install via pip from GitHub:<br>\n<code>python -m pip install \"git+https://github.com/napari/napari.git#egg=napari[all]\"</code></p>"], "78660": ["<p>Hi all,</p>\n<p>OMERO newbie here. I\u2019m currently trying to access an OMERO server through my Ubuntu terminal. I can log in through a web browser, but when I try the steps described here: <a href=\"https://omero.readthedocs.io/en/stable/users/cli/installation.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Installation \u2014 OMERO documentation</a></p>\n<p>I get as far as the \u201comero login\u201d command. When I put in the server address and login details, I get the following:</p>\n<p>WARNING:omero.client:\u2026Ignoring error in client.<strong>del</strong>:&lt;class \u2018Ice.ConnectFailedException\u2019&gt;<br>\nInternalException: Failed to connect: Ice.ConnectFailedException:<br>\nNo route to host</p>\n<p>This is in a miniconda environment, where I have installed omero-py. It\u2019s likely that I may have missed an obvious step somewhere, but I\u2019m completely new to OMERO and also don\u2019t have a lot of experience with Linux systems.</p>\n<p>Thanks in advance for any help!</p>\n<p>Tim</p>", "<p>Hi Tim,</p>\n<p>\u201cNo route to host\u201d means the client can\u2019t connect to the server. I suspect you\u2019re using the wrong hostname / ip address. It might be different from the hostname / url of the webclient. Best to ask the admin of your omero server to give you the hostname or ip address of the OMERO server. If you\u2019re sure that you have the correct one, you should also check that you can actually access it (i.e. not blocked by a firewall or in a different network, etc.). Does a <code>ping hostname</code> work?</p>\n<p>Kind Regards,<br>\nDominik</p>", "<p>Hi Dominik,</p>\n<p>Thanks for your reply. I was using a correct hostname, but it was the hostname of the web server. Just tried with the back end hostname and that worked. I had assumed logins, imports and such like had to be done through the web server.</p>\n<p>Thanks,</p>\n<p>Tim</p>"], "78661": ["<p>Hello all,</p>\n<p>First to say I\u2019m new to programming i.e python etc so please forgive me and please correct me if I use the wrong terminology!</p>\n<p>Secondly, Cellpose is brilliant, so thank you to the creators.</p>\n<p>I have a question about how I could potentially utilise the Cellpose cell segmentation with multi class instance segmentation.</p>\n<p>I have human tissue FFPE sections that are H&amp;E stained, I\u2019ve uploaded an example of what the tissue looks like, and I am trying to segment cells and then assign a class to each cell \u201cinstance\u201d e.g. lymphocyte, plasma cell, tumour cell etc.</p>\n<p>I\u2019ve partly trained a Cellpose custom model,  which works fairly well on my H&amp;E images and currently I have been using the Cellpose GUI to get an idea of its potential. From my understanding Cellpose isn\u2019t built for H&amp;E images, however this has worked well so far.</p>\n<p>Is there a way that I can also use Cellpose cell segmentation and multi class prediction? Is there a way of scripting in python to then predict cell type?<br>\nWould I need to create another deep learning model to then predict cell type? or can this be integrated into the Cellpose model?</p>\n<p>Depending on how this is possible will depend on how I create my masks. One method I could think of was including a class dictionary which contains a label for each cells instance ID.</p>\n<p>Thank you in advance for your help, if any further information is required please let me know <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>FYI I had considered using Stardist but not all my cells may be star convex, tumour cells can have all sorts of shapes and are atypical by nature so I wanted to see if I could avoid running into this problem.</p>\n<p>Furthermore, I have already created python script using unet and Tensorflow to perform semantic segmentation of tumour and stroma. So I have some basic (very) understanding. Although I know that Cellpose uses PyTorch.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/unPmejI5AuBJQDsWaRfQu6NXFxn.tif\">TCGA-BA-4074-01Z-00-DX1 [x=71168,y=11776,w=512,h=512].tif</a> (768.2 KB)</p>"], "78662": ["<p>Hello,<br>\nI would like to record an animation/video of my reconstruction of the neuron using FIJI, i tried to use Macro plugin but it gave me errors. Any help on the procedure to do so?</p>", "<p><a class=\"mention\" href=\"/u/zahraa_sweidan\">@Zahraa_Sweidan</a><br>\nWhat type of data do you have, what method are you using to process the data and what errors did you receive?</p>", "<p>Hi christian,<br>\nI have an lsm file and i did a reconstruction of it using SNT. I\u2019d like to record a video of my reconstruction on the neuron. I used the macro plugin, and then record. But i get confused about the steps i should do inorder to generate a video at the end.</p>", "<p>You likely don\u2019t want to record a macro for this. The macro recording function is to make it easier to write scripts to repeat things you have done before, or batch-process many files, for example.</p>\n<p>Is your data an image stack? What type of video do you want?</p>\n<p>If it is a stack, and you want a video to just go through the slices, all you have to do is save it as and .avi file (With \u201cSave as\u2026\u201d).</p>\n<p>If you\u2019d like to show it as a 3D object, and do a virtual rotation around it, the 3D viewer can do that very easily. Just open it in Plugins &gt; 3D Viewer,  and go to View &gt; Record 360 deg rotation.</p>", "<p>Hi <a class=\"mention\" href=\"/u/zahraa_sweidan\">@Zahraa_Sweidan</a> ,</p>\n<p>There are several ways you could do this, but if you want a 3D animation you could use the reconstruction viewer (<em>3D tab &gt; Open Reconstruction Viewer</em>). Double-clicking will toggle a rotation animation. In the utilities menu (wrench icon), there is a <em>Record rotation</em> option which will save the frames of one 360 degree rotation to the snapshot directory, which can be changed in the <em>Global preferences\u2026</em> of the gear menu. See <a href=\"https://imagej.net/plugins/snt/reconstruction-viewer\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">SNT \u203a Reconstruction Viewer</a></p>\n<p>The frames can be rendered as an .mp4 with ffmpeg, e.g.,<br>\n<code>ffmpeg -framerate 30 -i %5d.png video.mp4</code></p>\n<p></p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/540a9967b51f7a64c9cddde4ff20c3d5be6d47a6.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/540a9967b51f7a64c9cddde4ff20c3d5be6d47a6.mp4\" rel=\"noopener nofollow ugc\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/540a9967b51f7a64c9cddde4ff20c3d5be6d47a6.mp4</a>\n    </source></video>\n  </div><p></p>\n<p>There are many options for customizing the rendering of trees and meshes, including morphometric color-mapping, thickness, transparency. The viewer is also completely scriptable, see <em>Scripts &gt; Demos &gt; Reconstruction Viewer Demo</em> for an example.</p>\n<p>For a 2D animation, you could use the Fill Manager to reconstruct the fluorescent volume around the skeleton, then export the segmentation as a grayscale/binary mask and save the stack as an .avi or view it in the 3D viewer, as per <a class=\"mention\" href=\"/u/jbehnsen\">@jbehnsen</a> 's suggestion.</p>"], "78663": ["<p>Hi,</p>\n<p>I am working with imaging mass cytometry data and see variability in my nuclear staining that I would like to minimize. I want to split the multi.tiff into it\u00b4s 25 channels, edit the DNA channel, and then save the image as H5 to import into ilastik. However, when I try splitting the channels with \u201cColorToGray\u201d - Channels, and then specifiy the channels, CP complains that the input image isn\u00b4t in color (which is fair). What are alternative ways to split these channels and edit only one of them?</p>\n<p>Thanks!</p>"], "78665": ["<p>We are labeling spiders. We\u2019ve labeled a few neural networks, but one thing that we\u2019ve always run into is that legs hide underneath other things, and the tips of the legs are never tracked very well. Even if the tip is not there, the network will try to label it. We have an idea of just labeling where the tips of the legs even if we can\u2019t see them so the nueral network has an idea of where the tip is supposed to be relative to everything else. Sometimes it overlaps with other dots, or is on a different body part. I was just wondering if this is something that makes sense.<br>\nThanks!</p>", "<p>Generally I wouldn\u2019t recommend labeling invisible bodyparts, this can skew the model into frequent prediction errors. The model has to try to label all the bodyparts but if they aren\u2019t really there the detections should have very low likelihood and can be easily filtered out. For a more relative approach to determining bodypart position you can use multianimal DLC on one animal. The PAF graphs should be helpful here.</p>"], "78666": ["<p>Hi<br>\nI am using the cellpose 2.2 in cell segmentation.<br>\nThis system was built in WEB platform and the cellpose was realized by using command.</p>\n<p>Firstly, while researching the cellpose, I found that cellpose provide 14 pretrained models.<br>\nBut I am not sure what the differences(and their utilizing examples) are between them because I am not an expert in this field.<br>\nPlease give me good advice about this problem.<br>\nSecondly, I also found that the cellpose provide \u201c*_output.png\u201d file when we get cell segment result by using commands.<br>\nBut sometimes there are some cases that it\u2019s not created(Because I used the same command, I think that it\u2019s related with the kind of image file).<br>\nWhich case is it created or not in?<br>\nAny help would be appreciated.</p>"], "78668": ["<p>I am trying to opens PerkinElmer Nuance im3 files (cube files) in ImageJ or Fiji.</p>\n<p>I found the IM3Reader (<a href=\"https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java\" rel=\"noopener nofollow ugc\">Source Code</a>, <a href=\"https://bio-formats.readthedocs.io/en/latest/metadata/IM3Reader.html\" rel=\"noopener nofollow ugc\">Supported Metadata Fields</a>)<br>\nI am not sure if what I am doing is right and if it is possible to import the Bio-Format that is in the source code</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java\" target=\"_blank\" rel=\"noopener nofollow ugc\">ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java</a></h4>\n\n\n      <pre><code class=\"lang-java\">/*#%L\n * BSD implementations of Bio-Formats readers and writers\n * %%\n * Copyright (C) 2005 - 2017 Open Microscopy Environment:\n *   - Board of Regents of the University of Wisconsin-Madison\n *   - Glencoe Software, Inc.\n *   - University of Dundee\n * %%\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * \n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n * \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/ome/bioformats/blob/2590eb180ca5fbda8c5b59d3d09b79eb3063185f/components/formats-bsd/src/loci/formats/in/IM3Reader.java\" target=\"_blank\" rel=\"noopener nofollow ugc\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\ninto ImgeJ or Fiji through &gt;Plugins&gt;Bio-Formats&gt;Bio-Formats Importer.</p>\n<p>I tried to save the script from the link and copy into the text editor and import it as a IM3Reader.java file into the Bio-Format but it is still not opening the images with the im3 extension.</p>\n<p>In Fiji I have the following \u201cException\u201d</p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_202 [64-bit]; Mac OS X 10.16; 296MB of 13333MB (2%)</p>\n<p><em>java.lang.IllegalStateException: Too early in import process: current step is READER, but must be after STACK</em></p>\n<ul>\n<li>at loci.plugins.in.ImportProcess.assertStep(ImportProcess.java:778)*</li>\n<li>at loci.plugins.in.ImportProcess.getReader(ImportProcess.java:306)*</li>\n<li>at loci.plugins.in.Importer.run(Importer.java:99)*</li>\n<li>at loci.plugins.LociImporter.run(LociImporter.java:78)*</li>\n<li>at ij.IJ.runUserPlugIn(IJ.java:237)*</li>\n<li>at ij.IJ.runPlugIn(IJ.java:203)*</li>\n<li>at ij.Executer.runCommand(Executer.java:152)*</li>\n<li>at ij.Executer.run(Executer.java:70)*</li>\n<li>at java.lang.Thread.run(Thread.java:748)*</li>\n</ul>\n<p>Can anyone help with this?</p>\n<p>Thank you</p>"], "78670": ["<p>Hello! I performed brain slices alignment in ABBA and exported the atlas as annotations in QuPath. Now I would like to export the original image and the annotations in a single multichannel image. I am using this code:</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\n\n// Define output path (relative to project)\ndef outputDir = buildFilePath(PROJECT_BASE_DIR, 'export')\nmkdirs(outputDir)\ndef name = GeneralTools.getNameWithoutExtension(imageData.getServer().getMetadata().getName())\ndef path = buildFilePath(outputDir, name + \"-labels.ome.tif\")\n\n// Define how much to downsample during export (may be required for large images)\ndouble downsample = 8\n\n// Create an ImageServer where the pixels are derived from annotations\ndef labelServer = new LabeledImageServer.Builder(imageData)\n  .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n  .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported\n  .addLabel('Other', 1)\n  .setBoundaryLabel('Boundary*', 2) // Define annotation boundary label\n  .multichannelOutput(true) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)\n  .build()\n\n// Write the image\nwriteImage(labelServer, path)\n</code></pre>\n<p>And I\u2019m able to obtain a multichannel image with 3 channels: a black background, a binary image with the annotations filled in white and the annotation\u2019s boundary. Is there a way to add also the original image, without the annotations, as a fourth channel?</p>", "<p>Hey <a class=\"mention\" href=\"/u/morgan_marz\">@morgan_marz</a><br>\nI don\u2019t have a full script worked out, but maybe something with TransformedServerBuilder would help. If you got your <code>labelServer</code> as you are doing now, and got your normal image server with <code>def imageserver = getCurrentServer()</code>, you could combine them with:</p>\n<pre><code class=\"lang-auto\">def serverMerged = new TransformedServerBuilder(labelServer)\n    .concatChannels(imageServer)\n    .build()\n</code></pre>\n<p>taken from <a href=\"https://forum.image.sc/t/qupath-multiple-channel-in-separate-files-how-to-merge-them/29455/22\">here</a>.  Then pass serverMerged into your writeImage function. I haven\u2019t tested this, but it might run into image type issues, since your labelServer is probably 8 bit? which may or may not match your data type. Important! This will not be a 4th channel. It will be either the last 3 channels (for an RGB image) or, for fluorescence, however many channels you have. If it runs at all, it will take a LOT longer to write, because there is significantly more information.</p>", "<p>Thank you so much <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a>, it was the correct idea! I just needed to concatenate the labelServer to the imageServer (so the viceversa of your solution) because otherwise the original image is black and white. Now I have an image with 6 channel as expected (RGB for the original + 3 that I have defined for the labelServer). This is the final version of the code that works for me:</p>\n<pre><code class=\"lang-auto\">import static qupath.lib.gui.scripting.QPEx.*\n\nimport javafx.application.Platform\nimport qupath.lib.images.ImageData\nimport qupath.lib.images.servers.ImageServerProvider\nimport qupath.lib.images.servers.TransformedServerBuilder\n\ndef imageData = getCurrentImageData()\n\n// Define output path (relative to project)\ndef outputDir = buildFilePath(PROJECT_BASE_DIR, 'export')\nmkdirs(outputDir)\ndef name = GeneralTools.getNameWithoutExtension(imageData.getServer().getMetadata().getName())\ndef path = buildFilePath(outputDir, name + \"-merged.tiff\")\n\n// Define how much to downsample during export (may be required for large images)\ndouble downsample = 8\n\n// Create an ImageServer where the pixels are derived from annotations\ndef labelServer = new LabeledImageServer.Builder(imageData)\n  .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n  .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported\n  .addLabel('Other', 1)\n  .setBoundaryLabel('Boundary*', 2) // Define annotation boundary label\n  .multichannelOutput(true) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)\n  .build()\n\ndef imageServer = getCurrentServer()\n\ndef serverMerged = new TransformedServerBuilder(imageServer)\n    .concatChannels(labelServer)\n    .build()\n\n\n// Write the image\nwriteImage(serverMerged, path)\n\n</code></pre>\n<p>On a side note, my goal was to open the multichannel image in Imaris, which works. ImageJ is not really able to split the channels, so this code might not work for every software.</p>", "<p>I\u2019m glad this works for you!</p>\n<p>Could you describe what you mean by \u201cImageJ is not really able to split the channels\u201d. What did you see?</p>", "<p>Unfortunately I can\u2019t share the image, but basically:</p>\n<ul>\n<li>If I export in the .tiff format, ImageJ sees the different channels but they are all superimposed even when you switch between them (you always see the original images and the annotations)</li>\n<li>If I export in the .ome.tif format, then ImageJ is able to split the images contained in each channel, but the original image is in black and white.</li>\n</ul>\n<p>That said, I am not an ImageJ user, so I might just be missing something!</p>"], "78673": ["<p>I take following error in every platform</p>\n<p>napari was tested with QT library <code>&gt;=5.12.3</code>.<br>\nThe version installed is 5.9.7. Please report any issues with<br>\nthis specific QT version at <a href=\"https://github.com/Napari/napari/issues\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Issues \u00b7 napari/napari \u00b7 GitHub</a>.<br>\nwarn(message=warn_message)<br>\nWARNING: QOpenGLWidget is not supported on this platform.</p>", "<p>Can you provide some info like what platform you are on, how you set up your python env, how you installed napari, and what version of napari?<br>\n(e.g. output of <code>napari --info</code>)?</p>", "<p>Thank you very much for your reply,<br>\nI work in a conda environment in ubuntu which I create with:</p>\n<p>module load anaconda/3.21.05<br>\nmodule load cuda/11.3<br>\nmodule load cudnn/8.2.1/cuda-11.X<br>\nmodule load gcc/9.3.0<br>\nmodule load cmake/3.24.0<br>\nmodule load Qt/5.12.12</p>\n<p>and</p>\n<p>output of <code>napari --info</code>:</p>\n<p>.local/lib/python3.8/site-packages/napari/_qt/<strong>init</strong>.py:50: UserWarning:<br>\nnapari was tested with QT library <code>&gt;=5.12.3</code>.<br>\nThe version installed is 5.9.7. Please report any issues with<br>\nthis specific QT version at <a href=\"https://github.com/Napari/napari/issues\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Issues \u00b7 napari/napari \u00b7 GitHub</a>.<br>\nwarn(message=warn_message)<br>\nWARNING: QOpenGLWidget is not supported on this platform.<br>\nWARNING:vispy:QOpenGLWidget is not supported on this platform.<br>\nSegmentation fault (core dumped)</p>\n<p>and</p>\n<p>I installed napari with:<br>\npython -m pip install \u201cnapari[all]\u201d<br>\npython -m pip install \u201cnapari[all]\u201d --upgrade<br>\nfor the last time</p>\n<p>thanks a lot\u2026<br>\nseher.</p>", "<p>I\u2019m not a linux expert, but this <code>module load anaconda/3.21.05</code> seems like a very old version of anaconda. Also anaconda typically comes with a number of packages, which in this case are likely to be out-dated, which is probably why you end up with very old Qt that we don\u2019t support.</p>\n<p>Perhaps you can consider using miniforge or mambaforge?</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/conda-forge/miniforge\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/conda-forge/miniforge\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/988810d7da03e54faa4eb0126c38bf5c2bd0a983.png 2x\" data-dominant-color=\"F2F4F1\"></div>\n\n<h3><a href=\"https://github.com/conda-forge/miniforge\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - conda-forge/miniforge: A conda-forge distribution.</a></h3>\n\n  <p>A conda-forge distribution. Contribute to conda-forge/miniforge development by creating an account on GitHub.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nThis will give you a modern conda with the <code>conda-forge</code> channel by default, which is also most up-to-date.<br>\nThen you can use</p>\n<pre><code class=\"lang-auto\">conda create -y -n napari-env -c conda-forge python=3.9\nconda activate napari-env\npython -m pip install \"napari[all]\"\n</code></pre>\n<p>if you want to use pip or just:</p>\n<pre><code class=\"lang-auto\">conda create -y -n napari-env -c conda-forge python=3.9 napari\n</code></pre>\n<p>If you want a pure conda solution.</p>", "<p>Thanks.<br>\nQt version is the only available one in my school\u2019s clusters, that\u2019s why I used that. Since I was already in a conda environment, I did not create one more inside it, and also I am not sure if it is the right move. I\u2019ll try this in my computer\u2019s anaconda environment (in spider), where I am freer, and I\u2019ll let you know. Thanks again.</p>", "<p>You are using a conda env for your napari here right? So only what\u2019s inside the env should matter\u2026<br>\nAre you making a fresh env using conda then installing using pip and still getting pyqt5 5.9.7? or you have something else in the env or it has full (old) anaconda in it?</p>\n<p>Or try making a fresh env using conda-forge channel:<br>\n<code>conda create -y -n napari-env -c conda-forge python=3.9</code><br>\nthen you can either try pip again or conda:<br>\n<code>conda install -c conda-forge napari</code></p>"], "78678": ["<p>I am trying to use the ilastik to quantify the percentage of virus-infected cells upon gene overexpression. In a word, I need to know the number of cells overexpressing the target genes (red) and the number of virus-infected cells upon gene overexpression (red+ green). I have attached the picture here fo<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806.jpeg\" data-download-href=\"/uploads/short-url/rPqtcsJl9mLOtn2jqXN4BseXW9E.jpeg?dl=1\" title=\"DeconvolvedTsfDAPI 377,447+TsfGFP 469,525+TsfTexas Red 586,647_D9_1_001\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_500x500.jpeg\" alt=\"DeconvolvedTsfDAPI 377,447+TsfGFP 469,525+TsfTexas Red 586,647_D9_1_001\" data-base62-sha1=\"rPqtcsJl9mLOtn2jqXN4BseXW9E\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30ae7ba1187b0afdaf5d80c3efec4bddabb5806_2_1000x1000.jpeg 2x\" data-dominant-color=\"160D08\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">DeconvolvedTsfDAPI 377,447+TsfGFP 469,525+TsfTexas Red 586,647_D9_1_001</span><span class=\"informations\">1920\u00d71920 315 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nr your reference. May I ask what is the rational workflow of ilastik to achieve this? Additionally, sometimes the red signal is too strong and it covers the green signal so I cannot identify the green signals by my eyes during the training, is there any way to spilt the channel or another way to distinguish the signal (see the D9 picture for reference)\uff1f</p>\n<p>percentage of virus-infected cells upon gene overexpression = the number of virus-infected cells upon gene overexpression (red+green) / the number of cells overexpressing the target genes (red)</p>\n<p>Thanks for your time!</p>", "<p>Hello <a class=\"mention\" href=\"/u/zoeyzhengg\">@Zoeyzhengg</a>,</p>\n<p>first of all, welcome to the image.sc community <img src=\"https://emoji.discourse-cdn.com/twitter/handshake.png?v=12\" title=\":handshake:\" class=\"emoji\" alt=\":handshake:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/tada.png?v=12\" title=\":tada:\" class=\"emoji\" alt=\":tada:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Since you want to reason about cells as entities, the default route here would be (more or less what we have in our <a href=\"https://youtu.be/F6KbJ487iiU\">i2k  tutorial</a>:</p>\n<p>1.) Pixel Classification to segment cell-pixels vs background.<br>\n2.) Object classification with the probability maps exported from 1) to classify cells into the classes <em>red</em>, <em>red+green</em> and probably a third class where you\u2019d put <em>incomplete, or bad segmentations</em>. In the end you\u2019d get a classification table that you can then use (with Excel, Python, R,\u2026) to get the ratio you\u2019re looking for.</p>\n<p>Things to know that might help you on the way:</p>\n<ul>\n<li>ilastik will in general work on all channels you give it - there is no way to subselect only a single channel <em>for computation</em>.</li>\n<li>there are different ways of <em>viewing</em> multi-channel images:\n<ul>\n<li>per default (if you have 2-3 channels) they will render in color (with first channel being red, second green, third blue). Sometimes it is then difficult to see all the additional colors you get in ilastik (at least for me). In situations like this I often choose to <em>look</em> at the channels in grayscale (one channel at a time - again, this will not change anything for the computation, where always <em>all</em> channels are included). See <a href=\"https://www.ilastik.org/documentation/basics/dataselection#properties\">dataset properties - display mode/channel display</a>. When switching to <em>grayscale</em> you can navigate between channels in the layerstack (bottom right) like this:</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bc912daf8631e256489a2278df593139a26ba1ef.gif\" alt=\"switch-channels\" data-base62-sha1=\"qU8O5pwRgxkBxemoZkcConfyZNt\" width=\"690\" height=\"485\" class=\"animated\"></p>\n<p>Hope this helps! Please update this thread in case you have questions related to the same analysis <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Cheers<br>\nDominik</p>", "<p>Hi k-dominik,</p>\n<p>Thanks for your advice! I still have one more question. How can I distinguish the red signal from the red+green signal when doing object classification? What kind of object feature selection I should use?</p>", "<p>I mean how to classify the cells into red signal and the red+green signal?</p>", "<aside class=\"quote no-group\" data-username=\"Zoeyzhengg\" data-post=\"3\" data-topic=\"78678\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/zoeyzhengg/40/69138_2.png\" class=\"avatar\"> Zoeyzhengg:</div>\n<blockquote>\n<p>How can I distinguish the red signal from the red+green signal when doing object classification? What kind of object feature selection I should use?</p>\n</blockquote>\n</aside>\n<p>for this the intensity features could work well (they are computed per-channnel). So if you use something like <em>Mean Intensity</em> will include this information.</p>", "<p>Thanks! One more question.<br>\nWhen I use the threshold and size filter when doing the objection classification, The red signal is merged together so that the object identification can not distinguish the merged cells one by one. If increase the threshold and smooth choice, some small staining cells will be ignored. If decrease the threshold and smooth choice, the cells will merge into one object. Is there any solution for this?</p>", "<p>Hm, okay. So the probability maps already don\u2019t separate the cells - is that correct? There are options\u2026</p>\n<ol>\n<li>Would you say that there is a visible boundary between those cells? If so, then you could try to train accordingly in Pixel Classification - adding a \u201cboundary\u201d class to help separate those.</li>\n<li>If that doesn\u2019t help, or is not feasible, you could also try to live with it - in object clasifiaction you could add an additional class of \u201cbad segmentations\u201d and exclude those clusters from the analysis.</li>\n<li>is there by any chance some additional channel in your data, e.g. on the nuclei (assuming what I am seeing in your example data is full cells and not nuclei :)). Then you could exploit the fact that cells usually only come with one nucleus and try the <em>Hysteresis Thresholding</em> (<a href=\"https://www.youtube.com/watch?v=_ValtSLeAr0&amp;t=2042s\">video turorial</a>, <a href=\"https://www.ilastik.org/documentation/objects/objects#hysteresis-thresholding\">documentation</a>). For this you\u2019d need to train to separate pixel classification projects, one for the whole cell bodies, and one for the nuclei - combining the probability maps in object classification.</li>\n</ol>\n<p>Hope this helps!</p>\n<p>Cheers<br>\nDominik</p>"], "78679": ["<p>I wrote a plugin to open a certain image format. I have the problem that two image instances are opened when using the ImagePlus.show function in run when using drag and drop. If I remove the ImagePlus.show function it opens a single instance but then if I use the plugin via Plugins-&gt;Input-Output it doesn\u2019t open the image.</p>\n<p>Thanks,<br>\nDon</p>"], "62294": ["<p>Hello everyone! First, I wish everyone on this forum a happy new year, may it bring you hapiness and luck.</p>\n<p>I am facing a problem I can\u2019t resolve on my own.<br>\nI am trying to automatically mesure the diamater of cellulose fibers, from a \u00b5CT.<br>\nI already have a working segmentation algorithm and it\u2019s working very well.<br>\nHowever, the cellulose fibers are from a very specific kind of paper, full of minerals and additives. Consequently, the volume (even after segmentation) is very \u201cnoisy\u201d :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47b9aef8c4486ceedc97d851319a59c54ba803e7.png\" data-download-href=\"/uploads/short-url/aevIYylst02DVgdAoygW4hJE8yr.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47b9aef8c4486ceedc97d851319a59c54ba803e7.png\" alt=\"image\" data-base62-sha1=\"aevIYylst02DVgdAoygW4hJE8yr\" width=\"433\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/7/47b9aef8c4486ceedc97d851319a59c54ba803e7_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">779\u00d7898 33.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nCross section :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e04ec44262456868cac40c462ae17f809dad40d.png\" data-download-href=\"/uploads/short-url/6z6vG2CCvKAetOxjgdlFXifEjgN.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e04ec44262456868cac40c462ae17f809dad40d_2_690x47.png\" alt=\"image\" data-base62-sha1=\"6z6vG2CCvKAetOxjgdlFXifEjgN\" width=\"690\" height=\"47\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e04ec44262456868cac40c462ae17f809dad40d_2_690x47.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e04ec44262456868cac40c462ae17f809dad40d_2_1035x70.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e04ec44262456868cac40c462ae17f809dad40d_2_1380x94.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e04ec44262456868cac40c462ae17f809dad40d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1519\u00d7105 16.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>You can distinguish fibers, but it\u2019s way more complex than what I\u2019ve seen in litterature.</p>\n<p>Cellulose fibers aren\u2019t perfect tubes :</p>\n<ul>\n<li>The walls very often collapse and tracking these fibers from their hollow middle won\u2019t work</li>\n<li>The walls often open and the cross section looks like a \u201cC\u201d (instead of an \u201cO\u201d)</li>\n<li>The fibers sometimes turn at very sharp angles.</li>\n</ul>\n<p><strong>I\u2019ve tried DiameterJ with no luck.</strong><br>\n<strong>I\u2019ve tried 3rd party programs like Theba with no luck. (specialized software from a paper on analyzing fibers)</strong></p>\n<p><strong>Is there a simple way to achieve this I\u2019m missing?</strong></p>\n<p>I am working with a 2048x2048x177 volume, already segmented. For reference, this is how it looks like before segmentation and everything :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41fa58262f206f808f3324e6d34c94585b625950.jpeg\" data-download-href=\"/uploads/short-url/9pFoxBknqgp6HTYCv0FsTzQFRh6.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41fa58262f206f808f3324e6d34c94585b625950_2_483x500.jpeg\" alt=\"image\" data-base62-sha1=\"9pFoxBknqgp6HTYCv0FsTzQFRh6\" width=\"483\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41fa58262f206f808f3324e6d34c94585b625950_2_483x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41fa58262f206f808f3324e6d34c94585b625950.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41fa58262f206f808f3324e6d34c94585b625950.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/41fa58262f206f808f3324e6d34c94585b625950_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">698\u00d7722 198 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nCross section<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/2/e277b65c6efb664dcbae3cb4bffce6b83c643a7f.jpeg\" data-download-href=\"/uploads/short-url/wjqoep4FUEiks3rTuDebY3Sx8wD.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/2/e277b65c6efb664dcbae3cb4bffce6b83c643a7f_2_690x73.jpeg\" alt=\"image\" data-base62-sha1=\"wjqoep4FUEiks3rTuDebY3Sx8wD\" width=\"690\" height=\"73\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/2/e277b65c6efb664dcbae3cb4bffce6b83c643a7f_2_690x73.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/2/e277b65c6efb664dcbae3cb4bffce6b83c643a7f_2_1035x109.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/2/e277b65c6efb664dcbae3cb4bffce6b83c643a7f_2_1380x146.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/2/e277b65c6efb664dcbae3cb4bffce6b83c643a7f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2029\u00d7217 113 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>YOu could try quanfima:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/rshkarin/quanfima\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/rshkarin/quanfima\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/39f3a6b1840a3d2a4940f1478ee0b5c8dda98550_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/39f3a6b1840a3d2a4940f1478ee0b5c8dda98550_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/39f3a6b1840a3d2a4940f1478ee0b5c8dda98550_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/9/39f3a6b1840a3d2a4940f1478ee0b5c8dda98550.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/39f3a6b1840a3d2a4940f1478ee0b5c8dda98550_2_10x10.png\"></div>\n\n<h3><a href=\"https://github.com/rshkarin/quanfima\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - rshkarin/quanfima: Quanfima (Quantitative Analysis of Fibrous...</a></h3>\n\n  <p>Quanfima (Quantitative Analysis of Fibrous Materials) - GitHub - rshkarin/quanfima: Quanfima (Quantitative Analysis of Fibrous Materials)</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0215137\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e8fd02215c3e0ebc8bd7379c51a1f3cd38b1c24e.png\" class=\"site-icon\" width=\"48\" height=\"48\">\n\n      <a href=\"https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0215137\" target=\"_blank\" rel=\"noopener nofollow ugc\">journals.plos.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:320/216;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/aeb776a4af8a12248fba2ed961be26d0d408efd8.png\" class=\"thumbnail\" width=\"320\" height=\"216\"></div>\n\n<h3><a href=\"https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0215137\" target=\"_blank\" rel=\"noopener nofollow ugc\">Quanfima: An open source Python package for automated fiber analysis of...</a></h3>\n\n  <p>Hybrid 3D scaffolds composed of different biomaterials with fibrous structure or enriched with different inclusions (i.e., nano- and microparticles) have already demonstrated their positive effect on cell integration and regeneration. The analysis of...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nThe paper is quite impressive and it does 3D.</p>\n<p>I used it for 2D analysis of bacterial cellulose nano fibers:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/psobolewskiPhD/SEM_fiber_analysis\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/psobolewskiPhD/SEM_fiber_analysis\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4f49040db281ae45ae2638194b7cadce0c0ed39_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4f49040db281ae45ae2638194b7cadce0c0ed39_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4f49040db281ae45ae2638194b7cadce0c0ed39_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4f49040db281ae45ae2638194b7cadce0c0ed39.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4f49040db281ae45ae2638194b7cadce0c0ed39_2_10x10.png\"></div>\n\n<h3><a href=\"https://github.com/psobolewskiPhD/SEM_fiber_analysis\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - psobolewskiPhD/SEM_fiber_analysis: SEM image analysis to get fiber...</a></h3>\n\n  <p>SEM image analysis to get fiber parameters (diameter, orientation) - GitHub - psobolewskiPhD/SEM_fiber_analysis: SEM image analysis to get fiber parameters (diameter, orientation)</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>FYI: I had to fork it to get it working on my Apple M1 mac, due to python version issues. Also, the 3D library wasn\u2019t available to me.</p>", "<p>Thank you for your reply, I\u2019ll try this out!</p>", "<p>Good luck!<br>\nAlso, don\u2019t use my fork for your 3D, cause it\u2019s the <code>vigra</code> library I had issue with\u2014prolly due to being on arm64\u2014and I commented it out, since I was just doing 2D. You may want to check this commit if you are using python3:</p><aside class=\"onebox githubcommit\" data-onebox-src=\"https://github.com/psobolewskiPhD/quanfima/commit/57764db5315c6ca68085eedb86340a785c766657\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/psobolewskiPhD/quanfima/commit/57764db5315c6ca68085eedb86340a785c766657\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/psobolewskiPhD/quanfima</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Commit\">\n    <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/psobolewskiPhD/quanfima/commit/57764db5315c6ca68085eedb86340a785c766657\" target=\"_blank\" rel=\"noopener nofollow ugc\">Bump version pins for python3.9</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        committed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-10-15\" data-time=\"19:30:56\" data-timezone=\"UTC\">07:30PM - 15 Oct 21 UTC</span>\n      </div>\n\n      <div class=\"user\">\n        <a href=\"https://github.com/psobolewskiPhD\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"psobolewskiPhD\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/5/252e840432703c8e2923871b838b1f1269bb48be.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          psobolewskiPhD\n        </a>\n      </div>\n\n      <div class=\"lines\" title=\"changed 2 files with 7 additions and 7 deletions\">\n        <a href=\"https://github.com/psobolewskiPhD/quanfima/commit/57764db5315c6ca68085eedb86340a785c766657\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <span class=\"added\">+7</span>\n          <span class=\"removed\">-7</span>\n        </a>\n      </div>\n    </div>\n  </div>\n</div>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hi <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a>,</p>\n<p>I\u2019m trying to install quanfima, but I get this when trying to \u2018pip install\u2019</p>\n<pre><code class=\"lang-auto\">(quanfima) C:\\Users\\owner&gt;pip install quanfima\nERROR: Could not find a version that satisfies the requirement quanfima\nERROR: No matching distribution found for quanfima\n\n(quanfima) C:\\Users\\owner&gt;\n</code></pre>\n<p>Any clues?</p>\n<p>I\u2019ve just updated python to the latest version as of 24.01.22, and I\u2019m using anaconda cmd prompt.</p>\n<p>Cheers,<br>\nDaniel</p>", "<aside class=\"quote no-group\" data-username=\"Daniel_Waiger\" data-post=\"5\" data-topic=\"62294\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/daniel_waiger/40/24019_2.png\" class=\"avatar\"> Daniel_Waiger:</div>\n<blockquote>\n<p>I\u2019ve just updated python to the latest version as of 24.01.22, and I\u2019m using anaconda cmd prompt.</p>\n</blockquote>\n</aside>\n<p>As I mentioned above the package is kinda old and has some issues with python3. Looking at pypi it\u2019s just a python2 wheel:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pypi.org/project/quanfima/#files\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/14c21e9cefe5a019cee7f0125c1731d09cdf3e7a.png\" class=\"site-icon\" width=\"32\" height=\"30\">\n\n      <a href=\"https://pypi.org/project/quanfima/#files\" target=\"_blank\" rel=\"noopener nofollow ugc\">PyPI</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fbd3be5698076d694ade5b76ea8b3484d745cd4c.jpeg\" class=\"thumbnail onebox-avatar\" width=\"300\" height=\"300\">\n\n<h3><a href=\"https://pypi.org/project/quanfima/#files\" target=\"_blank\" rel=\"noopener nofollow ugc\">quanfima</a></h3>\n\n  <p>The package for morphological analysis and visualization of fibrous materials.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>You could try installing from the quanfima GitHub too, there was a few commits newer than the release, some also fixing python3 issues.</p>\n<p>Otherwise, you may try to use my fork, which I updated some of the depends to get it running on my M1 mac, but I commented out the 3d package <code>vigra</code>.</p>", "<p>Hi, were you eventually able to solve this?</p>"], "78682": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696.png\" data-download-href=\"/uploads/short-url/niGg8fh64SeHXsnBpAYUyDaci1M.png?dl=1\" title=\"\u622a\u5c4f2023-03-16 \u4e0a\u53485.03.27\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696_2_690x487.png\" alt=\"\u622a\u5c4f2023-03-16 \u4e0a\u53485.03.27\" data-base62-sha1=\"niGg8fh64SeHXsnBpAYUyDaci1M\" width=\"690\" height=\"487\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696_2_690x487.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a34e9d850228ecccc698106c78cdff4b220ad696.png 2x\" data-dominant-color=\"E6E5E6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\u622a\u5c4f2023-03-16 \u4e0a\u53485.03.27</span><span class=\"informations\">989\u00d7699 70 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nWhen using the pixel classification, I cannot upload the PNG image for Raw Data part of Input Data. May I ask what is the solution to this situation? If PNG format is not acceptable, what kind of software or apps can be used for stack transformation from PNG to HDF5?</p>\n<p>Thanks!</p>", "<p>Hello <a class=\"mention\" href=\"/u/zoeyzhengg\">@Zoeyzhengg</a>,</p>\n<p>This is probably a bug in ilastik related to the special characters you have in your filename. Specifically all the square brackets <code>Deconvolved|TsfDAPI377,447]+Tsf{GFP 469,525]+Tsf[ TexasRed 586,647]]_82_1_001.png</code>. If you replaces them by something else, then it will work.</p>\n<p>So this really is a problem on the ilastik side, sorry you ran into this.</p>\n<p>Cheers<br>\nDominik</p>\n<p>edit: corresponding issue in our issuetracker: <a href=\"https://github.com/ilastik/ilastik/issues/2391\" class=\"inline-onebox\">File names with square brackets cannot be opened \u00b7 Issue #2391 \u00b7 ilastik/ilastik \u00b7 GitHub</a></p>"], "78685": ["<p>Recently I\u2019ve been generating a lot of multi-channel (this is what the Nikon software calls multiple acquisition settings) images and have been getting the same error when trying to import these into ImageJ/Fiji.<br>\nImageJ recognises the correct number of channels but I think gets confused between RGB channels and the ND2 channels, leading to N channels being opened in ImageJ (N being the number of channels taken with the microscope) but each is a mono rather than RGB image and seems to be made up of LCD-display-like pixels, with the image being recognisable but jumbled (see <a href=\"https://zenodo.org/record/7738355#.ZBLpFnbP1aR\" rel=\"noopener nofollow ugc\">Zenodo upload</a>).<br>\nIf it helps, I think the python nd2reader plugin can recognise the dimensions properly.</p>\n<p>For those reading who also have this problem, the best fix I can come up with is exporting multi-channel ND2s as TIFFs in NIS E Viewer and then stacking them with ImageJ. Please let me know if you have a better idea!.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://zenodo.org/record/7738355#.ZBLpFnbP1aR\">\n  <header class=\"source\">\n      \n\n      <a href=\"https://zenodo.org/record/7738355#.ZBLpFnbP1aR\" target=\"_blank\" rel=\"noopener nofollow ugc\">Zenodo</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://zenodo.org/record/7738355#.ZBLpFnbP1aR\" target=\"_blank\" rel=\"noopener nofollow ugc\">20230315 ImageJ bio-format importer multi-channel ND2 bug</a></h3>\n\n  <p>A bug wherein ND2 images taken with different camera settings are read incorrectly by ImageJ's bio-format importer imageJ_test.nd2 - The ND2 file produced by NIS Elements AR on a Nikon LV100 microscope. 11 time steps, 2 channels (images taken with...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thank you Nick for raising the issue and providing sample data and investigation. I can confirm that this does look to be a bug in the Bio-Formats ND2Reader, unfortunately we do not an immediate fix for the problem. I have opened a Bio-Formats GitHub Issue for tracking this problem: <a href=\"https://github.com/ome/bioformats/issues/3964\" class=\"inline-onebox\">ND2: 8 bit RGB channels being read as single 8 bit channels \u00b7 Issue #3964 \u00b7 ome/bioformats \u00b7 GitHub</a></p>"], "78688": ["<p>I have an opening for a postdoctoral researcher position in my group in Warsaw:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://dioscuricentrebacteria.com/jobs/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/ae921e180b49cad37cdd981f59955c1898cd2e7c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://dioscuricentrebacteria.com/jobs/\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"10:51PM - 18 May 2020\">Dioscuri Centre for Physics and Chemistry of Bacteria \u2013 18 May 20</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:440/134;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/6/267924fc84fd9ccad70c0bf34c1802a2ffb32847.png\" class=\"thumbnail\" width=\"440\" height=\"134\"></div>\n\n<h3><a href=\"https://dioscuricentrebacteria.com/jobs/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Jobs</a></h3>\n\n  <p>A postdoctoral researcher in biological physics We seek a talented post-doctoral researcher to contribute to the project \u201cTransition from genetic to phenotypic antibiotic resistance in de novo bact\u2026</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>The project will involve creating a very large microfluidic device for optical imaging of de novo bacterial mutations, and will combine experiments, image analysis, and mathematical modelling.</p>\n<p>Although the job is ideally suited for a person with experimental background, theorists and computational researchers interested in image processing and seeking to gain wet lab experience are encouraged to apply. I have had some success in converting such researchers into half-time experimentalists. This project is an ideal place to make such a transition; theoretical/computational skills will be very useful and there is another post-doc (a biologist) to help with the \u201cbiology\u201d part of the project.</p>\n<p>Deadline for applications is 31 March 2023.</p>\n<p>Best wishes,<br>\nBartek Waclaw</p>"], "78691": ["<p>Hi there,</p>\n<p>I have no experience with scripting, other that running them and changing specific variables. I have detected the annotations with a pixel classifier and then added Intensity features to them to get the mean intensity of the structure.</p>\n<p><strong>Could you provide a script that allows me to select some of my annotations based on their intensity</strong>? e.g. I would like to select the ones that have a value &gt;750, or 550-750.<br>\nI did it manually for some of my images, but I believe there must be a more clever way to do it <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Also it would be great if there was apossibility to <strong>exclude some of them based on their intensity values</strong>.</p>\n<p>And finally I would like to be able to set a new class through the script: e.g from Islet \u2192 NE-islet</p>\n<p>Thanks so much in advance!</p>\n<p>Best,<br>\nEry</p>", "<p>Out of curiousity, I asked this question to Bing Compose (which recently is revealed to be using GPT-4):</p>\n<blockquote>\n<p>Write a QuPath Groovy script to select annotations with intensity greater than 750 and change their class name from \u201cIslet\u201d to \u201cNE-islet\u201d</p>\n</blockquote>\n<p>and got this result:</p>\n<pre><code class=\"lang-auto\">// Get all annotations in the current image\ndef annotations = getAnnotationObjects()\n\n// Loop through each annotation\nfor (annotation in annotations) {\n  // Get the mean intensity value of the annotation\n  def intensity = annotation.getMeasurementList().getMeasurementValue('Intensity')\n  \n  // Check if the intensity is greater than 750 and the class name is \"Islet\"\n  if (intensity &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {\n    // Change the class name to \"NE-islet\"\n    annotation.setPathClass(getPathClass('NE-islet'))\n  }\n}\n\n// Update the display\nfireHierarchyUpdate()\n</code></pre>\n<p>Honestly, I think that\u2019s impressive and I wouldn\u2019t change much, if at all, to the structure of the script.</p>\n<p>However, you will need to change the measurement name to match the exact measurement you made to the annotations, i.e. change <code>'Intensity'</code> to whatever measurement name you got. Also, you probably don\u2019t need to use <code>fireHierarchyUpdate()</code> in QuPath v0.4.x anymore.</p>\n<p>EDIT: I forgot that <code>getMeasurementValue()</code> is now deprecated, and get() should be used instead:</p>\n<pre><code class=\"lang-auto\">// Get all annotations in the current image\ndef annotations = getAnnotationObjects()\n\n// Loop through each annotation\nfor (annotation in annotations) {\n  // Get the mean intensity value of the annotation\n  def intensity = annotation.measurements.get('Intensity') // or annotation.measurements['Intensity']\n  \n  // Check if the intensity is greater than 750 and the class name is \"Islet\"\n  if (intensity &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {\n    // Change the class name to \"NE-islet\"\n    annotation.setPathClass(getPathClass('NE-islet'))\n  }\n}\n</code></pre>", "<p>thanks! I will try!</p>", "<p>Hi so I tried the following, and it gave me an error at line 10. I am also using version 0.3.2 so I kept the fireHierarchyUpdate. So i think the problem is with my \u2018ROI: 0.50 \u03bcm per pixel: MHC-I: Mean\u2019 value.</p>\n<pre><code class=\"lang-auto\">// Get all annotations in the current image\ndef annotations = getAnnotationObjects()\n\n// Loop through each annotation\nfor (annotation in annotations) {\n  // Get the mean intensity value of the annotation\n  def intensity = annotation.measurements.get('ROI: 0.50 \u03bcm per pixel: MHC-I: Mean') // or annotation.measurements['Intensity']\n  \n  // Check if the intensity is greater than 750 and the class name is \"Islet\"\n  if (ROI: 0.50 \u03bcm per pixel: MHC-I: Mean &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {\n    // Change the class name to \"H-islet\"\n    annotation.setPathClass(getPathClass('H-islets'))\n  }\n}\n// Update the display\nfireHierarchyUpdate()\n\nThis is the error:\nRROR: MultipleCompilationErrorsException at line 10: startup failed:\nScript14.groovy: 11: Unexpected input: '{\\n  // Get the mean intensity value of the annotation\\n  def intensity = annotation.measurements.get('ROI: 0.50 \u03bcm per pixel: MHC-I: Mean') // or annotation.measurements['Intensity']\\n  \\n  // Check if the intensity is greater than 750 and the class name is \"Islet\"\\n  if (ROI:' @ line 11, column 10.\n     if (ROI: 0.50 \u03bcm per pixel: MHC-I: Mean &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet')) {</code></pre>", "<aside class=\"quote no-group\" data-username=\"EPet\" data-post=\"4\" data-topic=\"78691\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/epet/40/46601_2.png\" class=\"avatar\"> Ery Petropoulou:</div>\n<blockquote>\n<p><code>if (ROI: 0.50 \u03bcm per pixel: MHC-I: Mean &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet'))</code></p>\n</blockquote>\n</aside>\n<p>You can just use <code>intensity</code> here as you have defined it as a variable earlier, i.e.:</p>\n<pre><code class=\"lang-auto\">if (intensity &gt; 750 &amp;&amp; annotation.getPathClass() == getPathClass('Islet'))\n</code></pre>"], "78694": ["<p>Hi guys\uff0cI am conducting research on atherosclerotic plaques of internal carotid artery on CT angiography. In order to analyze plaque components, I need to use the plug-in Polymeasure, but I cannot find the download method of this plug-in from any aspect. Can anyone help me?</p>"], "78696": ["<p>Hello! Thanks you for the open source software, It is proving to be really useful for my current dataset. It has been great reading the discussion in the forum.</p>\n<p>I do however need to clear a number of questions that I could not find answers to in the FAQ and the forum.</p>\n<p>My dataset is stacks of time series microCT data and although I primarily use Avizo, due to the circumstances of the acquisition of these images there are differences in contrast in the regions of the image for same phases and to some extent across time points as well.</p>\n<p>To give an example:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d.jpeg\" data-download-href=\"/uploads/short-url/8Oec2wSztLNcILhogh0tx5vd19P.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d_2_690x419.jpeg\" alt=\"image\" data-base62-sha1=\"8Oec2wSztLNcILhogh0tx5vd19P\" width=\"690\" height=\"419\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d_2_690x419.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d_2_1035x628.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3dbeee6c4b94d346299e1723b1ff02a17af88d0d.jpeg 2x\" data-dominant-color=\"949996\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1132\u00d7688 76.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here the region by the red arrow is the same phase and as you can see the results are great when trained with ilastik.</p>\n<p>The problem is that this took about 9-10 hours to process a stack of 1500 images(2016x2016) using an autocontext workflow, which considering the number of scans I have is a problem.</p>\n<p>Reading through this forum I have optimized the workflow a lot: by using smaller set of training data, sparse annotations, reducing the number of features selected and modifying the config files. It is now down to about 5 hours per stack, still not ideal but if I cannot reduce it further then its livable.</p>\n<p>Now to the questions:</p>\n<ul>\n<li>\n<p>I have to train a new workflow for each type of sample(considerable differences in structure), this is fine, however It seems that the project requires accompanying .h5 files to function, this balloons already large amount of data. But looking at the size of the .ilp file it looks like it already includes the raw data? It\u2019s a bit redundant having to save the same data twice. <em>Is it possible to use the trained model without having the raw files with it?</em></p>\n</li>\n<li>\n<p>There are 4 phases that I have to separate, reading through the forum it is recommended to export the probabilities and then threshold(using a value of 0.5) these to obtain the phases. I some scans where a specific region could correspond to one of two classes and they might have very similar probabilities, My question is what happens in regions where no class has the value 0.5. <em>Is it not possible to segment that region? Also could the probability export be in 8-bit?</em><br>\neg:( probabilities of the two phases, zoomed-in)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb.jpeg\" data-download-href=\"/uploads/short-url/tqN7rK25Wqxkif2e7FoTcxOAy8j.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb_2_689x364.jpeg\" alt=\"image\" data-base62-sha1=\"tqN7rK25Wqxkif2e7FoTcxOAy8j\" width=\"689\" height=\"364\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb_2_689x364.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb_2_1033x546.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce465e063c9c70782b9746b57fdcc6b8629c17eb.jpeg 2x\" data-dominant-color=\"8B8B8B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1279\u00d7676 69.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c.jpeg\" data-download-href=\"/uploads/short-url/kaNwJdg95Ubu7RnGKDi1UsuNYEk.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c_2_689x376.jpeg\" alt=\"image\" data-base62-sha1=\"kaNwJdg95Ubu7RnGKDi1UsuNYEk\" width=\"689\" height=\"376\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c_2_689x376.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c_2_1033x564.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d63fab035ce873b1ee961b57e1dffb721d9780c.jpeg 2x\" data-dominant-color=\"7CA2AC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1268\u00d7691 86.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>Regarding the auto-context workflow, generally I\u2019ve seen that I need to annotate the 4 classes in the 1st stage and then the foreground and the background in the 2nd stage. <em>But how do I get the 4 classes from the export of 2nd stage then?</em> The way I\u2019m doing it right now is in reverse, where separating the foreground in step 1 which works great and then annotating the classes in stage 2. <em>Am I approaching this wrong?</em></p>\n</li>\n</ul>\n<p>*And finally regarding the <em>OOB error, what value is acceptable enough?</em> I know that it need to be as low as possible but what is an objective difference between 0.0001 and 0.001 when visually I cannot tell much difference between the two but the computational time is long. Some minor differences I do notice is for very few pixels and those are not really relevant in the context of the whole volume. Another issue is the multiple workflows for each volume, so might be good to know the maximum acceptable value to aim for each.</p>\n<p>The quality of data I have is not the best and there is an acceptable degree of error that can be attributed to the circumstances of the scans, so a degree of uncertainty if unavoidable. But it would be great if I can optimize it further to have as best as feasible.</p>\n<p>Thank you again! Any information regarding this would be helpful.</p>", "<p>Hello <a class=\"mention\" href=\"/u/ujjwal\">@ujjwal</a>,</p>\n<p>first of all, welcome to the image.sc forum <img src=\"https://emoji.discourse-cdn.com/twitter/tada.png?v=12\" title=\":tada:\" class=\"emoji\" alt=\":tada:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12\" title=\":star_struck:\" class=\"emoji\" alt=\":star_struck:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>second, thank you for taking the time to look at all the previous information that there is. I know it\u2019s not always straight forward to find! This is some very proficient usage of ilastik!</p>\n<p>Before I go on answering the questions, I have a few as well:</p>\n<ul>\n<li>just to clarify: when I read through your post it seems that you converted your input data to h5, is this correct? (all input data) Did you use our fiji plugin?</li>\n<li>to what format are you exporting?</li>\n<li>what kind of machine are you using for your processing (how many cores, how much ram?)</li>\n</ul>\n<p>Now to your questions?</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"1\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>I have to train a new workflow for each type of sample(considerable differences in structure), this is fine, however It seems that the project requires accompanying .h5 files to function, this balloons already large amount of data. But looking at the size of the .ilp file it looks like it already includes the raw data? It\u2019s a bit redundant having to save the same data twice. <em>Is it possible to use the trained model without having the raw files with it?</em></p>\n</blockquote>\n</aside>\n<p>What version of ilastik are you using. When the classifier is saved to the project file, the raw data should not be needed anymore in headless processing (which I assume you\u2019re doing). ilastik would only try to access it to train the classifer. I verified with ilastik <code>1.4.0</code> that this works in principle.</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"1\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>There are 4 phases that I have to separate, reading through the forum it is recommended to export the probabilities and then threshold(using a value of 0.5) these to obtain the phases. I some scans where a specific region could correspond to one of two classes and they might have very similar probabilities, My question is what happens in regions where no class has the value 0.5. <em>Is it not possible to segment that region? Also could the probability export be in 8-bit?</em><br>\neg:( probabilities of the two phases, zoomed-in)</p>\n</blockquote>\n</aside>\n<p>The probabilities can safely be exported to 8-bits (in that case you have to renormalize from <code>0..1</code> to <code>0..255</code>.<br>\nFor the thresholds - of course you could use lower values, or just the largest value (if documented properly). You are right of course, that this means that some regions might not get a segmentation. But I would think about it differently - by enforcing at least 0.5 (or any value you see fit) you will get some pixels that are not part of any class - and you will know which ones those are - isn\u2019t that good for any downstream analysis, not to include those (after all they might be too different)?</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"1\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>Regarding the auto-context workflow, generally I\u2019ve seen that I need to annotate the 4 classes in the 1st stage and then the foreground and the background in the 2nd stage. <em>But how do I get the 4 classes from the export of 2nd stage then?</em> The way I\u2019m doing it right now is in reverse, where separating the foreground in step 1 which works great and then annotating the classes in stage 2. <em>Am I approaching this wrong?</em></p>\n</blockquote>\n</aside>\n<p>The advice here is empirical: In general, what we suggest is to annotate everything that is in the image in the first stage (so this is usually many classes) and then only what you are interested in in the second stage. So if you are interested in 4 classes, then you should have 4 classes in the second stage. For the first stage you\u2019d probably also want to have at least 4 classes. Compared to your current setup this means having twice as many computations overall, so it would slow you down even more. (And it would also require more RAM). But probably deliver the highest fidelity. I don\u2019t really dare to ask, but you\u2019ve tried \u201cjust\u201d pixel classification, and it was not good enough, yes?</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"1\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>*And finally regarding the <em>OOB error, what value is acceptable enough?</em> I know that it need to be as low as possible but what is an objective difference between 0.0001 and 0.001 when visually I cannot tell much difference between the two but the computational time is long. Some minor differences I do notice is for very few pixels and those are not really relevant in the context of the whole volume. Another issue is the multiple workflows for each volume, so might be good to know the maximum acceptable value to aim for each.</p>\n</blockquote>\n</aside>\n<p>Gold standard to assess the quality of your segmentation would be to annotate at least a part of the volume densely (all pixels) and then compare that to the final segmentation result (after thresholding and all).<br>\nThe OOB is only an indication - data points are never tested against the full classifier, only against those trees in the forest that didn\u2019t include them as training data. A value <code>0.001</code> would mean that only 1 in 1000 pixels (from the training data) gets classified incorrectly in this setup. <code>0.0001</code> would even be 1 in 10000. For me this would also be extremely hard to see. Both these values are extremely low. But keep in mind that this <em>only</em> includes the annotated training data.<br>\nYou seem have looked at the data and the result (the <em>most</em> important thing here) and found both equally good, which is the important (albeit less objective) measure. And again, to get a \u201creal\u201d indicator, you\u2019d have to annotate some volume densely.</p>\n<p>I hope I could clear up <em>some</em> of your concerns. Happy to go into any of the topics in more detail in case you have further questions!</p>\n<p>Cheers<br>\nDominik</p>", "<p>Hi <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>, Thank you for the welcome and taking time to answer the questions!</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<ul>\n<li>just to clarify: when I read through your post it seems that you converted your input data to h5, is this correct? (all input data) Did you use our fiji plugin?</li>\n</ul>\n</blockquote>\n</aside>\n<p>Indeed, initially I used the tiff stacks but it was quite some work to move the files back and forth from the network drive and reading through the suggestions I chose h5.<br>\nSince Avizo is the most used software in our group, I have the most experience with using that, so the preprocessed files are converted using a macro in Fiji using the ilastikj plugin to h5.</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>to what format are you exporting?</p>\n</blockquote>\n</aside>\n<p>The output from ilastik till now was the 8-bit simple segmentation stage 2 in h5 but changing the axis order to \u2018zxyc\u2019. The reverse conversion (this time to tiff sequence) is again done using a macro in Fiji.</p>\n<p>(extra note: Apparently Fiji does not like to read these h5 files using the native hdf5 plugin and so I use the ilastikj plugin again for that, do you know why that might be? It seems that it is also a little slower as well\u2026?)</p>\n<p>I briefly tried to use the probability map but it creates 4 tiff stacks,one for each class. So I still need to figure that out.</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>what kind of machine are you using for your processing (how many cores, how much ram?)</p>\n</blockquote>\n</aside>\n<p>It\u2019s a workstation class machine, so 14 core, 256-384GB ram. There is a particularly powerful GPU as well (A5000) but ilastik does not benefit from it <img src=\"https://emoji.discourse-cdn.com/twitter/upside_down_face.png?v=12\" title=\":upside_down_face:\" class=\"emoji\" alt=\":upside_down_face:\" loading=\"lazy\" width=\"20\" height=\"20\">.<br>\nI did create a config file and during processing it does use 100% CPU but yeah\u2026compute time is still relatively same when compared to a machine with 8 cores and 128GB memory.<br>\nAny idea how to probably optimize it?</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>What version of ilastik are you using. When the classifier is saved to the project file, the raw data should not be needed anymore in headless processing (which I assume you\u2019re doing). ilastik would only try to access it to train the classifer. I verified with ilastik <code>1.4.0</code> that this works in principle.</p>\n</blockquote>\n</aside>\n<p>It is 1.4.0 initially it was 1.4.0rc8 but I updated it last month. I am currently working form the GUI but I plan to use it in headless mode once I am happy with the segmentation result. Right now if I move the project/raw files I receive the error that the files have been moved/ it is too late to change.</p>\n<p>Is it possible to have a single self-contained .ilp file, because the raw data for training is smaller than the stacks. In that case I can keep a single file for each trained model and not the data files separately.</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>The probabilities can safely be exported to 8-bits (in that case you have to renormalize from <code>0..1</code> to <code>0..255</code>.<br>\nFor the thresholds - of course you could use lower values, or just the largest value (if documented properly). You are right of course, that this means that some regions might not get a segmentation. But I would think about it differently - by enforcing at least 0.5 (or any value you see fit) you will get some pixels that are not part of any class - and you will know which ones those are - isn\u2019t that good for any downstream analysis, not to include those (after all they might be too different)?</p>\n</blockquote>\n</aside>\n<p>If the probability map is renormalized to 0\u2026255 does the threshold of 0.5(or other value) still remain valid?<br>\nI was able to export it to 8-bit map but the I might need to write a macro to possibly combine the 4 channels into a single tiff stack(by setting the appropriate threshold value eg.0.5). Because right now when trying to export it just creates 4 stacks for each of the class with some garbage value.<br>\nThe reasoning does indeed make sense here, and based on the phenomenon observed, chances of one class over other is more which can be classified properly while post-processing</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>I don\u2019t really dare to ask, but you\u2019ve tried \u201cjust\u201d pixel classification, and it was not good enough, yes?</p>\n</blockquote>\n</aside>\n<p>I did <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"> but yeah, Auto-context was definitely better.<br>\nPerhaps a question, does the amount of annotations affect the computation time? or is it just the amount of features selected?</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>So if you are interested in 4 classes, then you should have 4 classes in the second stage. For the first stage you\u2019d probably also want to have at least 4 classes.</p>\n</blockquote>\n</aside>\n<p>Currently for my approach, Separating from the background it required only 4 features in stage 1 and then for stage 2 it was 17-19 features. If I select do the way you describe, does it then mean 17-19 features in each stage? The ram isn\u2019t the biggest issue, it\u2019s the time taken to process the 1500 image stack (currently about 5hrs) Is this around the time that is normally expected?</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>Gold standard to assess the quality of your segmentation would be to annotate at least a part of the volume densely (all pixels) and then compare that to the final segmentation result (after thresholding and all).<br>\nThe OOB is only an indication - data points are never tested against the full classifier, only against those trees in the forest that didn\u2019t include them as training data.</p>\n</blockquote>\n</aside>\n<p>That\u2019s probably not possible in this case as the contrast is not that good, in certain regions the grey values change too much for the same phase, However, everything is more or less correctly classified apart from very small region(which I can better classify from the probability map).</p>\n<aside class=\"quote no-group\" data-username=\"k-dominik\" data-post=\"2\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/k-dominik/40/34149_2.png\" class=\"avatar\"> k-dominik:</div>\n<blockquote>\n<p>But keep in mind that this <em>only</em> includes the annotated training data.</p>\n</blockquote>\n</aside>\n<p>This does clear some of my concern and would help me better argue the observed results.</p>\n<p>Apologies if my discussion is becoming too wordy. But the results obtained are much better that other approaches and I really want to get a clear understanding so that I can include it in a prospective article.</p>\n<p>Best,<br>\nUjjwal</p>", "<p>Hello <a class=\"mention\" href=\"/u/ujjwal\">@ujjwal</a>,</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"3\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>(extra note: Apparently Fiji does not like to read these h5 files using the native hdf5 plugin and so I use the ilastikj plugin again for that, do you know why that might be? It seems that it is also a little slower as well\u2026?)</p>\n</blockquote>\n</aside>\n<p>I\u2019ve tried the plugin with some random result from ilastik and the normal reader works for me with the \u201cindividual hyperstacks\u201d option (with correct dataset layout). The build-in plugin does appear to be faster. With the ilastik plugin you get some added convenience, that axistags are recognized correclty.</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"3\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>I did create a config file and during processing it does use 100% CPU but yeah\u2026compute time is still relatively same when compared to a machine with 8 cores and 128GB memory.<br>\nAny idea how to probably optimize it?</p>\n</blockquote>\n</aside>\n<p>From your initial post I take it you already <a href=\"https://www.ilastik.org/documentation/basics/installation#controlling-cpu-and-ram-resources\">set the number of threads for ilastik</a> via the config file\u2026 Even if you did, the speedup <a href=\"https://github.com/ilastik/ilastik/issues/1458\">you get beyond 8 threads is not linear, last time I checked</a>, but with your available amount of memory definitely worth a try.</p>\n<p>Another way to speed up the computation would be to start two instances of ilastik (say, with 7 cores each) to work on different images (if having multiple images available is a situation that occurs for you).</p>\n<p>Going beyond that, <a href=\"https://www.ilastik.org/documentation/basics/headless#running-distributed-ilastik-via-mpi-potentially-trough-slurm\">there is the option to run ilastik distributed on a cluster using MPI</a>.</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"3\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>Is it possible to have a single self-contained .ilp file, because the raw data for training is smaller than the stacks. In that case I can keep a single file for each trained model and not the data files separately.</p>\n</blockquote>\n</aside>\n<p>You can <a href=\"https://www.ilastik.org/documentation/basics/dataselection#properties\">copy the training data to your project file in the data selection applet</a>, by changing the <em>storage</em> property to <em>Copied to project file</em>. After that you can move your project file to different machines/operating systems and can always open/modify/work with it.</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"3\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>If the probability map is renormalized to 0\u2026255 does the threshold of 0.5(or other value) still remain valid?</p>\n</blockquote>\n</aside>\n<p>Threshold would be <code>128</code> then.</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"3\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>Perhaps a question, does the amount of annotations affect the computation time? or is it just the amount of features selected?</p>\n</blockquote>\n</aside>\n<p>The number of annotated pixels has an influence on training time. If you annotate a lot, training tends to slow down quite a bit. Prediction time is not affected by it. This indeed is affected by the number of features you select.</p>\n<aside class=\"quote no-group\" data-username=\"ujjwal\" data-post=\"3\" data-topic=\"78696\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/u/e0b2c6/40.png\" class=\"avatar\"> Ujjwal:</div>\n<blockquote>\n<p>Currently for my approach, Separating from the background it required only 4 features in stage 1 and then for stage 2 it was 17-19 features. If I select do the way you describe, does it then mean 17-19 features in each stage? The ram isn\u2019t the biggest issue, it\u2019s the time taken to process the 1500 image stack (currently about 5hrs) Is this around the time that is normally expected?</p>\n</blockquote>\n</aside>\n<p>I am assuming that your image has only a single channel here. So when you select 4 features in the first stage, you have to compute (let\u2019s oversimplify here and assume that all the features produce only a single channel) 4 additional channels. In the second stage, the predictions from the first round are stacked on top of the image. So if you had 2 classes in the first round, you\u2019ll have three channels going into the second round (1 channel raw data, 2 channels probabilities).  The features you select in the second round, are computed for each of those channels. So if you select 20 features, there will be 60 additional channels computed. So choosing many in the second round comes at quite the computational cost.<br>\nNot having seen your data you could potentially speed up the computation quite a bit if you use 2D features instead of 3D ones - this can make sense if the data resolution is very anisotropic.<br>\nilastik is giving quite some output during batch processing. You might gain some insight inspecting the log file (<em>Settings \u2192 Open Log Folder</em>).</p>\n<p>Hope this helps in getting your processing a  little faster\u2026</p>\n<p>Cheers<br>\nDominik</p>"], "60265": ["<p>Dear all,<br>\nI would really appreciate any help with the following.</p>\n<p>I would like to trial deepcell segmentation in our image analysis workflow, I got good results using the website (<a href=\"https://www.deepcell.org/predict\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DeepCell</a>) but would now like to try the QuPath integration.<br>\nQuPath (0.3.0) on windows.<br>\nI have tried using Pete\u2019s <a href=\"https://gist.github.com/petebankhead/db8548a0112bad089492061bf8046430\" rel=\"noopener nofollow ugc\">demo script</a>  the only change I make is to initialise the channels to match my nuclear and cytoplasmic markers i.e. int<span class=\"chcklst-box fa fa-square-o fa-fw\"></span> channels = [7,6]  -with channel 7 being DAPI.</p>\n<p>However every time I run it I get the following response</p>\n<pre><code class=\"lang-auto\">ERROR: IOException: Unexpected code Response{protocol=h2, code=400, message=, url=https://deepcell.org/api/predict/}\n\nERROR: org.vanvalenlab.KioskHttpClient.sendHttpRequest(KioskHttpClient.java:83)\n    org.vanvalenlab.KioskHttpClient.createJob(KioskHttpClient.java:214)\n    org.vanvalenlab.KioskJob.create(KioskJob.java:95)\n    org.vanvalenlab.KioskJob$create.call(Unknown Source)\n    org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)\n    Script3.runJob(Script3.groovy:158)\n    Script3$runJob.callStatic(Unknown Source)\n    org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:217)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:240)\n    Script3.detectCells(Script3.groovy:124)\n    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    java.base/java.lang.reflect.Method.invoke(Unknown Source)\n    org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)\n    groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\n    org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:351)\n    org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:61)\n    org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)\n    org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n    Script3$_run_closure1.doCall(Script3.groovy:79)\n    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    java.base/java.lang.reflect.Method.invoke(Unknown Source)\n    org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)\n    groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\n    org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:263)\n    groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)\n    groovy.lang.Closure.call(Closure.java:412)\n    groovy.lang.Closure.call(Closure.java:406)\n    java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    java.base/java.lang.Thread.run(Unknown Source)\n\n</code></pre>\n<p>That would suggest there is an issue with the request but I am not sure what to try changing.<br>\nThank you in advance for any help</p>", "<aside class=\"quote no-group\" data-username=\"ShadySmith\" data-post=\"1\" data-topic=\"60265\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/shadysmith/40/37834_2.png\" class=\"avatar\"> ShadySmith:</div>\n<blockquote>\n<p>channel 7 being DAPI.</p>\n</blockquote>\n</aside>\n<p>Quick check, that means you have 8 channels at least, and the 8th channel is DAPI?</p>\n<p>Nevermind, I get the same result when running the script on a standard demo image. Perhaps <a class=\"mention\" href=\"/u/noah_greenwald\">@Noah_Greenwald</a> will have some idea. I will go ahead and link to this topic on their issues page.</p>\n<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> or <a class=\"mention\" href=\"/u/melvingelbard\">@melvingelbard</a> might have some idea if some part of QuPath\u2019s communication with the server might have changed.</p>\n<p>Similar but different error seen here: <a href=\"https://forum.image.sc/t/deepcell-tiling/46105\" class=\"inline-onebox\">DeepCell - Tiling</a><br>\nIndicating it is probably something <em>related</em> to accessing the server.</p>", "<p>Thank you for the quick response, I noticed the previous topic you referred to so tried different ROI sizes but I still get the same http 400 response.</p>\n<p>Thanks again</p>\n<p>PS I am using a 10 channel Opal/polaris image.</p>", "<p>It appears DeepCell may have changed substantially since that script was written.</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/vanvalenlab/kiosk-imageJ-plugin/issues/39#issuecomment-975790061\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/vanvalenlab/kiosk-imageJ-plugin/issues/39#issuecomment-975790061\" target=\"_blank\" rel=\"noopener\">github.com/vanvalenlab/kiosk-imageJ-plugin</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/vanvalenlab/kiosk-imageJ-plugin/issues/39#issuecomment-975790061\" target=\"_blank\" rel=\"noopener\">Improved QuPath integration</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-03-12\" data-time=\"17:35:41\" data-timezone=\"UTC\">05:35PM - 12 Mar 21 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/ngreenwald\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"ngreenwald\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/c/1c7556445e87099276aa3f2776fe933f786229dd.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          ngreenwald\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          documentation\n        </span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          enhancement\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Pete put together the following script for improved QuPath usability: https://gi<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">st.github.com/petebankhead/db8548a0112bad089492061bf8046430\n\nOnce we figure out next steps for QuPath, we can decide what we want to do with this script.</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nI guess neither job type exists, and there are now different requirements for the data shape, based on the error messages.</p>", "<p>So you need to pass two channels, even if you only have one, and it should now return nuclear and cytoplasmic outlines, which the script might not handle. I am not really sure how to change the script to handle multiple ROIs, or how they are returned. Might be up to <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> if/when he has time. Hopefully you can make do with either/or cell/nuclear outlines in the meantime.</p>", "<p>Thank you for the follow up</p>", "<p>FYI I changed the jobtype to \u201cmesmer\u201d as per the discussion and left everything else the same and it worked perfectly for me!<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/698bb686e04b712dccd82a9e571ae17c1b893699.png\" alt=\"image\" data-base62-sha1=\"f3Ht1pCGWDrVtpOPgCVw88qStKh\" width=\"476\" height=\"466\"></p>", "<p>Yep, it returns results, but it isn\u2019t really working as intended at the moment.</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"2\" data-topic=\"60265\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> Research_Associate:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a></p>\n</blockquote>\n</aside>\n<p>Dear All<br>\nI was able to run the Deepcell script that Pete wrote on a 16-channels image with the only modification of setting the DAPI and Membrane channels  as:<br>\n<strong>int[] channels = [0,13] // Optionally specify channels to extract (0-based).</strong><br>\nBut when I look at or export the measurements, I basically get only the results for those 2 channels (DAPI and Membrane).<br>\nI need help extracting the measurements for all channels.<br>\nSincerely.<br>\nKhoi</p>", "<p>I don\u2019t remember how the DeepCell script generates measurements, but I would start by using QuPath\u2019s built in <em>Add intensity measurements</em>, or a script.</p><aside class=\"quote quote-modified\" data-post=\"36\" data-topic=\"27906\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/36\">QuPath Intro: Choose your own analysis(adventure)</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Adding Features\nThe Calculate features menu has been split into the Calculate features and Spatial analysis menus in 0.2.0. \n<details><summary>Add Intensity features</summary>The largest and most flexible feature generator summarizes intensity data in or around a given detection. You can select a single object (cell) and run this, or everything, or an annotation. A simple use would be to create a full image annotation and collect the mean value of all channels/deconvolutions for all of your images, and take a look at the r\u2026</details>\n  </blockquote>\n</aside>\n", "<p>Thanks, I was able to generate the measurements I wanted.</p>", "<p>I am using QuPath 3.0 on whole slide fluorescent images with 3 channels + DAPI. I want to use ideally the other 3 markers, but I\u2019ll settle for just panCK as a membrane marker for segmentation. With the current expansion of the nucleus to create the cell boundaries, panCK(green)+ tumor cells with HLA-A (red) and/or HLA-DR (purple)+ non tumor cells next to them are classified as positive when they actually lack expression or non-tumor cells inermixed within the tumor are classified as tumor cells. See illustration bellow (white).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b3a03aa151ff421523ec466f6b43e1fce0777e20.jpeg\" data-download-href=\"/uploads/short-url/pD2KTg87ExhzWoP33E8n7rBKSmQ.jpeg?dl=1\" title=\"HLADRtouching\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b3a03aa151ff421523ec466f6b43e1fce0777e20_2_546x500.jpeg\" alt=\"HLADRtouching\" data-base62-sha1=\"pD2KTg87ExhzWoP33E8n7rBKSmQ\" width=\"546\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b3a03aa151ff421523ec466f6b43e1fce0777e20_2_546x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b3a03aa151ff421523ec466f6b43e1fce0777e20_2_819x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b3a03aa151ff421523ec466f6b43e1fce0777e20.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b3a03aa151ff421523ec466f6b43e1fce0777e20_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">HLADRtouching</span><span class=\"informations\">918\u00d7840 158 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I tried running Pete\u2019s improved QuPath integration with DeepCell referenced in this post only changing jobtype to mesmer and adding the channels (DAPI and panCK).</p>\n<ul>\n<li>\n</li>\n<li>Set the main parameters<br>\n*/<br>\ndef options = new KioskOptions(jobType: \u201cmesmer\u201d) // Specify jobType (may be multiplex or segmentation)<br>\nboolean includeMeasurements = true // Optionally include default shape/intensity measurements<br>\nint<span class=\"chcklst-box fa fa-square-o fa-fw\"></span> channels = [1,2] // Optionally specify channels to extract (0-based)<br>\ndouble requestedPixelSize = -1 // Optionally specify the target pixel size for resizing the image<br>\nint nThreads = PathPrefs.numCommandThreadsProperty().value // Do multithreading</li>\n</ul>\n<p>I run into the following error:</p>\n<p><em>INFO: Waiting for 1 task(s) to complete</em><br>\n<em>ERROR: KioskJobFailedException: Traceback (most recent call last):</em></p>\n<ul>\n<li>File \u201c/usr/src/app/redis_consumer/consumers/base_consumer.py\u201d, line 201, in consume*</li>\n<li>status = self._consume(redis_hash)*</li>\n<li>File \u201c/usr/src/app/redis_consumer/consumers/mesmer_consumer.py\u201d, line 136, in _consume*</li>\n<li>results = app.predict(image, batch_size=batch_size,*</li>\n<li>File \u201c/usr/local/lib/python3.8/site-packages/deepcell/applications/mesmer.py\u201d, line 305, in predict*</li>\n<li>return self._predict_segmentation(image,*</li>\n<li>File \u201c/usr/local/lib/python3.8/site-packages/deepcell/applications/application.py\u201d, line 442, in _predict_segmentation*</li>\n<li>output_images = self._run_model(*</li>\n<li>File \u201c/usr/local/lib/python3.8/site-packages/deepcell/applications/application.py\u201d, line 381, in _run_model*</li>\n<li>output_tiles = self._batch_predict(tiles=tiles, batch_size=batch_size)*</li>\n<li>File \u201c/usr/local/lib/python3.8/site-packages/deepcell/applications/application.py\u201d, line 338, in _batch_predict*</li>\n<li>batch_outputs = self.model.predict(batch_inputs, batch_size=batch_size)*</li>\n<li>File \u201c/usr/src/app/redis_consumer/grpc_clients.py\u201d, line 384, in predict*</li>\n<li>output = self.send_grpc(inputs)*</li>\n<li>File \u201c/usr/src/app/redis_consumer/grpc_clients.py\u201d, line 335, in send_grpc*</li>\n<li>prediction = self._client.predict(req_data, settings.GRPC_TIMEOUT)*</li>\n<li>File \u201c/usr/src/app/redis_consumer/grpc_clients.py\u201d, line 260, in predict*</li>\n<li>response = self._retry_grpc(request, request_timeout)*</li>\n<li>File \u201c/usr/src/app/redis_consumer/grpc_clients.py\u201d, line 241, in _retry_grpc*</li>\n<li>raise err*</li>\n<li>File \u201c/usr/src/app/redis_consumer/grpc_clients.py\u201d, line 208, in _retry_grpc*</li>\n<li>response = api_call(request, timeout=request_timeout)*</li>\n<li>File \u201c/usr/local/lib/python3.8/site-packages/grpc/_channel.py\u201d, line 923, in <strong>call</strong>*</li>\n<li>return _end_unary_response_blocking(state, call, False, None)*</li>\n<li>File \u201c/usr/local/lib/python3.8/site-packages/grpc/_channel.py\u201d, line 826, in _end_unary_response_blocking*</li>\n<li>raise _InactiveRpcError(state)*<br>\n<em>grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:</em>\n</li>\n<li>status = StatusCode.FAILED_PRECONDITION*</li>\n<li>details = \u201cTensors with name \u2018serving_default_input_0:0\u2019 from different tasks have different shapes and padding is turned off.Set pad_variable_length_inputs to true, or ensure that all tensors with the same namehave equal dimensions starting with the first dim.\u201d*</li>\n<li>debug_error_string = \u201c{\u201ccreated\u201d:\u201d<span class=\"mention\">@1648683435.629539673</span>\",\u201cdescription\u201d:\u201cError received from peer ipv4:10.84.10.36:8500\u201d,\u201cfile\u201d:\u201csrc/core/lib/surface/call.cc\u201d,\u201cfile_line\u201d:1062,\u201cgrpc_message\u201d:\u201cTensors with name \u2018serving_default_input_0:0\u2019 from different tasks have different shapes and padding is turned off.Set pad_variable_length_inputs to true, or ensure that all tensors with the same namehave equal dimensions starting with the first dim.\u201d,\u201cgrpc_status\u201d:9}\"*<br>\n<em>&gt;</em>\n</li>\n</ul>\n<p><em>ERROR: java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</em></p>\n<ul>\n<li>java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)*</li>\n<li>java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)*</li>\n<li>java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)*</li>\n<li>java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)*</li>\n<li>org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:72)*</li>\n<li>org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105)*</li>\n<li>org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:59)*</li>\n<li>org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:263)*</li>\n<li>org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:277)*</li>\n<li>Script3.runJob(Script3.groovy:162)*</li>\n<li>Script3$runJob.callStatic(Unknown Source)*</li>\n<li>org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55)*</li>\n<li>org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:217)*</li>\n<li>org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:240)*</li>\n<li>Script3.detectCells(Script3.groovy:124)*</li>\n<li>java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)*</li>\n<li>java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)*</li>\n<li>java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)*</li>\n<li>java.base/java.lang.reflect.Method.invoke(Unknown Source)*</li>\n<li>org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)*</li>\n<li>groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)*</li>\n<li>org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:362)*</li>\n<li>org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:61)*</li>\n<li>org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)*</li>\n<li>org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)*</li>\n<li>Script3$_run_closure1.doCall(Script3.groovy:79)*</li>\n<li>java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)*</li>\n<li>java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)*</li>\n<li>java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)*</li>\n<li>java.base/java.lang.reflect.Method.invoke(Unknown Source)*</li>\n<li>org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)*</li>\n<li>groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)*</li>\n<li>org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)*</li>\n<li>groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)*</li>\n<li>groovy.lang.Closure.call(Closure.java:412)*</li>\n<li>groovy.lang.Closure.call(Closure.java:406)*</li>\n<li>java.base/java.util.concurrent.FutureTask.run(Unknown Source)*</li>\n<li>java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)*</li>\n<li>java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)*</li>\n<li>java.base/java.lang.Thread.run(Unknown Source)*</li>\n</ul>\n<p>Any help or advise will be appreciated.</p>\n<p>Thanks,</p>\n<p>Paula</p>", "<p>Hopefully someone gets back to you, but in the mean time, I would recommend trying the nuclear channel mean for cells like that, it usually works well. Alternatively, adjust the cell expansion and use a more complex classifier - decision trees or similar, to find a particularly problematic cell type.</p>", "<p><a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> What do mean by \u201cnuclear channel mean\u201d? Just DAPI on cell detection?<br>\nI have played with cell expansion, the problem is that if I make the cells smaller I miss some of the expression on membranes of bigger tumor cells.<br>\nI do use an object classifier that I train using problematic and non-problematic regions. This works fine (although could be better) with whole sections (biopsies or resections) form a single tumor type (TNBC). However in this case I have 88 TMAs containing all types of breast cancer which can look very different histologically, so I can seem to train a \u201cone-size-fits-all\u201d classifier.  I either miss-classify tumors with low CK expression or have the problem above, or a little of both.</p>", "<p>Sorry, \u201cnuclear mean\u201d. Early here. <em>Most</em> cytoplasmic staining can be detected using the nuclear channel of the cell rather than in the cell expansion.</p>\n<p>It is also entirely possible you will not be able to use a single setting or classifier for widely varying samples. Classifiers are intended for use with samples that are either similar in staining and intensity (for a given outcome) or data that has already been normalized in some way.</p>\n<p>There may also be normalization options available for some types of classification. I know the pixel classifier has it, which can be used as a cell classifier.</p>", "<p>Hi Paula,</p>\n<p>The DeepCell Kiosk was down for a few hours yesterday, but it should be operational now. I suspect that was the cause of <code>ERROR: KioskJobFailedException</code>. Can you let me know if you continue to have problems with DeepCell predictions?</p>\n<p>All the best,<br>\nMorgan</p>", "<p><a class=\"mention\" href=\"/u/msschwartz21\">@msschwartz21</a> tried it 20 min ago and it worked. It did a weird job, it seemed to be using both DAPI and CK as nuclear (probably on me), so I run it again with just the DAPI and got a  KioskJobFailedException error again.</p>\n<p>INFO: Waiting for 1 task(s) to complete<br>\nERROR: KioskJobFailedException: Traceback (most recent call last):<br>\nFile \u201c/usr/src/app/redis_consumer/consumers/base_consumer.py\u201d, line 201, in consume<br>\nstatus = self._consume(redis_hash)<br>\nFile \u201c/usr/src/app/redis_consumer/consumers/mesmer_consumer.py\u201d, line 124, in _consume<br>\nimage = self.validate_model_input(image, model_name, model_version,<br>\nFile \u201c/usr/src/app/redis_consumer/consumers/base_consumer.py\u201d, line 300, in validate_model_input<br>\nraise ValueError(errtext)<br>\nValueError: Invalid image shape: [(1, 1209, 1457)]. The mesmer job expects images of shape[(-1, 256, 256, 2)]</p>\n<p>ERROR: java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)<br>\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)<br>\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)<br>\njava.base/java.lang.reflect.Constructor.newInstance(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:72)<br>\norg.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:59)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:263)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:277)<br>\nScript7.runJob(Script7.groovy:162)<br>\nScript7$runJob.callStatic(Unknown Source)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:217)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:240)<br>\nScript7.detectCells(Script7.groovy:124)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)<br>\norg.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:362)<br>\norg.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:61)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)<br>\nScript7$_run_closure1.doCall(Script7.groovy:79)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)<br>\norg.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)<br>\ngroovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)<br>\ngroovy.lang.Closure.call(Closure.java:412)<br>\ngroovy.lang.Closure.call(Closure.java:406)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>", "<p>I think certain job types require multiple channels, that came up in another deepcell thread.</p>", "<p>Mesmer expects a two channel input: nuclear and whole cell. If you want to run predictions just on nuclear, you need to give it a dummy channel for whole cell which can consist of all zeros. The relevant part of the error message is quoted below and points out the incorrect shape of the data. In this case, the number of channels is the cause of the error.</p>\n<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"17\" data-topic=\"60265\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> paulage:</div>\n<blockquote>\n<p>ValueError: Invalid image shape: [(1, 1209, 1457)]. The mesmer job expects images of shape[(-1, 256, 256, 2)]</p>\n</blockquote>\n</aside>", "<p>I see, my ROI was also too big. I reduced the size and added the second channel and it\u2019s running without errors. However it\u2019s not doing a good job.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/92ffa78933e9900f27307fdace632ca0724e696c.jpeg\" data-download-href=\"/uploads/short-url/kYprLjdfETS9M06sSniinvx8nTe.jpeg?dl=1\" title=\"Mesmer\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/2/92ffa78933e9900f27307fdace632ca0724e696c_2_647x500.jpeg\" alt=\"Mesmer\" data-base62-sha1=\"kYprLjdfETS9M06sSniinvx8nTe\" width=\"647\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/2/92ffa78933e9900f27307fdace632ca0724e696c_2_647x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/92ffa78933e9900f27307fdace632ca0724e696c.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/92ffa78933e9900f27307fdace632ca0724e696c.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/2/92ffa78933e9900f27307fdace632ca0724e696c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Mesmer</span><span class=\"informations\">812\u00d7627 191 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It does a better job on the web page or on ImageJ. Any input would be appreciated.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/te0AU4RRV6Rqo64g9LbfVgcLM2q.tif\">APT30III1_CBCS3_HLAA-AF555_CK-AF488_HLADR-AF647_9-17-21_c1,2_feature_0.tif</a> (1019.1 KB)</p>\n<p>One more question, how do you go from this to segmenting a whole slide image?</p>\n<p>Thanks,</p>\n<p>Paula</p>"], "78698": ["<p>Hi all,</p>\n<p>I am currently working on my thesis about pose estimation using deep learning algorithms. Now, I am at a stage that I need obtain a comparison between software tools DeepLabCut and SLEAP on the same set of datasets. For the purpose, I need to manipulate the loss functions implemented in DeepLabCut and change it to become the same as in SLEAP, to get comparison of performances under that circumstance. I want to ask, from which portion of DeepLabCut\u2019s source code can I play with the implemented loss functions?</p>\n<p>Best regards,<br>\n\u00d6zg\u00fcr</p>", "<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py\" target=\"_blank\" rel=\"noopener nofollow ugc\">DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py</a></h4>\n\n\n      <pre><code class=\"lang-py\">#\n# DeepLabCut Toolbox (deeplabcut.org)\n# \u00a9 A. &amp; M.W. Mathis Labs\n# https://github.com/DeepLabCut/DeepLabCut\n#\n# Please see AUTHORS for contributors.\n# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS\n#\n# Licensed under GNU Lesser General Public License v3.0\n#\nimport warnings\n\n\nclass PoseNetFactory:\n    _nets = dict()\n\n    @classmethod\n    def register(cls, type_):\n        def wrapper(net):\n            if type_ in cls._nets:\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/nnets/factory.py\" target=\"_blank\" rel=\"noopener nofollow ugc\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>losses are an output of the <code>create</code> function of the <code>PoseNetFactory</code> class.</p>\n<p>Usage is here: <a href=\"https://github.com/DeepLabCut/DeepLabCut/blob/6c429271b338cdd010348f640b30f62cfc986edc/deeplabcut/pose_estimation_tensorflow/core/train.py#L173\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DeepLabCut/train.py at 6c429271b338cdd010348f640b30f62cfc986edc \u00b7 DeepLabCut/DeepLabCut \u00b7 GitHub</a></p>"], "76654": ["<p>Hello,</p>\n<p>I am an undergraduate student working on retraining DeepLabCut-Live to track a person wearing an exoskeleton for exoskeleton fit evaluation. The mobile net architecture in DeepLabCut has given me faster inferences in real-time using DLC-Live. However, I would like to initialize model training using the pre-trained full_human DLC model, which is resnet 101. Is there a way to convert the full_human model to mobile net or is there an existing DLC pre-trained human model using the mobile net architecture? I would greatly appreciate some assistance with this matter. Thank you very much.</p>", "<p>You cannot convert a model to a different architecture. What you can do is take the <a href=\"http://human-pose.mpi-inf.mpg.de/#download\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">MPII Human Pose Database</a> dataset and train your own model with desired backbone</p>", "<p>If possible, can I receive some guidance on how to train my own model with the desired backbone? How would I train a mobile net architecture from scratch? Thank you.</p>", "<p>You can use DLC MobileNet backbone, just make MPII dataset into accepted format. Why do you want to use full_human anyway? It won\u2019t perform better than your own model (specific &gt; general)</p>", "<p>Thank you for the advice. I am working on training a DLC model on MPII. However, I am having trouble finding the annotated MPII images to place in the labeled-data folder. Would you be able to give some guidance on where I can find the annotated images? Thank you.</p>", "<p>There is a download section on the website I linked. You should download images and annotations and then from annotations take out the pose estimation part of them (the joints) to create a table like the one in CollectedData file in a DLC project.</p>", "<p>Yes I have done that, but DLC requires annotated images for training and I cannot find the annotated MPII images. I planned to put the annotated images in the labeled data folder, but I am not sure where I can download the annotated images from. I thought it is not possible to train a DLC model without annotated images, so I am not sure how to work around this. Thank you.</p>", "<p>The annotations match the images. You just have to take them out of the <code>.mat</code> file and create your own <code>CollectedData</code> file by hand assigning image names to annotations</p>", "<p>I understand that part, but don\u2019t I need the annotated images in the _labeled folder that is located in labeled-data? I thought you needed the csv file and annotated images for DLC training since users are usually required to annotate frames which are then stored as labeled data. Is it possible to simply train using the csv only and without seeing the annotations on the images? Thank you.</p>", "<p>The images are just images. Annoations are stored in a table in the <code>.h5</code> file. When you label images in DLC you\u2019re not changing the images in any way, you\u2019re just storing information on where what is in a separate file</p>", "<p>Okay I understand that part now thank you. DLC has annotations stored in csv and h5 files, so do I need to convert the MPII annotations to both of these formats? In addition, my conversion of the .mat file to csv resulted in a csv that has a format slightly different from the csv annotations format of DLC. The information (coordinates of annotations and image names are organized in the same manner) is not organized too differently from DLC, but some of the headings are different. Will DLC training work with this slightly different format or do I have to convert it to the exact format DLC outputs in the labeled-data subdirectory? Thank you very much.</p>", "<p>The data structure of the mat file is explained on the website - you should load speicifc parts of it that are related to keypoint annotation and then save as <code>.csv</code> and <code>.h5</code> that look identically to what DLC would create</p>", "<p>Thank you for the help!</p>", "<p>Hi, I am still working on training a DLC model using MPII dataset. However, when using deeplabcut.create_training_dataset I am getting a ValueError that states that the dimensions of the array across axis 0 is not the same; index 0 has a dimension of 16 while index 1 has a dimension of 6. Am I getting this error because I have a path to the folder containing the MPII images and do not have paths to videos in my config.yaml file? Would you be able to suggest any solutions to resolve this error? Thank you very much.</p>", "<p>Can you post the full traceback?</p>", "<p>I have uploaded an image of the deeplabcut command I entered and the error I received. Thank you!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/0/c0ef333e964cad6910897c7f99a6b8ee55208694.png\" data-download-href=\"/uploads/short-url/rwM9ODEbyQ7rqNcBrsHq6YG027W.png?dl=1\" title=\"Screenshot 2023-02-27 at 8.17.16 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0ef333e964cad6910897c7f99a6b8ee55208694_2_690x131.png\" alt=\"Screenshot 2023-02-27 at 8.17.16 PM\" data-base62-sha1=\"rwM9ODEbyQ7rqNcBrsHq6YG027W\" width=\"690\" height=\"131\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0ef333e964cad6910897c7f99a6b8ee55208694_2_690x131.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0ef333e964cad6910897c7f99a6b8ee55208694_2_1035x196.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0ef333e964cad6910897c7f99a6b8ee55208694_2_1380x262.png 2x\" data-dominant-color=\"F0F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-27 at 8.17.16 PM</span><span class=\"informations\">2244\u00d7427 194 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>You should make up a path to a video matching the name of the folder in labeled data. Does you annotation data have proper header and index?</p>", "<p>I am a little unclear about the part regarding making up a path to a video matching the name of the folder in labeled data. The folders in labeled data are MPII and MPII_labeled. Does this mean I would have to make name an arbitrary video MPII and then set the path of that video to my video path in config.yaml? I have attached the annotations csv file which I believe should be formatted appropriately. Thank you.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/3vvfEP11Q1hs5Nvh1sDHc5hrFUw.csv\">CollectedData_nishat-3.csv</a> (3.0 MB)</p>", "<p>You can just make up a path. The video doesn\u2019t have to exist. So something like C:\\data\\MPII.mp4, add it to config, specify resolution and that\u2019s it.</p>\n<p>Your index column should be a mutlindex split based on <code>\\</code>. Bodyparts really shouldn\u2019t have whitespaces in the names. Basically no strings in python should have whitespaces if they are meant to be read, sliced etc. Do bodypart names match the ones in the config?</p>", "<p>Thank you for the clarification regarding the video. I have also edited the csv and config files to have the bodyparts named with an _ instead of white space. The bodyparts are the same in the config and csv. I am a little confused about the index column. What does it mean for the index column to be a multi index split based on ? Thank you.</p>"], "78702": ["<p>Hello.<br>\nIt\u2019s my first time using DeepLabCut. Everytime I try to create a new project i receive the following error:</p>\n<p>DirectWrite: CreateFontFaceFromHDC() failed (Indica um erro em um arquivo de entrada, como um arquivo de fonte.) for QFontDef(Family=\u201c8514oem\u201d, pointsize=9, pixelsize=20, styleHint=5, weight=400, stretch=100, hintingPreference=0) LOGFONT(\u201c8514oem\u201d, lfWidth=0, lfHeight=-20) dpi=144</p>\n<p>Couldn\u2019t find any solution for this.</p>\n<p>Thank you in advance.</p>", "<p>Copy of the answer I gave on gitter:<br>\nSeems like a PyQT issue. But maybe let\u2019s start with the basics. Are you turning on the terminal as administrator? What happens if you run without going into ipython with <code>python -m deeplabcut</code> ?</p>"], "78704": ["<p>Hello,</p>\n<p>We encounter issues while importing specific .tif images on OMERO.<br>\nTiff Images are generated by a Vilber Lourmat machine (software FusionCapt Advanced Solo 7) and there are actually two types of tiffs we can use to export:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/3/33ed950f53aae4e061ce65edefb504a08f9e1a1a.png\" alt=\"image\" data-base62-sha1=\"7pnmR5dX25irzpvYgMoDcvdIJKO\" width=\"645\" height=\"66\"></p>\n<p>If images are encoded using <code>16 bits Tagged Image File Format</code>, then no images can be open by bioformats. It throws the following exception when we try to import them on OMERO, using the latest version of OMERO.insight.</p>\n<pre><code class=\"lang-auto\">java.io.EOFException: Attempting to read beyond end of file.\n\tat loci.common.NIOFileHandle.readInt(NIOFileHandle.java:415)\n\tat loci.common.RandomAccessInputStream.readInt(RandomAccessInputStream.java:564)\n\tat loci.common.RandomAccessInputStream.readUnsignedInt(RandomAccessInputStream.java:574)\n\tat loci.formats.tiff.TiffParser.getIFDValue(TiffParser.java:644)\n\tat loci.formats.tiff.TiffParser.fillInIFD(TiffParser.java:526)\n\tat loci.formats.in.MinimalTiffReader.initFile(MinimalTiffReader.java:483)\n\tat loci.formats.in.BaseTiffReader.initFile(BaseTiffReader.java:609)\n\tat loci.formats.FormatReader.setId(FormatReader.java:1443)\n\tat loci.formats.in.TiffDelegateReader.setId(TiffDelegateReader.java:92)\n\tat loci.formats.ImageReader.setId(ImageReader.java:849)\n\tat ome.formats.importer.OMEROWrapper$4.setId(OMEROWrapper.java:167)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat loci.formats.ChannelFiller.setId(ChannelFiller.java:234)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat loci.formats.ChannelSeparator.setId(ChannelSeparator.java:293)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat loci.formats.Memoizer.setId(Memoizer.java:662)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat ome.formats.importer.ImportCandidates.singleFile(ImportCandidates.java:427)\n\tat ome.formats.importer.ImportCandidates.handleFile(ImportCandidates.java:576)\n\tat ome.formats.importer.ImportCandidates.execute(ImportCandidates.java:384)\n\tat ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:222)\n\tat ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:174)\n\tat org.openmicroscopy.shoola.env.data.OMEROGateway.getImportCandidates(OMEROGateway.java:5894)\n\tat org.openmicroscopy.shoola.env.data.OmeroImageServiceImpl.importFile(OmeroImageServiceImpl.java:1104)\n\tat org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.importFile(ImagesImporter.java:103)\n\tat org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.access$000(ImagesImporter.java:49)\n\tat org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter$1.doCall(ImagesImporter.java:127)\n\tat org.openmicroscopy.shoola.env.data.views.BatchCall.doStep(BatchCall.java:144)\n\tat org.openmicroscopy.shoola.util.concur.tasks.CompositeTask.doStep(CompositeTask.java:226)\n\tat org.openmicroscopy.shoola.env.data.views.CompositeBatchCall.doStep(CompositeBatchCall.java:126)\n\tat org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.exec(ExecCommand.java:165)\n\tat org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.run(ExecCommand.java:276)\n\tat org.openmicroscopy.shoola.util.concur.tasks.AsyncProcessor$Runner.run(AsyncProcessor.java:91)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.nio.BufferUnderflowException\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:509)\n\tat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:373)\n\tat loci.common.NIOFileHandle.readInt(NIOFileHandle.java:413)\n\t... 34 more\n\n\tat org.openmicroscopy.shoola.env.data.util.Status.update(Status.java:608)\n\tat ome.formats.importer.ImportCandidates.safeUpdate(ImportCandidates.java:536)\n\tat ome.formats.importer.ImportCandidates.singleFile(ImportCandidates.java:471)\n\tat ome.formats.importer.ImportCandidates.handleFile(ImportCandidates.java:576)\n\tat ome.formats.importer.ImportCandidates.execute(ImportCandidates.java:384)\n\tat ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:222)\n\tat ome.formats.importer.ImportCandidates.&lt;init&gt;(ImportCandidates.java:174)\n\tat org.openmicroscopy.shoola.env.data.OMEROGateway.getImportCandidates(OMEROGateway.java:5894)\n\tat org.openmicroscopy.shoola.env.data.OmeroImageServiceImpl.importFile(OmeroImageServiceImpl.java:1104)\n\tat org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.importFile(ImagesImporter.java:103)\n\tat org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter.access$000(ImagesImporter.java:49)\n\tat org.openmicroscopy.shoola.env.data.views.calls.ImagesImporter$1.doCall(ImagesImporter.java:127)\n\tat org.openmicroscopy.shoola.env.data.views.BatchCall.doStep(BatchCall.java:144)\n\tat org.openmicroscopy.shoola.util.concur.tasks.CompositeTask.doStep(CompositeTask.java:226)\n\tat org.openmicroscopy.shoola.env.data.views.CompositeBatchCall.doStep(CompositeBatchCall.java:126)\n\tat org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.exec(ExecCommand.java:165)\n\tat org.openmicroscopy.shoola.util.concur.tasks.ExecCommand.run(ExecCommand.java:276)\n\tat org.openmicroscopy.shoola.util.concur.tasks.AsyncProcessor$Runner.run(AsyncProcessor.java:91)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.EOFException: Attempting to read beyond end of file.\n\tat loci.common.NIOFileHandle.readInt(NIOFileHandle.java:415)\n\tat loci.common.RandomAccessInputStream.readInt(RandomAccessInputStream.java:564)\n\tat loci.common.RandomAccessInputStream.readUnsignedInt(RandomAccessInputStream.java:574)\n\tat loci.formats.tiff.TiffParser.getIFDValue(TiffParser.java:644)\n\tat loci.formats.tiff.TiffParser.fillInIFD(TiffParser.java:526)\n\tat loci.formats.in.MinimalTiffReader.initFile(MinimalTiffReader.java:483)\n\tat loci.formats.in.BaseTiffReader.initFile(BaseTiffReader.java:609)\n\tat loci.formats.FormatReader.setId(FormatReader.java:1443)\n\tat loci.formats.in.TiffDelegateReader.setId(TiffDelegateReader.java:92)\n\tat loci.formats.ImageReader.setId(ImageReader.java:849)\n\tat ome.formats.importer.OMEROWrapper$4.setId(OMEROWrapper.java:167)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat loci.formats.ChannelFiller.setId(ChannelFiller.java:234)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat loci.formats.ChannelSeparator.setId(ChannelSeparator.java:293)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat loci.formats.Memoizer.setId(Memoizer.java:662)\n\tat loci.formats.ReaderWrapper.setId(ReaderWrapper.java:650)\n\tat ome.formats.importer.ImportCandidates.singleFile(ImportCandidates.java:427)\n\t... 16 more\nCaused by: java.nio.BufferUnderflowException\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:509)\n\tat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:373)\n\tat loci.common.NIOFileHandle.readInt(NIOFileHandle.java:413)\n\t... 34 more\n</code></pre>\n<p>If we now encode them using <code>Compatibility Plus 16 bit Tagged Image File Format</code>, then some images can be imported on OMERO but some other not and we don\u2019t know why (all images have been taken with the same machine). For images which cannot be imported, the exception is the same.</p>\n<p>I attach here a zip file with a couple of images to that you can test and try to reproduce the bug.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/eOUPEP6nm63KpsrkGzIHT1VKNtH.zip\">images.zip</a> (11.9 MB)</p>\n<p>Do you know where it is coming from and what we can do to import those files on OMERO ?<br>\nThanks for your help,</p>\n<p>R\u00e9my.</p>", "<p>None of those images conforms to the TIFF specification, even the \u201cworking\u201d one. See for example the warnings of libtiff\u2019s <code>tiffinfo</code> tool. Check if there is a software update that fixes the issue or contact Vilber support.</p>\n<pre><code class=\"lang-auto\">&gt; tiffinfo imageNotWorking.Tif\nTIFFReadDirectoryCheckOrder: Warning, Invalid TIFF directory; tags are not sorted in ascending order.\nTIFFReadDirectory: Warning, Unknown field with tag 1807 (0x70f) encountered.\nTIFFReadDirectory: Warning, Unknown field with tag 1808 (0x710) encountered.\nTIFFFetchNormalTag: Warning, ASCII value for tag \"ImageDescription\" does not end in null byte.\nTIFFFetchNormalTag: Warning, IO error during reading of \"Tag 1807\"; tag ignored.\nTIFFFetchNormalTag: Warning, IO error during reading of \"Tag 1808\"; tag ignored.\n=== TIFF directory 0 ===\nTIFF Directory at offset 0x8006be (8390334)\n  Subfile Type: (0 = 0x0)\n  Image Width: 2048 Image Length: 2048\n  Resolution: 300, 300 (unitless)\n  Bits/Sample: 16\n  Compression Scheme: None\n  Photometric Interpretation: min-is-black\n  Samples/Pixel: 1\n  Rows/Strip: 2048\n  Planar Configuration: single image plane\n  ImageDescription:\n</code></pre>", "<p>Hello <a class=\"mention\" href=\"/u/cgohlke\">@cgohlke</a>,</p>\n<p>Thanks for your quick answer !<br>\nI\u2019ll do that and probably contact Vilber support.</p>\n<p>I didn\u2019t know about this library for Tiff checking. How can I install it ?</p>\n<p>Best,<br>\nR\u00e9my.</p>", "<p><a href=\"http://www.libtiff.org/man/tiffinfo.1.html\">Tiffinfo</a> is part of the the <a href=\"http://www.libtiff.org/\">libtiff</a> library. You can usually simply install it via your linux package manager. And there\u2019s also a Windows binary for download on the website, but I\u2019ve never tried that.<br>\nKind Regards,<br>\nDominik</p>"], "78705": ["<p>A few people made the macros and eventually left and I have been tasked with figuring it all out even though I HAVE NO experience in FIJI whatsoever. In short, I\u2019ve hit a wall. I looked through the command list and compared it to the Macros Code and I see the line that is unrecognized but I am not sure why its not recognized because the line is there and there isn\u2019t anything I can see that is wrong with it.<br>\nThe window that open says Macro Error unrecognized command \u201cenhance local contrast (CLAHE)\u201d, \u201cblocksize=127 histogram=256 maximum=3 mask = *None fast_(less_accurate)\u201d &lt;)&gt; ;<br>\nI opened the debug window and got nada. Can someone please help me to get this macro to work! Thank you so much!!</p>", "<p>Hi <a class=\"mention\" href=\"/u/nobel\">@nobel</a>,<br>\nreplace the line with</p>\n<pre><code class=\"lang-auto\">run(\"Enhance Local Contrast (CLAHE)\", \"blocksize=127 histogram=256 maximum=3 mask=*None* fast_(less_accurate)\");\n</code></pre>\n<p>You can activate the macro recorder and run the command from the menu <code>(Process \u203a Enhance Local Contrast (CLAHE)</code>, to check the syntax.</p>\n<p>Best,<br>\nVolker</p>", "<p>Thanks for that suggestion! I got the same error after doing this. After completing this step a dialog box is supposed to pop up for the next step but this does not happen. I am pretty sure I have followed everything else as needed so I think this is a macro problem but not sure of any other ideas to fix it. Thanks!!</p>", "<p>Hi <a class=\"mention\" href=\"/u/nobel\">@nobel</a>,</p>\n<p>you still get the error</p>\n<p><code>Macro Error unrecognized command \u201cenhance local contrast (CLAHE)\u201d, \u201cblocksize=127 histogram=256 maximum=3 mask = *None fast_(less_accurate)\u201d &lt;)&gt; ;</code><br>\n?</p>\n<p>Could you post the macro?</p>\n<p>Best,<br>\nVolker</p>", "<p>I got it! Thanks SO much for the feedback. I hope to one day contribute to this forum.</p>"], "78707": ["<p>The Nikon Imaging Center at Harvard Medical School (NIC@HMS) is seeking candidates for a 2-3 year (renewable annually) post-doctoral training fellowship in advanced optical microscopy techniques and core facility management.</p>\n<p>Optical microscopy has become central to progress in many areas of science. At the same time, the complexity of instruments and quantitative imaging experiments has dramatically increased, with many requiring extensive expertise to operate. Microscopy facilities managed by PhD-level scientists who advise and train researchers on imaging experimental design, the best instruments to use for their experiments, and proper use of instruments have become essential sources of expertise in many research institutions, and core facility management has become a stimulating career path for scientists with experience in advanced quantitative microscopy techniques and an interest in facilitating science broadly.</p>\n<p>The NIC@HMS Fellow will learn advanced quantitative microscopy techniques including confocal, TIRF, FRET, FRAP, photo-activation, single-molecule imaging, light sheet and super-resolution microscopy, and key skills needed to manage a large, heavily-used core facility. The Fellow will learn to train core facility users to select and apply the appropriate techniques, using a wide range of biological specimens and experimental approaches. The Fellow will also have the opportunity to identify additional responsibilities that match her/his interests, such as: organizing and teaching microscopy courses in the NIC@HMS Educational Program; organizing microscopy discussion groups/journal clubs; troubleshooting equipment problems; developing protocols for testing equipment performance; designing and/or implementing novel or custom imaging techniques.</p>\n<p>We are seeking candidates with optical microscopy experience, a PhD in biology, physics, or a related discipline, and interest in a career in optical microscopy. Start date is flexible. For more information and to apply, please visit <a href=\"https://microfellows.hms.harvard.edu/\">microfellows.hms.harvard.edu</a>. Applications will be considered as they are received, but should be submitted no later than April 6, 2023.</p>\n<p>Jennifer C. Waters, PhD<br>\nDirector of the Nikon Imaging Center &amp; Lecturer in Cell Biology, Harvard Medical School<br>\nChan Zuckerberg Initiative Imaging Scientist<br>\n<a href=\"https://twitter.com/jencwaters\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://twitter.com/jencwaters</a></p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.youtube.com/microcourses\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d17df8ddb48817e528e9038bc66d304cf122d9c5.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://www.youtube.com/microcourses\" target=\"_blank\" rel=\"noopener\">YouTube</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ad3eced709e9129a82a1497a98768c64a11eebd2.jpeg 2x\" data-dominant-color=\"EAEAEA\">\n\n<h3><a href=\"https://www.youtube.com/microcourses\" target=\"_blank\" rel=\"noopener\">Microcourses</a></h3>\n\n  <p>We are a team of light microscopists from core facilities at Harvard Medical School. We teach microscopy at HMS, and run the Quantitative Imaging: From Acquisition to Analysis course at Cold Spring Harbor Laboratory. On this channel, you can find...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "78708": ["<p>Hi,</p>\n<p>I am analysing Picroserius red stained skin sections viewed under polarised light. I want select that red, green and yellow areas. To do this I use 2 threshold, one for red and one for green. To get the yellow I apply the green threshold to the red annotation to get the overlap. This works well but means that I need to open each image, I have a lot of images and qupath tends to get problems when I frequently open and close images. Currently I have 3 scripts and this is the workflow:</p>\n<p>1 Run the script that detects the workflow<br>\nSelect the dermal annotation<br>\n2 Run the script that detects the green and red areas<br>\nSelect the red annotation<br>\n3 Run the script that detects green area but creates a detection called yellow as it is only applied to the red area</p>\n<p>I would like to have this as one script that automatically selects the dermis annotation before running the red:green script and then automatically selects the red annotation before running the yellow script.</p>\n<p>Sadly I don\u2019t have any experience with groovy or java so I\u2019m currently trying to learn.</p>\n<p>This is an example of how I would like the annotation to look in the end. The green strip is a membrane used for embedding but present in some of the pictures so I don\u2019t mind opening those specific ones.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934.jpeg\" data-download-href=\"/uploads/short-url/gBp0csd9UbU0PhqndvD1f3A1KTi.jpeg?dl=1\" title=\"Screenshot 2023-03-16 at 16.15.40\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_690x477.jpeg\" alt=\"Screenshot 2023-03-16 at 16.15.40\" data-base62-sha1=\"gBp0csd9UbU0PhqndvD1f3A1KTi\" width=\"690\" height=\"477\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_690x477.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_1035x715.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/745d2b9f89ba2c630dd0799a04ef29e3b8858934_2_1380x954.jpeg 2x\" data-dominant-color=\"1C2F17\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-16 at 16.15.40</span><span class=\"informations\">1920\u00d71328 145 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you in advance</p>", "<p>If you right click on the class and select by class (as shown in your image), you can generate a line of code in the Workflow that should read something like<br>\n<code>selectObjectsByClassification(\"Dermis\");</code><br>\nAs long as you get the class name or class names right, it will select those objects at that point in the script. You can also resetSelection(), I think, to unselect previously selected objects.</p>"], "29558": ["<p>I am implementing QuPath functions into software for an image capture device as plugins. Currently WatershedCellDetection is working, but I\u2019m trying to pass custom stain vector colors as well, so the user can utilize this feature within our current workflow.</p>\n<p>This is my first attempt at creating a plugin using QuPath features (someone else built the cell detection plugin that\u2019s currently being used) and I\u2019d like to add an option prior to running cell detection to easily calibrate the stain color between slides.</p>\n<p>The goal is to obtain the custom stain vector results in the same manner (ex. H&amp;E: select ROI &gt; set Hematoxylin, Eosin) to be written to a JSON object for integration into the program.</p>\n<p>Does anyone have any experience passing QuPath stain vectors to other software as a plugin?</p>", "<p>I\u2019m afraid I don\u2019t really understand exactly what you want to do. For example, with <em>\u2018for integration into the program\u2019</em> I\u2019m not entirely sure which one is \u2018the program\u2019, or whether the user is interacting through QuPath\u2019s GUI or another one. Without knowing more about your software and how the integration looks I find it too abstract to give an answer.</p>\n<p>However, if you check the workflow tab after setting the stain vectors in QuPath and choose \u2018Create script\u2019 you can see the scripting command that is logged there. Not sure if that\u2019s a useful starting point or not.</p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> The user (pathologist) is using a separate GUI which takes an image feed from the microscope and lets them use ImageJ, QuPath, and other plugins within this GUI minus some of the more complex options (no changing the sigma, threshold, etc.). The goal being to fit the workflow of a less tech-savvy pathologist.</p>\n<p>I will look into the scripts for stain vectors and see if that helps, I think that will be a good place to start. Cheers!</p>", "<p>Ah, ok. If you\u2019re integrating directly with the QuPath Java libraries then you\u2019ll need the <code>ImageData</code>, from which you can set/get the <code>ColorDeconvolutionStains</code>.</p>", "<p>Excellent, I was vastly overthinking this, and am on the right track now. Thanks for that <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>, although after implementation it seems like the stain params aren\u2019t being used correctly.</p>\n<p>Most of the <code>params</code> are being passed via JSON object from the application to the QuPath plugin and vice versa. The ROI and <code>PixelSize</code> are currently included.</p>\n<p>It looks like once I have the <code>ImageData</code> set, and my <code>params</code>, which now include the Stain Name, Stain 1, Stain 2, and the RGB optical density values for each stain, I can pass this to QuPath objects as necessary.</p>\n<p>My final question on this topic:<br>\nDo I need to create a new <code>StainVector</code> from the JSON encoded stain/value information within the plugin first, or should I create a new <code>ColorDeconvolutionStains</code> directly from this string using the <code>parseColorDeconvolutionStainsArg</code> method?</p>", "<p>Good! I think the parse method is probably the way to go.</p>", "<p>Sorry to resurrect an old thread, but here is a QuPath script to write the stain vectors into a JSON file and to read them back. So yes, \u201cpassing stain vectors to a third-party application\u201d or in my case, <a class=\"mention\" href=\"/u/cdgatenbee\">@cdgatenbee</a>\u2019s <a href=\"https://github.com/MathOnco/valis\" rel=\"noopener nofollow ugc\">VALIS</a> automated registration scripts.</p>\n<p>After I get my stain vectors into Python from the JSON file (I\u2019ll write this bit next), I will apply colour deconvolution as a pre-processing step to VALIS. Most likely, the Haematoxylin channel between two slides stained with non-overlapping markers, or maybe DAPI in a fluorescent slide vs Haematoxylin in an H&amp;E slide.</p>\n<p>The JSON file is saved in the same folder as the current image, with the same name but with <code>\"_stains.json\"</code> appended to it instead of the original extension.</p>\n<pre><code class=\"lang-java\">import qupath.lib.scripting.QP\nimport java.nio.charset.StandardCharsets\n\nimport java.nio.file.Files \nimport java.nio.file.Paths\nimport org.apache.commons.io.IOUtils\n\nimport qupath.lib.io.GsonTools\nimport com.google.gson.Gson\n\nimport qupath.lib.color.ColorDeconvolutionStains\nimport qupath.lib.color.StainVector\n\ndef write_stains(stains) {\n    def server = QP.getCurrentImageData().getServer()\n\n    //*********Get a JSON filename automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()\n    path = path[0..&lt;path.lastIndexOf('.')]+\"_stains.json\";\n    println path;\n    \n    boolean prettyPrint = true\n    def gson = GsonTools.getInstance(prettyPrint)\n\n    // write (save) the json file\n    try (Writer writer = new FileWriter(path)) {\n        gson.toJson(stains, writer);\n    }\n}    \n\ndef read_stains() {\n    def server = QP.getCurrentImageData().getServer()\n\n    //*********Get a JSON filename automatically based on naming scheme \n    def path = GeneralTools.toPath(server.getURIs()[0]).toString()\n    path = path[0..&lt;path.lastIndexOf('.')]+\"_stains.json\";\n    println path;\n\n    def JSONfile = new File(path)\n    if (!JSONfile.exists()) {\n        println \"No stains file for this image...\"\n        return\n    }\n\n    Gson gson = new Gson(); \n    map = gson.fromJson(JSONfile.text, Map.class);\n    StainVector stain1 = StainVector.createStainVector(map.stain1.name, map.stain1.r, map.stain1.g, map.stain1.b)\n    StainVector stain2 = StainVector.createStainVector(map.stain2.name, map.stain2.r, map.stain2.g, map.stain2.b)\n    StainVector stain3 = StainVector.createStainVector(map.stain3.name, map.stain3.r, map.stain3.g, map.stain3.b)\n    \n    ColorDeconvolutionStains stains = new ColorDeconvolutionStains(map.name, stain1, stain2, stain3, map.maxRed, map.maxGreen, map.maxBlue);\n    \n    return stains\n}\n\n//Here we write the stains\ndef imageData = getCurrentImageData();\ndef stains = imageData.getColorDeconvolutionStains()\nwrite_stains(stains)\n\n//Here we read the stains\nstains = read_stains()\nprintln stains\n</code></pre>\n<p>I\u2019ll add some Python code to this thread when I\u2019m done writing it.</p>\n<p>Cheers,<br>\nEgor</p>", "<p>Here\u2019s where I got to in terms of <em>passing custom stain vectors to a Python application</em>.</p>\n<p>The strict minimum Python code to actually make use of the JSON export would be something along these lines:</p>\n<pre><code class=\"lang-python\">import json\n\ndef get_stain(stains, val):\n    str_stain = None\n    if isinstance(val,int):\n        str_stain = f'stain{val+1}'\n    else:\n        for i in range(3):\n            str_stain = f'stain{i+1}'\n            if stains[str_stain]['name'] == val:\n                break\n            else:\n                str_stain = None\n\n    if str_stain is not None:\n        ret = [stains[str_stain]['r'], stains[str_stain]['g'], stains[str_stain]['b']]\n    else:\n        ret = None\n\n    return ret\n\ndef get_max(stains):\n    return [stains['maxRed'], stains['maxGreen'], stains['maxBlue']]\n</code></pre>\n<p>I used the stain vectors I generated for <a href=\"https://user-images.githubusercontent.com/11299568/83185126-8318de00-a0f8-11ea-91fe-efb1bc84a860.png\" rel=\"noopener nofollow ugc\">this image</a> (from the thread: <a href=\"https://forum.image.sc/t/pathology-image-color-separation-scaling-artifacts/38393\" class=\"inline-onebox\">Pathology image color separation scaling artifacts</a>) and generated a JSON file with the groovy script above. Then in Python:</p>\n<pre><code class=\"lang-python\">fn = r\"83185126-8318de00-a0f8-11ea-91fe-efb1bc84a860_stains.json\"\nwith open(fn) as f:\n    stains = json.load(f)\n\n# Using the stain index:\nprint(get_stain(stains, 0))\n\n# Using the stain name\nprint(get_stain(stains, 'Hematoxylin'))\n\n# The background value:\nprint(get_max(stains))\n</code></pre>\n<p>Now, take the following with a large pinch of salt, here\u2019s how I think colour deconvolution would be done in Python\u2026</p>\n<pre><code class=\"lang-auto\">import numpy as np\nimport pyvips\nfrom numpy.linalg import inv\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# Get the stains matrix and invert it\n\ndef get_matrix(stains):\n    mat = np.zeros((3,3),float)\n    for i in range(3):\n        mat[i,:] = get_stain(stains,i)        \n    return mat\n\nmat = get_matrix(stains)\nmax_stains = get_max(stains)\nmatinv = inv(mat)\n\n# Load the image using pyvips and convert to a numpy array\nimage = pyvips.Image.new_from_file('83185126-8318de00-a0f8-11ea-91fe-efb1bc84a860.png', access='sequential')\narr = image.numpy()\n\n# Image normalisation using QuPath's background value\narr1 = arr[:,:,0:3]/[[max_stains]]\narr1[arr1 &gt; 1] = 1\n\n# Deconvolve and display the stain images\nfig, axes = plt.subplots(1,3,figsize=(16,8))\nfor i,ax in enumerate(axes):\n    data = np.sum(arr1*matinv[:,i],axis=2)\n    im = ax.imshow(data, cmap='gray')\n    ax.axis('off')\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"8%\", pad=0.1)\n    cb = plt.colorbar(im, cax=cax)\n</code></pre>\n<p>The color deconvolution happens in this line (i: HEM=0, DAB=1, Residual=2). Be careful that numpy understands what I mean, but there probably are more correct ways to write this! <img src=\"https://emoji.discourse-cdn.com/twitter/innocent.png?v=12\" title=\":innocent:\" class=\"emoji\" alt=\":innocent:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<pre><code class=\"lang-auto\">data = np.sum(arr1*matinv[:,i],axis=2)\n</code></pre>\n<p>And the result:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5.jpeg\" data-download-href=\"/uploads/short-url/n36JfPs6voqwBz8l8s9on3yPRxr.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5_2_690x179.jpeg\" alt=\"image\" data-base62-sha1=\"n36JfPs6voqwBz8l8s9on3yPRxr\" width=\"690\" height=\"179\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5_2_690x179.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5_2_1035x268.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a18bf51323b1ea825293adecd88b9c97a7ea9bd5.jpeg 2x\" data-dominant-color=\"C7C7C7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1300\u00d7339 76.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Do not hesitate to correct me everywhere I made mistakes!</p>\n<p>Cheers,<br>\nEgor</p>", "<p><a class=\"mention\" href=\"/u/ep.zindy\">@EP.Zindy</a> very quick response to say that I think you\u2019re missing a logarithm in there somewhere\u2026 In any case, I linked to some implementations of colour deconvolution (including some on Python) at</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"38725\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/color-deconvolution-implementations-best-practice/38725\">Color deconvolution implementations &amp; best practice</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    The posts about colour deconvolution today have reminded me of a question I had some time ago\u2026 \nColor deconvolution, as described by Ruifrok and Johnston, involves generating a 3x3 stain matrix using three stain vectors. \nI understand that if two stains are available, then the remaining elements can be created by generating a third (pseudo)stain that is orthogonal to the first two. \nAs far as I can tell, this third stain is generated using the cross product in several places: \n\nQuPath (<a href=\"https://github.com/qupath/qupath/blob/a03756328188999c0b7f12c290cda0589c50bd4b/qupath-core/src/main/java/qupath/lib/color/StainVector.java#L316\">code</a>)\nsci\u2026\n  </blockquote>\n</aside>\n", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"9\" data-topic=\"29558\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> Pete:</div>\n<blockquote>\n<p>logarithm in there somewhere</p>\n</blockquote>\n</aside>\n<p>Possibly inverted too? Based on the color scale the background has the highest OD? Or maybe this is a different measurement system showing transparency.<br>\nEdit (or I guess negative log if that is the case)</p>", "<p>You can do the colour deconvolution in pyvips, fwiw. It should be a lot quicker than numpy, and will work for any size image without running out of memory. You could run it on an entire slide image, for example.</p>\n<p>Sample code here:</p>\n<aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/libvips/pyvips/issues/289#issuecomment-994539912\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/libvips/pyvips/issues/289#issuecomment-994539912\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/libvips/pyvips</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/libvips/pyvips/issues/289#issuecomment-994539912\" target=\"_blank\" rel=\"noopener nofollow ugc\">Save image to tif</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-12-09\" data-time=\"11:29:45\" data-timezone=\"UTC\">11:29AM - 09 Dec 21 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/SikangSHU\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"SikangSHU\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8533c1c50978d0086b67fccf80e94e3545e8e98.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          SikangSHU\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Dear Professor:\n      I consider to save images to tif,  but there is an error <span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">occured as follows. I don't know how to save. I would greatly appreciate it if you could help me.\n     This is my code:\n\n```python\nimg = pyvips.Image.new_from_file('E:\\\\testpicture_jupyter\\\\ometif_def6.tif', access='sequential')\npatch_size = 512\nn_across = img.width // patch_size\nn_down = img.height // patch_size\nx_max = n_across - 1\ny_max = n_down - 1\nprint(x_max, y_max)\n\nfor y in range(0, n_down):\n    print(\"row {} ...\".format(y))\n    for x in range(0, n_across):\n        patch = img.crop(x * patch_size, y * patch_size,\n                         patch_size, patch_size)\n        image = pyvips.Image.arrayjoin(patch, across=img.width)\nimage.tiffsave(\"huge.tif\")\n```\n\n     And the error occurred is:\n\n```\n(wsi2.py:1468): GLib-GObject-WARNING **: 19:19:47.663: value \"34102784\" of type 'gint' is invalid or out of range for property 'width' of  type 'gint'\n```</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "78711": ["<p>Hi,<br>\nI\u2019m attempting to use MIB for segementaiton of FIB-SEM data. I\u2019ve created a model from a subsection of the a datastack and I\u2019m trying to used DeepMIB to train a 2D U-net model to detect 4 materials.</p>\n<p>I\u2019ve followed your 2D U-net tutorial and it works well until I get to the training step. I use the following input file and parameters:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be1d7062de5936bf6d8226c2feb09dc2d45f477a.png\" data-download-href=\"/uploads/short-url/r7PN76nOH9qyTictOXyOyd83aFs.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be1d7062de5936bf6d8226c2feb09dc2d45f477a.png\" alt=\"image\" data-base62-sha1=\"r7PN76nOH9qyTictOXyOyd83aFs\" width=\"373\" height=\"500\" data-dominant-color=\"F2F2F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">398\u00d7533 8.77 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4b715201fe72f80cc70c65720701ca36b0d3d0a.png\" data-download-href=\"/uploads/short-url/pMGcdW4xre64apbFAjd0HShCNyG.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4b715201fe72f80cc70c65720701ca36b0d3d0a.png\" alt=\"image\" data-base62-sha1=\"pMGcdW4xre64apbFAjd0HShCNyG\" width=\"690\" height=\"429\" data-dominant-color=\"CFE4F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">986\u00d7614 30.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When I start training rapidly I get the following error. I\u2019m not sure how to proceed.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/2/e216e51f2d8d0ffcdd812f671adee749a9453ca8.png\" alt=\"image\" data-base62-sha1=\"wg4XyJfDfvfDiJexqxJSlgEPEso\" width=\"437\" height=\"213\"></p>\n<p>I use this version of MIB.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/b/5b1217fe9793b501560ab2f99aae1483fc2afc73.jpeg\" alt=\"image\" data-base62-sha1=\"cZEdmaG0KUv8xGTpjPN2w9fc3F9\" width=\"668\" height=\"441\"></p>\n<p>Cheers<br>\nJon</p>"], "78713": ["<p>I am a student and my project group wants to convert a very large OpenCV Matrix (about ten gigabytes) into 3D Texture, but the maximum resource capacity we have is 2 gigabytes for the 3D texture. we don\u2019t know how to optimize this matrix to be able to create the texture. we are working with UNITY tool and it is for 3d visualization project</p>"], "78714": ["<p>Per the title, I am trying to control a ViALUX V-7000 digital micromirror device in Micro-Manager. Unfortunately, there does not appear to be a device driver for this particular DMD. However, since it is based on the same Texas Instruments DMD Discovery\uf8ea 4100 chipset used in just about every other DMD, I was hoping perhaps someone else has looked into this, and knows of some way to control this device in MM?</p>\n<p>For reference, I have contacted ViALUX, but to their knowledge, no plugin currently exists. They do provide full API support for their controller suite, ALP-4 <em>basic</em>, even going as far as including an example visual studio project, so I guess it is not out of the question that someone already tried to write a device adapter for it.</p>\n<p>Unfortunately, since this particular device does not show up as a monitor, I cannot use the GenericSLM adapter. The <a href=\"https://micro-manager.org/GenericSLM\" rel=\"noopener nofollow ugc\">GenericSLM</a> page does mention that a very similar product from Digital Light Innovations can be converted to DVI with an adapter, but as far as I can tell, ViALUX does not have a similar converter. If someone knows a way, that too would be most helpful!</p>"], "78718": ["<p>I am attempting to colocalize 3 fluorescent signals using the Colocalization plugin (Pierre Bourdoncle). I am donig this by first colocalizing two of the signals then using the 8-bit output from that colocalization to colocalize with the 3rd. For some reason when I do this in one order Ch0:Ch02 then Ch03, this works great\u2026but when I do it in another order ch0:ch03 then ch02, the plugin will not recognize the colocalization. Any suggestions?</p>", "<p>That code looks to be a bit old <a href=\"https://imagej.nih.gov/ij/plugins/colocalization.html\" class=\"inline-onebox\">Colocalization</a></p>\n<p>Have you tried <a href=\"https://imagej.net/plugins/coloc-2\" class=\"inline-onebox\">Coloc 2</a></p>\n<p>If you wanted someone to test, we would need sample images.</p>"], "78719": ["<p>Hi all,<br>\nI am new here! I am writing a python fiji script with the help of chatgpt. I would like to sort the rois in the roimanager and chatgpt suggested the following command:</p>\n<pre><code class=\"lang-auto\">rm.runCommand('Sort', 'by=Size')\n</code></pre>\n<p>In the documentation for runCommand I cannot find a second argument for the Sort.<br>\nThe command works but I am trying to understand how chatgpt was able to suggest such a command when the documentation does not say anything about it.<br>\nCan anyone point me to a documentation that has this information or the code?</p>\n<p>Thanks in advance<br>\nFotos</p>", "<aside class=\"quote no-group\" data-username=\"Fotos_Stylianou\" data-post=\"1\" data-topic=\"78719\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fotos_stylianou/40/69173_2.png\" class=\"avatar\"> Fotos Stylianou:</div>\n<blockquote>\n<p>I would like to sort the rois in the roimanager and chatgpt suggested the following command:</p>\n<pre><code class=\"lang-auto\">rm.runCommand('Sort', 'by=Size')\n</code></pre>\n</blockquote>\n</aside>\n<p>It\u2019s amazing that ChatGPT knows about such an obscure ImageJ method. Unfortunately, it made up a non-existent option for that method. The only thing the rm.runCommand(\u2018Sort\u2019) method does is sort the ROI names into alphanumeric order.</p>"], "78720": ["<p>I have an image of a battery separator (i.e what separates the anode from the cathode in a battery).</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/000f35c2e25864dc13866ace5edd0ab035232d21.png\" alt=\"background\" data-base62-sha1=\"wApwx13k0z6tutzKFr2RCbbeF\" width=\"187\" height=\"199\"></p>\n<p>I am able to segment this image and find the pores (i.e the black holes) in the image effectively.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/efc70ac8621c64fa0b8e4391484726b18eb2981b.png\" alt=\"overlay_prototype\" data-base62-sha1=\"ydayNiKqoOBapcZlK62ZXoKoC0z\" width=\"187\" height=\"199\"></p>\n<p>However, another important quantitative measurement I\u2019ll like to obtain is the length/diameter of the fibers. I tried using some signal processing analysis using scikit-image and opencv (e.g morphological operations, canny, edge detection etc.) but nothing works.</p>\n<p>Furthermore, I also found this package (<a href=\"https://github.com/rshkarin/quanfima/tree/master\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - rshkarin/quanfima: Quanfima (Quantitative Analysis of Fibrous Materials)</a>) that estimates fiber properties from this related forum topic (<a href=\"https://forum.image.sc/t/automatically-measuring-cellulose-fiber-diameter-and-length-from-a-ct/62294/2\" class=\"inline-onebox\">Automatically measuring cellulose fiber diameter and length from a \u00b5CT - #2 by psobolewskiPhD</a>), however, I\u2019m unable to verify the veracity of the \u2018estimate_fiber_measurements\u2019 method. I tried to look at the documentation but I couldn\u2019t find any on here (<a href=\"https://quanfima.readthedocs.io/en/latest/quanfima.html#quanfima-morphology-module\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">API Reference \u2014 quanfima 0.1a1 documentation</a>), so I tried reading the source code and stepping through the code but I stumped.</p>\n<p>Any help will be much appreciated. Thanks!!</p>", "<p>You can get some extra info about that package (<code>quanfima</code>) in their publication: <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0215137\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Quanfima: An open source Python package for automated fiber analysis of biomaterials</a></p>", "<p>Thanks a lot! Hopefully it helps.</p>", "<p>Hi Peter,</p>\n<p>I read the paper, and I\u2019ve looked at the code again and I don\u2019t think I understand how the cast_ray function is calculating the fiber diameter. Please do you understand how it does it or is there any other link you can send that helps explain this. I tried googling other ray casting functions, but I couldn\u2019t find any that does something similar to what I think the cast_ray function in quanfima does.</p>", "<p>Sorry, I don\u2019t have any other resources about this. Perhaps you can try contacting the authors?</p>", "<p>Thank you so much, no worries. I tried and I\u2019m waiting on a response.</p>"], "78722": ["<p>I would like to know if all the open source BIA software in the list below are self-contained and if, in Windows, there would be any issue in solely archiving/copying back this folder for redeployment to the same machine.</p>\n<p>ImageJ, CellProfiler, QuPath, ilastik, Python Anaconda.</p>"], "78725": ["<p>I am currently working with IF images and have classified the cells into two classes( for simplicity call them Class A and B). My goal is to draw a line from the centroid of Class A cells to the centroid of the closest Class B cell. Furthermore, I would like to do this for every detected Class A cell. Since I will need to do this for potentially hundreds of thousands of cells, I would like to create a script to automate this process.</p>\n<p>I am able to use the LineROI function to draw a single line, but I\u2019m not sure how to first find the cell with closest distance and get the coordinates to draw a line between these two cells. Since there is a Shortest Distance to Annotation function, I imagine that it would be possible to write script for this. However, I am not sure where to start. I apologize if this is a simple task as I am fairly new to qupath.</p>", "<aside class=\"quote no-group\" data-username=\"AWallE\" data-post=\"1\" data-topic=\"78725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/awalle/40/69183_2.png\" class=\"avatar\"> AWallE:</div>\n<blockquote>\n<p>I am able to use the LineROI function to draw a single line</p>\n</blockquote>\n</aside>\n<p>Getting the distance to the nearest cell is quite easy, there is a built in function for that. Drawing the line is less so, but why is the line important? Is this for visualization or is there something about the angle or relative direction that is important?</p>", "<p>Yes, I am able to get the distance, but I am unable to obtain the coordinates of the detected cell which is closest. My purpose is mostly for visualization.</p>", "<p>Hmm, I can think of a few roundabout ways to approach it, like cycling through every class A cell, changing it to class B, and then performing Delaunay clustering and checking the connections for that cell for the shortest one, get the centroids, then changing the class back to A and repeat. Maybe <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> has something more efficient though.</p>\n<p>If you have a constraint in terms of maximum distances that might open up some more options by limiting the search radius, but if you want it open ended that could be challenging.</p>", "<p>I am working with whole slide images which contains a TMA grid. Hence, the constraint is that I only want to draw lines between cells which are in the same TMA core. I don\u2019t know if this would simplify the problem.</p>\n<p>Since there is a function for finding the shortest distance to a detected cell of some specified class, I assumed that there might be a way to change the detectionToAnnotationDistances script to fit my needs. I have been reading the gitHub linked below:</p>\n<p><a href=\"https://github.com/qupath/qupath/blob/main/qupath-core/src/main/java/qupath/lib/analysis/DistanceTools.java\" rel=\"noopener nofollow ugc\">https://github.com/qupath/qupath/blob/1368912885c1a191beaea32c28d85a3707f657f8/qupath-core/src/main/java/qupath/lib/analysis/DistanceTools.java</a></p>", "<p>Yeah, unfortunately most of those functions use a distance map, which doesn\u2019t help identify the particular cell you are interested in.</p>", "<p>A modification of this <a href=\"https://forum.image.sc/t/qupath-script-nearest-neighbors/31824\" class=\"inline-onebox\">QuPath Script: Nearest Neighbors</a> might be closer to what you want, since you can minimize the loop to your class of interest, and also loop over the members of each TMA.</p>", "<p>Hey <a class=\"mention\" href=\"/u/awalle\">@AWallE</a> ,<br>\nHere\u2019s how I would approach it.</p>\n<ol>\n<li>Get the centroids of all ClassB cells. Turn them into a <a href=\"https://locationtech.github.io/jts/javadoc/org/locationtech/jts/geom/MultiPoint.html\">multipoint geometry.</a>\n</li>\n<li>Run a loop over each ClassA cell. Turns it\u2019s centroid into a single <a href=\"https://locationtech.github.io/jts/javadoc/org/locationtech/jts/geom/Point.html\">point geometry</a>.</li>\n<li>Use <a href=\"https://locationtech.github.io/jts/javadoc/org/locationtech/jts/operation/distance/DistanceOp.html#nearestPoints--\">NearestPoints</a> to search for the closest centroid between CellA and the entire ClassB</li>\n</ol>", "<p>Thanks for the suggestions!</p>", "<p><a class=\"mention\" href=\"/u/smcardle\">@smcardle</a>\u2019s answer would be much faster I suspect, though you\u2019d probably want to draw the line based off that nearest coordinate rather than trying to tie it to the nearest XY coordinate cell.</p>"], "78726": ["<p>Hi community,</p>\n<p>I have a dump question but I rather ask than assuming.</p>\n<p>When exporting intensity measurement, it gives two value, Area and mean intensity as below.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/88b5ceedae8a22769a7f45fc98fa2ce03809b1c6.png\" alt=\"image\" data-base62-sha1=\"jvosqsSuFoI9vGGmyHxMCnW8dn0\" width=\"349\" height=\"21\"><br>\nHere does \u201c1.29Aum per pixel: NPs: Mean\u201d indicate mean intensity/pixel or mean intensity/um2.</p>\n<p>I was thinking the former but just wanted to make sure if that is correct.<br>\nThanks!</p>", "<p>It\u2019s simply the mean intensity, not area based. Or I suppose per pixel might be a way of looking at it. But the average intensity is just the average intensity based on the image acquisition parameters. It isn\u2019t going to double because you double the estimated pixel size (the ROI 1.29um), which you can test by changing that value.</p>\n<p>The measurement is <em>roughly</em> independent of the pixel size used for downsampling (which could sort-of also be thought of as \u201cwhich layer of the pyramidal image am I using to calculate the mean intensity\u201d). Though there would be differences based on edges, which will generally be small if the estimated pixel size is small relative to the objects being measured. If your pixel size is the whole image\u2026 well, it will be the average intensity across the entire image and might be way off.</p>", "<p>Thanks for the quick response. I do not think I understood it correctly. Let me elaborate more to make sure that I am on the same page with you.</p>\n<p>So let\u2019s say I apply a pixel classifier to get an detection/annotation object. Then I will do the intensity measurement in this detection object. The QuPah will spit out the \u201cArea(um2)\u201d of detection object and \u201cmean intensity\u201d as discussed above. In this case \u201cmean intensity\u201d would be the integration of intensity obtained from each pixel in the object divided by # of pixels in that detection object? Is it the right way to interpret or can you elaborate on this specific example?</p>\n<p>Thanks</p>", "<aside class=\"quote no-group\" data-username=\"BUMJUN_KIM\" data-post=\"3\" data-topic=\"78726\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bumjun_kim/40/17896_2.png\" class=\"avatar\"> Bumjun Kim:</div>\n<blockquote>\n<p>In this case \u201cmean intensity\u201d would be the integration of intensity obtained from each pixel in the object divided by # of pixels in that detection object? Is it the right way to interpret or can you elaborate on this specific example?</p>\n</blockquote>\n</aside>\n<p>It could be, depends on what your actual pixel size is compared to the pixel size you chose when making the measurement. Either way the result should be pretty much the same, you are effectively pre-averaging some of the pixels together for larger pixel sizes is all.</p>"], "78727": ["<p>Hi folks,<br>\nI\u2019m working on using the convex hull analysis in SNT from FIJI to calculate the volume of an axon plexus. SNT doesn\u2019t seem to be following the global set scale feature in FIJI and I\u2019m wondering what the units are. Are they pixels or is it in um?</p>\n<p>Do you folks have any tips for me? Here is a screenshot with the trace of an axonal plexus, the convexhull, and the corresponding size calculation with confusing units.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8.jpeg\" data-download-href=\"/uploads/short-url/9pXcqjFwyqkHJ6JcixE0YmEAhF6.jpeg?dl=1\" title=\"axon1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8_2_527x500.jpeg\" alt=\"axon1\" data-base62-sha1=\"9pXcqjFwyqkHJ6JcixE0YmEAhF6\" width=\"527\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8_2_527x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8_2_790x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4202a790867ab80aa59fce142f8cf3c8000718d8.jpeg 2x\" data-dominant-color=\"18191E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">axon1</span><span class=\"informations\">1046\u00d7991 94.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/sharkweekshane\">@sharkweekshane</a> ,</p>\n<p>Did you set the scale before starting SNT or after? I can reproduce this if I start SNT with an uncalibrated image, then set the scale. If the scale is set before starting, it seems to work as expected.</p>"], "78728": ["<p>Hi,</p>\n<p>I was trying brainreg plugin in Napari on a zebrafish image using the mpin_zfish_1um atlas. I wanted to check the orientation of my data first, so followed these <a href=\"https://docs.brainglobe.info/brainreg-napari/checking-orientation\" rel=\"noopener nofollow ugc\">instructions</a>, but for some reason it is generating views based on, what seems to me, a mouse brain atlas (top row), even though I selected the zebrafish atlas. Screenshot below:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240.png\" data-download-href=\"/uploads/short-url/f7aZp4wG4qwKSxiC5XD9lw2mtLW.png?dl=1\" title=\"Screenshot\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240_2_690x404.png\" alt=\"Screenshot\" data-base62-sha1=\"f7aZp4wG4qwKSxiC5XD9lw2mtLW\" width=\"690\" height=\"404\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240_2_690x404.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240_2_1035x606.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69f04eaa9f4dbcea348db4413bb390cc45c10240.png 2x\" data-dominant-color=\"35383C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot</span><span class=\"informations\">1339\u00d7784 153 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Any idea why this might be happening?</p>\n<p>ping <a class=\"mention\" href=\"/u/adamltyson\">@adamltyson</a></p>\n<p>Thank you!</p>", "<p>Hey, sorry about this. For some reason that part of the plugin has a mouse brain image hardcoded. I\u2019ve raised an issue to track the fix <a href=\"https://github.com/brainglobe/brainreg-napari/issues/53\">here</a>.</p>\n<p>Thanks,<br>\nAdam</p>", "<p>This fix should be released soon. In the mean time if you want to test it, then please install the release candidate with <code>pip install brainreg-napari==0.1.2rc0</code>.</p>\n<p>Adam</p>", "<p>Thank you <a class=\"mention\" href=\"/u/adamltyson\">@adamltyson</a> , will check it out!</p>\n<p>Ved</p>"], "78729": ["<p>I just received a set of lightsheet data (.tif files) from a prototype instrument that someone wants to visualize.  The stacks are of course still skewed due to the angle of the lightsheet.  I\u2019ve got little experience with lightsheet data, and about all I know is I need to shift each slice by z*cos (lightsheet angle).</p>\n<p>Before I start reinventing the wheel, are there plugins or programs that already exist that deskew lightsheet data where you can input z and the angle?  (Note that I\u2019m most familiar with FIJI/ImageJ, but I have a basic working knowledge of napari and Matlab.)</p>", "<p>You might want to look into <a href=\"https://www.napari-hub.org/plugins/napari-ndtiffs\" rel=\"noopener nofollow ugc\">napari-nd-tiff</a> developed by <a class=\"mention\" href=\"/u/talley\">@talley</a> and also <a href=\"https://www.napari-hub.org/plugins/napari-lattice\" rel=\"noopener nofollow ugc\">napari-lattice-lightsheet</a> developed by <a class=\"mention\" href=\"/u/pr4deepr\">@pr4deepr</a> . The latter works quiet well for lattice-lightsheet-data acquired with a Zeiss ligthsheet, I am not sure if this would work on other LLSM datasets.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/praveen\">@Praveen</a>.  Hope things are working well for you.</p>\n<p><a class=\"mention\" href=\"/u/maria_traver\">@Maria_Traver</a> , do you know the angle and direction of skew (Y or X).  It should work on other LLSM datasets, provided you know the angle and direction of skew (X or Y). We recently identified a \u201cbug\u201d, where I realized that the direction of stage-scan matters too! This will be fixed soon on pyclesperanto-prototype library upstream.</p>\n<p>napari-lattice-lightsheet has been inspired a lot from <a class=\"mention\" href=\"/u/talley\">@talley</a> 's code.</p>", "<p>Within FIJI, an option is to use CLIJ.<br>\nYou can use the <a href=\"https://clij.github.io/clij2-docs/reference_affineTransform3D\" rel=\"noopener nofollow ugc\">AffineTransform3D</a> class for deskew, followed by a rotation for coverslip rotation.</p>\n<p>Other options:</p>\n<ul>\n<li><a href=\"https://monash-merc.github.io/llsm/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">LLSM</a></li>\n<li><a href=\"https://gist.github.com/jdmanton/722b8a5618365062a854c190a8bc366a\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ImageJ/Fiji script for deskewing stage-scan light sheet data (inc. OPM) \u00b7 GitHub</a></li>\n</ul>", "<p>Hello all,</p>\n<p>I would also recommend <a href=\"https://imagej.net/plugins/bigstitcher/index\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">BigStitcher</a>, even if you do not need stitching <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nIn the new version it has Arrange Views \u2192 (De)Skew Images\u2026 which works really well (and probably you would need to scale one of the axis (or just provide proper voxel sizes).<br>\nYou can export result as tiff or BDV HDF5 (skipping stitching step).</p>\n<p>Cheers,<br>\nEugene</p>"], "78731": ["<p>Hi</p>\n<p>I am using the cellpose 2.2 in cell segmentation.<br>\nThis system was built in WEB platform and the cellpose was realized by using command.<br>\nFirstly, while researching the cellpose, I found that cellpose provide 14 pretrained models.<br>\nBut I am not sure what the differences(and their utilizing examples) are between them because I am not an expert in this field.<br>\nPlease give me good advice about this problem.<br>\nSecondly, I also found that the cellpose provide \u201c*_output.png\u201d file when we get cell segment result by using commands.<br>\nBut sometimes there are some cases that it\u2019s not created(Because I used the same command, I think that it\u2019s related with the kind of image file).<br>\nWhich case is it created or not in?</p>\n<p>Any help would be appreciated.</p>", "<p>Hey!<br>\nRegarding CellPose models, they were trained on different data and perform better/worse depending on your specific situation. We use the base models to train our own domain specific models to improve performance for our use case.</p>\n<p>Regarding files created/missing, an example of your issues might be appropriate.<br>\nIn general, sharing something you tried and the issue you experienced is typically a better approach than soliciting for \u201cgeneral advice\u201d <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Good luck!<br>\nLeo</p>", "<p>Hey,</p>\n<p>Like Leo said, these models were trained on different data.<br>\nHere is a figure to explain them, in a paper: <a href=\"https://www.nature.com/articles/s41592-022-01663-4\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Cellpose 2.0: how to train your own model | Nature Methods</a><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8.jpeg\" data-download-href=\"/uploads/short-url/u0R9Xgsm23jLDChQ2DdYMloKYY0.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_690x451.jpeg\" alt=\"image\" data-base62-sha1=\"u0R9Xgsm23jLDChQ2DdYMloKYY0\" width=\"690\" height=\"451\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_690x451.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_1035x676.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_1380x902.jpeg 2x\" data-dominant-color=\"CCCDCD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1751\u00d71146 228 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best<br>\nYuan</p>", "<p>Hello everyone,</p>\n<p>First of all, really thank you for all of your kind helpness.<br>\nAnd also thank you for the cellpose developers and experts.<br>\nI am a really fresh developer in this field and so curious about the powerful functions of cellpose 2.2.<br>\nNow I just started the cell segment by using cellpose 2.2 and succeed (I am not sure how you think about this but it\u2019s certainly big success for me.) to get  \u201c<em>_seg.npy\" and \"</em>_cp_output.jpg\u201d files from origine images by using windows command.<br>\nThis is the result.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d.jpeg\" data-download-href=\"/uploads/short-url/vPEVnpF0ctIpkD4pADsSgPXzX8V.jpeg?dl=1\" title=\"029_img.ome_cp_output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg\" alt=\"029_img.ome_cp_output\" data-base62-sha1=\"vPEVnpF0ctIpkD4pADsSgPXzX8V\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1380x344.jpeg 2x\" data-dominant-color=\"A5A7A3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">029_img.ome_cp_output</span><span class=\"informations\">3600\u00d7900 239 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And now I want to get more detailed images from \u201c<em>_seg.npy\" file.<br>\nI mean that I want to get these 4 individual images from the \"</em>_seg.npy\u201d file and also to get the correct position of ROIS.<br>\nI really want to do this but I am not sure how to do this because I just have some knowledge about Python and image processing.<br>\nIf possible, please share with me the python source code to get these individual images and correct position of ROIS from the \u201c*_seg.npy\u201d file.</p>\n<p>Thank all of you again.</p>", "<p>Since QuPath is linked in this, if you run CellPose through QuPath, you will get the positions of all of the cells, along with measurements, as ROI/detection objects. I am not as sure about the outlines and cell poses though, maybe <a class=\"mention\" href=\"/u/oburri\">@oburri</a> knows more.  I kind of doubt it since that would be a huge amount of data by default if run across whole slide images.</p>"], "78735": ["<p>Hi all,</p>\n<p>I used to use CellProfiler years ago adn recently got excited when I saw the implementation of Stardist and Cellpose.  So I thought I\u2019d give it a go\u2026</p>\n<p>I tried a few approaches to install CellProfiler from source.<br>\nThis one on the <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Source-installation-%28Windows%29\" rel=\"noopener nofollow ugc\">github</a> repo did work.<br>\nI failed trying to do it using anaconda.</p>\n<p>Having navigated through this I discovered you need to also clone the <a href=\"https://github.com/CellProfiler/CellProfiler-plugins\" rel=\"noopener nofollow ugc\">plugins</a>.</p>\n<p>I followed the Use in the README.md.</p>\n<p>when I got to step 2 (b).</p>\n<pre><code class=\"lang-auto\">pip install -r requirements-windows.txt\n</code></pre>\n<p>cellh5 and keras installed<br>\nbut cntk did not</p>\n<pre><code class=\"lang-auto\">ERROR: Could not find a version that satisfies the requirement cntk (from versions: none)\nERROR: No matching distribution found for cntk\n</code></pre>\n<p><strong>Is there a solution for this ?</strong></p>\n<hr>\n<p>I also then found the <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/blob/0c2e7cab5c13d17fa376b2bd42ca6a7e5db04960/Instructions/Install_environment_instructions_windows.md\" rel=\"noopener nofollow ugc\">Beginner Guide</a><br>\nASIDE:This suggests using anaconda which is at odds with the cellprofiler github page that recommends PIP.</p>\n<p>This works well.The resulting install has the runstardist and runcellpose modules.  Havent tested they work though. Only obvious issues</p>\n<pre><code class=\"lang-auto\">Could not load variancetransform\nTraceback (most recent call last):\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\CP_plugins\\lib\\site-packages\\cellprofiler_core\\utilities\\core\\modules\\__init__.py\", line 71, in add_module\n    m = __import__(mod, globals(), locals(), [\"__all__\"], 0)\n  File \"C:\\Users\\Admin\\OneDrive - The University of Sydney (Students)\\Documents\\GitHub\\CP2\\CellProfiler-plugins\\variancetransform.py\", line 20, in &lt;module&gt;\n    import pandas\nModuleNotFoundError: No module named 'pandas'\ncould not load these modules: variancetransform\n</code></pre>\n<p>NB. The anaconda based approach \u2192 CellProfiler 4.2.1<br>\nThe PIP approach \u2192 CellProfiler 5.0.0b1 but no runStarDist</p>\n<p>Tried to  solve with</p>\n<pre><code class=\"lang-auto\">pip install stardist csbdeep --no-deps\npip install omnipose\n</code></pre>\n<p>did not work ;(</p>\n<p>Cheers.</p>\n<p>James</p>", "<p>If anyone can advise how to get GPU working for stardist and cellpose it would be much appreciated.<br>\nI cannot work it out</p>"], "23445": ["<p>I\u2019m new to ImageJ. I\u2019m trying to measure some structures made in optical<br>\nmicroscopy. I downloaded Fiji and am using Microscope Measurement Tools and<br>\nthe plugin works pretty well!</p>\n<p>But I\u2019d like to know if there is a way to change the position (and text and<br>\nbackground color) of the measured \u201cline\u201d when I select \u201cDraw Measurement<br>\nLine\u201d. The black background and green font almost always appears in an<br>\nincovenient point and is very large, so it generally covers some important<br>\nfeature of the structure I\u2019m measuring\u2026</p>\n<p>Any advices are welcome!</p>", "<p>If You look inside the user_settings file, there are also settings for the font size and background color etc. See if those can help you out. Eventually, rather than a fixed pixel size, I was going to make the font size be some fraction of the total image size so it always comes out around the same size no matter the resolution.</p>\n<p>Positioning is a bit more complicated, you may have to go into the code depending on what you want.<br>\nI already coded it so that the text always shows up at the <strong>end</strong> of where you drag your line marker. So, wherever you start dragging the Line will not have text, and the text will always show up after the end of where you stop dragging your line. Hopefully that helps, otherwise, open up the code and you can see some lines that deal with whether the text shows up on the top, bottom, etc. You are welcome to add  an option in the user settings, that maybe puts the text in the center instead. If you want to code that up, fork the github code, and we can go from there.</p>", "<p>Thanks, demis! I\u2019m gonna try this.</p>", "<p>Were you able to adjust the text to your liking?<br>\nI was recently revisiting this code, and also noticed I have a setting in the <em>user_settings.py</em> file, that lets you choose whether the text shows up at the left or right of the Line ROI.<br>\nI think it would be useful to make a separate \u201cDraw Measurement - Line \u2026\u201d that pops up options for the annotation.  However this would only be useful after making it an Overlay, so if you don\u2019t like it you can just delete the annotation (right now it\u2019s still drawn directly to the image, uneditable).</p>", "<p>Hello guys,<br>\nI am trying to use the Microscope Measurement Tools with Fiji as well, but I am encountering a problem.<br>\nI followed the installation instructions step by step, but when I try to run the Choose Microscope Calibration\" or \u201cDraw Measurement Line\u201d functions I get an error. It seems the module \u201cMicroscope_Calibrations_user_settings\u201d cannot be imported.<br>\nI am wondering if I need to install some additional library or software\u2026</p>\n<p>I would appreciate your help. I should be missing a very simple step.</p>\n<p>This is the error shown:</p>\n<pre><code class=\"lang-auto\">[ERROR] Traceback (most recent call last):\n  File \"Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py\", line 42, in &lt;module&gt;\nImportError: No module named Microscope_Calibrations_user_settings\n\n\tat org.python.core.Py.ImportError(Py.java:329)\n\tat org.python.core.imp.import_first(imp.java:1230)\n\tat org.python.core.imp.import_module_level(imp.java:1361)\n\tat org.python.core.imp.importName(imp.java:1528)\n\tat org.python.core.ImportFunction.__call__(__builtin__.java:1285)\n\tat org.python.core.PyObject.__call__(PyObject.java:433)\n\tat org.python.core.__builtin__.__import__(__builtin__.java:1232)\n\tat org.python.core.imp.importOneAs(imp.java:1564)\n\tat org.python.pycode._pyx0.f$0(Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py:212)\n\tat org.python.pycode._pyx0.call_function(Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py)\n\tat org.python.core.PyTableCode.call(PyTableCode.java:173)\n\tat org.python.core.PyCode.call(PyCode.java:18)\n\tat org.python.core.Py.runCode(Py.java:1687)\n\tat org.python.core.__builtin__.eval(__builtin__.java:497)\n\tat org.python.core.__builtin__.eval(__builtin__.java:501)\n\tat org.python.util.PythonInterpreter.eval(PythonInterpreter.java:255)\n\tat org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:57)\n\tat org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:31)\n\tat javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:157)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n</code></pre>\n<p>Thanks!</p>", "<p>Hi,</p>\n<p>I also keep getting the same message after trying the instructions. Were you able to find any solutions so far? I am using Fiji in MacOS.</p>", "<aside class=\"quote no-group\" data-username=\"Jose_Cabot\" data-post=\"5\" data-topic=\"23445\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jose_cabot/40/57310_2.png\" class=\"avatar\"> Jose Cabot:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">[ERROR] Traceback (most recent call last):\n  File \"Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py\", line 42, in &lt;module&gt;\nImportError: No module named Microscope_Calibrations_user_settings\n\n\tat org.python.core.Py.ImportError(Py.java:329)\n\tat org.python.core.imp.import_first(imp.java:1230)\n\tat org.python.core.imp.import_module_level(imp.java:1361)\n\tat org.python.core.imp.importName(imp.java:1528)\n\tat org.python.core.ImportFunction.__call__(__builtin__.java:1285)\n\tat org.python.core.PyObject.__call__(PyObject.java:433)\n\tat org.python.core.__builtin__.__import__(__builtin__.java:1232)\n\tat org.python.core.imp.importOneAs(imp.java:1564)\n\tat org.python.pycode._pyx0.f$0(Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py:212)\n\tat org.python.pycode._pyx0.call_function(Analyze/Microscope Measurement Tools/Choose_Microscope_Calibration.py)\n\tat org.python.core.PyTableCode.call(PyTableCode.java:173)\n\tat org.python.core.PyCode.call(PyCode.java:18)\n\tat org.python.core.Py.runCode(Py.java:1687)\n\tat org.python.core.__builtin__.eval(__builtin__.java:497)\n\tat org.python.core.__builtin__.eval(__builtin__.java:501)\n\tat org.python.util.PythonInterpreter.eval(PythonInterpreter.java:255)\n\tat org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:57)\n\tat org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:31)\n\tat javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:157)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n</code></pre>\n</blockquote>\n</aside>\n<p>Follow the steps here, and download the latest version: <a href=\"https://github.com/demisjohn/Microscope-Measurement-Tools\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - demisjohn/Microscope-Measurement-Tools: Microscope Measurement/Calibration plugin for FIJI</a></p>\n<p>Place your unzipped Microscope Measurement Tools folder in the PLUGINS directory of your Fiji folder (NOT in the SCRIPT folder, as indicated in some other instructions), then follow instructions as usual.</p>\n<p>Once installed in Fiji, you will find this tool in your Fiji software in the Plugins&gt;Analyze&gt;Microscope Measurement Tools and your calibrations will appear.</p>\n<p>Hope this helps!</p>"], "78742": ["<p>Hello everyone!<br>\nI\u2019m new to QuPath so not sure if my problem is relevant at all. I\u2019m trying to run a sensitivity analysis on SimpleTissueDetection2 parameters. I would need to make each value of the parameters to vary within a specific range and export the measurement for each combination. The goal is to analyses how parameters influence tissue detection outcome for a set of images and to define a range of reliability for each parameters for a specific dataset in a consistent way.</p>\n<h3>\n<a name=\"challenges-1\" class=\"anchor\" href=\"#challenges-1\"></a>Challenges</h3>\n<ul>\n<li>\n<p>What stops you from proceeding?<br>\nI think I would need to use a for loop somewhere inside or outside the plugin but I\u2019m not sure if the plugin can admit something different than a number as parameter.</p>\n</li>\n<li>\n<p>What have you tried already?<br>\nI have tried to define a function getNumbersInRange() to create a list of integer within a specific range and use this function for each quantitative parameter of SimpleTissueDetection2.</p>\n</li>\n<li>\n<p>Have you found any related forum topics? If so, cross-link them.<br>\nI haven\u2019t found any related topic for QuPath and/or SimpleTissueDetection2 plugins specifically.</p>\n</li>\n<li>\n<p>What software packages and/or plugins have you tried?<br>\nI\u2019m working with QuPath-0.4.3 and SimpleTissueDetection2 (<a href=\"https://qupath.github.io/javadoc/docs/qupath/imagej/detect/tissue/SimpleTissueDetection2.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">SimpleTissueDetection2 (QuPath 0.4.0)</a>).</p>\n</li>\n</ul>\n<h3>\n<a name=\"sample-image-andor-code-2\" class=\"anchor\" href=\"#sample-image-andor-code-2\"></a>Sample image and/or code</h3>\n<p>Whole slide images in open-access: <a href=\"https://openslide.cs.cmu.edu/download/openslide-testdata/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">openslide-testdata</a></p>\n<pre><code class=\"lang-auto\">\npublic List&lt;Integer&gt; getNumbersInRange(int start, int end) {\n    List&lt;Integer&gt; result = new ArrayList&lt;&gt;();\n    for (int i = start; i &lt; end; i++) {\n        result.add(i);\n    }\n    return result;\n}\n\nrunPlugin('qupath.imagej.detect.tissue.SimpleTissueDetection2', ' {\n    \"threshold\": getNumbersInRange(0, 255),  \n    \"requestedPixelSizeMicrons\": getNumbersInRange(0, 100),  \n    \"minAreaMicrons\": getNumbersInRange(0, 100000000),  \n    \"maxHoleAreaMicrons\": getNumbersInRange(0, 100000000),\n    \"darkBackground\": false,  \n    \"smoothImage\": true,  \n    \"medianCleanup\": true,  \n    \"dilateBoundaries\": true,  \n    \"smoothCoordinates\": true,  \n    \"excludeOnBoundary\": true,  \n    \"singleAnnotation\": true\n    }');\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/valentin\">@Valentin</a> ,<br>\nBecause the runPlugin command expects a string, you\u2019ll need to it inside a nested loop, where you can deal with string conversion. Also, I recommend not actually testing every single integer in all of those ranges, because as it stands right now, you\u2019re testing 2e18 combinations.</p>\n<pre><code class=\"lang-auto\">double pixelSize=getCurrentImageData().getServer().getPixelCalibration().getAveragedPixelSize()\n\nfor (t=0; t&lt;226; t++){\n    for (p=1; p&lt;20; p++){\n        for (min=0; min&lt;10000000; min+=10000){\n            for (max=0; max&lt;10000000; max+=10000){\n                runPlugin('qupath.imagej.detect.tissue.SimpleTissueDetection2', '{\"threshold\":'+t+',\"requestedPixelSizeMicrons\":'+p*pixelSize+',\"minAreaMicrons\":'+min+',\"maxHoleAreaMicrons\":'+max+',\"darkBackground\":false,\"smoothImage\":true,\"medianCleanup\":true,\"dilateBoundaries\":false,\"smoothCoordinates\":true,\"excludeOnBoundary\":false,\"singleAnnotation\":true}')\n//MEASUREMENTS GO HERE\n            }   \n         }\n    }  \n}\n</code></pre>\n<p>I\u2019ve also changed the pixel size parameter to simply be a multiple of image pixel size, because I\u2019m pretty sure QuPath uses the pre-defined pyramid levels, not arbitrary downsampling.</p>", "<p>Also want to throw out there that SimpleTissueDetection has been deprecated as of the development of the thresholder. <a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/thresholding.html\" class=\"inline-onebox\">Detecting tissue \u2014 QuPath 0.4.3 documentation</a></p>\n<p>Any particular reason you are looking into it?</p>"], "78743": ["<p>Hello everyone.</p>\n<p>I\u2019m asking for your help because I can\u2019t make a root segmentation.</p>\n<p>Indeed, the intensity of the pixels at the end of my root is similar to the intensity of pixels in the background of the image. (Cf Photo) Conclusion, the segmentation doesn\u2019t work well \u2026<br>\n<a class=\"attachment\" href=\"/uploads/short-url/vRbdVGkZDE1X3L1li9HyMOfcEqo.tif\">DSC_215546-1.tif</a> (2.7 MB)</p>\n<p>Do you have any pre-processing or methods to advise me so that I can finally have my root in full despite this difference in pixel intensity?</p>\n<p>Thank you in advance,</p>\n<p>Bastien</p>", "<p>What about improving the focus?<br>\nDid you tried dark field microscopy?</p>", "<p>This area of the root is more transparent than the rest of the root. It is therefore normal to see a difference.</p>\n<p>I tried to increase and improve the focus but it did not change anything since the transparency of the root is a reality.</p>\n<p>No we don\u2019t have this type of microscope.</p>\n<p>It\u2019s frustrating since only the root tip is good in segmentation\u2026</p>\n<p>Thanks</p>", "<p>Autofluorescence might be another option.</p>", "<aside class=\"quote no-group\" data-username=\"Bastien\" data-post=\"3\" data-topic=\"78743\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/b/51bf81/40.png\" class=\"avatar\"> Bastien:</div>\n<blockquote>\n<p>No we don\u2019t have this type of microscope.</p>\n</blockquote>\n</aside>\n<p>If you have a bright field microscope, you do\u2026<br>\nI suggest doing a bit of reading.</p>", "<p>You will likely struggle to segment the root via intensity contrast, because, as you said, there isn\u2019t any.</p>\n<p>If you only have a few of those images, it might be quicker to create a selection (polygon tool) and create a mask by hand.</p>\n<p>You could als try to enhance the contrast (maybe with the local contrast CLAHE filter) and try an edge detection filter first.</p>", "<p>Possible entry\u2026</p>\n<pre><code class=\"lang-auto\">//-------------------------\nrun(\"Duplicate...\", \"title=1\");\nrun(\"Duplicate...\", \"title=2\");\nrun(\"Invert\");\nimageCalculator(\"Subtract create\", \"2\",\"1\");\nselectWindow(\"Result of 2\");\nrun(\"Unsharp Mask...\", \"radius=30 mask=0.60\");\nsetAutoThreshold(\"Default dark\");\nsetThreshold(0, 94);\n//run(\"Threshold...\");\nsetOption(\"BlackBackground\", true);\nrun(\"Convert to Mask\");\nrun(\"Median\", \"radius=3\");\nrun(\"Erode\");\nrun(\"Erode\");\nrun(\"Fill Holes\");\ndoWand(1220, 104, 0.0, \"Legacy\");\nsetBackgroundColor(0,0,0);\nrun(\"Clear Outside\");\nselectWindow(\"1\");\nrun(\"Restore Selection\");\n//-------------------------\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4.jpeg\" data-download-href=\"/uploads/short-url/6h7IMr4AVEKGc5plz2hl3wj79yY.jpeg?dl=1\" title=\"obtenir\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_139x250.jpeg\" alt=\"obtenir\" data-base62-sha1=\"6h7IMr4AVEKGc5plz2hl3wj79yY\" width=\"139\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_139x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_208x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bfc995dc9e3bcf2649b9a06d65b77a2e36460d4_2_278x500.jpeg 2x\" data-dominant-color=\"4A4A4A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">obtenir</span><span class=\"informations\">1254\u00d72244 161 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Thank you for your answer.</p>\n<p>I\u2019ll give it a try!</p>\n<p>Bastien</p>", "<p>Hello Gabriel,</p>\n<p>In order to obtain these images I do not use a microscope but a classic camera.</p>\n<p>Bastien</p>", "<p>Hello,</p>\n<p>Thank you I will try this method, please excuse my time of answer I had some problems of connection to the forum these last times after posting my topic.</p>\n<p>Do you think it is possible to apply this method on a stack of images?</p>\n<p>Bastien</p>", "<p>Hello,<br>\nThere are one or two mistakes but your macro that you provided me with gives me a good basis to explore my problem!</p>\n<p>Thanks a lot !</p>", "<aside class=\"quote no-group\" data-username=\"Bastien\" data-post=\"10\" data-topic=\"78743\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/b/51bf81/40.png\" class=\"avatar\"> Bastien:</div>\n<blockquote>\n<p>In order to obtain these images I do not use a microscope but a classic camera.</p>\n</blockquote>\n</aside>\n<p>You can still use a dark background and more than one strong side illumination sources. Most likely that will solve the problem.</p>", "<p>Ok I will try this !</p>\n<p>Thank\u2019s a lot !</p>\n<p>Bastien</p>"], "78746": ["<p>I\u2019m using the latest version of DLC with version 4.0.7 of Python on a Windows 10</p>\n<p>I\u2019m able to create a new project, extract frames, configure the yaml file, and label the frames, yet when I do Save All Layers, I get a series of error messages and endless lines of addresses. Afterward, it says it *** END STACK TRACE POINTER ***</p>\n<p>Did I download something wrong? What can I do to fix this problem?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f.png\" data-download-href=\"/uploads/short-url/p3q6T22xrEphqM2rXM3cXsBfGsn.png?dl=1\" title=\"Screenshot 2023-03-17 040327\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_690x365.png\" alt=\"Screenshot 2023-03-17 040327\" data-base62-sha1=\"p3q6T22xrEphqM2rXM3cXsBfGsn\" width=\"690\" height=\"365\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_690x365.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_1035x547.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af9959f261ce9f723f636299486dcd80f1aca09f_2_1380x730.png 2x\" data-dominant-color=\"1B1B1B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-17 040327</span><span class=\"informations\">1920\u00d71018 72.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>You should save just the point data layer</p>", "<p>Thank you so much, that worked for me!</p>"], "48027": ["<p>Hello,</p>\n<p>I get an error message, when <em>passing the variable</em> <em><strong>windowName</strong></em> (which is the name of the currently open stack) to 3D viewer for display.</p>\n<pre><code class=\"lang-auto\">windowName=getTitle();\nselectWindow(windowName);\nrun(\"3D Viewer\");\ncall(\"ij3d.ImageJ3DViewer.setCoordinateSystem\", \"false\");\ncall(\"ij3d.ImageJ3DViewer.add\", \"+windowName+\", \"None\", \"+windowName+\", \"0\", \"true\", \"true\", \"true\", \"2\", \"0\");\n</code></pre>\n<p><strong>error message:</strong></p>\n<pre><code class=\"lang-auto\">(Fiji Is Just) ImageJ 2.1.0/1.53c; Java 1.8.0_172 [64-bit]; Windows 7 6.1; 222MB of 11720MB (1%)\n\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat ij.macro.Functions.call(Functions.java:4515)\n\tat ij.macro.Functions.getStringFunction(Functions.java:276)\n\tat ij.macro.Interpreter.getStringTerm(Interpreter.java:1475)\n\tat ij.macro.Interpreter.getString(Interpreter.java:1453)\n\tat ij.macro.Interpreter.doStatement(Interpreter.java:333)\n\tat ij.macro.Interpreter.doStatements(Interpreter.java:264)\n\tat ij.macro.Interpreter.run(Interpreter.java:160)\n\tat ij.macro.Interpreter.run(Interpreter.java:93)\n\tat ij.macro.Interpreter.run(Interpreter.java:104)\n\tat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:161)\n\tat ij.IJ.runMacro(IJ.java:153)\n\tat ij.IJ.runMacro(IJ.java:142)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1148)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1144)\n\tat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1095)\n\tat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1144)\n\tat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:157)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:165)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat ij3d.ContentCreator.getImages(ContentCreator.java:165)\n\tat ij3d.ContentCreator.createContent(ContentCreator.java:74)\n\tat ij3d.Image3DUniverse.addContent(Image3DUniverse.java:831)\n\tat ij3d.ImageJ3DViewer.add(ImageJ3DViewer.java:164)\n\t... 30 more\n</code></pre>\n<p>The error does not appear when I use the full window name (<em>\u201cI:/201213/control.lif - Series002 - C=2\u201d</em>) instead of the variable.</p>\n<p>Thanks for help <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>I found the mistake:</p>\n<p>instead of using: <strong>\u201c+<em>windowName</em>+\u201d</strong>, it is just <em>windowName</em></p>", "<p>Hello,</p>\n<p>I get an error message like Chris although I used the full window name(\u201cC1-initialcleaning_1_z.oir Group:1 Level:1 Area:1\u201d).</p>\n<pre><code class=\"lang-auto\">//Folder setting\nshowMessage(\"Select Open Folder\");\nopenDir = getDirectory(\"Choose a Directory\");\nshowMessage(\"Select Save Folder\");\nsaveDir = getDirectory(\"Choose a Directory\");\nlist = getFileList(openDir);\n\n//connection a name\n//Array.show(list);\n//print(openDir);\nname = openDir + list[0];\n\n//Open Oir file\nrun(\"Viewer\", \"open=[name]\");\n\n//Split channels\nrun(\"Split Channels\");\nclose();\nclose();\nredname = \"C1-\"+list[0]+\" Group:1 Level:1 Area:1\"+\".oir\";\n\n//Enter min and Max\nmin = getNumber(\"Min (0-255):\", 0);\nmax = getNumber(\"Max (0-255):\", 1000);\n\n//\u62e1\u5f35\u5b50\u3088\u308a\u3082\u524d\u5074\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\nname = getTitle();\ndotIndex = lastIndexOf(name,\".\");\ntitle = substring(name,0,dotIndex);\nprint(title);\n\n//Contrast Adjust\nsetMinAndMax(min, max);\nrun(\"Apply LUT\", \"stack\");\n\n//\u624b\u52d5\u3067ROI\u3092\u9078\u629e\u3059\u308b\nsetTool(\"rectangle\")\nwaitForUser(\"ROI selection\", \"Select ROI using rectangle tool then click \\\"OK\\\".\");\nrun(\"Crop\");\n\n//3D Viewer\nrun(\"3D Viewer\");\ncall(\"ij3d.ImageJ3DViewer.setCoordinateSystem\", \"false\");\ncall(\"ij3d.ImageJ3DViewer.add\", \"C1-initialcleaning_1_z.oir Group:1 Level:1 Area:1\", \"None\", \"C1-initialcleaning_1_z.oir Group:1 Level:1 Area:1\", \"0\", \"true\", \"true\", \"true\", \"2\", \"0\");//**error**\n</code></pre>\n<p><strong>error message</strong></p>\n<pre><code class=\"lang-auto\">(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_202 [64-bit]; Mac OS X 10.16; 198MB of 7915MB (2%)\nMacro line number: 46\n \njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat ij.macro.Functions.call(Functions.java:4620)\n\tat ij.macro.Functions.getStringFunction(Functions.java:277)\n\tat ij.macro.Interpreter.getStringTerm(Interpreter.java:1520)\n\tat ij.macro.Interpreter.getString(Interpreter.java:1498)\n\tat ij.macro.Interpreter.doStatement(Interpreter.java:336)\n\tat ij.macro.Interpreter.doStatements(Interpreter.java:267)\n\tat ij.macro.Interpreter.run(Interpreter.java:163)\n\tat ij.macro.Interpreter.run(Interpreter.java:93)\n\tat ij.macro.Interpreter.run(Interpreter.java:107)\n\tat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)\n\tat ij.IJ.runMacro(IJ.java:158)\n\tat ij.IJ.runMacro(IJ.java:147)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)\n\tat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)\n\tat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)\n\tat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:164)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -32048\n\tat vib.NaiveResampler$MaxLikelihood.add(NaiveResampler.java:72)\n\tat vib.NaiveResampler.resample(NaiveResampler.java:233)\n\tat vib.NaiveResampler.resample(NaiveResampler.java:155)\n\tat vib.NaiveResampler.resample(NaiveResampler.java:159)\n\tat voltex.VoltexGroup.&lt;init&gt;(VoltexGroup.java:102)\n\tat ij3d.ContentInstant.displayAs(ContentInstant.java:153)\n\tat ij3d.ContentCreator.createContent(ContentCreator.java:106)\n\tat ij3d.ContentCreator.createContent(ContentCreator.java:74)\n\tat ij3d.Image3DUniverse.addContent(Image3DUniverse.java:831)\n\tat ij3d.ImageJ3DViewer.add(ImageJ3DViewer.java:164)\n\t... 30 more\n\n</code></pre>\n<p>I\u2019m so sorry to revive this topic. I need your help.</p>"], "78753": ["<p>Hello.<br>\nI\u2019m using DeepLabCut for the first time and everytime I try to creat a new project I receive the following error:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec.png\" data-download-href=\"/uploads/short-url/9shjHyFWj7s5irX9MVhwkj9Pqc4.png?dl=1\" title=\"errodeeplabcut\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_690x370.png\" alt=\"errodeeplabcut\" data-base62-sha1=\"9shjHyFWj7s5irX9MVhwkj9Pqc4\" width=\"690\" height=\"370\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_690x370.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_1035x555.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4245ebbceece1626fba15751c2adce8351938eec_2_1380x740.png 2x\" data-dominant-color=\"28323C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">errodeeplabcut</span><span class=\"informations\">1920\u00d71032 51.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you for your time.</p>", "<p>Something is wrong with your location. It repeats itself multiple times.</p>"], "78759": ["<p>Hi everyone, I am very new to CellProfiler but I have read a couple of recent papers using this software to calculate nuclear envelope invaginations in cells. I would like to do something similar, but am not sure where to start. I have attached the images I want to use this technique for. I have stained for DAPI, Lamin B1, and Actin. A lot of the papers specified they used the amount of Lamin B1 fluorescence in the nucleus to calculate this occurrence. However, I know I am asking a lot with little insight so I do understand if no one is willing to help.<br>\nThank you in advanced. <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/tIUmm4aZIFXYdOvyJotMnxYosGr.pdf\">Screen Shot 2023-03-17 at 10.57.23 AM.pdf</a> (3.2 MB)</p>"], "78760": ["<p>Dear Qupath specialist,</p>\n<p>I try to find a way to quantify cells interactions in Qupath.<br>\nMy first cell type are well detected with cells detection  or cellpose. The second type of cells are difficult/impossible  to segment so I just identify the labelling above a threshold with create thresholder that create a annoation (phagocyte). I would like to quantify the region overlap in my cytoplasm (area percentage or area size in um\u00b2) of each cell and add a measure for this. Any help will be great ! Thanks</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2.jpeg\" data-download-href=\"/uploads/short-url/upRxrLGmwHu1u5x3EcbZrHHZxqW.jpeg?dl=1\" title=\"image1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_690x388.jpeg\" alt=\"image1\" data-base62-sha1=\"upRxrLGmwHu1u5x3EcbZrHHZxqW\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d52daa6a41be3d26f591fa719a2464be30e9bfb2_2_1380x776.jpeg 2x\" data-dominant-color=\"685964\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image1</span><span class=\"informations\">1914\u00d71077 240 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>I try the subcellular detection  and it seems to work but can I classify a cell based on the subcellular component area inside ? I resume I want to classify cell in a  class interaction if there are subcelluar components of area &gt; 2 um\u00b2 (at least one)</p>", "<p>Here\u2019s a helpful thread doing some of what you want: <a href=\"https://forum.image.sc/t/need-help-quantifying-nuclear-and-cytoplasmic-rnascope-foci/51077/5\" class=\"inline-onebox\">Need help quantifying nuclear and cytoplasmic RNAscope foci - #5 by Research_Associate</a></p>", "<p>ok not easy to script, I try something that is not working yet (no error but no measure also !) :</p>\n<p>def imageData = getCurrentImageData()<br>\ndef hierarchy = imageData.getHierarchy()<br>\ndef annotations = hierarchy.getAnnotationObjects()<br>\ndef server = imageData.getServer()</p>\n<p>Area = 0<br>\nString objectType = \u201csubcell\u201d</p>\n<p>if(objectType == \u201ccytoplasm\u201d){<br>\ngetCellObjects().each{<br>\nArea = it.getChildObjects()[0].getMeasurementList().getMeasurementValue(\u201cSubcellular Cluster: Channel 3: Area\u201d)<br>\nprintln(Area)<br>\nit.getMeasurementList().putMeasurement(\u201cArea_phagocytes_inside_cyto\u201d, Area)<br>\n}<br>\n}</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816.jpeg\" data-download-href=\"/uploads/short-url/4nFRE3g5s6EQ1D4jjv5SMexdNmm.jpeg?dl=1\" title=\"Clipboard\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816_2_689x360.jpeg\" alt=\"Clipboard\" data-base62-sha1=\"4nFRE3g5s6EQ1D4jjv5SMexdNmm\" width=\"689\" height=\"360\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816_2_689x360.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816_2_1033x540.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1eb5d1277c94d0fda16ee628a6614230f2d42816.jpeg 2x\" data-dominant-color=\"4E2E55\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Clipboard</span><span class=\"informations\">1279\u00d7669 132 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Does the print statement show the correct results?<br>\nAlso, the script as written will only work for the first child object, and only if it is a cluster, due to the [0]</p>", "<p>If I remove the if statement, I have some print for only the first subcellular cluster of each cell so I think I need to get all child object with a list, I try to change the [1] by other but do not work \u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26.jpeg\" data-download-href=\"/uploads/short-url/aHPAQ4BpNw8iETF9rtrAJxRhDDM.jpeg?dl=1\" title=\"Clipboard\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_690x346.jpeg\" alt=\"Clipboard\" data-base62-sha1=\"aHPAQ4BpNw8iETF9rtrAJxRhDDM\" width=\"690\" height=\"346\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_690x346.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_1035x519.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b0a2ae590ad77790fd302b1484b424a8b34cf26_2_1380x692.jpeg 2x\" data-dominant-color=\"403F46\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Clipboard</span><span class=\"informations\">1524\u00d7765 134 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78761": ["<p>Hello,<br>\nI am acquiring images from Olimpus VS200 scanner. When I opened the images on imageJ I have the information in pixels and  I do not know how to calibrate the image to add a scale bar.</p>\n<p>So, I started to use Qupath which shows me the metadata and I have the pixel width and pixel height<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ffdf58135fb4c22f484950ad57ba36c7497a746d.png\" alt=\"image\" data-base62-sha1=\"AvygzN2pmEmjRfgZ1UOIzXY2Qzj\" width=\"378\" height=\"123\"></p>\n<p>Unfortunately, Qupath cannot add a scale bar or change colors or dimensions.</p>\n<p>Can someone help me to figure out how to add a scale bar from Qupath or in ImageJ.</p>\n<p>I have a Mac and I cannot use the viewer from Olympus.</p>\n<p>Thank you<br>\nPablo</p>", "<p>Hi Pablo,</p>\n<p>So QuPaths current approach to scale bars is an adaptable one in the lower left side of the screen that changes depending on the current view. If you\u2019re unable to see it then check out the \u201cView\u201d dropdown menu and you will find \u201cshow scalebar\u201d. From there you can take a screenshot of the viewers content with the scalebar via \u201cFile\u201d \u2192 \u201cExport snapshot \u2026\u201d \u2192 \u201ccurrent viewer content\u201d.</p>\n<p>This is however is a small and un-constomisable scale bar (currently) so to I\u2019d advise sending the screenshot over to imageJ within QuPath and then create a scale bar there as the transfer handles setting the correct scale. How to do that can be found <a href=\"https://qupath.readthedocs.io/en/0.4/docs/advanced/imagej.html#sending-image-regions-to-imagej\" rel=\"noopener nofollow ugc\">here</a>.</p>\n<p>Alternativley, to set the scale in ImageJ without the need for QuPath at all, go to \u201cAnalyze\u201d \u2192 \u201cSet Scale\u201d. From here you can fill in the window with your scanners scale values and then you\u2019re ready to create the bars.</p>\n<p>Hope this helps!</p>\n<p>Fiona</p>", "<p>Hi All,</p>\n<p>I think since 0.4.0 there is a way to change the appearance of the scale bar in Preferences, it often is sufficient for my needs, but going to ImageJ gives you more options.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e96402fd2d7aac30d82e52c11767cc8a2ef0887.png\" data-download-href=\"/uploads/short-url/fMiqOlHSXouJarvDbHhwQZ4x33F.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6e96402fd2d7aac30d82e52c11767cc8a2ef0887.png\" alt=\"image\" data-base62-sha1=\"fMiqOlHSXouJarvDbHhwQZ4x33F\" width=\"690\" height=\"303\" data-dominant-color=\"98A59A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">740\u00d7325 25.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Pable, this a bit outdated video on  <a href=\"https://www.youtube.com/watch?v=VW4kAbaSYMU\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Data visualization in QuPath - YouTube</a> might be helpful if you are looking for ways to customize the channels.</p>\n<p>Best, Z.</p>", "<p>Thank you very much. I tried to set the scale on ImageJ but I need a reference that I cannot get from the Olympus metadata that I see on QUpath. So, I exported it as OME tiff and I got the information, is it Ok for the values exported as OME tiff?<br>\nThanks</p>", "<p>Thank you very much. I will try to adjust it this way.</p>"], "78762": ["<p>Hi all,<br>\nI\u2019m writing a small macro that would create a 3D heatmap from ROIs found in a stack.  At the end of the macro, I have a stack that contains a series of spheres of value=1 for each ROI.  Where the spheres intersect, the values are added.  The stack is visualised using 3D viewer.<br>\nMy issue is the last line of the macro: I\u2019m trying to apply transparency to the 3D image using <code>call(\"ij3d.ImageJ3DViewer.setTransparency\", \"0.75\");</code>  as recorded when called manually from the 3D viewer.  Unfortunately, the command is not implemented\u2026 What am I doing wrong?</p>\n<p>M.</p>\n<p>Here is the full macro:</p>\n<pre><code class=\"lang-auto\">\n//setting minimum required measurements\nrun(\"Set Measurements...\", \"area centroid stack display redirect=None decimal=3\");\n\n//Getting file information\nTitle=getTitle();\ngetVoxelSize(width, height, depth, unit);\nVoxelWidth=width;\nVoxelHeight=height;\nVoxelDepth=depth;\nVoxelunit=unit;\nRatioXYZ=VoxelDepth/VoxelWidth; //unless the image is isotropic, Z step is longer than XY, so the sphere is actually skewed like a rugby ball\n\ngetDimensions(width, height, channels, slices, frames);\n\n//ask user for sphere radius, which will determine how far in the vicinity one needs to look for neighbours\nSphereRadius=getNumber(\"What is the average radius of the sphere that represents a ROI, in pixels\", 20);\nSphereRadiusZ=SphereRadius/RatioXYZ;\n\n//collecting ROIs coordinates -currently using regular 2D, will have to adapt to get from 3D suite\nNumberOfRois=roiManager(\"count\");\nroiManager(\"Measure\");\n\nXRois=newArray();\nYRois=newArray();\nZRois=newArray();\n\nfor (j = 0; j &lt; NumberOfRois; j++) {\n\tX=getResult(\"X\", j);\n\tXRois=Array.concat(XRois,X);\n\tY=getResult(\"Y\", j);\n\tYRois=Array.concat(YRois,Y);\n\tZ=getResult(\"Slice\", j);\n\tZRois=Array.concat(ZRois,Z);\n}\n\nroiManager(\"reset\");\n\nsetBatchMode(true);\n\n/* Creating the spheres for 3D heat map, one sphere per ROI\n *  Each sphere is created as a stack of discs with increasing and decreasing radii.\n * Each sphere is created in a new stack and stacks are added one by one to the first stack\n * Each sphere is given the value of 1.  As they are added to the stack, regions of overlapping spheres have increased values\n*/\nfor (i = 0; i &lt; NumberOfRois; i++) {\n\tnewImage(\"Sphere_\"+i, \"8-bit color-mode\", width, height, 1, slices, 1);\n\ttopSlice=round(ZRois[i]-SphereRadiusZ+1);\n\tbottomSlice=round(ZRois[i]+SphereRadiusZ-1);\n\tNumberOfSlices=round(2*SphereRadiusZ);\n\tSphereSlice=1;\n\tselectWindow(\"Sphere_\"+i);\n\t\n\tfor (sl= topSlice; sl &lt; bottomSlice; sl++) { //i.e. from top of sphere to bottom\n\t\t\n\t\tH=sl-ZRois[i];  //this is the distance between the centre of the sphere and the centre of the disc being added to the stack\n\t\t\n\t\t//the following commands is necessary if a sphere is created by the edge of the stack, with coordinates outwith, which leads to error\n\t\tif(sl&lt;1){  //if Z coordinate is above the stack, then Z is automatically at the first slice\n\t\t\tZ=1;\n\t\t}\n\t\telse{\n\t\t\tif(sl&gt;slices){ //if Z coordinate is below the stack, then Z is automatically at the last slice.\n\t\t\t\tZ=slices;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tZ=sl;\n\t\t\t}\n\t\t}\n\t\tStack.setSlice(Z);\n\t\tCurrentRadius=sqrt(pow(SphereRadius,2)-pow(H,2)); //using Pythagoras to calculate the radius of each disc\n\t\t\n\t\tmakeOval(XRois[i]-CurrentRadius, YRois[i]-CurrentRadius, CurrentRadius*2, CurrentRadius*2);\n\t\troiManager(\"add\");\n\t\trun(\"Set...\", \"value=1 slice\");\n\n\t\tSphereSlice=SphereSlice+1;\n\n\t\t//roiManager(\"reset\");\n\t}\n\troiManager(\"reset\");\n\t//adding the newly formed stack to the first stack\n\tif(i&gt;0){\n\timageCalculator(\"Add stack\", \"Sphere_0\",\"Sphere_\"+i);\n\tclose(\"Sphere_\"+i);\n\t}\n}\t\n\n//visualising result in 3D viewer\n\nselectWindow(\"Sphere_0\");\nsetBatchMode(false);\nrename(\"3D_HeatMap\");\nrun(\"glasbey_on_dark\");\nrun(\"3D Viewer\");\ncall(\"ij3d.ImageJ3DViewer.setCoordinateSystem\", \"false\");\ncall(\"ij3d.ImageJ3DViewer.add\", \"3D_HeatMap\", \"None\", \"3D_HeatMap\", \"0\", \"true\", \"true\", \"true\", \"2\", \"0\");\ncall(\"ij3d.ImageJ3DViewer.setTransparency\", \"0.75\");\n\n</code></pre>\n<p>\u2026and here is a simple macro to create a volume with random ROIs:</p>\n<pre><code class=\"lang-auto\">newImage(\"HyperStack\", \"16-bit grayscale-mode\", 500, 500, 1, 500, 1);\ngetDimensions(width, height, channels, slices, frames);\nfor (i = 0; i &lt; 200; i++) {\n\tx=random*width;\n\ty=random*height;\n\tz=round(random*slices);\n\tsetSlice(z);\n\tmakePoint(x, y);\n\troiManager(\"add\");\n\n}\n\n</code></pre>"], "78768": ["<p>This is the version of the ImageJ I am using:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png\" data-download-href=\"/uploads/short-url/wHHVEARNewpXphnJDKGeFnnveO6.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e5366ae8352a4b651457f70cb90c9d9d8b013232.png\" alt=\"image\" data-base62-sha1=\"wHHVEARNewpXphnJDKGeFnnveO6\" width=\"674\" height=\"500\" data-dominant-color=\"4D4A1D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">758\u00d7562 103 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I am operating on Windows 10.<br>\nThe problem is when I open a colorized stack composite in 3D viewer plugin, I go to view, and take snapshot, and nothing is in frame, and a black screen results:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238.jpeg\" data-download-href=\"/uploads/short-url/g6LaNsmsUqFvLDHHVK3g9nw25As.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg\" alt=\"image\" data-base62-sha1=\"g6LaNsmsUqFvLDHHVK3g9nw25As\" width=\"690\" height=\"398\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_690x398.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1035x597.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/0/70e66e7e90493b1e18871fce80879bf403647238_2_1380x796.jpeg 2x\" data-dominant-color=\"171714\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1467\u00d7847 28.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg\" data-download-href=\"/uploads/short-url/bM3txLoZ9CSkkGWNbCPKzu7ylSh.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg\" alt=\"image\" data-base62-sha1=\"bM3txLoZ9CSkkGWNbCPKzu7ylSh\" width=\"690\" height=\"404\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_690x404.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9_2_1035x606.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52869300748ab2cb4f366669a13e3f6b2c1e76d9.jpeg 2x\" data-dominant-color=\"100E0D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1350\u00d7791 54.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It will take a snapshot of the bounding box, but the sample is missing from the snapshot. I tried on a mac laptop, and it worked fine, but I\u2019m wondering if anyone else is having this problem in ImageJ Windows 10? Is there a solution to this?</p>"], "78769": ["<p>Hi,</p>\n<p>How would I overlay these images and find the area of the red parts of the brain in the right image that is covered by the white parts of the brain in the left image.</p>\n<p>Thanks<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8.jpeg\" data-download-href=\"/uploads/short-url/gAlWGG7caBuCsUTj4vEbBePbDiE.jpeg?dl=1\" title=\"Screen Shot 2023-03-17 at 1.09.30 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_690x194.jpeg\" alt=\"Screen Shot 2023-03-17 at 1.09.30 PM\" data-base62-sha1=\"gAlWGG7caBuCsUTj4vEbBePbDiE\" width=\"690\" height=\"194\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_690x194.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_1035x291.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/743ece30b43a7d4a8570957c448a695f0b59c5f8_2_1380x388.jpeg 2x\" data-dominant-color=\"475157\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-17 at 1.09.30 PM</span><span class=\"informations\">1920\u00d7542 38.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi,<br>\nCreate two mask images from the source images with the Threshold\u2026 or Color Threshold\u2026 commands. Then compute the intersection of these masks with Image Calulator\u2026 using the AND operator. Measure the intersecting area on the resulting image.</p>\n<p>Jerome</p>"], "78770": ["<p>I am trying to analyze the change of intensity in different time frames. I am selecting an irregular shaped particle by freehand and measuring the intensity. There are 5 pictures of the same sample to measure the change of intensity with time. How can I select the same particle in all 5 images?<br>\nIf anyone can help me in this regard, that would be great.<br>\nThanks in advance.</p>", "<p>Hi <a class=\"mention\" href=\"/u/roympc\">@roympc</a> . You can convert your images into a stack. Draw your ROI on the first image of the stack. Add this to the ROI Manager. Assuming your object is at the same position in all your images, you can just go to \u2018More\u2019 on the ROI Manager, then select Multi-Measure.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/jomaydc\">@jomaydc</a> I tried to do the way you mentioned. But in the results how can I check the time interval? in the measurement table its mentioning that the mean, min and max values. But I am not sure about the time interval e.g. 0 min, 5min or 15 min</p>", "<p>If that information is not included in the metadata of the original file, there is no way for Fiji to know that information, unfortunately. Much like pixel size, some of that information needs to come from the file, or from you.<br>\nIf you opened the image through BioFormats, there should be a checkbox to look at the image metadata that you can select. Alternatively, you can check Image \u2192 Properties\u2026 or Image \u2192 Show info to see if the information is stored there.</p>", "<p>Hi <a class=\"mention\" href=\"/u/roympc\">@roympc</a></p>\n<p>It would be much easier for us to help you if you uploaded an image (or better: several).</p>"], "78771": ["<p>Hello everyone,</p>\n<p>First of all, really thank you for the cellpose developers and experts.<br>\nI am a really fresh developer in this field and so curious about the powerful functions of cellpose 2.2.<br>\nNow I just started the cell segment by using cellpose 2.2 and succeeded to get  \u201c<em>_seg.npy\" and \"</em>_cp_output.jpg\u201d files from origine images by using windows command.<br>\nThis is the result.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d.jpeg\" data-download-href=\"/uploads/short-url/vPEVnpF0ctIpkD4pADsSgPXzX8V.jpeg?dl=1\" title=\"029_img.ome_cp_output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg\" alt=\"029_img.ome_cp_output\" data-base62-sha1=\"vPEVnpF0ctIpkD4pADsSgPXzX8V\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1380x344.jpeg 2x\" data-dominant-color=\"A5A7A3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">029_img.ome_cp_output</span><span class=\"informations\">3600\u00d7900 239 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And now I want to get more detailed images from the \u201c<em>_seg.npy\" file.<br>\nI really want to do this but I am not sure how to do this because I just have some knowledge about Python and image processing.<br>\nIf possible, please share with me the python source code to get these individual images and correct position of ROIS from the \"</em>_seg.npy\u201d file.</p>\n<p>Thank all of you again.</p>", "<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\" target=\"_blank\" rel=\"noopener\">MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb</a></h4>\n\n\n      <pre><code class=\"lang-ipynb\">{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"accelerator\": \"GPU\",\n    \"colab\": {\n      \"provenance\": [],\n      \"include_colab_link\": true\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nFrom github repository, it looks like the flags are default set to false to output those images individually (see last cell).</p>"], "78773": ["<p>Dear community (esp. python-microscope),<br>\nI see there is code online to drive a Linkam CMS196 online here:<br>\n<a href=\"https://github.com/python-microscope/microscope/tree/master/microscope/stages\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">microscope/microscope/stages at master \u00b7 python-microscope/microscope \u00b7 GitHub</a><br>\nBut there doesn\u2019t seem to be any guide about how to implement it, could I have some hints?</p>\n<p>Also this stage seems to be catered for explicitly, looking through the source code, but the stage doesn\u2019t appear here:<br>\n<a href=\"https://python-microscope.org/doc/architecture/supported-devices.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Supported Devices \u2014 Python Microscope</a></p>\n<p>Given a few hints, we\u2019d be happy to feedback a howto writeup,</p>\n<p>Thanks,</p>\n<p>Edward<br>\n(EPFL Center for Imaging)</p>", "<p>Hi</p>\n<p>The Linkam CMS196 stage was the first stage device we implemented in Python-Microscope before we had settled on a stage interface. As such, the interface is slightly different and does not implement <code>microscope.abc.Stage</code>. That\u2019s the reason why it was not listed under supported devices. It\u2019s used in production in a few places but we never went back to \u201cretrofit\u201d it.</p>\n<p>To use the implementation you need the LinkamSDK which is sold separately by Linkam (the SDK supports Linux and Windows). Here\u2019s a short code snippet on how to use it (untested - I no longer have access to the hardware).</p>\n<pre><code class=\"lang-auto\">from microscope.stages.linkam import LinkamCMS\n\nstage - LinkamCMS(uid=\"\")  # I think you can leave the UID empty if you only have one stage connected\n\n# Returns dict with connection state, temperature, light, etc\nprint(stage.get_status())\n\nassert stage.get_status().get(\"connected\")\n\nposition = stage.get_position()\n\nprint(\"current position is\", stage.get_position())  # dict with X and Y keys\nstage.move_to((xpos, ypos))  # 2 element tuple of floats\n\nprint(\"x limit is\", stage.get_value_limits(\"MotorSetpointX\")\nprint(\"y limit is\", stage.get_value_limits(\"MotorSetpointY\")\n</code></pre>", "<p>Thanks or much for the fast reply, that\u2019s great!</p>\n<p>We\u2019re not quite up and running yet, but hopefully not too far away.</p>\n<p>Using microscope 0.6.0 from pip, on Ubuntu 20.04, with the stage plugged in over USB.<br>\nWe\u2019re running everything as root, and we confirm that the SimpleC programme that came with the SDK compiles and runs OK with:<br>\n<code>SimpleC -c USB -vi 16da -pi 0007</code></p>\n<p>Using microscope we do not succeed in completing <code>init_sdk()</code> \u2013 it manages to load the DLL (although we hard to write the path in hard)</p>\n<pre><code class=\"lang-auto\">__class__._lib = ctypes.CDLL(\"libLinkamSDK.so\")\n</code></pre>\n<p>We\u2019re then able to read the license correctly, however when we get to:</p>\n<pre><code class=\"lang-auto\">        cfunc = ctypes.CFUNCTYPE(_uint32_t, _CommsHandle, _ControllerStatus)(\n            __class__._on_new_value\n        )\n</code></pre>\n<p>We get this output:</p>\n<blockquote>\n<p>*** stack smashing detected ***: terminated<br>\nAborted</p>\n</blockquote>\n<p>Have you seen this before?<br>\nWe\u2019ve tried this both with the \u201clatest\u201d 3.0.16 <code>libLinkamSDK.so</code> (~20.7MB) and the one that came on the official USB stick with the license (17.9MB) whose version it is not easy to understand (apparently &gt;3.0).</p>\n<p>We\u2019d really appreciate some tips, we\u2019re working towards a projects that will be open sourced and given back to the community!</p>\n<p>Edward</p>", "<p>I had not seen that before but after some debugging I think I\u2019ve found the problem. I\u2019ve reported it on <a href=\"https://github.com/python-microscope/microscope/issues/273\" class=\"inline-onebox\">LinkamCMS segfaults in Linux \u00b7 Issue #273 \u00b7 python-microscope/microscope \u00b7 GitHub</a> Basically, we\u2019re doing something that is not supported by Python and while it does work in Windows (or some of its architectures) it doesn\u2019t work in Linux.</p>\n<p>I no longer have access to a Linkam CMS and neither do the most active project developers. All the places I know using this code use Windows. If you\u2019re comfortable hacking the code, I can provide some hints but this is now moving into bug fixing and maybe it should moved from this forum to the bug tracker.</p>", "<p>Thanks very much, I\u2019ll happily move to the bug tracker!<br>\nI should have access to the stage soon to make the test you suggest.</p>", "<p>Hi Edward,</p>\n<p>I\u2019ve had to do quite some troubleshooting with a Linkam CMS196 in Python, so I think I may be able to help out - feel free to contact me. Some code can be found here (but mind that there are some hard-coded bits in there):</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py\" target=\"_blank\" rel=\"noopener nofollow ugc\">bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py</a></h4>\n\n\n      <pre><code class=\"lang-py\">from ctypes import *\nimport Parameters as prm\nlsdk = None\nstageHandle = c_int(0)\nlinkamProcessMessageCommon = None\nstageLims = [(100.0, 10000.0), (100.0, 2600.0)]\nconnection = False\nsimulated = False\nsimulatedPosition = [0.0, 0.0]\nfrom Utility import *\nfrom os import system\n\nclass CommsInfo(Structure):\n    _fields_ = [\n        ('info', c_char*124),\n    ]\n\nclass USBCommsInfo(Structure):\n    _fields_ = [\n        ('vendorID', c_uint16),\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/bionanopatterning/cryoscope_2/blob/764b8a445b9c4f305a658a5026d7312a50eb8cb5/CryoStage.py\" target=\"_blank\" rel=\"noopener nofollow ugc\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hi Mart</p>\n<p>Thank you for sharing the code (both in this answer and the Cryoscope project in general).</p>\n<p>I\u2019ve looked at how you handle the bit-fields for the flags (which is at the root of this issue) and I\u2019m surprised the way you do it works correctly. You\u2019re doing:</p>\n<pre><code class=\"lang-auto\">class ConnectionStatus(Structure):\n    _fields_ = [(\"flags\", c_bool*32), (\"value\", c_uint32)]\n</code></pre>\n<p>which maps to a struct with 32 <code>_Bool</code>s (each taking 8bit) <em>and</em> one <code>uint32_t</code>. That\u2019s a total of 288 bits (32*8 + 32). Instead, the SDK takes a union of one <code>uint32_t</code> and one struct with 32 bit-fields. The union should have a size of 32 bits.</p>\n<p>I would guess that the real value of the flags is merged into <code>ConnectionStatus.flags[:4]</code></p>\n<p>But maybe you\u2019re working with a different SDK version (we have 3.0.0100 and 3.0.15)?</p>", "<p>Hi David,</p>\n<p>I\u2019m using 3.0.15. It\u2019s been a while since I did anything with this SDK or with ctypes in general, so I\u2019m not sure why mine would or wouldn\u2019t work\u2026 I had assumed that a c_bool was 1 bit.</p>\n<p>Some of the structs that are in my driver are just left over from when I was testing the SDK. After I got the stage to work (i.e. moving the motors, setting condenser intensity, reading temperatures), I left it at that. The python-microscope/microscope code is much more sophisticated so it could be that the ConnectionStatus struct doesn\u2019t actually work for me, but that I can successfully ignore it because of the crude driver.</p>", "<p>A warm thanks to both of you for your inputs, we will make it work!</p>\n<p>I propose to continue to discuss the cryoscope option here, and the python-microscope on the github issue.<br>\nWe tried to use the code on Linux, loading the <code>libLinkamSDK.so</code> from our path (both of the versions we have).</p>\n<p>In both cases we have a segfault during <code>CryoStage.Connect()</code> when calling <code>linkamProcessMessageCommon()</code></p>"], "78774": ["<p>Hey all,</p>\n<p>Is there a way to combine these two processes? Currently, \u2018Find Edges\u2019 is overriding watershedding? I say override, but in actuality what is happening is after running the \u2018Find Edges\u2019 tool, analyze particles is no longer picking up the watershedding that is taking place.</p>\n<p>Any help is appreciated!</p>", "<p>It would probably help to show some example pictures. Intuitively I would not expect find edges and watershedding to interact well together (lines versus areas), but not sure what you are looking at.</p>", "<p>This is an image of watershedding. The cells are being being divided up.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/58f882e7943d46e2de1a0381142167645357176b.png\" data-download-href=\"/uploads/short-url/cH4rVJtV4acH1BKBpsW41SjqIZl.png?dl=1\" title=\"Screen Shot 2023-03-17 at 2.57.10 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_472x500.png\" alt=\"Screen Shot 2023-03-17 at 2.57.10 PM\" data-base62-sha1=\"cH4rVJtV4acH1BKBpsW41SjqIZl\" width=\"472\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_472x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_708x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58f882e7943d46e2de1a0381142167645357176b_2_944x1000.png 2x\" data-dominant-color=\"252525\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-17 at 2.57.10 PM</span><span class=\"informations\">1378\u00d71458 187 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>This is the results of running analyze particles.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac.png\" data-download-href=\"/uploads/short-url/mg7DmazvudExyOfJxx3RLHyvbyY.png?dl=1\" title=\"Screen Shot 2023-03-17 at 10.52.29 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_469x500.png\" alt=\"Screen Shot 2023-03-17 at 10.52.29 AM\" data-base62-sha1=\"mg7DmazvudExyOfJxx3RLHyvbyY\" width=\"469\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_469x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_703x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/c/9c026749c7dfc29a55fbc5f4925fbaad91fdd0ac_2_938x1000.png 2x\" data-dominant-color=\"22221D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-17 at 10.52.29 AM</span><span class=\"informations\">1370\u00d71458 64.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>This is watershedding + Find Edges<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9.png\" data-download-href=\"/uploads/short-url/khHhAvjuo1VjfciqfAr10nFdYoN.png?dl=1\" title=\"Screen Shot 2023-03-17 at 3.03.23 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_475x500.png\" alt=\"Screen Shot 2023-03-17 at 3.03.23 PM\" data-base62-sha1=\"khHhAvjuo1VjfciqfAr10nFdYoN\" width=\"475\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_475x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_712x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e2ba200835ef931c78c20ee17d61ef93b4d63b9_2_950x1000.png 2x\" data-dominant-color=\"0F0F0F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-17 at 3.03.23 PM</span><span class=\"informations\">1382\u00d71452 235 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And this is analyze particles. What we see is that the number of cells actually decrease because the watershedding (even though the watershedding lines are visible) isn\u2019t being considered.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254.png\" data-download-href=\"/uploads/short-url/njDUKygKOWB9Mhpppmuvxb2nDc8.png?dl=1\" title=\"Screen Shot 2023-03-17 at 3.03.46 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_492x500.png\" alt=\"Screen Shot 2023-03-17 at 3.03.46 PM\" data-base62-sha1=\"njDUKygKOWB9Mhpppmuvxb2nDc8\" width=\"492\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_492x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_738x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36a758a200c464f41de5d19f11ada57b8b81254_2_984x1000.png 2x\" data-dominant-color=\"0C0C08\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-17 at 3.03.46 PM</span><span class=\"informations\">1382\u00d71404 77 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Find Edges seems great in that it helps eliminate the small debris when the size parameter within Analyze Particles can\u2019t do it alone. But without watershedding, multiple cells are being counted as a single cell.</p>\n<p>I hope this helps lol.</p>", "<p>Yes, once you find edges, you have lines, which are now all connected (for touching cells), so that is the expected result.<br>\nYou would need to do something like shrink the ROI by one pixel or two to make sure the touching edges are not a single line. Unfortunately, standard erosion won\u2019t work since that\u2019s on the pixels and not the ROI.</p>\n<aside class=\"quote no-group\" data-username=\"Robert_Schweickart\" data-post=\"4\" data-topic=\"78774\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/robert_schweickart/40/67178_2.png\" class=\"avatar\"> Robert Schweickart:</div>\n<blockquote>\n<p>small debris when the size parameter within Analyze Particles can\u2019t do it alone</p>\n</blockquote>\n</aside>\n<p>Not sure what you gain from the outline then. You should be able to get a perimeter from the objects after analyze particles + Measure, or any of a variety of other measurements to filter the ROIs down more.<br>\nCould be wrong, but I don\u2019t think there is an easy way to find the shared edges once you have removed all of the information about shared edges by using Find Edges (it becomes an image with pixels, objects are gone).</p>\n<p>If you wanted to, you could probably load the old ROIs into the Find Edges result\u2026 and maybe do some sort of ROI intersection? Not sure.</p>", "<p>I may have found a convoluted method to get what I want.</p>\n<p>Run Find Edges followed my analyze particles to eliminate the small debris, use fill x2, run watershed, and then another analyze particles.</p>\n<p>It seems to give me what I would be expecting but it is super roundabout.</p>", "<p>I\u2019m glad you found your solution, but I\u2019d like to offer another approach.</p>\n<p>I\u2019ve tried layering these methods but ended up giving up due to having to optimize too many parameters at once (i.e. threshold, watershed, particle size). For my purposes, this finetuning is unacceptable due to the data volume we handle. I now use <a href=\"https://www.cellpose.org/\" rel=\"noopener nofollow ugc\">CellPose</a> to avoid the threshold/watershed/segment loop entirely.<br>\nSpecifically, we trained a custom model that has a performance we like and now deploy this model en-masse.</p>\n<p>Here\u2019s a relevant example from <a href=\"https://nbviewer.org/github/leogolds/MicroscopyPipeline/blob/main/walkthrough.ipynb\" rel=\"noopener nofollow ugc\">our repo</a>.</p>", "<p>CellPose is fantastic at differentiating the cells. If only I could implement that into my ImageJ macro and have CellPose create the DAPI mask to overlay across the other channels for cell counting\u2026it\u2019d be perfect.</p>", "<aside class=\"quote no-group\" data-username=\"Robert_Schweickart\" data-post=\"8\" data-topic=\"78774\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/robert_schweickart/40/67178_2.png\" class=\"avatar\"> Robert Schweickart:</div>\n<blockquote>\n<p>DAPI mask to overlay across the other channels for cell counting</p>\n</blockquote>\n</aside>\n<p>If cellpose has already counted the cells, do you need that?</p>\n<p>Alternatively, there are disucssions about combining StarDist for nuclear detection and CellPose for cell boundary detection for combined cell objects.</p>", "<p>My ImageJ macro produces a cell count file for each channel which I then run through my R macro and get a nice spreadsheet.</p>\n<p>I am sure it is doable it would just require me overhauling my entire pipeline.</p>"], "78776": ["<p>Hi all!</p>\n<p>I am unable to download the concentric circles plug in. I am unsure if it\u2019s my Mac and I should just try on a pc or if there is another way to download this plugin if anyone else has encountered this error before. please help if able. Thank you very much, I hope to be able to contribute to this platform someday!<br>\nWhen I go to my folder download my computer says I am unable to download this plugin because it can\u2019t recognize if it\u2019s malware. How can I get around that?<br>\nThanks!!</p>", "<p>Try downloading the java source file from <a href=\"https://imagej.nih.gov/ij/plugins/download/Concentric_Circles.java\">https://imagej.nih.gov/ij/plugins/download/Concentric_Circles.java</a></p>\n<p>Save it as Concentric_Circles.java inside your plugins folder, open it with ImageJ and se the Compile and Run\u2026 command from the editor File menu.</p>\n<p>Jerome</p>"], "78777": ["<p>I may be missing an obvious way to do this (I hope), but in a project can I rotate an individual image without applying the same rotation to all images in the project? When I have an image opened and use View &gt; Rotate image on it, my rotation is applied to all images, which is not what I want.</p>", "<p><em>View \u2192 Rotate</em> rotates the contents of the viewer. It doesn\u2019t make any changes to any images (but will impact all images opened in that viewer).</p>\n<p>If you want to rotate the image itself by an increment of 90 degrees, you can do that during import:<br>\n<a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/projects.html#add-images\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://qupath.readthedocs.io/en/0.4/docs/tutorials/projects.html#add-images</a></p>"], "78781": ["<p>This script, worked out with <a href=\"https://forum.image.sc/t/combining-two-imagej-macros-matching-in-output-directories-bio-formats/78534/7\">help from</a> <a class=\"mention\" href=\"/u/dgault\">@dgault</a> (thank you!), whose purpose is to convert images in a .lif file to .tifs and also adjust them (brightness/contrast).</p>\n<pre><code class=\"lang-auto\">run(\"Bio-Formats Macro Extensions\");\nsetBatchMode(true); \ninput = getDirectory(\"Input directory, folder where your .lif file is and nothing else\");\noutput = getDirectory(\"Output directory, where you'd like your adjusted .tiff files to go\");\n\n\nsuffix = \".lif\";\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n}\n\nfunction processFile(input, output, file) {\n\tExt.setId(input + file);\n\tExt.getSeriesCount(count);\n\tprint(\"Processing: \" + input + file+ \" - \"+d2s(count,0)+\" images\");\n\n\tfor (f=0;f&lt;count;f++) {\n\t\topenLif(input+file,f);\n\t\tprint(\"Saving to: \" + output);\n\t\tlistImages();\n\t}\n}\n\nfunction openLif(input,f){\n\tExt.setSeries(f);\n\trun(\"Bio-Formats Importer\", \"open=[\" +input +\"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_\" + d2s(f+1,0));\n}\n\n//Create string \"image_1 image_2 image_3 image_4....\"\nfunction seriesN(num){\n\tstr = \"\";\n\tfor (i=0; i&lt;num; i++){\n\t\tii = i+1;\n        str = str+\"image_\"+ii+\" \";\n     }\n     return str;\n}\n\n//Save\nfunction listImages(){\n\timageList = getList(\"image.titles\");\n\tif (imageList.length==0)\n\t     print(\"No image windows are open\");\n\telse {\n    \tprint(\"Image windows:\");\n     \tfor (i=0; i&lt;imageList.length; i++){\n        \tprint(\"   \"+imageList[i]);\n        \t// TODO: Add condition for saving unadjusted files\n                saveTiff(imageList[i]);\n                \n                // Process the files as per the second script\n                processTiff(output, output, imageList[i]);\n     \t}\n\t}\n\tprint(\"\");\n}\n\n\nfunction saveTiff(winName){\n\tselectWindow(winName); \n\tsaveAs(\"Tiff\", output + winName);\n\tclose();\n}\n\nfunction processtiff(input, output, file) {\n\n\tprint(\"processing: \" + input + file);\n\t\n\t\n\topen(input+file+\".tif\");\n\trendercolor(file);\n\tbrightnessncontrast(file);\n\tdeleteslices(file);\n\n\tsaveas(\"tiff\", output + file + \"_bcadjusted\");\n\trename(file);\n\t\n\trgbmerge(file);\n\tscalebar();\n\tmakemontage(file);\n\n\tselectwindow(\"montage1to5\");\n\tsaveas(\"jpeg\", output + file + \"_montage1to5\");\n\tselectwindow(\"montagehorizontal\");\n\tsaveas(\"jpeg\", output + file + \"_bcmontage\");\n\n\tprint(\"saving to: \" + output);\n\trun(\"close all\");\n}\n\n\n\n// Give colors for each slice\nfunction renderColor(file){\n\tcolor = newArray(\"Blue\",\"Green\",\"Red\",\"Grays\");\n\t//color = newArray(\"Blue\",\"Green\",\"Red\");\n\n\trun(\"Make Composite\", \"display=Color\");\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\trun(color[i]);\n\t}\n}\n// Change brightness and contrast\nfunction brightnessNcontrast(file){\n\t//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\tsetMinAndMax(BC_range[2*i],BC_range[2*i+1]);\n\t}\n\t\n}\n\n// Delete slices. You want to delete slice a, b\nfunction deleteSlices(file){\n\tdeleteA = 0;\n\tdeleteB = 0;\n\t//deleteB &gt; deleteA. deleteB should be larger than deleteA\n\n\tprint(\"deleting the slice \"+deleteA+\" and \"+deleteB);\n\tselectWindow(file);\n\tif(deleteB&gt;0){\n\t\tsetSlice(deleteB);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t\t}\n\tif(deleteA&gt;0){\n\t\tsetSlice(deleteA);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t}\t\n}\n\n// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.\nfunction RGBmerge(file){\n\trun(\"Duplicate...\", \"title=4channels duplicate\");\n\trun(\"RGB Color\");\n\t\t\n\tselectWindow(file);\n\trun(\"Duplicate...\", \"title=Merge duplicate\");\n\tStack.setDisplayMode(\"composite\");\n\trun(\"RGB Color\");\n\trun(\"Copy\");\n\n\tselectWindow(\"4channels (RGB)\");\n\tsetSlice(nSlices);\n\trun(\"Add Slice\"); \n\trun(\"Paste\"); \n\n\tclose(\"4channels\");\n\tclose(\"Merge\");\n\tclose(\"Merge (RGB)\");\n\t\n\t// \"4channels (RGB)\" is made.\n}\n\n// Add scale bar\nfunction scaleBar(){\n\tsetSlice(nSlices);\n\trun(\"Set Scale...\", \"distance=311.0016 known=100 pixel=1 unit=\u00b5m\");\n\t//run(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] hide\");\n\trun(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] bold\");\n}\n\nfunction makeMontage(input){\n\t//selectWindow(\"4channels (RGB)\");\n\t//run(\"Make Montage...\", \"columns=1 rows=5 scale=0.25 border=2\");\n\t//rename(\"Montage1to5\");\n\tselectWindow(\"4channels (RGB)\");\n\trun(\"Make Montage...\", \"columns=\"+nSlices+\" rows=1 scale=0.5 border=2\");\n\trename(\"MontageHorizontal\");\n}\nfunction GetTime(){\n     MonthNames = newArray(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\");\n     DayNames = newArray(\"Sun\", \"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\");\n     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);\n     TimeString =\"Date: \"+DayNames[dayOfWeek]+\" \";\n     if (dayOfMonth&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+dayOfMonth+\"-\"+MonthNames[month]+\"-\"+year+\"\\nTime: \";\n     if (hour&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+hour+\":\";\n     if (minute&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+minute+\":\";\n     if (second&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+second;\n     print(TimeString);\n}\n</code></pre>\n<p>Unfortunately, I run into an error of:</p>\n<pre><code class=\"lang-auto\">Undefined identifier in line 61\n(called from line 29)\n(called from line 17)\n(called from line 9)\n\n&lt;processTiff&gt; (output,output,imageList[i]);\n</code></pre>\n<p>And I don\u2019t know why? I saw <a href=\"https://forum.image.sc/t/unidentified-identifier-error/53920/4\">this post</a> which has a similar problem and the solution was related to variables being global/not, but I don\u2019t understand how that would apply here</p>", "<p>it\u2019s the uppercase T in the function name.</p>\n<p>Jerome</p>", "<p>Yes, that did it!</p>\n<p>Now the next line to error out is 80 <img src=\"https://emoji.discourse-cdn.com/twitter/upside_down_face.png?v=12\" title=\":upside_down_face:\" class=\"emoji\" alt=\":upside_down_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/20161cca7abaf3b5471b37c4dba74826a9b1453b.png\" alt=\"image\" data-base62-sha1=\"4zQEB15G12QW80SnpW4ybyxMtvR\" width=\"353\" height=\"480\"></p>\n<p>I believe line 80 is</p>\n<pre><code class=\"lang-auto\">\trendercolor(file);\n</code></pre>\n<p>Which is inside <code>processTiff</code></p>\n<p>Interesting there\u2019s no other mention of <code>rendercolor</code> in the whole forum, or on Google\u2026<br>\nEdit: nevermind it\u2019s not interesting, it\u2019s just the name of the function named in the script <img src=\"https://emoji.discourse-cdn.com/twitter/person_facepalming.png?v=12\" title=\":person_facepalming:\" class=\"emoji\" alt=\":person_facepalming:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>That also looks to be due to the function calls in processTiff all being lowercase for some reason, I believe in the original they had the correct cases so not sure what has happened there. The correct version should be as below:</p>\n<pre><code class=\"lang-auto\">function processTiff(input, output, file) {\n\tprint(\"Processing: \" + input + file);\n\n\topen(input+file);\n\trenderColor(file);\n\tbrightnessNcontrast(file);\n\tdeleteSlices(file);\n\n\tsaveAs(\"Tiff\", output + file + \"_BCadjusted\");\n\trename(file);\n\t\n\tRGBmerge(file);\n\tscaleBar();\n\tmakeMontage(file);\n\n\t//selectWindow(\"Montage1to5\");\n\t//saveAs(\"Jpeg\", output + file + \"_montage1to5\");\n\tselectWindow(\"MontageHorizontal\");\n\tsaveAs(\"Jpeg\", output + file + \"_BCmontage\");\n\n\tprint(\"Saving to: \" + output);\n\trun(\"Close All\");\n}\n</code></pre>", "<p>Yup, that was the issue there! What an odd thing to have happened.</p>\n<p>When I run the code now, it successfully prompts me for everything it should be (input/output directories, and brightness parameters) but then I get this Macro error</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7.jpeg\" data-download-href=\"/uploads/short-url/pCOKQukZuSIdXXcjSRk1lBidzX9.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_690x294.jpeg\" alt=\"image\" data-base62-sha1=\"pCOKQukZuSIdXXcjSRk1lBidzX9\" width=\"690\" height=\"294\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_690x294.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_1035x441.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/3/b399b1b963babc399c3c957202f8c42a1fd701b7_2_1380x588.jpeg 2x\" data-dominant-color=\"DDDBDB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2936\u00d71252 622 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>That looks to be coming from the <code>selectWindow(file);</code> command in deleteSlices. Is there a window open at the time of failure with the same name? Or is the name in the macro error incorrect for the open windows?</p>", "<p>Here is the full code</p>\n<pre><code class=\"lang-auto\">run(\"Bio-Formats Macro Extensions\");\nGetTime();\nsetBatchMode(true); \ninput = getDirectory(\"Input directory, folder where your .lif file is and nothing else\");\noutput = getDirectory(\"Output directory, where you'd like your adjusted .tiff files to go\");\n\nrun(\"Input/Output...\", \"jpeg=100 gif=-1 file=.xls use_file copy_row save_column save_row\");\nDialog.create(\"File type\");\nDialog.addString(\"File suffix: \", \".tif\", 5);\nDialog.addNumber(\"Ch1:\", 0);\nDialog.addNumber(\"Ch1:\", 65535);\nDialog.addNumber(\"Ch2:\", 0);\nDialog.addNumber(\"Ch2:\", 65535);\nDialog.addNumber(\"Ch3:\", 0);\nDialog.addNumber(\"Ch3:\", 65535);\nDialog.addNumber(\"Ch4:\", 0);\nDialog.addNumber(\"Ch4:\", 65535);\nDialog.addNumber(\"Ch5:\", 0);\nDialog.addNumber(\"Ch5:\", 65535);\nDialog.show();\nsuffix = Dialog.getString();\nRange1 = Dialog.getNumber();\nRange2 = Dialog.getNumber();\nRange3 = Dialog.getNumber();\nRange4 = Dialog.getNumber();\nRange5 = Dialog.getNumber();\nRange6 = Dialog.getNumber();\nRange7 = Dialog.getNumber();\nRange8 = Dialog.getNumber();\nRange9 = Dialog.getNumber();\nRange10 = Dialog.getNumber();\n\nBC_range = newArray(Range1,Range2,Range3,Range4,Range5,Range6,Range7,Range8,Range9,Range10);\n\nprint(\"Brightness and contrast range is: \");\nArray.print(BC_range);\nprint(\"Blue\",\"Green\",\"Red\",\"Grays\",\"Yellow\");\n\nsuffix = \".lif\";\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n}\n\nfunction processFile(input, output, file) {\n\tExt.setId(input + file);\n\tExt.getSeriesCount(count);\n\tprint(\"Processing: \" + input + file+ \" - \"+d2s(count,0)+\" images\");\n\n\tfor (f=0;f&lt;count;f++) {\n\t\topenLif(input+file,f);\n\t\tprint(\"Saving to: \" + output);\n\t\tlistImages();\n\t}\n}\n\nfunction openLif(input,f){\n\tExt.setSeries(f);\n\trun(\"Bio-Formats Importer\", \"open=[\" +input +\"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_\" + d2s(f+1,0));\n}\n\n//Create string \"image_1 image_2 image_3 image_4....\"\nfunction seriesN(num){\n\tstr = \"\";\n\tfor (i=0; i&lt;num; i++){\n\t\tii = i+1;\n        str = str+\"image_\"+ii+\" \";\n     }\n     return str;\n}\n\n//Save\nfunction listImages(){\n\timageList = getList(\"image.titles\");\n\tif (imageList.length==0)\n\t     print(\"No image windows are open\");\n\telse {\n    \tprint(\"Image windows:\");\n     \tfor (i=0; i&lt;imageList.length; i++){\n        \tprint(\"   \"+imageList[i]);\n        \t// TODO: Add condition for saving unadjusted files\n                saveTiff(imageList[i]);\n                \n                // Process the files as per the second script\n                processTiff(output, output, imageList[i]);\n     \t}\n\t}\n\tprint(\"\");\n}\n\n\nfunction saveTiff(winName){\n\tselectWindow(winName); \n\tsaveAs(\"Tiff\", output + winName);\n\tclose();\n}\n\nfunction processTiff(input, output, file) {\n\n\tprint(\"Processing: \" + input + file);\n\n\topen(input+file+\".tif\");\n\trenderColor(file);\n\tbrightnessNcontrast(file);\n\tdeleteSlices(file);\n\n\tsaveAs(\"Tiff\", output + file + \"_BCadj\");\n\trename(file);\n\t\n\tRGBmerge(file);\n\tscaleBar();\n\tmakeMontage(file);\n\n\t//selectWindow(\"Montage1to5\");\n\t//saveAs(\"Jpeg\", output + file + \"_montage1to5\");\n\tselectWindow(\"MontageHorizontal\");\n\tsaveAs(\"Jpeg\", output + file + \"_BCmont\");\n\n\tprint(\"Saving to: \" + output);\n\trun(\"Close All\");\n}\n\n\n\n// Give colors for each slice\nfunction renderColor(file){\n\tcolor = newArray(\"Blue\",\"Green\",\"Red\",\"Grays\",\"Yellow\");\n\t//color = newArray(\"Blue\",\"Green\",\"Red\");\n\n\trun(\"Make Composite\", \"display=Color\");\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\trun(color[i]);\n\t}\n}\n// Change brightness and contrast\nfunction brightnessNcontrast(file){\n\t//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\tsetMinAndMax(BC_range[2*i],BC_range[2*i+1]);\n\t}\n\t\n}\n\n// Delete slices. You want to delete slice a, b\nfunction deleteSlices(file){\n\tdeleteA = 0;\n\tdeleteB = 0;\n\t//deleteB &gt; deleteA. deleteB should be larger than deleteA\n\n\tprint(\"deleting the slice \"+deleteA+\" and \"+deleteB);\n\tselectWindow(file);\n\tif(deleteB&gt;0){\n\t\tsetSlice(deleteB);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t\t}\n\tif(deleteA&gt;0){\n\t\tsetSlice(deleteA);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t}\t\n}\n\n// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.\nfunction RGBmerge(file){\n\trun(\"Duplicate...\", \"title=4channels duplicate\");\n\trun(\"RGB Color\");\n\t\t\n\tselectWindow(file);\n\trun(\"Duplicate...\", \"title=Merge duplicate\");\n\tStack.setDisplayMode(\"composite\");\n\trun(\"RGB Color\");\n\trun(\"Copy\");\n\n\tselectWindow(\"4channels (RGB)\");\n\tsetSlice(nSlices);\n\trun(\"Add Slice\"); \n\trun(\"Paste\"); \n\n\tclose(\"4channels\");\n\tclose(\"Merge\");\n\tclose(\"Merge (RGB)\");\n\t\n\t// \"4channels (RGB)\" is made.\n}\n\n// Add scale bar\nfunction scaleBar(){\n\tsetSlice(nSlices);\n\trun(\"Set Scale...\", \"distance=311.0016 known=100 pixel=1 unit=\u00b5m\");\n\t//run(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] hide\");\n\trun(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] bold\");\n}\n\nfunction makeMontage(input){\n\t//selectWindow(\"4channels (RGB)\");\n\t//run(\"Make Montage...\", \"columns=1 rows=5 scale=0.25 border=2\");\n\t//rename(\"Montage1to5\");\n\tselectWindow(\"4channels (RGB)\");\n\trun(\"Make Montage...\", \"columns=\"+nSlices+\" rows=1 scale=0.5 border=2\");\n\trename(\"MontageHorizontal\");\n}\nfunction GetTime(){\n     MonthNames = newArray(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\");\n     DayNames = newArray(\"Sun\", \"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\");\n     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);\n     TimeString =\"Date: \"+DayNames[dayOfWeek]+\" \";\n     if (dayOfMonth&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+dayOfMonth+\"-\"+MonthNames[month]+\"-\"+year+\"\\nTime: \";\n     if (hour&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+hour+\":\";\n     if (minute&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+minute+\":\";\n     if (second&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+second;\n     print(TimeString);\n}\n</code></pre>\n<p>The error goes away when I comment out <code>selectWindow(file)</code> in</p>\n<pre><code class=\"lang-auto\">// Delete slices. You want to delete slice a, b\nfunction deleteSlices(file){\n\tdeleteA = 0;\n\tdeleteB = 0;\n\t//deleteB &gt; deleteA. deleteB should be larger than deleteA\n\n\tprint(\"deleting the slice \"+deleteA+\" and \"+deleteB);\n\t//selectWindow(file);\n\tif(deleteB&gt;0){\n\t\tsetSlice(deleteB);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t\t}\n\tif(deleteA&gt;0){\n\t\tsetSlice(deleteA);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t}\t\n}\n</code></pre>\n<p>But I don\u2019t think this is desirable as when I test this out, the adjusted .tiff seems to be identical to the raw .tiff, but the montage seems to be (correctly) affected by the brightness adjustments.</p>\n<p>As to your questions, the name in the macro error corresponds to the name of the first image, which is what should be processed first as expected, and matches the name in the \u201cProcessing\u201d line above it. There isn\u2019t anything else popping up or open besides that in the screenshot.</p>", "<p>Ok, it looks as though the windows were not opening at all, this is due to the line <code>setBatchMode(true);</code><br>\nIf you remove that line you should see the windows open and close as it runs through the processes.</p>\n<p>The last change is that the selectWindow command in deleteSlices will also need the .tif suffix added as below:<br>\n<code>selectWindow(file+\".tif\");</code></p>\n<p>With those 2 changes I was able to run the script successfully to completion.</p>", "<p>Appreciate the help.</p>\n<p>Neither adjustment seems to change the fact that the \u201c_BCadj\u201d tiff that is saved, which is supposed to reflect the brightness changes requested, is identical to the raw tiff. The saved montage continues to correctly reflect the brightness changes.</p>"], "78782": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe.jpeg\" data-download-href=\"/uploads/short-url/kv1pZgB7bkBKuG1Fb2omCEEymuO.jpeg?dl=1\" title=\"09a\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_450x375.jpeg\" alt=\"09a\" data-base62-sha1=\"kv1pZgB7bkBKuG1Fb2omCEEymuO\" width=\"450\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_450x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_675x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fad3a8f82ca1b447ed6c52b8144291868b71abe_2_900x750.jpeg 2x\" data-dominant-color=\"879D89\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">09a</span><span class=\"informations\">1060\u00d7881 323 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>I am using QuPath through command line.  I can\u2019t however figure out how to save the final image along with its annotation as a jpeg/png.</p>\n<p>I have this script, but obviously the annotations aren\u2019t saved:</p>\n<p><em>def server = getCurrentServer()</em><br>\n<em>// Downsample image</em><br>\n<em>def srv = RegionRequest.createInstance(server, 2)</em><br>\n<em>def img_dir = \u2018D:/KI67_data/Annotated images\u2019</em><br>\n<em>mkdirs(img_dir)</em><br>\n<em>def name = GeneralTools.getNameWithoutExtension(getProjectEntry().getImageName())</em><br>\n<em>def path = buildFilePath(img_dir, name + \u2018.jpg\u2019)</em><br>\n<em>writeImageRegion(server, srv, path)</em></p>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<p>getCurrentViewer(), getOverlayOptions() return errors (<em>javax.script.ScriptException: Cannot invoke method getOverlayOptions() on null object in KI_annotation.groovy at line number 106</em>) when running through command line.</p>\n<p>Therefore, I cannot use this in my script (from <a href=\"https://qupath.readthedocs.io/en/0.3/docs/advanced/exporting_images.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Exporting images \u2014 QuPath 0.3.0 documentation</a>)</p>\n<p><em>// Write the full image, displaying objects according to how they are currently shown in the viewer</em><br>\n<em>def viewer = getCurrentViewer()</em><br>\n<em>writeRenderedImage(viewer, \u2018/path/to/export/rendered.png\u2019)</em></p>\n<p>Is it possible to write a script without using getCurrentViewer(), getOverlayOptions() and save the image along with its annotation?</p>", "<p>You will probably have to create a Viewer, though not sure if that comes with even more complications without a GUI.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://qupath.github.io/javadoc/docs/qupath/lib/gui/viewer/QuPathViewer.html#%3Cinit%3E(qupath.lib.images.ImageData,qupath.lib.gui.images.stores.DefaultImageRegionStore,qupath.lib.gui.viewer.OverlayOptions,qupath.lib.display.ImageDisplay)\">\n  <header class=\"source\">\n\n      <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/gui/viewer/QuPathViewer.html#%3Cinit%3E(qupath.lib.images.ImageData,qupath.lib.gui.images.stores.DefaultImageRegionStore,qupath.lib.gui.viewer.OverlayOptions,qupath.lib.display.ImageDisplay)\" target=\"_blank\" rel=\"noopener\">qupath.github.io</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/gui/viewer/QuPathViewer.html#%3Cinit%3E(qupath.lib.images.ImageData,qupath.lib.gui.images.stores.DefaultImageRegionStore,qupath.lib.gui.viewer.OverlayOptions,qupath.lib.display.ImageDisplay)\" target=\"_blank\" rel=\"noopener\">QuPathViewer (QuPath 0.4.0)</a></h3>\n\n  <p>declaration: package: qupath.lib.gui.viewer, class: QuPathViewer</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>You can create <code>OverlayOptions</code> without creating a viewer. See <a href=\"https://forum.image.sc/t/export-qupath-images-and-overlays-without-a-viewer/64302\" class=\"inline-onebox\">Export QuPath images and overlays without a viewer</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> Thank you very much! This worked and has solved the issue.</p>"], "54207": ["<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0151c040108e2a8330e8817b5e9700e7523d13e.png\" alt=\"image\" data-base62-sha1=\"vYkbas83dXvzQa3UNL070spv56u\" width=\"194\" height=\"179\"></p>\n<p>Hello. I am trying to run the example images to test out this macros called Axon Tracer but I run into this issue with variable counter undefined. Does anyone else have a similar issue when running this macros?</p>\n<p>I am using a windows PC with the correct plugins installed.</p>\n<p>These links should have everything:</p>\n<p><a href=\"https://www.poplawski-lab.com/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://www.poplawski-lab.com/</a><br>\n<a href=\"https://drive.google.com/drive/folders/0Bw-67C_yWGvgdzJyZmlPU1ExTVE?resourcekey=0-sBfiz4kaZ0J-KfOI6Cc1Eg\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://drive.google.com/drive/folders/0Bw-67C_yWGvgdzJyZmlPU1ExTVE?resourcekey=0-sBfiz4kaZ0J-KfOI6Cc1Eg</a><br>\n<a href=\"https://imagescience.org/meijering/software/featurej/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://imagescience.org/meijering/software/featurej/</a></p>", "<p>Hi Samuel,</p>\n<p>I had a quick attempt at running the macro code selecting the \u2018Entire Image\u2019 option using the sample images downloaded from the website. I got a similar error message (\u2018Undefined variable: counter =  + 1\u2019), but in my case it hit the error in line 331. Looking at the code, the variable counter is used throughout in various while loops.<br>\nI could get the \u2018Entire Image\u2019 analysis to run by adding the instruction \u2018counter = 0\u2019 just before the while loop, in my case line 322.<br>\nSeeing that your error occurred in line 3108, I suspect that you tried to use option 3: \u2018Manual ROI - 1 Axon Channel\u2019, which appears to use a copy of the same while loop structure (I think it it would have been nicer/more efficient if the re-used code was defined as a function rather than just copied).<br>\nAnyway, defining the variable by setting it to 0 at the start of the macro e.g. just after setting the values for enlargeROI and reduceROI should probably work for all four of the options.</p>\n<pre><code class=\"lang-auto\">counter = 0;\n</code></pre>\n<p>I haven\u2019t tested all the various macro options, but it seems to be working for the \u2018Entire Image\u2019 option.<br>\nHope this helps,<br>\nVolko</p>", "<p>Thank you Volko.</p>\n<p>I remember adding a counter = 0 to try to get it work for the other analysis but not the entire image analysis. Now that you helped confirm with me that the entire image analysis does run, it really helps out a bunch.</p>", "<p>I\u2019m having an issue where the macro does not give me the option which analysis to run in the beginning, specifically the entire image. How can I resolve this?</p>"], "78787": ["<p>Hi All,</p>\n<p>I am using QuPath 0.4.2 on a Mac OSX 12.6.3 M1 Pro to annotate H&amp;E-stained sections with a pixel classifier. I made a training set of rectangular annotations from about 20 slides differing somewhat in stain intensity and created a pixel classifier from manual annotation of the tissue.</p>\n<p>The classifier gives very different results for when applied to images on the training set compared to their originals from the same project. I can find no differences in the corresponding descriptions on the Image tab (pixel resolution and stain vectors are the same), except the Server type is OpenSlide for the original and Sparse image server for the training slide.</p>\n<p>What might be wrong?</p>\n<p>Thanks for your help.</p>\n<p>Cheers,<br>\nMark</p>", "<p>Hi All,</p>\n<p>As an update, I reproduced the issue described in my earlier post by creating a new project from a single slide, annotating a single rectangle surrounding a section on that slide with class set to Region*, generating a training set from this annotation, and using the training set to create various pixel classifiers. The classifiers were created at Very high resolution, scales 1, 2, 4, and 8, and 1\u20133 of the following features: gaussian, weighted deviation, and structure tensor coherence.</p>\n<p>I found the problem most pronounced when weighted deviation was included in the classifier. However, fine details differed even when the simpler settings for the pixel classifier were used. For example, a pixel classifier created at Moderate resolution, a single scale of 1, and a gaussian as the only feature showed many subtle differences between the training image and the original.</p>\n<p>I wonder how the OpenSlide and Sparse image servers might differ in image parameters. My images are brightfield images of hematoxylin and eosin slides scanned at 20x using a Hamamatsu NanoZoomer 2.0 (.ndpi format). Are .ndpi slides known to be problematic when analyzed by this approach (that is, by using a training set to train a pixel classifier in QuPath)?</p>\n<p>Cheers,<br>\nMark</p>", "<aside class=\"quote no-group\" data-username=\"mpkrebs\" data-post=\"2\" data-topic=\"78787\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/46a35a/40.png\" class=\"avatar\"> Mark Krebs:</div>\n<blockquote>\n<p>I wonder how the OpenSlide and Sparse image servers might differ in image parameters.</p>\n</blockquote>\n</aside>\n<p>Have you looked at the visualization for the channels/features in the sparse image areas and made sure there are not any edge effects for the largest size filters used? Or did you stick to areas far enough from the edges of each patch to make sure there\u2019d not be any edge effects.</p>\n<p>Somewhat terrible example showing edge of image effects for a poorly chosen set of image regions<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426.jpeg\" data-download-href=\"/uploads/short-url/84JOhBjmk8T7BwqRA3OtYzbvxfE.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_690x436.jpeg\" alt=\"image\" data-base62-sha1=\"84JOhBjmk8T7BwqRA3OtYzbvxfE\" width=\"690\" height=\"436\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_690x436.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_1035x654.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/389a86ecfc831a6d127351132356b7242fab3426_2_1380x872.jpeg 2x\" data-dominant-color=\"4D4B4C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1567\u00d7991 84.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"mpkrebs\" data-post=\"2\" data-topic=\"78787\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/46a35a/40.png\" class=\"avatar\"> Mark Krebs:</div>\n<blockquote>\n<p>OpenSlide and Sparse image servers might differ in image parameters</p>\n</blockquote>\n</aside>\n<p>Also, did you use staining or RGB values as inputs to the pixel classifier, and if stains were used, are the stain vectors the same in both images before you started the pixel classifier?</p>", "<p>Dear MicroscopyRA,</p>\n<p>I have been using RGB as inputs to the pixel classifier. The stain vectors are identical in both images, although I infer from your question the stain vectors don\u2019t apply to RGB values, which makes sense.</p>\n<p>Cheers,<br>\nMark</p>", "<p>Dear MicroscopyRA,</p>\n<p>Regarding edge effects, I think I am using the classifier in a slightly different way than in your image. The tissue I study (mouse retina) is highly structured with natural boundaries in staining color and intensity. Thus, although each section selected for the training set is at the center of a larger rectangle, and therefore free from edge effects, there are prominent edges within the tissue layer at all scales and channels.</p>\n<p>But why would these be differentially detected in a training image and its corresponding original?</p>\n<p>Cheers,<br>\nMark</p>", "<p>Afraid I don\u2019t know. Have used NDPI files for this, so generally it shouldn\u2019t be an issue, though usually not with OpenSlide.<br>\n<a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> offhand thought, is there any chance the re-centering of OpenSlide coordinates (from way back in the day, something about the boundaries) somehow mistranslates into the training image, so that the training annotations end up access pixels that are not expected?</p>\n<p>Alternatively, <a class=\"mention\" href=\"/u/mpkrebs\">@mpkrebs</a>, have you tried using BioFormats when Importing the files, needs to be done in a new project, I think, unfortunately.</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"7\" data-topic=\"78787\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> offhand thought, is there any chance the re-centering of OpenSlide coordinates (from way back in the day, something about the boundaries) somehow mistranslates into the training image, so that the training annotations end up access pixels that are not expected?</p>\n</blockquote>\n</aside>\n<p>I wouldn\u2019t expect so \u2013 and I don\u2019t think it generally affects .ndpi.</p>\n<aside class=\"quote no-group\" data-username=\"mpkrebs\" data-post=\"2\" data-topic=\"78787\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/46a35a/40.png\" class=\"avatar\"> Mark Krebs:</div>\n<blockquote>\n<p>I found the problem most pronounced when weighted deviation was included in the classifier. However, fine details differed even when the simpler settings for the pixel classifier were used.</p>\n</blockquote>\n</aside>\n<p>That makes me think <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>\u2019s assumption about edge artifacts could well be correct. All the filters are applied in image tiles. The tiles overlap to try to reduce/eliminate boundary issues, however padding needs to be used at the edge of the image (since QuPath doesn\u2019t know what is beyond the image boundary).</p>\n<p>See here for more info: <a href=\"https://bioimagebook.github.io/chapters/2-processing/4-filters/filters.html#filtering-at-image-boundaries\" class=\"inline-onebox\">Filters \u2014 Introduction to Bioimage Analysis</a></p>\n<p>When you create the training image, the features you get at the edge of a training region will be slightly different from the features that you get in the same region of the original image: in the training image, QuPath will need to pad the image, because it doesn\u2019t know what the pixels \u2018outside\u2019 the cropped region should be.</p>\n<p>The effect of this is much greater when you use large filters. Some features (e.g. weighted deviation) are also likely to be more affected.</p>\n<p>To mitigate this, you can create training images that include quite large regions \u2013 and only annotate the central parts.</p>", "<p>Dear MicroscopyRA,</p>\n<p>Thanks very much for your input.</p>\n<p>I didn\u2019t try BioFormats, but I should point out that in order to work efficiently I had downloaded slide images in .ndpi format from our local OMERO server. These were used for annotation, pixel classification and subsequent testing that revealed the issue of different results on original versus training images.</p>\n<p>By contrast, when instead I created a project in which the slide was linked to the OMERO server by its URL and used that slide to create a training slide for pixel classification, the resulting classifier gave a much more similar annotation for both the training and original images. As a control, using the local .ndpi file to train the classifier again yielded training and original annotations that differed greatly.</p>\n<p>So it\u2019s looking to me like the culprit may be the use of the local .ndpi file downloaded from OMERO.  I\u2019ll let you know as I continue testing whether this pans out, but to me the solution (albeit potentially less efficient) is to use OMERO-served images rather than local files for training. Maybe something was altered in the .ndpi  download from OMERO (metadata?) but I really have no idea.</p>\n<p>Cheers,<br>\nMark</p>", "<p>No experience with omero, but adding the omero tags in case someone more familiar might know how things could change. I recall some issues with multichannel images and the representations of the images being changed, what omero shows vs what the original data is. But not sure if/how that relates to the current issue.</p>\n<p>If you could host/share a small project that might help, but not sure if your images are sharable. Again, not an issue that comes up normally that I can recall, but I haven\u2019t tried since 0.4.2 or on a Mac.</p>"], "78788": ["<p>I am currently working on the corrections of an article in which I perform anisotropy measurements on cubic bone volumes extracted from femoral heads. To do so, I use the BoneJ plugin in Fiji. One of my reviewers mentioned the existence of an \u201cedge and corner bias\u201d with cubic VOIs. Indeed, it seems that higher component values in orientations towards the edges or corners are found compared to spherical volumes (Ketcham &amp; Ryan, 2004, Journal of Microscopy, 213). I was wondering if BoneJ takes this bias into account when measuring anisotropy with cubic volumes or not at all.</p>", "<aside class=\"quote no-group\" data-username=\"jgonet\" data-post=\"1\" data-topic=\"78788\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/j/ea666f/40.png\" class=\"avatar\"> Jordan_G\u00f4net:</div>\n<blockquote>\n<p>BoneJ plugin in Fiji</p>\n</blockquote>\n</aside>\n<p>Which version of BoneJ are you using? The old 1.x downloaded from <a href=\"http://bonej.org\">bonej.org</a> or the newer 2.x from the ImageJ updater?</p>\n<p>In the old Anisotropy, spherical \u2018balls\u2019 of sampling vectors were made in random positions around the image until the result stabilised, and the vectors did not touch the sides of the image. In the new one, sampling vectors pass through the entire image volume. When the sampling vector passes through an image boundary, I am not sure if it counts the \u2018outside\u2019 as foreground, background, or doesn\u2019t count it at all when it is counting foreground-background interfaces: we would have to take a look at the code. I would imagine that this would be the important implementation detail that could contribute to the edge and corner bias you mention. If image edges counted as interfaces, texture at corners and edges would look like smaller features than they really are, because of the way they have been \u2018cut\u2019 out of the parent texture. However, a way to get around that is not to count artefactual \u2018interfaces\u2019 at the image sides in the calculation of MIL.</p>\n<p>There is usually a lot of uncertainty in estimating anisotropy using mean intercept length, due to random sampling effects and variation within the sample, even if you come up with a perfect unbiased sampling scheme. There are some other interesting effects that result from things like the rectangular pixel grid interacting with texture scale and sampling frequency. So its possible that the magnitude of this edge-and-corner bias may be negligible in the face of the stochasticity and uncertainties inherent in the method.</p>\n<p>Note that some implementations of mean intercept length use a single sampling sphere placed inside the image, with all sampling vectors radiating from a single point. This sampling approach means that points closer to the centre of the sampling sphere are relatively over sampled compared to points towards the ends of the sampling vectors, and only the point in the centre is sampled from different directions.</p>\n<p>In BoneJ 1.x we used many sampling spheres randomly placed inside the image volume, so that each point in the texture could be sampled from many directions, but that means each sampling sphere centre is relatively oversampled compared to the sphere\u2019s periphery. To overcome this in BoneJ2, Anisotropy does a \u2018rotating bed of nails\u2019 sampling strategy, so that all the pixels have an equal chance of being sampled from all directions.</p>", "<p>Thanks for the detailed response. I used BoneJ 1.4.3 for this paper. If I understand correctly, I am not particularly affected by this bias, since the vectors do not touch the sides of the image?</p>", "<aside class=\"quote no-group\" data-username=\"jgonet\" data-post=\"3\" data-topic=\"78788\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/j/ea666f/40.png\" class=\"avatar\"> Jordan_G\u00f4net:</div>\n<blockquote>\n<p>I am not particularly affected by this bias, since the vectors do not touch the sides of the image?</p>\n</blockquote>\n</aside>\n<p>That\u2019s correct. In BoneJ 1.4.3 the centres of the sampling spheres are randomly distributed inside the image stack, but never closer than their sphere\u2019s radius to the sides of the image.</p>\n<p><a href=\"https://doi.org/10.1111/j.1365-2818.2004.01277.x\">Ketcham and Ryan wrote this</a>:</p>\n<blockquote>\n<p>the uneven sampling inherent in a cubic VOI makes features orientated near 45\u00b0 to one or more axes more likely to be included and measured than near-orthogonal features, particularly with small specimens and/or when the VOI is small compared with the fabric elements</p>\n</blockquote>\n<p>So basically the problem is that you might have an interaction between the shape of the image (cuboid) and the orientation of the texture, because the diagonal is longer edge-to-edge and corner-to-corner, which would mean sampling in those directions is more frequent than sampling in the face-to-face direction, which might tend to over-represent some texture directions. This interaction would only become meaningful when the size of the cuboid ROI approaches the frequency of the texture, and I expect for most specimens is likely to be drowned out by uncertainty in the sampling and variation in the specimen. As they acknowledge:</p>\n<blockquote>\n<p>it is difficult to gauge whether these effects are an artefact or reflect true specimen features</p>\n</blockquote>\n<p>(in other words, you may sample a different range of textures and get a different DA when you change the ROI a bit, which is not unexpected given local variations in trabecular bone geometry)</p>\n<p>Note that the \u2018m\u2019 in MIL stands for \u2018mean\u2019, such that textures in each direction are averaged over the length of line probes in that direction, so the increased amount of sampling in a particular direction is removed by dividing the intersection count by the amount of sampling in that direction. To be honest, I don\u2019t think that this is a real sampling bias problem, and is more of a problem of specimen variation and sampling uncertainty.</p>", "<aside class=\"quote no-group\" data-username=\"mdoube\" data-post=\"2\" data-topic=\"78788\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mdoube/40/30454_2.png\" class=\"avatar\"> Michael Doube:</div>\n<blockquote>\n<p>When the sampling vector passes through an image boundary, I am not sure if it counts the \u2018outside\u2019 as foreground, background, or doesn\u2019t count it at all when it is counting foreground-background interfaces: we would have to take a look at the code.</p>\n</blockquote>\n</aside>\n<p>To confirm, each vector passes through only the image pixels and doesn\u2019t sample outside the image.</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209\" target=\"_blank\" rel=\"noopener\">bonej-org/BoneJ2/blob/6968b3e039a2115ef8992d00ac4e7418b86d41b1/Modern/ops/src/main/java/org/bonej/ops/mil/ParallelLineMIL.java#L209</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-java\">\n      <ol class=\"start lines\" start=\"199\" style=\"counter-reset: li-counter 198 ;\">\n          <li>\t * Only affects the random offset 0 &amp;lt; o &amp;lt; increment added to the sampling points</li>\n          <li>\t * (see {@link #sampleSegment(RandomAccessible, Segment, Vector3dc, double)})</li>\n          <li>\t * &lt;/p&gt;</li>\n          <li>\t * @param seed seed value</li>\n          <li>\t */</li>\n          <li>\tpublic static void setSeed(final long seed) {</li>\n          <li>\t\tParallelLineMIL.seed = seed;</li>\n          <li>\t}</li>\n          <li>\n          </li>\n<li>\t// region -- Helper methods --</li>\n          <li class=\"selected\">\tprivate static &lt;B extends BooleanType&lt;B&gt;&gt; long countPhaseChanges(</li>\n          <li>\t\tfinal RandomAccessible&lt;B&gt; interval, final Vector3d start,</li>\n          <li>\t\tfinal Vector3dc gap, final long samples)</li>\n          <li>\n          </li>\n<li>\t{</li>\n          <li>\t\tfinal RandomAccess&lt;B&gt; access = interval.randomAccess();</li>\n          <li>\t\tboolean previous = false;</li>\n          <li>\t\tlong phaseChanges = 0;</li>\n          <li>\t\tfor (long i = 0; i &lt; samples; i++) {</li>\n          <li>\t\t\tfinal boolean current = getVoxel(access, start);</li>\n          <li>\t\t\tif (current != previous) {</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thank you very much!</p>"], "78792": ["<p>Hii</p>\n<p>I have just started exploring napari for visualizing multiphoton microscopy data. I am having trouble getting the 3d view. The 3d view from side angles looks like hazy or probably more stretched out.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3.jpeg\" data-download-href=\"/uploads/short-url/As0gr6AVUkz5a4lz4XFv0UqKiFd.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3_2_676x500.jpeg\" alt=\"image\" data-base62-sha1=\"As0gr6AVUkz5a4lz4XFv0UqKiFd\" width=\"676\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3_2_676x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff78a8a2fb99e0d3bd2e89d3e7c52aa4d6a5a6f3.jpeg 2x\" data-dominant-color=\"18131D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">855\u00d7632 79 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The image file format is .oif which I load using <a href=\"https://pypi.org/project/oiffile/\" rel=\"noopener nofollow ugc\">oiffile</a>. The shape of the image is (3,25,512,512). The second axis is probably the number of z-stacks. I use the following scaling factors while loading the image : (25,1,1). [example image above and the video below has more number of z-stacks somewhere close to 70!] Each stack during acquisition was 10um apart. I guess the cause of my issue here is scaling but I do not know how to get the correct scaling factors.</p>\n<p>The video of the 3d view can be found <a href=\"https://drive.google.com/file/d/1OW3g_we1p8GvaxI7ntDDHGrIDS6hUPDT/view?usp=share_link\" rel=\"noopener nofollow ugc\">here</a>.</p>", "<p>Hey <a class=\"mention\" href=\"/u/abdubey123\">@abdubey123</a> and welcome!</p>\n<p>The issue is that a spacing of (25, 1, 1) is rather extreme anisotropy. Is the xy resolution 400nm? (10\u00b5m/25) If so, then I personally don\u2019t know of a good way to display such data in 3D, other than perhaps as individual 2D slices in 3D space \u2014 though we don\u2019t have a mode for that kind of display. Empirically, I think any sampling more extreme than 5:1:1 or 10:1:1 at most looks bad.</p>\n<p>Does anyone else have suggestions for making it look better?</p>\n<p>I don\u2019t know the oif format but the exact spacing is probably in the file metadata somewhere\u2026?</p>"], "78794": ["<p>I was able to get the labeled frames into a training dataset, but when I try to train the network, I get a series of lines from the yaml file, and the final line says \u201cI tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc::354] MLIR V1 optimization pass is not enabled\u201d.</p>\n<p>I started the training through the GUI on the most up-to-date version of DLC. I\u2019m also using a Windows 10. Is this supposed to happen or what should I do?</p>", "<p>This is a warning about an experimental feature that still isn\u2019t stable in tensorflow so it\u2019s all fine.</p>", "<p>I see! Thank you so much!<br>\nAfter that message shows up, my terminal\u2019s filled with lines that say<br>\niteration: __000 loss: 0.0015 scmap loss: 00.0014 locref loss: 0.0000 limb loss: 0.0000 lr: 5e-05<br>\nI think at least 20 of these lines showed up in the 2 days I left it running. Previously, my computer crashed and after a day the terminal shut itself off, but it\u2019s been running for the past 3 days. Does this message mean my computer\u2019s learning? Or should I do something else?</p>", "<p>This is a prinout of the training progress. Not sure which iteration you\u2019re on, but this loss will probably not get much better.</p>"], "78795": ["<p>Hi, I am new to Cell Profiler and am hoping to analyze confocal zstacks.</p>\n<p>I exported my LEICA image as individual .tifs, but when I double click any image, I get a pipeline error (see below). Tutorial module images work fine and I added imagecodecs to PATH, but this image is still showing up. A similar error happens if I try with jpgs.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c64cabf9ab92a718242452748c13bba8cbacafb2.png\" data-download-href=\"/uploads/short-url/sieO4HWlSchcMrzmIvCD3Tlu9MK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c64cabf9ab92a718242452748c13bba8cbacafb2.png\" alt=\"image\" data-base62-sha1=\"sieO4HWlSchcMrzmIvCD3Tlu9MK\" width=\"690\" height=\"362\" data-dominant-color=\"DBDBDB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1332\u00d7700 69.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Marie,</p>\n<p>Could you upload one image, please?  It\u2019s difficult to help you without.</p>\n<p>M.</p>", "<p>Here are two images! I am hoping to segment the plaques from channels 00 and 01.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/bDndgZLusikoW0DSqdQZzZkPcvS.tif\">cell profiler test_2021111 CC_AD_089 PBB5 Iba1 a _z06_ch00.tif</a> (92.9 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/gLWUJq0BbfGEaWqkKnugmDlxtwP.tif\">cell profiler test_2021111 CC_AD_089 PBB5 Iba1 a _z06_ch01.tif</a> (81.2 KB)</p>"], "78796": ["<p>Does anyone know why when I put python -m deeplabcut into the terminal, I get a series of errors that ultimately ends with:<br>\n*** End stack trace ***<br>\nAbort trap: 6</p>\n<p>What should I do?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da.jpeg\" data-download-href=\"/uploads/short-url/ySfAPmXd6MSpZ5FbJICAF7fivj4.jpeg?dl=1\" title=\"Screen Shot 2023-03-18 at 7.43.32 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_690x491.jpeg\" alt=\"Screen Shot 2023-03-18 at 7.43.32 PM\" data-base62-sha1=\"ySfAPmXd6MSpZ5FbJICAF7fivj4\" width=\"690\" height=\"491\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_690x491.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_1035x736.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f46bdc6d5441750f30c3223f36d8d5ff5e49c1da_2_1380x982.jpeg 2x\" data-dominant-color=\"272A2B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-18 at 7.43.32 PM</span><span class=\"informations\">1552\u00d71106 156 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Can you reinstall <code>pyqt</code> and <code>pyside</code> and see if it helps?</p>", "<p>I reinstalled pyqt and pyside, but still get the same error. Do you know what this error means?</p>", "<p>Could you show <code>pyqt</code> and <code>pyside</code> versions installed in your environment?</p>"], "78801": ["<p>Hi, I\u2019m trying to get up and running with the latest DLC (use case is single subject) and am stuck on the GPU-related aspects. I recall from earlier installations that this is tricky business, and want to get it right and avoid a mess. I\u2019ve carefully read the installation instructions and tips and followed links to external (stackoverflow) threads, but I feel the install docs are not consistent. In short, what are the current recommendations for the GPU install?</p>\n<p>Here are some of the relevant texts from the install page (which I find confusing/inconsistent):</p>\n<ul>\n<li>Note, DeepLabCut is up to date with the latest CUDA and tensorflow versions!</li>\n<li>Install CUDA (versions up to CUDA11 are supported, together with TF2.5)</li>\n<li>All of the TensorFlow versions work with DeepLabCut</li>\n<li>We recommend TF2.10 now<br>\n<a href=\"https://deeplabcut.github.io/DeepLabCut/docs/installation.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How To Install DeepLabCut \u2014 DeepLabCut</a>\n</li>\n</ul>\n<p>The latest driver for my GPU (RTX 2070 Super, Windows 10) is 528.49 (I currently have 516.94).<br>\nThe latest CUDA version is 12.1 (I currently have 11.7)<br>\nThe latest Tensorflow version seems to be 2.11 (but not Win10 compatible?)</p>\n<p>At <a href=\"http://tensorflow.org\" rel=\"noopener nofollow ugc\">tensorflow.org</a>, I see the following (scary) message: \u201cTensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2, or install tensorflow-cpu and, optionally, try the TensorFlow-DirectML-Plugin\u201d</p>\n<p><a href=\"http://Tensorflow.org\" rel=\"noopener nofollow ugc\">Tensorflow.org</a>\u2019s install guide for Windows suggests an older CUDA, it seems:<br>\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0</p>\n<p>The \u201ctested build configurations\u201d (for building from source) don\u2019t show any CUDA greater than 11.2, even for TF2.11:  <a href=\"https://www.tensorflow.org/install/source_windows\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Tw\u00f3rz ze \u017ar\u00f3d\u0142a w systemie Windows \u00a0|\u00a0 TensorFlow</a></p>\n<p>Can I go ahead and install DLC or should I upgrade, or even downgrade? Recall I\u2019m currently on CUDA 11.7 and GPU 516.94.</p>\n<p>Finally: There seems to be an DLC install on this computer. Ideally I\u2019d like to set up a new conda environment for the latest DLC now to start fresh, and not delete/remove any old envs. Can I still follow the normal install instructions (git clone, conda create env), but somehow ensure that the new env gets a unique name (i.e. not DEEPLABCUT)? Thanks!</p>", "<p>Can you install <code>cudnn==8.4.1.50</code> and <code>cudatoolkit==11.8</code> and <code>tensorflow==2.10.0</code>?</p>\n<p>To create a new environment just open the yaml file used for env creation and change the name to whatever you want the env be named</p>", "<p>Dear Konrad, thank you for answering - I will try to obtain those versions. It\u2019s not clear to me, on windows, which if those three items I need to download and install independently, and which will be installed automatically when I create the conda env from your yaml? In the past, I believe I only downloaded the GPU driver and the CUDA toolkit as standalone installs. Your version syntax looks python-y, so I\u2019m wondering if you mean I should install some or all of those via some commands / package manager within python?</p>", "<p>After you create a new environment you can simply run in that new env <code>conda install -c conda-forge cudnn</code> and if your tensorflow isn\u2019t <code>2.10.0</code> just run <code>pip install --upgrade tensorflow==2.10.0</code></p>", "<p>Sorry for the slow reply, and thanks for your support. I ran into various issues still and wasn\u2019t able to put in a lot of time to sort this out. I switched to using SLEAP, which installed very easily including GPU support, and I was able to generate the data I need that way. I still intend to come back to DLC in the future, to compare results and to decide which system SLEAP or DLC will work better for me for processing a lot of additional data in the future.</p>\n<p>I don\u2019t understand the python/CUDA underpinnings that cause the installation process (in my hands) to be so delicate/difficult for DLC. SLEAP seems to use the same/similar libraries under the hood, yet installing (with GPU) was trivially easy for me. I know I\u2019m not the first user to run into version / GPU issues with DLC, so maybe there\u2019s an opportunity for some transfer learning here <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>I hope it won\u2019t be too long before I get a chance to try to fix my DLC setup and get up and running with your latest code again.</p>"], "78805": ["<p>Hello Everybody,</p>\n<p>First of all, thank you for giving me really good advice.<br>\nNowadays, I am trying to get 4 individual images from the \u201c_seg.npy\u201d file in cellpose 2.2.<br>\nAs you can see on the screen, the result shows individual 4 images but actually it is one image(I mean that those 4 images are inside one image).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e.jpeg\" data-download-href=\"/uploads/short-url/zQo3vr4uDifjbN1KadaLNNXMHN4.jpeg?dl=1\" title=\"028_img.ome_cp_output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_690x172.jpeg\" alt=\"028_img.ome_cp_output\" data-base62-sha1=\"zQo3vr4uDifjbN1KadaLNNXMHN4\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_1380x344.jpeg 2x\" data-dominant-color=\"A1B0A0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">028_img.ome_cp_output</span><span class=\"informations\">3600\u00d7900 207 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nBut I need the individual 4 images for the next step of processing.<br>\nIf you know the python source code to separate those 4 images from the \u201c*_seg.npy\u201d file, please share with me.<br>\nThis task is really important for me but I am not an expert in this field.<br>\nSo I wish for your kind helpness.</p>", "<p>Hi <a class=\"mention\" href=\"/u/jameswang80428\">@JamesWang80428</a>! I\u2019m not familiar with cellpose, but it sounds like youre trying to split out a multichannel image.</p>\n<p>If that\u2019s a simple numpy save file (as it appears), yous hould be able to do the following:</p>\n<pre data-code-wrap=\"pyhton\"><code class=\"lang-plaintext\">import numpy as np\n\ndata = np.load(image_filepath)\n</code></pre>\n<p><code>data</code> should now have a specific <code>shape</code> where one of the dimensions is the channel axis. This is usually the first dimension:</p>\n<pre><code class=\"lang-python\">data.shape  # something like (4, X, Y)\n\nfirst_channel = data[0]\nsecond_channel = data[1]\n</code></pre>\n<p>I recomment reading the <a href=\"https://numpy.org/doc/stable/user/absolute_beginners.html\" rel=\"noopener nofollow ugc\">introducting docs of numpy</a>, you need very little to get a long way <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I think it\u2019s a bit more complicated than that <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a><br>\nSee my post in the other thread about reading cellpose .npy:</p><aside class=\"quote quote-modified\" data-post=\"8\" data-topic=\"76319\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/importing-seg-npy-files-into-napari/76319/8\">Importing seg.npy files into napari?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Looking more closely at the cellpose save code: \n\nThe .npy file contains actually a dictionary with more than just the labels, which is what you\u2019d visualize with napari and further process with region props, etc. There\u2019s no way I don\u2019t think for napari builtin reader to a priori know that it needs to do: \ndata_to_load = np.load('_seg.npy', allow_pickle=True).item()\nlabels = data_to_load['masks']\n\nSeems like this requires a cellpose-specific npy reader plugin. \nNow cellpose offers a png output, w\u2026\n  </blockquote>\n</aside>\n\n<p>So what you need to do to load them is to mimic what cellpose does. For example to get the masks:</p>\n<pre><code class=\"lang-auto\">data_to_load = np.load('_seg.npy', allow_pickle=True).item()\nlabels = data_to_load['masks']\n</code></pre>"], "78809": ["<p>Good morning,</p>\n<p>I work on the feeding kinematic of salamanders and I would like to use Deeplabcut to be more efficient for the tracking of jaw and hyoid movement of the individuals on the video I made. The thing is, I try to understand the software for 2 months now and there are still some details I don\u2019t understand and I\u2019m not sure I do everything right.</p>\n<p>I do single animal project<br>\nI work with Deeplabcut version 2.2.3<br>\nMy operating system is windows 11<br>\nMy computer has a NVIDIA GeForce RTX 3080 Ti Laptop GPU</p>\n<p>I have created a project importing at first 13 videos with different individuals of the same species laterally filmed during suction feeding. I labeled 200 frames for each of these 13 videos and I trained the network with 500000 iterations. Then, when analyzing and creating labeled videos, I obtained satisfying results.</p>\n<p>After that, I added 20 other videos of individuals of the same species in the folder \u201cvideos\u201d of the project and then I restarted the analysis but the results for these 20 new videos were not as good as for the previous 13 ones.</p>\n<p>So, I wanted to refine outliers but:<br>\n-First, I can\u2019t choose myself the videos I want to refine because, only some of them present a h5 file when using extract frame (whatever the refining method chosen).<br>\n-Secondly, after refining videos proposed by the software, I feel the improvement of the tracking are really weak, even if I run 500000 iterations\u2026</p>\n<p>I don\u2019t know if there is a way to force the software to let me refine specific videos. I\u2019m also not sure if there is an ideal number of iterations.</p>\n<p>I always remove the folders of \u201cevaluation result\u201d when retraining and I also remove the fields in the folder videos as the software doesn\u2019t overwrite them. I wonder if I must also remove the folder \u201citeration\u201d in \u201ctraining-datasets\u201d when merging the data and recreating training dataset\u2026</p>\n<p>I\u2019m sorry if my questions are naive, but I would be very pleased if you have any advice to give me.</p>", "<p>It\u2019s always hard to say without knowing the data but quality of labelling is really important, so remember to be precise and consistent in the way you label the bodypart you want to track.</p>\n<p>As for the refinement - if you analyze a video and extract outliers from it, it will be added to the project and extracted frames will be added to <code>labeled-data</code> folder.</p>\n<p>If you bump the iteration in the config and create a new training dataset, evaluation will be done for the new iteration in a separate folder - within the same training set iteration but different snapshots, evaluation results are appended to the csv file with evaluation results.</p>\n<p>You can also consider modyfing augmentation parameters for your specific use case - hard to tell exactly how without seeing the data.</p>", "<p>I thank you very much for trying to help me.<br>\nI think I have found what was wrong thanks to Guillermo Hidalgo Gadea website <a href=\"https://guillermohidalgogadea.com/openlabnotebook/refining-your-dlc-model/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Refining your DeepLabCut Model \u2013 Learning from Mistakes | Guillermo Hidalgo Gadea</a><br>\nI was concerned by \u201cStep 4: Learning from Mistakes\u201d. In fact I kept using pretrained resnet_50n instead of using my own pretrained model so my analysis started from the beginning each time.<br>\nNow that I have changed the \u201cinit_weights\u201d in the \u201cpose_cfg.yaml\u201d like Guillermo Hidalgo Gadea explains,  I have noticed improvement in my refinement.</p>", "<pre><code class=\"lang-auto\">**Note:** \nThis step will only work if the links to the new videos are included in the config.yaml file (see\npermission issues [here](https://github.com/DeepLabCut/DeepLabCut/issues/1181) and [here]\n(https://stackoverflow.com/a/65504258)). Otherwise, you will just duplicate the previous training\ndataset, and spend several days training a new model that turns out to be exactly the same as \nthe old one. It happened to a friend of mine.\n</code></pre>\n<p>When you <code>extract_outlier_frames</code> the video from which you extracted the frames is added to the <code>config.yaml</code> (won\u2019t happen if you\u2019re not running the terminal as administrator - which you always should be doing).</p>\n<p>After merging, a new dataset is created with new iteration. It\u2019s not a huge issue if you train from scratch - the model would get better, but yes it makes more sense to train from a snapshot since you don\u2019t have to train for as many iterations.</p>\n<p>The actual issue in your case was more likely that the iteration was never updated (since you say that you had to remove the <code>evaluation-results</code>) - hence you actually were constantly training without using the refined data. To avoid issues like this you can follow the workflow as explained in the DLC cookbook: <a href=\"https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html\" class=\"inline-onebox\">DeepLabCut User Guide (for single animal projects) \u2014 DeepLabCut</a></p>"], "78810": ["<p>Dear Qupath,  I try to find a way to measure Mean Fluo intensity (MFI)  in the different channels of my annotations, by default I have only area and perimeter. How can I extract these measurents ? It will be great also to have the possibility to put a threshold to measure above a theshold ? The same if I do detections, how to obtain these informations (if by default) these measurements was not checked in Cellpose script or others. Thanks so much, Qupath is a great software.</p>", "<p>Make sure the objects are selected before running the <em>Analyze-Calculate Features-Add Intensity Features</em></p><aside class=\"quote quote-modified\" data-post=\"36\" data-topic=\"27906\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-intro-choose-your-own-analysis-adventure/27906/36\">QuPath Intro: Choose your own analysis(adventure)</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Adding Features\nThe Calculate features menu has been split into the Calculate features and Spatial analysis menus in 0.2.0. \n<details><summary>Add Intensity features</summary>The largest and most flexible feature generator summarizes intensity data in or around a given detection. You can select a single object (cell) and run this, or everything, or an annotation. A simple use would be to create a full image annotation and collect the mean value of all channels/deconvolutions for all of your images, and take a look at the r\u2026</details>\n  </blockquote>\n</aside>\n\n<p>There is no background subtraction, you would need to do that yourself.</p>", "<p>Great, any solution to have the mean intensity above a threshold like for ImageJ ?</p>", "<p>Only by thresholding first, as far as I know. So threshold each cell, then calculate features, move measurement to cell, delete object used for thresholding.</p>"], "78813": ["<p>Dear community</p>\n<p>Is there any publicly available DL models to segment the cerebellar dentate nucleus on MRI images? There are a few papers on it but no link to the actual models to try.</p>\n<p>Thank you for your help.</p>\n<p>Kai</p>"], "78814": ["<p>Hi <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> ,</p>\n<p>I\u2019m just trying to use the new <a href=\"https://qupath.readthedocs.io/en/0.4/docs/deep/stardist.html#improving-input-normalization\" rel=\"noopener nofollow ugc\">global normalisation feature</a> introduced in the StarDist extension v0.4.0 with deconvolved channels but I was unsuccessful due to an error.</p>\n<p><code>StarDist2D.imageNormalizationBuilder()</code> worked with RGB channels:</p>\n<pre><code class=\"lang-auto\">def stardist = StarDist2D.builder(pathModel)\n    .preprocess(\n        StarDist2D.imageNormalizationBuilder()\n            .maxDimension(4096)\n            .percentiles(0.2, 99.8)\n            .build()\n    )\n    ...\n    .build()\n</code></pre>\n<p>but not when I try to do deconvolution:</p>\n<pre><code class=\"lang-auto\">def stardist = StarDist2D.builder(pathModel)\n    .preprocess(\n        ImageOps.Channels.deconvolve(stains),\n        ImageOps.Channels.extract(1), // 0 = haematoxylin, 1 = DAB; can be summed for pseudo optical density sum (0,1)\n        ImageOps.Filters.gaussianBlur(1),\n        StarDist2D.imageNormalizationBuilder()\n            .maxDimension(4096)\n            .percentiles(0.2, 99.8)\n            .build()\n    )\n    ...\n    .build()\n</code></pre>\n<p>This is the error that was spat out:</p>\n<pre><code class=\"lang-auto\">ERROR: It looks like you've tried to access a method that doesn't exist.\n\n\nERROR: No signature of method: qupath.ext.stardist.StarDist2D$Builder.preprocess() is applicable for argument types: (qupath.ext.stardist.OpCreators$PercentileTileOpCreator, qupath.opencv.ops.ImageOps$Channels$ColorDeconvolutionOp...) values: [qupath.ext.stardist.OpCreators$PercentileTileOpCreator@59ee801b, ...]\nPossible solutions: preprocess([Lqupath.opencv.ops.ImageOp;), preprocess(qupath.ext.stardist.OpCreators$TileOpCreator) in QuPathScript at line number 23\n\nERROR: org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:72)\n    org.codehaus.groovy.vmplugin.v8.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:161)\n    org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n    QuPathScript.run(QuPathScript:23)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)\n    qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)\n    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)\n    qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)\n    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    java.base/java.lang.Thread.run(Unknown Source)\n\nERROR: \nFor help interpreting this error, please search the forum at https://forum.image.sc/tag/qupath\nYou can also start a new discussion there, including both your script &amp; the messages in this log.\n</code></pre>\n<p>Deconvolution works normally when I exclude <code>StarDist2D.imageNormalizationBuilder()</code> and use <code>.normalizePercentiles()</code> instead.</p>\n<p>Am I missing anything to get the global normalisation working?</p>", "<aside class=\"quote no-group\" data-username=\"ym.lim\" data-post=\"1\" data-topic=\"78814\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png\" class=\"avatar\"> Yau Mun Lim:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">ERROR: No signature of method: qupath.ext.stardist.StarDist2D$Builder.preprocess() is applicable for argument types: (qupath.ext.stardist.OpCreators$PercentileTileOpCreator, qupath.opencv.ops.ImageOps$Channels$ColorDeconvolutionOp...) values: [qupath.ext.stardist.OpCreators$PercentileTileOpCreator@59ee801b, ...]\nPossible solutions: preprocess([Lqupath.opencv.ops.ImageOp;), preprocess(qupath.ext.stardist.OpCreators$TileOpCreator) in QuPathScript at line number 23\n</code></pre>\n</blockquote>\n</aside>\n<p>This seems to suggest that <code>preprocess()</code> only accepts either <code>ImageOps</code> or <code>imageNormalizationBuilder</code> separately but not together in one <code>preprocess()</code>.</p>\n<p>As such, I have tried to put them in separate <code>preprocess()</code>:</p>\n<pre><code class=\"lang-auto\">def stardist = StarDist2D.builder(pathModel)\n    .preprocess(\n        StarDist2D.imageNormalizationBuilder()\n            .perChannel(true)\n            .maxDimension(4096)\n            .percentiles(0.2, 99.8)\n            .build()\n    )\n    .preprocess(\n        ImageOps.Channels.deconvolve(stains),\n        ImageOps.Channels.extract(1), // 0 = haematoxylin, 1 = DAB; can be summed for pseudo optical density sum (0,1)\n        ImageOps.Filters.gaussianBlur(1)\n    )\n    ...\n    .build()\n</code></pre>\n<p>This ran without errors but no detections were made.</p>", "<p>Hi <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a> I think I just didn\u2019t write code to handle that scenario. You can add preprocessing steps <em>after</em> the global normalization, but not before.</p>\n<p>A possible workaround would be to use a trick like the one described here: <a href=\"https://forum.image.sc/t/running-superpixels-plugin-on-a-single-channel/78640/4\" class=\"inline-onebox\">Running superpixels plugin on a single channel - #4 by petebankhead</a></p>\n<p><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/images/servers/TransformedServerBuilder.html#deconvolveStains(qupath.lib.color.ColorDeconvolutionStains,int...)\"><code>deconvolveStains</code></a> is one of the options when creating a <code>TransformedImageServer</code>.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"78814\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> Pete:</div>\n<blockquote>\n<p>A possible workaround would be to use a trick like the one described here: <a href=\"https://forum.image.sc/t/running-superpixels-plugin-on-a-single-channel/78640/4\">Running superpixels plugin on a single channel - #4 by petebankhead</a></p>\n<p><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/images/servers/TransformedServerBuilder.html#deconvolveStains(qupath.lib.color.ColorDeconvolutionStains,int...)\" rel=\"noopener nofollow ugc\"><code>deconvolveStains</code></a> is one of the options when creating a <code>TransformedImageServer</code>.</p>\n</blockquote>\n</aside>\n<p>If I am understanding you correctly, you mean to create a deconvolved imageData separately, and then run the StarDist builder with the global normalisation preprocess on the deconvolved imageData? e.g.:</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\ndef server = imageData.getServer()\ndef otherServer = new TransformedServerBuilder(server).extractChannels(0, 1).build()\ndef sneakyImageData = new ImageData&lt;&gt;(otherServer, imageData.getHierarchy(), imageData.getImageType())\n\ndef stardist = StarDist2D.builder(pathModel)\n    .preprocess(\n            StarDist2D.imageNormalizationBuilder()\n                .perChannel(true)\n                .maxDimension(4096)\n                .percentiles(0, 99.8)\n                .build()\n    )\n    .threshold(0.5)              // Probability (detection) threshold\n    .pixelSize(pixelSize)\n    .cellExpansion(cellExpansionPixels)          // Approximate cells based upon nucleus expansion\n    .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n    .measureShape()              // Add shape measurements\n    .measureIntensity()          // Add cell measurements (in all compartments)\n    .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n    .build()\n\n// Get annotations to run StarDist\ndef stardistParentAnno = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Tissue\")}\n\nstardist.detectObjects(sneakyImageData, stardistParentAnno)\nstardist.close()\n</code></pre>", "<p>Yes, but you\u2019ll need to switch</p>\n<pre><code class=\"lang-auto\">.extractChannels(0, 1)\n</code></pre>\n<p>to be</p>\n<pre><code class=\"lang-auto\">.deconvolveStains(stains, 0)\n</code></pre>\n<p>or something similar.</p>", "<p>I\u2019m happy to report it technically works! Although slightly sad that I can\u2019t use the gaussian blur ImageOps filter as my objects of interest are grainy.</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\ndef stains = imageData.getColorDeconvolutionStains()\ndef server = imageData.getServer()\ndef deconvServer = new TransformedServerBuilder(server).deconvolveStains(stains, 1).build() // 1 = haematoxylin, 2 = DAB\ndef deconvImageData = new ImageData&lt;&gt;(deconvServer, imageData.getHierarchy(), imageData.getImageType())\n\ndef stardist = StarDist2D.builder(pathModel)\n   .preprocess(\n        StarDist2D.imageNormalizationBuilder()\n            .maxDimension(4096)\n            .percentiles(0, 99.8)\n            .build()\n   )\n    .threshold(0.5)              // Probability (detection) threshold\n    .pixelSize(pixelSize)\n    .cellExpansion(cellExpansionPixels)          // Approximate cells based upon nucleus expansion\n    .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n    .measureShape()              // Add shape measurements\n    .measureIntensity()          // Add cell measurements (in all compartments)\n    .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n    .build()\n\n// Get annotations to run StarDist\ndef stardistParentAnno = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Tissue\")}\n\nstardist.detectObjects(deconvImageData, stardistParentAnno)\nstardist.close()\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"ym.lim\" data-post=\"6\" data-topic=\"78814\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png\" class=\"avatar\"> Yau Mun Lim:</div>\n<blockquote>\n<p>I\u2019m happy to report it technically works!</p>\n</blockquote>\n</aside>\n<p>Great!</p>\n<aside class=\"quote no-group\" data-username=\"ym.lim\" data-post=\"6\" data-topic=\"78814\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png\" class=\"avatar\"> Yau Mun Lim:</div>\n<blockquote>\n<p>Although slightly sad that I can\u2019t use the gaussian blur ImageOps filter as my objects of interest are grainy.</p>\n</blockquote>\n</aside>\n<p>Have you tried? It looks like you can add preprocessing ops <em>and</em> global normalization, but the preprocessing will occur afterwards:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906\" target=\"_blank\" rel=\"noopener\">qupath/qupath-extension-stardist/blob/5909e6bca1304ab902b7121fb06234cb1d768c3d/src/main/java/qupath/ext/stardist/StarDist2D.java#L906</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-java\">\n      <ol class=\"start lines\" start=\"896\" style=\"counter-reset: li-counter 895 ;\">\n          <li>\t\t\t\t.stream()</li>\n          <li>\t\t\t\t.filter(t -&gt; mask == null || mask.intersects(GeometryTools.createRectangle(t.getImageX(), t.getImageY(), t.getImageWidth(), t.getImageHeight())))</li>\n          <li>\t\t\t\t.collect(Collectors.toList());</li>\n          <li>\t\t</li>\n          <li>\t\t// Detect all potential nuclei</li>\n          <li>\t\tvar server = imageData.getServer();</li>\n          <li>\t\tvar cal = server.getPixelCalibration();</li>\n          <li>\t\tdouble expansion = cellExpansion / cal.getAveragedPixelSize().doubleValue();</li>\n          <li>\t\tvar plane = request.getImagePlane();</li>\n          <li>\t\t</li>\n          <li class=\"selected\">\t\t// Compute op with preprocessing</li>\n          <li>\t\tvar fullPreprocess = new ArrayList&lt;ImageOp&gt;();</li>\n          <li>\t\tfullPreprocess.add(ImageOps.Core.ensureType(PixelType.FLOAT32));</li>\n          <li>\t\t</li>\n          <li>\t\t// Do global preprocessing calculations, if required</li>\n          <li>\t\tif (globalPreprocess != null) {</li>\n          <li>\t\t\ttry {</li>\n          <li>\t\t\t\tvar normalizeOps = globalPreprocess.createOps(op, imageData, roi, request.getImagePlane());</li>\n          <li>\t\t\t\tfullPreprocess.addAll(normalizeOps);</li>\n          <li>\t\t\t} catch (IOException e) {</li>\n          <li>\t\t\t\tthrow new RuntimeException(\"Exception computing global normalization\", e);</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>That being said, I\u2019m not sure how much it will help \u2013 since there are convolutions in the StarDist model and I\u2019m not sure if pre-convolving the image with a Gaussian filter would help or hurt.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"7\" data-topic=\"78814\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> Pete:</div>\n<blockquote>\n<p>Have you tried? It looks like you can add preprocessing ops <em>and</em> global normalization, but the preprocessing will occur afterwards:</p>\n</blockquote>\n</aside>\n<p>I have just tried using the Gaussian filter in a separate preprocess(), and it technically works, i.e. there are noticable differences with and without it in the script. But as you mentioned, it does not necessarily improve detection with the StarDist model.</p>\n<pre><code class=\"lang-auto\">def imageData = getCurrentImageData()\ndef stains = imageData.getColorDeconvolutionStains()\ndef server = imageData.getServer()\ndef deconvServer = new TransformedServerBuilder(server).deconvolveStains(stains, 2).build() // 1 = haematoxylin, 2 = DAB, 3 = residual\ndef deconvImageData = new ImageData&lt;&gt;(deconvServer, imageData.getHierarchy(), imageData.getImageType())\n\ndef stardist = StarDist2D.builder(pathModel)\n    .preprocess(\n        StarDist2D.imageNormalizationBuilder()\n            .maxDimension(4096)\n            .percentiles(0, 99.8) // Increase min percentile to detect lower intensity objects, decrease max percentile to clip too intense objects\n            .build()\n    )\n    .preprocess(\n        ImageOps.Filters.gaussianBlur(1)\n    )\n    .threshold(0.5)              // Probability (detection) threshold\n    .pixelSize(pixelSize)\n    .cellExpansion(cellExpansionPixels)          // Approximate cells based upon nucleus expansion\n    .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n    .measureShape()              // Add shape measurements\n    .measureIntensity()          // Add cell measurements (in all compartments)\n    .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n    .build()\n\n// Get annotations to run StarDist\ndef stardistParentAnno = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"Tissue\")}\n\nstardist.detectObjects(deconvImageData, stardistParentAnno)\nstardist.close()\n</code></pre>\n<p>Without Gaussian:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c.jpeg\" data-download-href=\"/uploads/short-url/rAKa657ukRt4ZDVS7lBISPo5rmA.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_501x499.jpeg\" alt=\"image\" data-base62-sha1=\"rAKa657ukRt4ZDVS7lBISPo5rmA\" width=\"501\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_501x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_751x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c162059f06acd3c8b13059d57784a860e421af5c_2_1002x998.jpeg 2x\" data-dominant-color=\"CFCCD9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1698\u00d71692 331 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>With Gaussian:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/07249347383fd201059326cfda784b736b8cb620.jpeg\" data-download-href=\"/uploads/short-url/11bHGbJgZFZkhTfNREvAvQuC1G0.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_501x499.jpeg\" alt=\"image\" data-base62-sha1=\"11bHGbJgZFZkhTfNREvAvQuC1G0\" width=\"501\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_501x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_751x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07249347383fd201059326cfda784b736b8cb620_2_1002x998.jpeg 2x\" data-dominant-color=\"CFCCD9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1698\u00d71692 329 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78815": ["<p>Dear <a class=\"mention-group notify\" href=\"/groups/ome\">@ome</a>,<br>\nI am experiencing some memory issues caused by data imports. As shown in the image below, every time a new dataset is uploaded the memory committed keeps increasing over time with slight decreases between uploads but never entirely returning to the baseline level. The memory drop seen on the 15th of March is due to a server restart.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/653fd86b780489ae513fd142443955d699ee251f.png\" data-download-href=\"/uploads/short-url/erH0MyC9o24hY3Csld7F846ljJR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653fd86b780489ae513fd142443955d699ee251f_2_680x499.png\" alt=\"image\" data-base62-sha1=\"erH0MyC9o24hY3Csld7F846ljJR\" width=\"680\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653fd86b780489ae513fd142443955d699ee251f_2_680x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653fd86b780489ae513fd142443955d699ee251f_2_1020x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/653fd86b780489ae513fd142443955d699ee251f.png 2x\" data-dominant-color=\"A6C69F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1137\u00d7835 279 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>We do not see anything out of the ordinary in the logs, but we have noticed the following process getting forked many times and remaining alive.</p>\n<pre><code class=\"lang-auto\">java -Xmx5005m -XX:MaxPermSize=1g -XX:+IgnoreUnrecognizedVMOptions -Djava.awt.headless=true -Dlogback.configurationFile=etc/logback.xml -Domero.logfile=var/log/${omero.name}.log -Domero.name=Blitz-0 ome.services.blitz.Entry --Ice.Config=/var/log/${omero.name}.log -Domero.name=Blitz-0 ome.services.blitz.Entry --Ice.Config=/opt/omero/server/OMERO.server-5.6.6-ice36/var/master/servers/Blitz-0/config/config\n</code></pre>\n<p>Bests,<br>\nRodrigo</p>", "<p><a class=\"mention\" href=\"/u/rodrigo_rb\">@Rodrigo_RB</a> sorry for the slow reply<br>\nAre you importing data using the Desktop client OMERO.insight?<br>\nWe are currently looking at a problem in the client not closing resources properly on the server.<br>\nThis could be the source of the problem see <a href=\"https://github.com/ome/omero-insight/pull/354\" class=\"inline-onebox\">Close all HandleTie servants created during import by sbesson \u00b7 Pull Request #354 \u00b7 ome/omero-insight \u00b7 GitHub</a></p>\n<p>Cheers</p>\n<p>Jean-Marie</p>", "<p>Dear Jan-Marie,</p>\n<p>As Sebastian Besson suggested in the linked post, I have looked at the Blitz logs and it seems it is precisely what we are suffering from. There are no 'remove servant \u2019 event logs resulting from that search! We are glad that it\u2019s a problem well on its way to being solved.</p>\n<p>Would you happen to know when these patches will be available for download at <a href=\"https://www.openmicroscopy.org/omero/downloads/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">OMERO Downloads | Open Microscopy Environment (OME)</a> ? It would allow us to ask our users to update their OMERO.insight.</p>\n<p>Bests,<br>\nRodrigo</p>", "<p>The Pull request is still a draft. We are looking at the best way to fix the issue.<br>\nHopefully we will have a release in the coming weeks</p>\n<p>Cheers</p>\n<p>Jean-Marie</p>"], "78816": ["<p>Hello everyone!</p>\n<p>I am working on segmenting and classifying cell nuclei on a TIF File with 3 channels. I ran into an Assertion error in the \u201cFeature Selection\u201d step for Ilastik 1.4.0. Interesting that I don\u2019t get with it in a previous build or while using Pixel Classification and Object Classification separately on the latest one.</p>\n<blockquote>\n<p>File \u201cD:\\ilastik-1.4.0-gpu\\lib\\site-packages\\lazyflow\\slot.py\u201d, line 1323, in maybe_call_within_transaction<br>\nself.graph.maybe_call_within_transaction(fn)<br>\nFile \u201cD:\\ilastik-1.4.0-gpu\\lib\\site-packages\\lazyflow\\graph.py\u201d, line 143, in maybe_call_within_transaction<br>\nfn()<br>\nFile \u201cD:\\ilastik-1.4.0-gpu\\lib\\site-packages\\lazyflow\\operator.py\u201d, line 500, in _setupOutputs<br>\nself.setupOutputs()<br>\nFile \u201cD:\\ilastik-1.4.0-gpu\\lib\\site-packages\\lazyflow\\operators\\opFeatureMatrixCache.py\u201d, line 99, in setupOutputs<br>\nassert self.FeatureImage.meta.getAxisKeys()[-1] == \u201cc\u201d<br>\nAssertionError</p>\n</blockquote>", "<p>Hello <a class=\"mention\" href=\"/u/nat.ovs\">@nat.ovs</a>,</p>\n<p>first of all, welcome to the image.sc community <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12\" title=\":star_struck:\" class=\"emoji\" alt=\":star_struck:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/tada.png?v=12\" title=\":tada:\" class=\"emoji\" alt=\":tada:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>and thank you for taking the time to report this problem with ilastik. This particular error message is unfortunately not very helpful in determining the root cause. I tried reproducing this on my side, without success. Maybe ilastik is misinterpreting the particular image. Do you think you could make it available (or something similar but small - it might be in the metadata). (I\u2019ll send you a DM with an upload link).</p>\n<p>Hope we can resolve this quickly.</p>\n<p>Cheers<br>\nD</p>", "<p>Hi, <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a></p>\n<p>You\u2019re probably right about file misinterpreting. It could be a problem with NIS-Elements (Nikon), used for image acquisition. Original files are read as time-lapse series by many soft except NIS-Elements. Conversion via NIS helps read channels as channels. I\u2019ve uploaded the original file (\u201csample_tyx\u201d) and the file after conversion (\u201csample_cyx\u201d) to the cloud.</p>\n<p>Best,<br>\nNatalia</p>", "<p>What you could do on your side would be using <a href=\"https://www.ilastik.org/documentation/fiji_export/plugin\">our fiji import/export plugin</a>, load your data as usual in fiji, but export it to hdf5 using our plugin.</p>", "<p>It works!<br>\nThank you for your help.</p>\n<p>Natalia</p>"], "78819": ["<p>Hi there,<br>\nI\u2019m trying to combine Live/Dead images for analysis. Files are named like<br>\nRun-10_23d802-01_B3_Treat5_Eth_1.tif</p>\n<p>and the metadata regex<br>\n^(?P.<em>)_(?P.</em>)<em>(?P[A-P][0-9])</em>(?P.<em>)_(?P.</em>)_(?P[0-9]{1}).tif<br>\ncreates the expected table of metadata correctly.</p>\n<p>When I then try to assign Live and Dead channels using the metadata, there is the dreaded \u201cSorry, \u2026 no valid image sets\u2026\u201d error. Images shall be matched by Plate, Well and Nr (=repeat). I cut it down to two images to no avail. I also tried to assign channels based on the filename (\u201ccontains: Eth\u201d) but that didn\u2019t help - no sets detected (it used to work with a few files with the \u201corder\u201d detection but we need the metadata).</p>\n<p>Can anyone point my mistake out? The path contains a space, but I used CP before like that without issues.</p>\n<p>Any help is greatly appreciated,<br>\nKonstantin</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/76903dda9a68a70886058df3e19e86986b91bca1.png\" alt=\"cp-1\" data-base62-sha1=\"gURnq4QzDYn09T8Y9bpxMZ3LV8R\" width=\"234\" height=\"33\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f306d25890600738d74f5876d98029528d83c78.png\" data-download-href=\"/uploads/short-url/i9aohOGZbTv93ZvGs1y2YfJdxe8.png?dl=1\" title=\"cp-2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f306d25890600738d74f5876d98029528d83c78.png\" alt=\"cp-2\" data-base62-sha1=\"i9aohOGZbTv93ZvGs1y2YfJdxe8\" width=\"690\" height=\"255\" data-dominant-color=\"E9E9E9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cp-2</span><span class=\"informations\">851\u00d7315 8.42 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1a515ef432a735be6f8134893dfc21fc6bc8ddf.png\" data-download-href=\"/uploads/short-url/n3Yzag3TGnm8C1zV2AlKsmOBWft.png?dl=1\" title=\"cp-3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1a515ef432a735be6f8134893dfc21fc6bc8ddf.png\" alt=\"cp-3\" data-base62-sha1=\"n3Yzag3TGnm8C1zV2AlKsmOBWft\" width=\"690\" height=\"458\" data-dominant-color=\"EAEAEA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cp-3</span><span class=\"informations\">902\u00d7600 20.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Konstantin,<br>\nI do have the same problem using CP4.2.5 (and earlier versions) with images acquired from spheroids through Opera Phenix and uploaded as *.tiff in CellProfiler.<br>\n1 - Trying to update the NamesAndTypes module, CellProfiler is giving me an identical error message (see Screenshot#1).<br>\n2 - Then when trying to visualize the acquired images through CellProfiler, it giving a second error message claiming that it failed to load the images (see Screenshot#2).<br>\n3 - Also tried to re-import the acquired images but the error message persisted!<br>\n<a>Uploading: Screenshot#1.png\u2026</a> i<br>\n<a>Uploading: Screenshot#2.png\u2026</a>s<br>\nAny help would be appreciated<br>\nThank you</p>"], "78823": ["<p>Hi,<br>\nI generated a classifier model using CPA 3.0.4 based on features from the actin cytoskeleton of cells. When I plug this model into the ClassifyObjects module in CP 4.2.4 and run the pipeline on new images, I get an error message indicating that some measurements don\u2019t exist, but the appropriate modules are present and running. I would appreciate any help or insight I can get for this issue. I am attaching here all the required information for others to look into (error message, CP pipeline, CPA model, sample images).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png\" data-download-href=\"/uploads/short-url/sAwr8M64Nvc1FyOISchV1Hie36Y.png?dl=1\" title=\"errorCellProfiler\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png\" alt=\"errorCellProfiler\" data-base62-sha1=\"sAwr8M64Nvc1FyOISchV1Hie36Y\" width=\"421\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 2x\" data-dominant-color=\"CBD2DA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">errorCellProfiler</span><span class=\"informations\">430\u00d7510 73.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/5v48dYmZdYzze8TvbWMxMGMPWX.zip\">CPAmodel_CPpipeline.zip</a> (9.8 MB)</p>\n<p>Thank you!</p>\n<p>Victoria</p>"], "78824": ["<p>Hi,</p>\n<p>I recently had the problem that Trackmate has assigned the same TrackID to 2 nuclei close to each other.<br>\nThe procedure that I am using is</p>\n<ol>\n<li>to segment nuclei with Stardist (in order to perform a cell tesselation)</li>\n<li>use the ROIs  detected by Stardist to create SpotCollection for Trackmate<br>\nAnd as you can see in the result table, the same TrackId is used for 2 nuclei in the same frame:</li>\n</ol>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297.png\" data-download-href=\"/uploads/short-url/6yW9QIugZFG98VpmOf9bKZKKEBN.png?dl=1\" title=\"Screen Shot 2023-03-20 at 14.04.45\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297_2_566x499.png\" alt=\"Screen Shot 2023-03-20 at 14.04.45\" data-base62-sha1=\"6yW9QIugZFG98VpmOf9bKZKKEBN\" width=\"566\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297_2_566x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297_2_849x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e001753fb742fe92569c75da391d2f894d1e297.png 2x\" data-dominant-color=\"E9E9E9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-20 at 14.04.45</span><span class=\"informations\">950\u00d7838 112 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>the columns correspond to:</p>\n<ol>\n<li>TrackID</li>\n<li>SpotID</li>\n<li>POSITION_X</li>\n<li>POSITION_Y</li>\n<li>POSITION_T</li>\n<li>FRAME</li>\n<li>QUALITY</li>\n</ol>\n<p>And as you can see in the screeshot, for the TrackID 44 there are 2 different positions for each frame. The 2 nuclei are on the following image close to each other<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28acb0a92939afba5155ca1ea940db16c0d716f3.png\" data-download-href=\"/uploads/short-url/5NP5j5olvxNF2BMqoBy9eFOs4YX.png?dl=1\" title=\"Screen Shot 2023-03-20 at 14.06.48\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28acb0a92939afba5155ca1ea940db16c0d716f3.png\" alt=\"Screen Shot 2023-03-20 at 14.06.48\" data-base62-sha1=\"5NP5j5olvxNF2BMqoBy9eFOs4YX\" width=\"473\" height=\"500\" data-dominant-color=\"0C0C0B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-20 at 14.06.48</span><span class=\"informations\">968\u00d71022 25.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Do you have any idea how to fix this problem ?</p>\n<p>thanks by advance</p>", "<p>Could it be that they belong to the same track? That they come from a cell that divided?</p>"], "78825": ["<p>Sometimes when I use the Ridge Detection plugin I get point selections, instead of lines.</p>\n<p>As I have a macro that loops through several files, whenever this happens, it halts the process (doing Plot Profiles for the ROIs).</p>\n<p>For most of my images, all works well, but some others it selects an ROI which is a Point, instead of the lines, as it should. I have Minimal Length set to 100.00 so I don\u2019t understand why this happens. Also, it seems to always be at the end of the list and preceeded by \u201cJP\u201d in the ROI Manager.</p>\n<p>I attach a tiff at the bottom with an image where this happens and present two screen grabs with the issue below.</p>\n<p>Thanks already for helping out!<br>\nNuno</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7.jpeg\" data-download-href=\"/uploads/short-url/yz3EvH5mEgHAZYjUjJN7oxdnwZ9.jpeg?dl=1\" title=\"Screen Shot 2023-03-20 at 14.45.34\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_690x406.jpeg\" alt=\"Screen Shot 2023-03-20 at 14.45.34\" data-base62-sha1=\"yz3EvH5mEgHAZYjUjJN7oxdnwZ9\" width=\"690\" height=\"406\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_690x406.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_1035x609.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f24075d64908e93bca9dcb28b701249e6e5c61c7_2_1380x812.jpeg 2x\" data-dominant-color=\"917D90\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-20 at 14.45.34</span><span class=\"informations\">1920\u00d71130 170 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1.jpeg\" data-download-href=\"/uploads/short-url/xVzHCsybOU6dpWgjhyhtwvOXMS5.jpeg?dl=1\" title=\"Screen Shot 2023-03-20 at 14.45.58\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_690x406.jpeg\" alt=\"Screen Shot 2023-03-20 at 14.45.58\" data-base62-sha1=\"xVzHCsybOU6dpWgjhyhtwvOXMS5\" width=\"690\" height=\"406\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_690x406.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_1035x609.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edc9e354570ec4f830e5b37b7e3b9fc622cd6ac1_2_1380x812.jpeg 2x\" data-dominant-color=\"846780\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-20 at 14.45.58</span><span class=\"informations\">1920\u00d71131 184 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/8eAdNttWIYoNiqoHBUWLoMXcMg3.tif\">ridge_detection_example.tif</a> (1.1 MB)</p>", "<p>I\u2019ll reply myself here, in case this helps someone else. I found another (unrelated ?) post that seems to have asked to delete certain entries on the ROI Manager based on name.</p>\n<p>It seems the \u201cJP\u201d entries added by the Ridge Detection plugin pertain to Junction Points, which can be useful, but in my case they break my analysis flow.</p>\n<p>This post shows a way to cycle through the ROI Manager and delete any that contain specific characters - in this case \u201cJP\u201d.</p>\n<aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"76747\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/lmurphy/40/43611_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/delete-roi-in-the-roimanager-based-on-names/76747/2\">Delete Roi in the RoiManager based on names</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    HI <a class=\"mention\" href=\"/u/christelle_gobet\">@christelle_gobet</a> \nThe below should work, it goes backwards through the ROI manager, selecting one at a time and uses a regular expression with the matches function to find if \u201cJP\u201d appears somewhere in the ROI name. The for loop goes backwards because if it went forwards the index would get deleted and the next increase in the for loop would skip an ROI. \nHope that makes sense! \nfor (i = roiManager(\"Count\")-1; i &gt;= 0; i--){ \n\troiManager(\"Select\", i);\n\tname = Roi.getName;\n\t\n\tif(matches(name, \"\u2026\n  </blockquote>\n</aside>\n\n<p>Thanks Community !</p>"], "78826": ["<p>Sorry I am a bit of a newb. I wanted to look at the source code for the Despeckle filter that comes with ImageJ. I have FIJI 1.53t<br>\nI found this: <a href=\"https://github.com/fiji/VIB/blob/794c4fa9cf89cc3505996ffba97dbebcc3269646/src/main/java/Despeckle_.java\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">VIB/Despeckle_.java at 794c4fa9cf89cc3505996ffba97dbebcc3269646 \u00b7 fiji/VIB \u00b7 GitHub</a><br>\nbut I don\u2019t think that\u2019s the same \u201cDespeckle\u201d because it opens a dialog, and the one I am using doesn\u2019t. Where can I find this code? It seems to be part of \u201cfilters-2.0.235\u201d but where does that come from?</p>", "<p>In RankFilters.java:</p>\n<pre><code class=\"lang-java\">if (filterType == DESPECKLE) {\n\t\t\tfilterType = MEDIAN;\n\t\t\tradius = 1.0;\n\t\t}\n</code></pre>", "<p>Ok I found it in ImageJ but not FIJI. Good enough\u2026</p>"], "78827": ["<p>Hello,</p>\n<p>I have a question regarding teh scalebar of OMERO.figure. Is it possible to set the scalebar width ? For some figure, it appears a bit thick and we would like to thin it a bit, with a drop-down menu (like for label font size) or simply manually.</p>\n<p>Thanks,<br>\nR\u00e9my.</p>", "<p>Thanks for the suggestion: I created an issue at <a href=\"https://github.com/ome/omero-figure/issues/503\" class=\"inline-onebox\">Scalebar line thickness \u00b7 Issue #503 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>", "<p>Hey,</p>\n<p>since we were actually having similar wishes this thread and Issue have prompted me to actually look into implementing it. PR Draft is linked against the GitHub issue. <a href=\"https://github.com/ome/omero-figure/pull/497\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Default figure filename to YYYY-MM-DD_hh-mm-ss by Tom-TBT \u00b7 Pull Request #497 \u00b7 ome/omero-figure \u00b7 GitHub</a><br>\n<a class=\"mention\" href=\"/u/rdornier\">@Rdornier</a> : Is this what you had in mind? (Ofc still work to do)</p>\n<p>// Julian</p>", "<p><a class=\"mention\" href=\"/u/julianhn\">@JulianHn</a>: I think you got the wrong PR: this one\u2026 <a href=\"https://github.com/ome/omero-figure/pull/504\" class=\"inline-onebox\">Add option to adjust Scalebar width by JulianHn \u00b7 Pull Request #504 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>", "<p>Hello <a class=\"mention\" href=\"/u/julianhn\">@JulianHn</a>,</p>\n<p>It\u2019s exactly what I had in mind, espacially to have a thinner height for the bar, as the default one is a bit large sometimes.<br>\nThank you for the PR !</p>\n<p>R\u00e9my.</p>"], "78829": ["<p>Hello,</p>\n<p>We experience a weird behavior of OMERO.figure when we add manual labels.<br>\nIf we add the same label as an existing one (same font size, same position, same color), then the label is added on the figure but not in the list of labels.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074.png\" data-download-href=\"/uploads/short-url/ki3kZzU1jnGu0dRipVYnW615GHW.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074_2_690x435.png\" alt=\"image\" data-base62-sha1=\"ki3kZzU1jnGu0dRipVYnW615GHW\" width=\"690\" height=\"435\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074_2_690x435.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e35ed47aacc9c2fcb8e607f7c5ca14564536074.png 2x\" data-dominant-color=\"EBEBED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">883\u00d7557 64.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>This is for me an issue because when I want to sort tags and manual labels, I need to manually rename each label to have them in the right order and when OMERO detects the two extact same labels, then it removes one from the list but not from the figure. My suggestion here is to keep the two extact labels in the list so that we can remove each one separetly.</p>\n<p>And maybe having options to :</p>\n<ol>\n<li>Select only tag we want to show (and not every tags by default)</li>\n<li>import all key-values at once (like the by-default for tags)</li>\n</ol>\n<p>would also be great if there are implemented.</p>\n<p>Regarding the label names, if the label contains many underscores <code>_</code>, then the label is not displayed correctly.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d.jpeg\" data-download-href=\"/uploads/short-url/ntSS0GFAlIp42Pla6qLdvmwGCrX.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d_2_554x500.jpeg\" alt=\"image\" data-base62-sha1=\"ntSS0GFAlIp42Pla6qLdvmwGCrX\" width=\"554\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d_2_554x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a492d296a122e280c1bd184f77af5b625dda498d.jpeg 2x\" data-dominant-color=\"CECDD0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">806\u00d7727 96.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks for having a look on that,<br>\nR\u00e9my.</p>", "<p>Hi R\u00e9my,</p>\n<p>The behaviour you\u2019re seeing is a feature that is designed to collapse all \u201cidentical\u201d labels together so they can be edited together. The idea is that if you\u2019ve got several panels with e.g. \u201cDAPI\u201d label coming from channel names, you can select all the panels and do a single edit of all the labels to \u201cDNA\u201d. It wasn\u2019t expected that you\u2019d have duplicate labels on a single panel.</p>\n<p>I wonder if we should try harder NOT to allow duplicate labels in the first place - or at least warn users. E.g. if creating labels from Tags, it could skip creating a label if it exists already? Created <a href=\"https://github.com/ome/omero-figure/issues/502\" class=\"inline-onebox\">Duplicate Labels can't be edited independently \u00b7 Issue #502 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>\n<ol>\n<li>\n<p>I thought about \u201cSelect only the Tag we want to Show\u201d when creating labels from Tags, but you might have different Tags coming from different panels and the UI for picking Tags for each panel would have just looked like the Figure itself - so it was simpler to create labels for All tags, then just remove the ones you don\u2019t want.<br>\nI can see this gets more confusing if they are mixed with existing Tags, but if we simply avoid creating duplicates, will that fix most of the issues you\u2019re seeing?</p>\n</li>\n<li>\n<p>Import all key-values \u2192 labels sounds like a useful feature that shouldn\u2019t be too hard to do. Created <a href=\"https://github.com/ome/omero-figure/issues/501\" class=\"inline-onebox\">Create Labels from ALL Key-Value pairs \u00b7 Issue #501 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>\n</li>\n<li>\n<p>I think what you\u2019re seeing with the underscores is that the label is being interpreted as <code>markdown</code> formatting, so the <code>_text_</code> is getting turned into <em>italics</em>! This can be escaped with <code>\\_text_</code> which will be displayed as _no-italics_. But I guess this isn\u2019t very user-friendly - will need to think about how to disable this\u2026  Added to <a href=\"https://github.com/ome/omero-figure/issues/486\" class=\"inline-onebox\">Label markdown formatting does too much \u00b7 Issue #486 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>\n</li>\n</ol>\n<p>Cheers,</p>", "<p>Hello Will,</p>\n<p>Thanks for your answer.</p>\n<blockquote>\n<p>It wasn\u2019t expected that you\u2019d have duplicate labels on a single panel.</p>\n</blockquote>\n<p>True</p>\n<blockquote>\n<p>E.g. if creating labels from Tags, it could skip creating a label if it exists already?</p>\n</blockquote>\n<p>This is a good idea, yes. Avoiding duplicate labels on the figure.</p>\n<blockquote>\n<p>I can see this gets more confusing if they are mixed with existing Tags, but if we simply avoid creating duplicates, will that fix most of the issues you\u2019re seeing?</p>\n</blockquote>\n<p>Indeed</p>\n<blockquote>\n<p>Import all key-values \u2192 labels sounds like a useful feature that shouldn\u2019t be too hard to do. Created <a href=\"https://github.com/ome/omero-figure/issues/501\" rel=\"noopener nofollow ugc\">Create Labels from ALL Key-Value pairs \u00b7 Issue #501 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>\n</blockquote>\n<p>Perfect. This option will be useful. I guess you will also keep the option to pick only key-value, right ?</p>\n<blockquote>\n<p>Added to <a href=\"https://github.com/ome/omero-figure/issues/486\" rel=\"noopener nofollow ugc\">Label markdown formatting does too much \u00b7 Issue #486 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>\n</blockquote>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/ok_hand.png?v=12\" title=\":ok_hand:\" class=\"emoji only-emoji\" alt=\":ok_hand:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Yes, keep the \u201cpick key-value\u201d option.</p>"], "78830": ["<p>Hi,<br>\nThanks for an excellent tool.</p>\n<p>Unfortunately, I\u2019ve run into issues running DeepMIB training (MIB v.2.84/Matlab R2021b). I\u2019m able to do preprocessing using my data when I try to follow your 2D U-net example from YouTube I get the following error.</p>\n<p>MATLAB:datastoreio:transformeddatastore:badTransformDef</p>\n<p>Using Resnet18 I get the following error:</p>\n<p>MATLAB:catenate:dimensionMismatch</p>\n<p>I\u2019m not sure how to fix this? Could it have something to do with the uneven dimensions of my dataset (1329:2388) which is different in size from my prediction dataset (1568:2040)? There are no error reported when I look at \u201cCheck network\u201d. Maybe I\u2019ve chosen an inappropriate number of patches (64)?</p>\n<p>FYI, I was able to initiate trainings and do prediction of the mitochondrial test dataset so I guess the issues lies in my dataset.</p>\n<p>Cheers<br>\nJon</p>", "<p>Hi Jon,<br>\ncould you post a snapshot from the training tab?<br>\nAlso, do not do preprocessing, you can skip it for most of the tasks in this version.</p>\n<p>Best regards,<br>\nIlya</p>", "<p>Here is the training tab for Resnet18.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03.png\" data-download-href=\"/uploads/short-url/c0H0lOLqHxYXHRlHzqyDUF2hZS3.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_690x223.png\" alt=\"image\" data-base62-sha1=\"c0H0lOLqHxYXHRlHzqyDUF2hZS3\" width=\"690\" height=\"223\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_690x223.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_1035x334.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/542e290eac4e8e328dca690772d7a7958c6eaf03_2_1380x446.png 2x\" data-dominant-color=\"D9E5F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1910\u00d7620 38.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here is the training tab for U-net. I did adjust the input patch size to get the training to initiate:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58.png\" data-download-href=\"/uploads/short-url/Ava3MjI6zsHL1CwtxOFqq56vf7y.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_690x223.png\" alt=\"image\" data-base62-sha1=\"Ava3MjI6zsHL1CwtxOFqq56vf7y\" width=\"690\" height=\"223\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_690x223.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_1035x334.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ffd40bb85b4a4d9040ce124fa3d60d136cf82d58_2_1380x446.png 2x\" data-dominant-color=\"D9E5F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1911\u00d7619 37.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Few quick comments:</p>\n<ul>\n<li>\n<p>do you images have 1 color channel?</p>\n</li>\n<li>\n<p>your input patch size seems to be quite large, you most likely will run out of GPU memory -  make it square and start with something as \u2018512 512 1 1\u2019. Also if I press Check network it gives an error indicating a wrong patch size:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff0a94ea01375a8b5a4026b23d84b0613fddcc4c.png\" alt=\"image\" data-base62-sha1=\"Aocqq5LILoqQc1YaadyEP1EafWQ\" width=\"459\" height=\"184\">. so make it square and smaller.</p>\n</li>\n<li>\n<p>after that check whether switching off augmentations will allow to start the training.</p>\n</li>\n</ul>", "<p>Turning down the patch size and turning off the augmentations made it possible to start the training. I thought the input patch size should be the size (or almost) the size of the input image.</p>\n<p>I have several classes I would like to predict. Would it be advisable to run these separately or together? And is masking the outside of the cell / ROIs also advisable?</p>\n<p>I\u2019m very new to this. Thanks for the help.</p>", "<aside class=\"quote no-group\" data-username=\"jonje219\" data-post=\"5\" data-topic=\"78830\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/j/e95f7d/40.png\" class=\"avatar\"> Jon Jerlstr\u00f6m Hultqvist:</div>\n<blockquote>\n<p>Turning down the patch size and turning off the augmentations made it possible to start the training.</p>\n</blockquote>\n</aside>\n<p>Does it work with augmentations?</p>\n<blockquote>\n<p>I thought the input patch size should be the size (or almost) the size of the input image.</p>\n</blockquote>\n<p>Perhaps in a perfect world, in reality you need to find a balance between available GPU memory and the input patch size. The idea to have the input patch size big enough to fit the context of the object, so that the network will be able to see it.</p>\n<blockquote>\n<p>I have several classes I would like to predict. Would it be advisable to run these separately or together?</p>\n</blockquote>\n<p>It depends, there is no direct answer. For example, it may depend on density of classes, if you have some very rare classes it may be better to crop those areas out and start training.<br>\nIf I am not completely mistaken, here is a video how to extract patches:</p><div class=\"onebox lazyYT lazyYT-container\" data-youtube-id=\"QrKHgP76_R0\" data-youtube-title=\"MIB Brief: Generation of patches for deep learning segmentation\" data-parameters=\"feature=oembed&amp;wmode=opaque\">\n  <a href=\"https://www.youtube.com/watch?v=QrKHgP76_R0\" target=\"_blank\" rel=\"noopener\">\n    <img class=\"ytp-thumbnail-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/9/99141162c80ce25eb5c591260ac25ecf11316772.jpeg\" title=\"MIB Brief: Generation of patches for deep learning segmentation\" width=\"480\" height=\"360\">\n  </a>\n</div>\n\n<blockquote>\n<p>And is masking the outside of the cell / ROIs also advisable?</p>\n</blockquote>\n<p>If you have not much experience, you should start with simple cases without masking (masking requires preprocessing). I typically tend to prepare the ground truth for training such that the mask is not needed, as it makes training faster. You can download sample networks from Menu-&gt;File-&gt;Example datasets. There are synthetic datasets with spots, they are very quick to do training.</p>\n<blockquote>\n<p>I\u2019m very new to this</p>\n</blockquote>\n<p>it is a rewarding process, but start with something simple that will give you quicker the results and would allow to play with parameters and options.</p>\n<p>One more thing, whenever possible try to use square input patches as it allows to use 90-degree rotations as one of augmentations.</p>", "<p>Thank you!</p>"], "78831": ["<p>Hello,</p>\n<p>I have question regarding the ROIs deletion in OMERO.figure.</p>\n<p>To delete ROIs on a specific image, we usually go on <code>ROIS-&gt;edit</code> and select one or more ROIs to delete. But we cannot select individual ROIs that are included in a larger one (and therefore cannot delete them).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339.jpeg\" data-download-href=\"/uploads/short-url/7gRM0LIYZl13kknBEFXht1yKWVP.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339_2_517x251.jpeg\" alt=\"image\" data-base62-sha1=\"7gRM0LIYZl13kknBEFXht1yKWVP\" width=\"517\" height=\"251\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339_2_517x251.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339_2_775x376.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/2/32f75455dfeb49dd9f86e40800429fc7dfa7d339.jpeg 2x\" data-dominant-color=\"DBD5D6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">797\u00d7387 59 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And sometime, for very small annotations, it is difficult to select them. Would it be possible to have a list of all ROIs so that we can individually select and delete them ?</p>\n<p>Thanks,<br>\nBest,</p>\n<p>R\u00e9my.</p>", "<p>Thanks for that - I created an issue at <a href=\"https://github.com/ome/omero-figure/issues/500\" class=\"inline-onebox\">Small ROIs lost behind bigger ROIs \u00b7 Issue #500 \u00b7 ome/omero-figure \u00b7 GitHub</a></p>\n<p>I would like it to \u201cjust work\u201d by automatically having the smaller shapes move to the front, so they can be clicked, instead of needing to find space and build a list for clicking on.</p>\n<p>I found a possible work-around that might help\u2026<br>\nIf you select the bigger ROI and \u201cCopy\u201d it, then delete it and delete others behind it, then you can \u201cPaste\u201d to add it back.</p>", "<blockquote>\n<p>I found a possible work-around that might help\u2026<br>\nIf you select the bigger ROI and \u201cCopy\u201d it, then delete it and delete others behind it, then you can \u201cPaste\u201d to add it back.</p>\n</blockquote>\n<p>Thanks for the trick !</p>"], "78837": ["<p>Hi! VERY beginner programmer here, I\u2019m trying to figure out if I can access the sholl profiles calculated from the normal semi-automated tracing functions in a python script to do further calculations without leaving SNT!</p>\n<p>I\u2019m hoping to calculate the Branching Index, an alternative to the Ramification index that biases more heavily towards distal branching, according to the paper cited below.  I have written a code that works if I use an exported csv file in a python compiler, but I was wondering if it would be possible to use with SNT for ease in the future!</p>\n<p>Luis Miguel Garcia-Segura, Julio Perez-Marquez.  A new mathematical function to evaluate neuronal morphology using the Sholl analysis, Journal of Neuroscience Methods, Volume 226, 2014, Pages 103-109, ISSN 0165-0270, <a href=\"https://doi.org/10.1016/j.jneumeth.2014.01.016\" rel=\"noopener nofollow ugc\">https://doi.org/10.1016/j.jneumeth.2014.01.016</a>. (<a href=\"https://www.sciencedirect.com/science/article/pii/S0165027014000272\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">A new mathematical function to evaluate neuronal morphology using the Sholl analysis - ScienceDirect</a>)</p>\n<p>Here is my code so far, it uses the pandas package on python.</p>\n<pre><code class=\"lang-python\"># reading CSV file\ndata = read_csdata = read_csv(\"/Users/gabriellafricklas1/Downloads/testSholl.csv\")\n\n# converting column data to list\nintersections = data['Inters.'].tolist()\nradius = data['Radius'].tolist()\nvector = list()\nfor i in range(1, len(radius)):\n    newVal = (intersections[i] - intersections[i - 1]) *(i+1)\n    if newVal &lt; 0:\n        vector.append(0)\n\n    else:\n        vector.append(newVal)\nBI = sum(vector)\n\nprint('BI', BI)\n</code></pre>", "<p>Yes! In SNT, Sholl profiles are obtained through dedicated parsers for images, SWC files, or preexisting tabular data. You can use a TabularParser to directly access the CSV data, as you would do using pandas. Here is an example:</p>\n<pre><code class=\"lang-python\">from sc.fiji.snt.analysis.sholl.parsers import TabularParser\n\n# Documentation Resources: https://imagej.net/plugins/snt/scripting\n# Latest SNT API: https://javadoc.scijava.org/SNT/\n\ntable_path = \"/Users/gabriellafricklas1/Downloads/testSholl.csv\"\nparser = TabularParser(table_path, 'Radius', 'Inters.') # file path, radius column heading, counts column heading (case sensitive)\nprofile = parser.getProfile() # access the Sholl profile object of (r, inters.) pairs. An exception is thrown if no valid profile exists (e.g., if column headings were invalid)\nintersections = profile.counts() # access intersection counts directly\n\nvector = list()\nfor i in range(1, len(intersections)):\n    newVal = (intersections[i] - intersections[i - 1]) * i\n    if newVal &lt; 0:\n        vector.append(0)\n    else:\n        vector.append(newVal)\nBI = sum(vector)\nprint('BI', BI)\n</code></pre>\n<p>I\u2019m surprised we don\u2019t include that branching index\u2019 in the default pool of metrics. You could use Fiji\u2019s script-editor autocompletion (<kbd>Ctrl</kbd> + <kbd>Space</kbd>) to check which <a href=\"https://javadoc.scijava.org/SNT/index.html?sc/fiji/snt/analysis/sholl/math/package-summary.html\">methods</a> are available:</p>\n<pre><code class=\"lang-python\">from sc.fiji.snt.analysis.sholl.math import LinearProfileStats \nstats = LinearProfileStats(profile)\nstats. # Press Ctrl+Space here\n</code></pre>", "<p>Also, looking quickly at the paper, it seems this line:</p>\n<pre><code class=\"lang-python\">    newVal = (intersections[i] - intersections[i - 1]) * (i+1)\n</code></pre>\n<p>ought to be:</p>\n<pre><code class=\"lang-python\">    newVal = (intersections[i] - intersections[i - 1]) * i\n</code></pre>\n<p>(I edited my snippet above to reflect this)</p>"], "78839": ["<p>Hello,</p>\n<p>I have very recently installed Micro-Manager for the first time (in Windows 10), to use it with a few generic USB microscopes via OpenCVGrabber (fancier hardware may happen in the future). I am hitting an issue and I have not been able to get around it.</p>\n<p>I have two USB microscopes connected to the computer (different views), and I can only get images from one of them (which is the least useful one, as Murphy\u2019s laws indicate). The only way I found to be able to get images from the other microscope is to physically unplug the first one. That is not really useful\u2026</p>\n<p>I have looked at all the properties in the OpenCVgrabber device, and I do not see anything I can change that lets me change the USB camera. I tried adding a second OpenCVgrabber device, but it seems to be also stuck on the same one. I tried to edit the \u201cCamera\u201d property in the Hardware ConfigurationWizard (which starts as \u201cundefined\u201d) to both a number and the USB camera name, and nothing seems to work.</p>\n<p>I do not need to acquire from both cameras at the same time (although that could be useful), but I would need to be able to switch between them, or at least to see them both as usable.</p>\n<p>Any help here would be truly appreciated.</p>\n<p>Thank you very much!</p>"], "78842": ["<p>Hello everyone,</p>\n<p>I have a Qupath project with multiple images and annotations on top of them. I successfully exported my annotations but I just realised that the level dimensions are off between the original whole slide images and the images in the Qupath project.</p>\n<p>For instance, one image .mrxs is of size (118216, 264186) at level 0, but when opening the Qupath project it is of size (110768, 146800). I was not able to find in the documentation a part mentioning any resizing made implicitly by QuPath.</p>\n<p>Do you know why this occurs and how I could translate my annotations in the original coordinate system of my slide?</p>\n<p>Many thanks in advance</p>\n<p>PAB</p>", "<p>Might be relevant <a href=\"https://forum.image.sc/t/actual-resolution-of-mirax-wsis-in-qupath/47968/2\" class=\"inline-onebox\">Actual resolution of Mirax WSIs in QuPath - #2 by petebankhead</a><br>\nSince the number of X pixels is about the same its unlikely to be a downsampling/resizing. More likely an XY shift.</p>\n<p>Alternatively, are the XY dimensions of pixels non-square?</p>", "<p>Hi <a class=\"mention\" href=\"/u/pab\">@PAB</a> this definitely sounds like the bounds issue <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> mentions.</p>\n<p>See here for some more info:</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"44475\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/i/df788c/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/roi-annotation-to-openslide-coordinates/44475\">Roi Annotation to Openslide Coordinates</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hello, \nI\u2019d like to extract a rectangle annotation to get its corresponding coordinates in the original slide using openslide python. \nTo do this, I\u2019m using following script: \ndef server = getCurrentServer()\nprint server.getMetadata()\n\ndef path = server.getPath()\ndef project = getProject()\ndef roi = getSelectedROI()\nprint roi\n\nThis gives me a roi of: INFO: Rectangle (19750, 19683, 1017, 723) . When I try exporting an image using this ROI directly using ImageJ, this works perfectly, but I\u2019d like \u2026\n  </blockquote>\n</aside>\n\n<p>I\u2019m not sure if you want to translate your annotations in QuPath, Python or elsewhere \u2013 but if it\u2019s QuPath then a Groovy script like this should work:</p>\n<pre><code class=\"lang-groovy\">def server = getCurrentServer()\ntranslateAllObjects(-server.boundsX, -server.boundsY)\n</code></pre>\n<p>(there still isn\u2019t a public method to access the bounds info, but this should probably be added to QuPath\u2026 for now, accessing the internal boundsX and boundsY values should still work)</p>\n<p>If you want to avoid the transform at all, then whenever you import images to a project you can provide a <code>--no-crop</code> as an optional argument \u2013 but that will cause lots of empty padding to be added.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84.png\" data-download-href=\"/uploads/short-url/s5DFzdUjbwL6sHGS1T7oPOD4PEU.png?dl=1\" title=\"Screenshot 2023-03-21 at 08.50.23\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84_2_690x406.png\" alt=\"Screenshot 2023-03-21 at 08.50.23\" data-base62-sha1=\"s5DFzdUjbwL6sHGS1T7oPOD4PEU\" width=\"690\" height=\"406\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84_2_690x406.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84_2_1035x609.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4e01420f24ee0f70bd44eb4053ba9a5ae929d84.png 2x\" data-dominant-color=\"C9CACB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-21 at 08.50.23</span><span class=\"informations\">1326\u00d7782 46.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78843": ["<p>Hello, I am trying to switch over from using SLEAP to DLC and I have a bunch of old .slp labels. I was wondering if DLC lets me import SLEAP labels?</p>", "<p>Could you open the file to show the data structure of sleap labelling?</p>"], "78845": ["<p>I\u2019d like to capture a grid with Micro-Magellan either at a specific Z location or with some autofocusing. I can create and capture the grid with the code below:</p>\n<pre><code class=\"lang-auto\">from pycromanager import Bridge,Acquisition\n\nbridge = Bridge()\nmagellan = bridge.get_magellan()\nmagellan.create_grid('my_grid', 3, 3, 0.0, 0.0)\nacq_settings = magellan.get_acquisition_settings(0)\nacq_settings.set_acquisition_name('experiment_1')\nacq_settings.set_saving_dir('D:\\test')\nacq_settings.set_xy_position_source('my_grid')\nacq = Acquisition(magellan_acq_index=0)\n</code></pre>\n<p>However, I seem unable to get Magellan to start at a specific Z height, neither by manually setting the microscope to a particular Z nor w/ MM Core <code>mmcore.set_position(z)</code>. Magellan always starts the grid at z = 0. Is there a way to set this? In the Explore controls, I can set the Z limits, but if I do this from Pycromanager, I get an error \u201c<code>java.lang.RuntimeException: Expected surface but didn't find one. Check acquisition settings</code>\u201d One other thing I noted is that in the Explore GUI, the Z Device for \u201cmy_grid\u201d is N/A.</p>\n<p>Maybe this would be easier with an XYTiledAcquision, but 0.13.2 doesn\u2019t have that enabled yet.</p>\n<p>This is with Micro-Manager 2.0.0 and Pycromanager 0.13.2.</p>", "<p>This is a bug I recently introduced. Hopefully will have a fix today</p>", "<p>Great, thanks for letting me know <a class=\"mention\" href=\"/u/henrypinkard\">@henrypinkard</a>. If I don\u2019t plan on using the Explore feature from Magellan and just want an xy grid (that\u2019s reasonably in focus), would it make more sense to set the focus automatically/manually at a landmark and then just run an XYTiled? I\u2019d like for it to be as automatic as possible, but going to a corner and focusing shouldn\u2019t be too difficult.</p>", "<p>It should be fixed now.</p>\n<p>Grids do not have any notion of Z position, so if you want them to run at a specific height, use set the position yourself, either through a z-stack with one slice or by moving to the correct position.  Or you can use a surface, which does have Z position</p>"], "78849": ["<p>Hi everyone,</p>\n<p>First time poster and new to Imagej, so I apologize for any mistakes in question format.<br>\nAnyhow, I am currently working on measuring the difference in neuronal activation via whole mount staining with cfos, dapi and tuj. The goal is the compare the cfos localization with the dapi and finally see if they are \u201cin/part of\u201d the tuj/neuronal body, thereby technically verifying that this is indeed neuronal activation. However, I haven\u2019t been able to find a good macro/technique to go about efficiently quantifying this. The problem with the few macros I have found and tried so far is that they don\u2019t automatically quantify for me the cfos co-localization with dapi and not in relation to the presence of tuj. I was wondering if you guys have any ideas or recommendations for how to better quantify this?</p>\n<p>Attached is a sample picture of what the stain looks like (it looks better when opened in imagej or lasx program).<br>\n<a class=\"attachment\" href=\"/uploads/short-url/ratzsmN1wOhiu4ZwTcPpDEsSxfL.tif\">sample wm stain.tif</a> (3.0 MB)</p>\n<p>Thanks!</p>", "<p>There is probably something similar you can do in Fiji, but in QuPath, you can perform thresholding on the three channels and then keep intersections between the channels that are over a given threshold.</p>\n<p>I believe Fiji has options in the ROI manager for Union and Intersect, which probably provide similar results.</p>\n<p>Colocalization experiments are tricky (colocalized doesn\u2019t really mean anything, or it means everything - <a href=\"https://youtu.be/P2JvFe0hB_M?t=209\" class=\"inline-onebox\">Deconstructing co-localisation workflows: A journey into the black boxes [NEUBIAS Academy@Home] - YouTube</a>), and even more so since you haven\u2019t stated what you want to measure. \u201cQuantify\u201d means very different things depending on whether you are looking at percentages of cells with a certain characteristic, percentage of area within given cell populations, Manders coefficients or Pearson\u2019s coefficients.</p>\n<p>Even worse if you have a 2D image of a 3D sample, which most are, since even the thinnest samples are actually 3D, and you are likely to have Z axis overlap without channels actually overlapping in Z. There is no fix for this, just something to be aware of when interpreting the results.</p>", "<p>Thanks for your reply. What I mean by \u201cmeasure\u201d is hopefully some macro that marks each \u201cactivated neuron\u201d in the picture (thereby allowing me to check the results afterwards) and produces an excel for each picture/field of view with the number of activated neurons. The goal is to get a general picture if there is significantly different activation levels by different treatments.</p>"], "78853": ["<p>Hi,<br>\nNeed some help scripting in QuPath.  I\u2019m analyzing multiplex IF images off a Polaris imaging platform.<br>\nOur current workflow is to import images into QuPath, run cell segmentation, then save the resulting detection measurements txt file with an appended ID column (unique integer for each cell).</p>\n<p>Thresholding/gating/phenotyping is then run using R scripts, yielding a csv file with the cell IDs and their assigned phenotypes.</p>\n<p>We then run a script to bring these phenotypes back into QuPath for visualization (see attached script).</p>\n<p>This script works for an individual image\u2026 user is prompted to open the phenotype.csv file associated with the image currently open in QuPath and phenotype classes are assigned appropriately.</p>\n<p>What I\u2019d like to be able to do is run this batchwise for all images in a project.  All of the associated phenotype.csv files would be located in one folder and the user would be prompted for that folder, rather than the individual csv file.</p>\n<p>Figured I could loop through the csv files with something like:</p>\n<p>def folder = Dialogs.promptForDirectory(null)<br>\nfolder.listFiles().each{file-&gt;<br>\ndef csvReader = new BufferedReader(new FileReader(file));</p>\n<p>But not sure how to link a given csv file to its associated image in QuPath.</p>\n<p>Thanks for you help,<br>\nMike</p>\n<pre><code class=\"lang-auto\">\n// assign phenotype class to each cell\n// from imported csv file\n\n\ncells = getCellObjects()\nnCells = cells.size()\n\n// Get location of csv\ndef file = Dialogs.promptForFile(null)\n\n// Create BufferedReader\ndef csvReader = new BufferedReader(new FileReader(file));\n\n// first row (header)\nrow = csvReader.readLine()\ndef columnNames = [:]; // Empty map\ndef cols = row.split(\",\");\nfor (i = 0; i &lt; cols.size(); i++){\n    columnNames[cols[i]] = i;\n}\n\nprint(columnNames)\n\ndef map = [:]; // Empty map\nwhile ((row = csvReader.readLine()) != null) {\n    def rowContent = row.split(\",\");\n    int id = rowContent[columnNames[\"ID\"]] as int;         // !!ID Col!!\n    phenotype = rowContent[columnNames[\"Cell.Type1\"]];    // Phenotype Col\n    map[id] = [phenotype];\n}\n\nprint \"nCells: \" + nCells;\nprint \"Map size: \" + map.size();\nprint \"_______________\";\n\n\nfor (i = 0; i &lt; nCells; i++){\n    def measurementList = cells[i].getMeasurementList();\n    int ID = measurementList.getMeasurementValue(\"ID\") as int;\n    print \"ID: \" + ID;\n    Phenotype  = map[ID];\n    print \"Assigned to: \" + Phenotype;\n    cells[i].setPathClass(getPathClass(Phenotype[0]));\n    print \"_______________\";\n}\n\nprint \"Done!\"\n\n\n</code></pre>", "<p>Try this: <a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/5\" class=\"inline-onebox\">There and back again, QuPath&lt;==&gt;CytoMAP cluster analysis - #5 by Mike_Nelson</a></p>\n<p>\u201cVariant script to import into QuPath a folder of CSV files that have been exported from CytoMAP\u201d</p>", "<aside class=\"quote no-group\" data-username=\"MJCampbell\" data-post=\"1\" data-topic=\"78853\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/ba9def/40.png\" class=\"avatar\"> Michael Campbell:</div>\n<blockquote>\n<p>But not sure how to link a given csv file to its associated image in QuPath.</p>\n</blockquote>\n</aside>\n<p>I hope the script helps - one thing to keep in mind is that the name association between the CSV files and the project entries, which is usually how you want to target the correct import location, needs to be maintained exactly.</p>", "<p>Hi Sara and Mike,<br>\nThanks.  Looks like that script should do the trick!<br>\nMike</p>"], "54277": ["<p>I recently got a new m1 MacBook air. I had been running QuPath with stardist on an intel chip and had no issues at all. I just built QuPath on my new computer, but every time I run my stardist cell detection script (which worked on the old machine), the program quits. I am not running it for a big batch or anything - just trying to get it going on one image at the moment! There is no error message generated, and I can\u2019t view the log to see what\u2019s going on. Has anyone else encountered this? I am still fairly new to QuPath, so it might be something very simple I am overlooking. Thank you!</p>", "<p>What\u2019s your QuPath install? You mention you built it\u2014does that mean it\u2019s Apple Silicon (arm64) native?<br>\nThen you likely the issue you have is architecture mismatch, because StarDist uses tensorflow and that\u2019s been a bit of an issue.<br>\nOn M1 everything has to be run in Rosetta or everything has to be native, you can\u2019t have a process that mixes architectures, which includes compiled libraries. I suspect it\u2019s the tensorflow part that\u2019s getting you, because I <em>think</em> that QuPath tensorflow is tensorflow-java, which isn\u2019t arm64 native yet. Perhaps <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> and <a class=\"mention\" href=\"/u/mweigert\">@mweigert</a> can comment.<br>\nThere was an Apple tensorflow fork, but getting it running was not trivial.<br>\nYou can take a look at the latest from Apple:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://developer.apple.com/metal/tensorflow-plugin/\">\n  <header class=\"source\">\n      <img src=\"https://developer.apple.com/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https://developer.apple.com/metal/tensorflow-plugin/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Apple Developer</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/362;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0741235d43e5210d9bc697486bc3cbeba1b080c3_2_690x362.jpeg\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0741235d43e5210d9bc697486bc3cbeba1b080c3_2_690x362.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0741235d43e5210d9bc697486bc3cbeba1b080c3_2_1035x543.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/0741235d43e5210d9bc697486bc3cbeba1b080c3.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0741235d43e5210d9bc697486bc3cbeba1b080c3_2_10x10.png\"></div>\n\n<h3><a href=\"https://developer.apple.com/metal/tensorflow-plugin/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Tensorflow Plugin - Metal - Apple Developer</a></h3>\n\n  <p>Find presentations, documentation, sample code, and resources for building macOS, iOS, and tvOS apps with the Metal framework.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nThere\u2019s been people indicating on twitter that this works already despite saying MacOS 12.0 required. However, getting tensorflow-java built has not been working:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/tensorflow/java/issues/252#issuecomment-859221268\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/tensorflow/java/issues/252#issuecomment-859221268\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/tensorflow/java</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/tensorflow/java/issues/252#issuecomment-859221268\" target=\"_blank\" rel=\"noopener nofollow ugc\">Does TensorFlow-Java support Apple Silicon Macs?</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-03-24\" data-time=\"07:48:55\" data-timezone=\"UTC\">07:48AM - 24 Mar 21 UTC</span>\n      </div>\n\n\n      <div class=\"user\">\n        <a href=\"https://github.com/Taiyx\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"Taiyx\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/8679400d4be67f6f826e3576d71c53071dd0bed0.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          Taiyx\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">&lt;em&gt;Please make sure that this is a bug. As per our [GitHub Policy](https://gith<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">ub.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template&lt;/em&gt;\n\n**System information**\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MacOs Big Sur ver11.0.1,  M1 slices** \n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n- TensorFlow installed from (source or binary):  Maven\n- TensorFlow version (use command below): Old tensorflow version(1.4.0/1.5.0), and new  tensorflow Java Version 0.2.0\n- JVM version:  1.8.0_162\n-  No GPU\n\n**Describe the current behavior**\nI just try to run tensorflow java offical example, and get tensorflow version for test. but It dosen't work. \nI have test different versions of tensorflow java interface, and only ver 1.13.1 works well. \nAnd all other versions can not work,  for example old tensorflow version(1.4.0/1.5.0), and new  tensorflow Java Version 0.2.0/0.3.0(tensorflow ver2.3.1/2.4.1) .\n\nThe Error shows below:\n\n&gt; A fatal error has been detected by the Java Runtime Environment:\n&gt; \n&gt; SIGILL (0x4) at pc=0x00000001290edc15, pid=6333, tid=0x0000000000001a03\n&gt; \n&gt; JRE version: Java(TM) SE Runtime Environment (8.0_162-b12) (build 1.8.0_162-b12)\n&gt; Java VM: Java HotSpot(TM) 64-Bit Server VM (25.162-b12 mixed mode bsd-amd64 compressed oops)\n&gt; Problematic frame:\n&gt; C  [libtensorflow_framework.2.dylib+0x14c15]  tensorflow::monitoring::MetricDef&lt;(tensorflow::monitoring::MetricKind)1, long long, 2&gt;::MetricDef&lt;char [11], char [7]&gt;(absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, char const (&amp;) [11], char const (&amp;) [7])+0x125\n&gt; \n&gt; Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n&gt; \n&gt; An error report file with more information is saved as:\n&gt;  /***/tf_test/hs_err_pid6333.log\n&gt; \n&gt; If you would like to submit a bug report, please visit:\n&gt; http://bugreport.java.com/bugreport/crash.jsp\n&gt; The crash happened outside the Java Virtual Machine in native code.\n&gt; See problematic frame for where to report the bug.\n&gt; \n\n**Code to reproduce the issue**\nJava code:\n```\n\nimport org.tensorflow.TensorFlow;\npublic class HelloTensorFlow {\n    public static void main(String[] args) throws Exception {\n        System.out.println(\"Hello TensorFlow \" +TensorFlow.version());\n    }\n}\n```\n\npom.xml\n```\n&lt;project&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;groupId&gt;org.myorg&lt;/groupId&gt;\n    &lt;artifactId&gt;hellotensorflow&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;exec.mainClass&gt;HelloTensorFlow&lt;/exec.mainClass&gt;\n    \n        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;\n    &lt;/properties&gt;\n\n    &lt;dependencies&gt;\n    \n        &lt;dependency&gt;\n            &lt;groupId&gt;org.tensorflow&lt;/groupId&gt;\n            &lt;artifactId&gt;tensorflow-core-platform&lt;/artifactId&gt;\n            &lt;version&gt;0.2.0&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n```</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Also, if you are running QuPath in Rosetta, it\u2019s possible StarDist and tensorflow will still fail because Rosetta might not be able to translate accelerated math functions targeting Intel AVX, as also mentioned in the above issue.</p>", "<aside class=\"quote no-group\" data-username=\"psobolewskiPhD\" data-post=\"2\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\"> psobolewskiPhD:</div>\n<blockquote>\n<p>I suspect it\u2019s the tensorflow part that\u2019s getting you, because I <em>think</em> that QuPath tensorflow is tensorflow-java, which isn\u2019t arm64 native yet.</p>\n</blockquote>\n</aside>\n<p>QuPath v0.2.3 was using TensorFlow via JavaCPP. QuPath v0.3.0 will support using it from <a href=\"https://github.com/tensorflow/java\" class=\"inline-onebox\">GitHub - tensorflow/java: Java bindings for TensorFlow</a> \u2013 which is being very actively developed. Not sure if/when it will resolve any Apple Silicon issues though.</p>\n<p>I notice that Java 17 (due in September) is expected to have a <a href=\"https://openjdk.java.net/jeps/391\">macOS/AArch64 Port</a>, although perhaps there are alternatives already available (e.g. <a href=\"https://www.azul.com/newsroom/azul-announces-support-of-java-builds-of-openjdk-for-apple-silicon/\">Zulu</a>).</p>\n<p>I\u2019m afraid I\u2019m not able to test any of these things until I get a new Mac myself\u2026</p>\n<p><strong>A bit of a sidenote</strong>, but <em>if</em> OpenCV works ok through QuPath on your Mac (e.g. the pixel classifier behaves properly) then there could be another option in the near-ish future.</p>\n<p>I\u2019ve seen that it\u2019s possible to convert a <a href=\"https://github.com/stardist/stardist-imagej/tree/master/src/main/resources/models/2D\">pretrained StarDist model</a> into a frozen .pb file using <a href=\"https://github.com/onnx/tensorflow-onnx\">tf2onnx</a> that is OpenCV-compatible. All it takes is one line:</p>\n<pre><code class=\"lang-auto\">python -m tf2onnx.convert --saved-model ./he_heavy_augment --output_frozen_graph ./he_heavy_augment.pb --opset 10\n</code></pre>\n<p>QuPath can then use it without any requirement for TensorFlow at all. It might be a bit slower (no GPU-acceleration), but the motivation extends beyond Apple M1 troubles: it would enable us to make StarDist available to all users very easily. Whenever TensorFlow <em>is</em> available, QuPath can switch between them depending upon what model file is provided.</p>\n<p><a class=\"mention\" href=\"/u/mweigert\">@mweigert</a> and <a class=\"mention\" href=\"/u/uschmidt83\">@uschmidt83</a> would you consider making your pretrained models also available as frozen .pb files somewhere for this purpose?</p>\n<p>It would mean StarDist can be used anywhere OpenCV is present via the <a href=\"https://docs.opencv.org/4.5.2/d2/d58/tutorial_table_of_content_dnn.html\">DNN module</a>.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>QuPath v0.2.3 was using TensorFlow via JavaCPP. QuPath v0.3.0 will support using it from <a href=\"https://github.com/tensorflow/java\" rel=\"noopener nofollow ugc\">GitHub - tensorflow/java: Java bindings for TensorFlow </a> \u2013 which is being very actively developed. Not sure if/when it will resolve any Apple Silicon issues though.</p>\n</blockquote>\n</aside>\n<p>OK, so this is what I thought might be the case. Similar as Fiji. At the moment, this isn\u2019t working native on Apple M1 (arm64), as seen in the issue I liked above.</p>\n<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>I notice that Java 17 (due in September) is expected to have a <a href=\"https://openjdk.java.net/jeps/391\" rel=\"noopener nofollow ugc\">macOS/AArch64 Port</a>, although perhaps there are alternatives already available (e.g. <a href=\"https://www.azul.com/newsroom/azul-announces-support-of-java-builds-of-openjdk-for-apple-silicon/\" rel=\"noopener nofollow ugc\">Zulu </a>).</p>\n</blockquote>\n</aside>\n<p>Azul does have Java JRE/JDK for Apple M1\u2014I run Fiji this way with a nice speedup over Rosetta\u2014but not yet have a Java 17 for Apple arm64:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.azul.com/downloads/?os=macos&amp;architecture=arm-64-bit\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c922b2b0e37e92ef395edff6358cde9b8ed0881.png\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https://www.azul.com/downloads/?os=macos&amp;architecture=arm-64-bit\" target=\"_blank\" rel=\"noopener nofollow ugc\">Azul | Better Java Performance, Superior Java Support</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://www.azul.com/downloads/?os=macos&amp;architecture=arm-64-bit\" target=\"_blank\" rel=\"noopener nofollow ugc\">Java Download | Java 8, Java 11, Java 13 - Linux, Windows &amp; macOS</a></h3>\n\n  <p>Download Java Builds of OpenJDK 8, 11, 13 &amp; 15. Azul Zulu Builds of OpenJDK runs on Linux, Windows, macOS &amp; Solaris on X86, Arm, SPARC &amp; PPC</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>I\u2019m afraid I\u2019m not able to test any of these things until I get a new Mac myself\u2026</p>\n</blockquote>\n</aside>\n<p>I\u2019m not a QuPath user, but I\u2019m willing to help test things, as long as they are reasonably documented, because I\u2019m not the most savvy person, particularly in Java.</p>\n<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p><strong>A bit of a sidenote</strong> , but <em>if</em> OpenCV works ok through QuPath on your Mac (e.g. the pixel classifier behaves properly) then there could be another option in the near-ish future.</p>\n<p>I\u2019ve seen that it\u2019s possible to convert a <a href=\"https://github.com/stardist/stardist-imagej/tree/master/src/main/resources/models/2D\" rel=\"noopener nofollow ugc\">pretrained StarDist model</a> into a frozen .pb file using <a href=\"https://github.com/onnx/tensorflow-onnx\" rel=\"noopener nofollow ugc\">tf2onnx</a> that is OpenCV-compatible. All it takes is one line:</p>\n<pre><code class=\"lang-auto\">python -m tf2onnx.convert --saved-model ./he_heavy_augment --output_frozen_graph ./he_heavy_augment.pb --opset 10\n</code></pre>\n<p>QuPath can then use it without any requirement for TensorFlow at all. It might be a bit slower (no GPU-acceleration), but the motivation extends beyond Apple M1 troubles: it would enable us to make StarDist available to all users very easily. Whenever TensorFlow <em>is</em> available, QuPath can switch between them depending upon what model file is provided.</p>\n<p><a class=\"mention\" href=\"/u/mweigert\">@mweigert</a> and <a class=\"mention\" href=\"/u/uschmidt83\">@uschmidt83</a> would you consider making your pretrained models also available as frozen .pb files somewhere for this purpose?</p>\n<p>It would mean StarDist can be used anywhere OpenCV is present via the <a href=\"https://docs.opencv.org/4.5.2/d2/d58/tutorial_table_of_content_dnn.html\" rel=\"noopener nofollow ugc\">DNN module</a>.</p>\n</blockquote>\n</aside>\n<p>I\u2019m not familiar with OpenCV, but it there is a native Apple Silicon build available via Homebrew:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://formulae.brew.sh/formula/opencv#default\">\n  <header class=\"source\">\n      <img src=\"https://formulae.brew.sh/assets/img/favicon.ico\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://formulae.brew.sh/formula/opencv#default\" target=\"_blank\" rel=\"noopener nofollow ugc\">Homebrew Formulae</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/7405ad2201bc0fb6c5032f52ccb825e2c28ef504.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://formulae.brew.sh/formula/opencv#default\" target=\"_blank\" rel=\"noopener nofollow ugc\">opencv</a></h3>\n\n  <p>Homebrew\u2019s package index</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I\u2019d be quite curious about this pathway to solve the issues with tensorflow-java, perhaps this route could also be available to Fiji?</p>", "<p>Small addendum:<br>\nJavaCPP is also has support for native on Apple M1 in the latest release:<br>\n<a href=\"https://github.com/bytedeco/javacpp/releases/tag/1.5.5\" rel=\"noopener nofollow ugc\">JavaCPP 1.5.5</a></p>\n<p>So it looks like the basic infrastructure is there or nearly there.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>I\u2019ve seen that it\u2019s possible to convert a <a href=\"https://github.com/stardist/stardist-imagej/tree/master/src/main/resources/models/2D\">pretrained StarDist model</a> into a frozen .pb file using <a href=\"https://github.com/onnx/tensorflow-onnx\">tf2onnx</a> that is OpenCV-compatible.</p>\n</blockquote>\n</aside>\n<p>That would be nice if it\u2019s really that easy.</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>QuPath can then use it without any requirement for TensorFlow at all \u2026 would enable us to make StarDist available to all users very easily.</p>\n</blockquote>\n</aside>\n<p>That opens the bigger discussion which model format (and runtime) to adopt. ONNX seems like a good choice and OpenCV as a runtime is widely used, but it doesn\u2019t have GPU support as you mentioned.</p>\n<p>I\u2019ve bee eyeing <a href=\"https://www.onnxruntime.ai\">ONNX Runtime</a> (<a href=\"https://github.com/microsoft/onnxruntime\">GitHub</a>) for a while as an inference runtime that works for many combinations of operating system, programming language, and accelerator (e.g. GPU).</p>\n<p>I haven\u2019t tried it in practice though, hence I\u2019m not sure if the ONNX format supports all the features/ops that we need and if ONNX Runtime is actually easy to install and use.</p>\n<p>Best,<br>\nUwe</p>", "<aside class=\"quote no-group quote-modified\" data-username=\"uschmidt83\" data-post=\"6\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/uschmidt83/40/37368_2.png\" class=\"avatar\"> uschmidt83:</div>\n<blockquote>\n<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>I\u2019ve seen that it\u2019s possible to convert a <a href=\"https://github.com/stardist/stardist-imagej/tree/master/src/main/resources/models/2D\">pretrained StarDist model</a> into a frozen .pb file using <a href=\"https://github.com/onnx/tensorflow-onnx\">tf2onnx</a> that is OpenCV-compatible.</p>\n</blockquote>\n</aside>\n<p>That would be nice if it\u2019s really that easy.</p>\n</blockquote>\n</aside>\n<p>Well, I first tried it some months ago &amp; tested it again with the latest version before posting. Still works!</p>\n<p>In a QuPath script I switch between passing a SavedModel and pb file, and it switches whether TensorFlow or OpenCV is used. Then I tried again without TensorFlow installed to check it wasn\u2019t cheating. Results look the same.</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"uschmidt83\" data-post=\"6\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/uschmidt83/40/37368_2.png\" class=\"avatar\"> uschmidt83:</div>\n<blockquote>\n<aside class=\"quote no-group quote-modified\" data-username=\"petebankhead\" data-post=\"3\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>QuPath can then use it without any requirement for TensorFlow at all \u2026 would enable us to make StarDist available to all users very easily.</p>\n</blockquote>\n</aside>\n<p>That opens the bigger discussion which model format (and runtime) to adopt. ONNX seems like a good choice and OpenCV as a runtime is widely used, but it doesn\u2019t have GPU support as you mentioned.</p>\n</blockquote>\n</aside>\n<p>True, although it supports different backends and there was a <a href=\"https://gist.github.com/YashasSamaga/a84cf2826ab2dc755005321fe17cd15d\">GSoC project on GPU support</a>. But so far I\u2019ve found accessing GPU-friendly OpenCV via Java was always too awkward and unreliable.</p>\n<aside class=\"quote no-group\" data-username=\"uschmidt83\" data-post=\"6\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/uschmidt83/40/37368_2.png\" class=\"avatar\"> uschmidt83:</div>\n<blockquote>\n<p>I\u2019ve bee eyeing <a href=\"https://www.onnxruntime.ai\">ONNX Runtime</a> (<a href=\"https://github.com/microsoft/onnxruntime\">GitHub</a>) for a while as an inference runtime that works for many combinations of operating system, programming language, and accelerator (e.g. GPU).</p>\n<p>I haven\u2019t tried it in practice though, hence I\u2019m not sure if the ONNX format supports all the features/ops that we need and if ONNX Runtime is actually easy to install and use.</p>\n</blockquote>\n</aside>\n<p>OpenCV supports ONNX supposedly, but if I use <code>tf2onnx</code> with StarDist and ONNX output I can\u2019t load the model. I have found frequent unsupported layer exceptions whenever I try to convert any arbitrary model into an OpenCV-friendly form, which can be frustrating. But StarDist is working nicely if I choose the frozen output rather than ONNX.</p>\n<aside class=\"quote no-group\" data-username=\"uschmidt83\" data-post=\"6\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/uschmidt83/40/37368_2.png\" class=\"avatar\"> uschmidt83:</div>\n<blockquote>\n<p>I haven\u2019t tried it in practice though, hence I\u2019m not sure if the ONNX format supports all the features/ops that we need and if ONNX Runtime is actually easy to install and use.</p>\n</blockquote>\n</aside>\n<p>I find <a href=\"https://github.com/bytedeco/javacpp-presets\">JavaCPP-presets</a> the easiest way to support native libraries in Java, including OpenCV \u2013 and it now seems to be adopted by the most \u2018official\u2019 TensorFlow Java. I\u2019ve also had an eye on ONNX-Runtime, but I <a href=\"https://github.com/bytedeco/javacpp-presets/tree/master/onnxruntime\">don\u2019t find the Java API very beautiful.</a>.</p>\n<p>OpenVINO also looks interesting, and has already <a href=\"https://github.com/qupath/qupath/pull/665\">been shown to work with QuPath + StarDist</a>.</p>\n<p>For QuPath, the plan is to support various frameworks via extensions \u2013 probably through JavaCPP in most cases. But cross-platform distribution of remains a pain. We\u2019ll likely add TensorFlow (mostly exists), PyTorch and ONNX Runtime extensions, but in the near future these are likely to require the end user to build them locally and grab the dependencies via gradle with whatever custom options they need. Therefore very much more a developer\u2019s option.</p>\n<p>OpenCV is the common denominator that is a QuPath dependency anyway, so by far the easiest way for us to make any model available to all users without the awkward build step.</p>", "<p>Thank you everyone for such comprehensive replies. I will start giving some of the suggestions a try, and work something out to use in the short term. I really appreciate all the help!</p>", "<aside class=\"quote no-group\" data-username=\"uschmidt83\" data-post=\"6\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/uschmidt83/40/37368_2.png\" class=\"avatar\"> uschmidt83:</div>\n<blockquote>\n<p>That opens the bigger discussion which model format (and runtime) to adopt. ONNX seems like a good choice and OpenCV as a runtime is widely used, but it doesn\u2019t have GPU support as you mentioned.</p>\n</blockquote>\n</aside>\n<p>Brief update to say\u2026 I\u2019ve now checked this on Windows and Linux, and OpenCV does indeed have GPU support and it works very well! It\u2019s a matter of setting the backend and target to CUDA, then hoping really really hard that the drivers and paths work out.</p>\n<p>I haven\u2019t tried it with Python yet, but in the Java world much of the pain can be resolved by using JavaCPP. This provides a GPU-friendly OpenCV distribution with CUDA support and even an option to include a bundled CUDA and cuDNN (albeit &gt; 1 GB) as Maven dependencies: <a href=\"https://github.com/bytedeco/javacpp-presets/tree/master/opencv#the-pomxml-build-file\" class=\"inline-onebox\">javacpp-presets/opencv at master \u00b7 bytedeco/javacpp-presets \u00b7 GitHub</a></p>\n<p>I\u2019ve also used this to run StarDist + classification through QuPath successfully. Currently, this is <em>only</em> possible using OpenCV + a frozen TensorFlow model, but I\u2019ll work on cleaning it up and hopefully supporting TensorFlow SavedModel this week.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"9\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>Brief update to say\u2026 I\u2019ve now checked this on Windows and Linux, and OpenCV does indeed have GPU support and it works very well! It\u2019s a matter of setting the backend and target to CUDA, then hoping really really hard that the drivers and paths work out.</p>\n</blockquote>\n</aside>\n<p>Thanks for the update!</p>\n<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"9\" data-topic=\"54277\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>Currently, this is <em>only</em> possible using OpenCV + a frozen TensorFlow model, but I\u2019ll work on cleaning it up and hopefully supporting TensorFlow SavedModel this week.</p>\n</blockquote>\n</aside>\n<p>In case you didn\u2019t know, the Fiji community is currently also thinking about how to integrate deep learning frameworks into Fiji, also as part of the <a href=\"https://forum.image.sc/t/fiji-hackathon-2021-online/53391/12\">currently ongoing hackathon</a>. I shared this forum post with them, since QuPath and Fiji share many of the same challenges in this regard.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/uschmidt83\">@uschmidt83</a> it would be great if Fiji and QuPath took a compatible approach. I\u2019d be very happy to chat more about it.</p>", "<p>I forgot about this thread.<br>\nIf the OP still wants to run StarDist on M1, I\u2019ve gotten the python side of things working native, as well as the napari plugin:</p><aside class=\"onebox twitterstatus\" data-onebox-src=\"https://twitter.com/psobolewskiPhD/status/1413479230051033091?s=20\">\n  <header class=\"source\">\n\n      <a href=\"https://twitter.com/psobolewskiPhD/status/1413479230051033091?s=20\" target=\"_blank\" rel=\"noopener\">twitter.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n<h4><a href=\"https://twitter.com/psobolewskiPhD/status/1413479230051033091?s=20\" target=\"_blank\" rel=\"noopener\">Peter Sobolewski</a></h4>\n<div class=\"twitter-screen-name\"><a href=\"https://twitter.com/psobolewskiPhD/status/1413479230051033091?s=20\" target=\"_blank\" rel=\"noopener\">@psobolewskiPhD</a></div>\n\n<div class=\"tweet\">\n  How about #StarDist native on #AppleSilicon \u2705\nSegmentation viewed in @napari_imaging\n(FYI: Just using the pre-trained model.)\n#Python #BioImageAnalysis\n    <div class=\"quoted\">\n      <a class=\"quoted-link\" href=\"https://twitter.com/psobolewskiPhD/status/1410162241454972930\" rel=\"noopener\">\n        <p class=\"quoted-title\">Peter Sobolewski <span>@psobolewskiPhD</span></p>\n      </a>\n\n      <div>Hi Patrik, are you still playing with this?\nI just tried it (M1, Big Sur 11.4). Do you also get \ndevice:GPU:0 with 0 MB memory\nI can do the basic tutorial fine, but it's odd to see that.\nhttps://www.tensorflow.org/tutorials/quickstart/beginner\u00a0\u2026\n#TensorFlow #AppleSilicon pic.twitter.com/r06Wl6NzwB</div>\n    </div>\n</div>\n\n<div class=\"date\">\n  <a href=\"https://twitter.com/psobolewskiPhD/status/1413479230051033091?s=20\" class=\"timestamp\" target=\"_blank\" rel=\"noopener\">5:44 AM - 9 Jul 2021</a>\n\n    <span class=\"like\">\n      <svg viewbox=\"0 0 512 512\" width=\"14px\" height=\"16px\" aria-hidden=\"true\">\n        <path d=\"M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z\"></path>\n      </svg>\n      22\n    </span>\n\n    <span class=\"retweet\">\n      <svg viewbox=\"0 0 640 512\" width=\"14px\" height=\"16px\" aria-hidden=\"true\">\n        <path d=\"M629.657 343.598L528.971 444.284c-9.373 9.372-24.568 9.372-33.941 0L394.343 343.598c-9.373-9.373-9.373-24.569 0-33.941l10.823-10.823c9.562-9.562 25.133-9.34 34.419.492L480 342.118V160H292.451a24.005 24.005 0 0 1-16.971-7.029l-16-16C244.361 121.851 255.069 96 276.451 96H520c13.255 0 24 10.745 24 24v222.118l40.416-42.792c9.285-9.831 24.856-10.054 34.419-.492l10.823 10.823c9.372 9.372 9.372 24.569-.001 33.941zm-265.138 15.431A23.999 23.999 0 0 0 347.548 352H160V169.881l40.416 42.792c9.286 9.831 24.856 10.054 34.419.491l10.822-10.822c9.373-9.373 9.373-24.569 0-33.941L144.971 67.716c-9.373-9.373-24.569-9.373-33.941 0L10.343 168.402c-9.373 9.373-9.373 24.569 0 33.941l10.822 10.822c9.562 9.562 25.133 9.34 34.419-.491L96 169.881V392c0 13.255 10.745 24 24 24h243.549c21.382 0 32.09-25.851 16.971-40.971l-16.001-16z\"></path>\n      </svg>\n      8\n    </span>\n</div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nAs far as I can tell it does use the GPU, so <code>tensorflow-metal</code> is working.<br>\nInstructions:</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"55051\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/napari-tensorflow-aicsimageio-stardist-pyclesperanto-running-native-on-apple-silicon-m1/55051\">Napari, TensorFlow, AICSImageIO, StarDist, CARE/N2V, pyclEsperanto: running native on Apple Silicon M1</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Ok, I\u2019ve been playing with this for a week and it seems functional. That said, a disclaimer: This is my first contact with anything in python, I\u2019m a copy-pasta coder, and I\u2019ve not read too many docs or learned much. So, there may be a better or easier path to success\u2014and there may be issues. \nThis was all on MBPro, M1, 16GB, Big Sur 11.4 \nYou\u2019ll need to have some comfort with the Terminal. \nYou\u2019ll need homebrew or some other way to install additional tools. \n\nNote: In some places specific packag\u2026\n  </blockquote>\n</aside>\n\n<p>Regarding OpenCV, I\u2019ve yet to play with it, but it does suggest it has OpenCL support too, which would help make it more universal than CUDA:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://opencv.org/opencl/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/5/953dd1be08ef7307640c6b77012ece8d7abc43d8.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://opencv.org/opencl/\" target=\"_blank\" rel=\"noopener\">OpenCV</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:640/480;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c73578802f111feb8d0f3da5a9f722e9a2baeb8.jpeg\" class=\"thumbnail\" width=\"640\" height=\"480\"></div>\n\n<h3><a href=\"https://opencv.org/opencl/\" target=\"_blank\" rel=\"noopener\">OpenCL - OpenCV</a></h3>\n\n  <p>Intro Open Computing Language (OpenCL) is an open standard for writing code that runs across heterogeneous platforms including CPUs, GPUs, DSPs and etc. In particular OpenCL provides applications with an access to GPUs for non-graphical computing...</p>\n\n  <p>\n    <span class=\"label1\">Est. reading time: 2 minutes</span>\n  </p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nAlso there are PRs working on arm64/M1 specific performance improvements, so that\u2019s a good sign!</p>", "<p>Lots of movement regarding deep learning deployment these days\u2026</p>\n<aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"55731\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/sebi06/40/21297_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/new-release-2-0-0-of-czmodel-pypi-package/55731\">New release 2.0.0 of CZMODEL PyPi package</a> <a class=\"badge-wrapper  bullet\" href=\"/c/announcements/10\"><span class=\"badge-category-bg\" style=\"background-color: #AB9364;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for announcements, such as new software releases and upcoming community events.\">Announcements</span></a>\n  </div>\n  <blockquote>\n    Hi all, \nwe just released a major update of our open-source python package: \n<a href=\"https://pypi.org/project/czmodel/\" rel=\"noopener nofollow ugc\">CZMODEL 2.0.0</a> \nIt allows to package deep-learning models together with metadata and use them inside the ZEN software platform(s) as part of various modules. To run \u201cpredictions\u201d in ZEN we use either TF2.SavedModel or ONNX (for better GPU memory management <img width=\"20\" height=\"20\" src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=10\" title=\"slight_smile\" alt=\"slight_smile\" class=\"emoji\"> . \nAn <a href=\"https://colab.research.google.com/github/zeiss-microscopy/OAD/blob/master/Machine_Learning/notebooks/czmodel/SingleClassSemanticSegmentation_2_0_0.ipynb\" rel=\"noopener nofollow ugc\">example notebook</a> is available for Google Colab. Please check it out to get started. \nThis package is also used on <a href=\"https://www.apeer.com/app/machine-learning/overview\" rel=\"noopener nofollow ugc\">APEER-ML Deep-Learning Cloud Pla\u2026</a>\n  </blockquote>\n</aside>\n", "<p>Hi all,</p>\n<p>I\u2019m completely new to coding/deep learning and I will admit most of this thread has flown over my head. I was wondering whether someone could clarify - can I use stardist with Qupath Version: 0.4.3 on an M1 Mac? I can\u2019t seem to open <a href=\"https://github.com/qupath/models/raw/main/stardist/he_heavy_augment.pb\" rel=\"noopener nofollow ugc\">he_heavy_augment.pb</a>  in qupath, and when I try running this script (from qupath v0.3.0 docs) using one of my mrxs files I get the error message that tensoflow is required</p>\n<p>import qupath.ext.stardist.StarDist2D</p>\n<p>// Specify the model file (you will need to change this!)<br>\nvar pathModel = \u2018/Volumes/VERBATIM HD/Slide Scanner/Flow through gels/CD68/FT31/slide-2023-02-24T09-20-06-R9-S10.mrxs\u2019</p>\n<p>var stardist = StarDist2D.builder(pathModel)<br>\n.threshold(0.5)              // Prediction threshold<br>\n.normalizePercentiles(1, 99) // Percentile normalization<br>\n.pixelSize(0.5)              // Resolution for detection<br>\n.build()</p>\n<p>// Run detection for the selected objects<br>\nvar imageData = getCurrentImageData()<br>\nvar pathObjects = getSelectedObjects()<br>\nif (pathObjects.isEmpty()) {<br>\nDialogs.showErrorMessage(\u201cStarDist\u201d, \u201cPlease select a parent object!\u201d)<br>\nreturn<br>\n}<br>\nstardist.detectObjects(imageData, pathObjects)<br>\nprintln \u2018Done!\u2019</p>\n<p>INFO: Cannot build model with qupath.opencv.dnn.OpenCVDnnModelBuilder@537857b6<br>\nERROR: Unable to load TensorFlow with reflection - are you sure it is available and on the classpath?<br>\nERROR: qupath.ext.tensorflow.TensorFlowTools<br>\njava.lang.ClassNotFoundException: qupath.ext.tensorflow.TensorFlowTools<br>\nat java.base/java.net.URLClassLoader.findClass(Unknown Source)<br>\nat java.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>\nat java.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>\nat java.base/java.lang.Class.forName0(Native Method)<br>\nat java.base/java.lang.Class.forName(Unknown Source)<br>\nat qupath.ext.stardist.StarDist2D$Builder.build(StarDist2D.java:676)<br>\nat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)<br>\nat QuPathScript.run(QuPathScript:7)<br>\nat org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\nat org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nat qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nat qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nat qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\nat java.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\nat java.base/java.lang.Thread.run(Unknown Source)<br>\nERROR: qupath.ext.tensorflow.TensorFlowTools in QuPathScript at line number 6</p>\n<p>ERROR: java.base/java.net.URLClassLoader.findClass(Unknown Source)<br>\njava.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>\njava.base/java.lang.ClassLoader.loadClass(Unknown Source)<br>\njava.base/java.lang.Class.forName0(Native Method)<br>\njava.base/java.lang.Class.forName(Unknown Source)<br>\nqupath.ext.stardist.StarDist2D$Builder.build(StarDist2D.java:676)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)<br>\nQuPathScript.run(QuPathScript:6)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>", "<p>Hi <a class=\"mention\" href=\"/u/li1234\">@Li1234</a> your script provides the image, but it needs to give the path to the model instead.</p>\n<p>The documentation was also updated for v0.4.3, so it would be better to use the scripts from there: <a href=\"https://qupath.readthedocs.io/en/0.4/docs/deep/stardist.html\" class=\"inline-onebox\">StarDist \u2014 QuPath 0.4.3 documentation</a></p>", "<p>Hi Pete,</p>\n<p>Thanks for the quick reply. Perfect, have managed to get it to run!</p>"], "78855": ["<p>I\u2019ve recently started to work with quPath after Definiens software is not supported anymore but even with all the YT tutorials i\u2019ve been struggling with simple IHC cell detection - so what I want to do is a simple IHC immune cell density analysis (pos cells/mm2) in tumor ROI followed by density maps.</p>\n<p>What I do is \u2026</p>\n<ul>\n<li>Image upload (works as a batch of 5-10 images)</li>\n<li>Tissue detection (done by Classify &gt; Train images &gt; Create regional annotation (run for project) with \u201cbelow treshold\u201d annotation *Ignore</li>\n<li>Manual correction for hair/dirt/bubbles/wrong positive areas with annotation for *Ignore (first unlock, then draw all annotations, merge them, lock)</li>\n<li>Create region annotation in all slides and create training images</li>\n<li>Train pixel classifier</li>\n</ul>\n<p> \u2192 Unfortunately all annotations of manually excluded bubbles and so on are gone after the pixel classifier - does anyone know how to deal with this problem?</p>\n<p>Thanks and greetings,<br>\nMarkus</p>", "<p>I\u2019d generally recommend using a completely separate project for training your pixel classifiers. If not, duplicate any images you want to use for training and rename them, but you still run the risk of accidentally running a script for \u201call images\u201d and overwriting your annotations that way. Better to keep the training completely separate so that you can improve it when you see a problem.</p>"], "78856": ["<p>Hi All,</p>\n<p>I\u2019m trying to use the ilastik pixel classifier as part of a larger FIJI macro workflow, but when running the macro using either Fiji\u2019s --headless option, or with batchMode(true) then the pixel classifier runs, but doesn\u2019t display a window so the pipeline won\u2019t continue because it\u2019s looking for a multiple channel image (the output from the classification). This is perhaps not surprising - although I\u2019d like to find a workaround if I could - but what has surprised me is that if I run the macrom from the command line but omit the --headless option, the same thing happens and the error message pops up. Once I click \u201cok\u201d the output to the classification does pop up.</p>\n<p>A code snippet to reproduce this issue:</p>\n<pre><code class=\"lang-auto\">setBatchMode(false);\nrun(\"Gel\");\nfname = getTitle();\nrun(\"Run Pixel Classification Prediction\", \"projectfilename=MyProject.ilp inputimage=\"+fname+\" pixelclassificationtype=Probabilities\");\ngetDimensions(width, height, channels, slices, frames);\nprint(width,height,channels,slices,frames);\n</code></pre>\n<p>I have a MyProject.ilp that creates with three classes so the output to the log should be:<br>\n\u201c276 467 3 1 1\u201d</p>\n<p>but if I set batch mode to true it becomes \u201c276 467 1 1 1\u201d</p>\n<p>futhermore if I set batch mode to False, but run from command line with (note the lack of headless flag, I was thinking of wrapping in xvfb-run if I couldn\u2019t get headless working):</p>\n<p>./ImageJ-linux64 -macro classify.ijm</p>\n<p>Then I get \u201c276 467 1 1 1\u201d again, and the classified image shows up <em>after</em> the print command has executed and the macro completed.</p>\n<p>I hope this is clear, other options I\u2019ve tried with no success:</p>\n<ul>\n<li>calling \u201cclassify.ijm\u201d from within another macro in the hope that the classified image would turn up before the wrapper macro continued</li>\n<li>putting wait commands or running other processes after the ilastik command in case it was some kind of race condition</li>\n<li>Considered moving to Jython, but the ilastik plugin doesn\u2019t return a reference to the output ImagePlus as far as I can tell so I can\u2019t work out how to select the output for processing.</li>\n</ul>\n<p>Any help would be greatly appreciated.</p>\n<p>Thanks,<br>\n-Lachie</p>", "<p>Hello <a class=\"mention\" href=\"/u/drlachie\">@DrLachie</a>,</p>\n<p>I have to admit that I ran into this problem myself (particularly the <code>--headless</code> part). To make it short - I didn\u2019t find a solution and gave up.</p>\n<p>Maybe <a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a> has an idea what to do in this situation? Does this have something to do with the virtual dataset? I remember discussing something related to that with you.</p>\n<p>Cheers<br>\nD</p>", "<aside class=\"quote no-group\" data-username=\"DrLachie\" data-post=\"1\" data-topic=\"78856\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/drlachie/40/17454_2.png\" class=\"avatar\"> DrLachie:</div>\n<blockquote>\n<p>the same thing happens and the error message pops up</p>\n</blockquote>\n</aside>\n<p>Hi, which error message is it?</p>\n<p>And, sorry I did not fully understand yet, is there any scenario where it <em>does</em> work?</p>\n<p>General comment: I would recommend using tools like NextFlow for creating image analysis workflows including ilastik.</p>", "<p>At least for me the problem was, that in batch mode, the image would not open at the right point in time (in the macro) but only at the end - and this of course screws with the flow. Headless was the same. So the error is subtle - you assume that a new image popped up - probably rename it, but you are actually doing it to the last image that was open, not to the output from the plugin.</p>", "<p>Yes, this is the issue. The next command in the actual workflow was to split the channels which doesn\u2019t work as the input image is single channel.</p>\n<p>I think the strangest thing is that if I run it NOT in batch mode and NOT in headless mode, but do call the macro from the command line, the same behaviour happens.</p>", "<p>Ok, maybe this helps?</p><aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"6234\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/etadobson/40/17_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/fiji-macro-does-not-wait-a-command-finished-before-running-the-next-command/6234/2\">Fiji macro does not wait a command finished before running the next command</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    <a class=\"mention\" href=\"/u/tryphon\">@Tryphon</a> \nI have never had to run such a thing in a macro\u2026 however, there are a ton of people on the Forum who have - of course! \nJust to get you started in testing a few things\u2026 there should be a few <a href=\"https://imagej.net/ij/developer/macro/functions.html\">Built-In Macro Functions</a> (a great place to search for applicable functions) that could help, such as: \nwait(n) \nDelays (sleeps) for n milliseconds. \nwaitForUser(string) \nHalts the macro and displays string in a dialog box. The macro proceeds when the user clicks \u201cOK\u201d. Unlike showMessage, the dialog\u2026\n  </blockquote>\n</aside>\n", "<p>I\u2019ve actually had that problem in the past and taken similar approaches (waits, while loops etc).</p>\n<p>I tried that here with a wait of over 30 seconds (when the ilastik classification takes &lt; 5 when not in batch mode or run from command line) to no avail. I didn\u2019t bother with a while loop as I figured it would just get into an infitite loop but I\u2019ll give it a shot today just to see what happens.</p>\n<p>I realsied I neglected to answer your other question, the only time it works (and works consistently I might add) is when I run the macro from an already open instance of FIJI and with batch mode set to false.</p>"], "78857": ["<p>Dear all, how can I make ROI and train pixel classifier in the certain ROI\uff1fThanks so much\uff01<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31.jpeg\" data-download-href=\"/uploads/short-url/Y2jVEQ4EkFYwWKVER3fqT3TcCR.jpeg?dl=1\" title=\"Snipaste_2023-03-21_11-07-46\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31_2_588x500.jpeg\" alt=\"Snipaste_2023-03-21_11-07-46\" data-base62-sha1=\"Y2jVEQ4EkFYwWKVER3fqT3TcCR\" width=\"588\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31_2_588x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06c96183e448663769d2cd4f1de1b3e9b39c6e31.jpeg 2x\" data-dominant-color=\"B973B7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Snipaste_2023-03-21_11-07-46</span><span class=\"informations\">812\u00d7690 150 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>You don\u2019t need the ROI, the pixel classifier trains on the pixels within the labeled annotations. You can apply the results in any annotation you select, it doesn\u2019t matter where you put the annotation. Putting a * at the end of an annotation\u2019s class name, like Ignore* or Region* will prevent it from being used during training.</p>\n<p>It\u2019s best to train classifiers across several images and keep the training data in a separate project so that it isn\u2019t overwritten by scripts run across the project.</p>", "<p>I\u2019m not sure what you mean by an annotation?  By annotation do you mean the name of each class? Also, how do you train in one project and then use the training in a second project? Thanks.</p>", "<p>I would recommend looking through some of the object types used in QuPath here <a href=\"https://qupath.readthedocs.io/en/0.4/docs/concepts/objects.html\" class=\"inline-onebox\">Objects \u2014 QuPath 0.4.3 documentation</a></p>\n<p>You can copy the <code>classifiers</code> folder from the project, or individual classifiers (.json files) from one project\u2019s classifier folder to another. Then, if you need to update the classifier, you go back to the original project, add more annotations, rebuild the classifier, and then move it over (and overwrite) the classifier in the active project.</p>"], "78858": ["<p>Is it possible to add a recent commands module in the menu bar so that the users can call some functions used recently.</p>", "<p>Depending on the command, it might show up in the workflow tab, along with the settings used.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458.png\" data-download-href=\"/uploads/short-url/hkoQ0dy0lFuOli7CqRUlf0iI4lO.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_438x375.png\" alt=\"image\" data-base62-sha1=\"hkoQ0dy0lFuOli7CqRUlf0iI4lO\" width=\"438\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_438x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_657x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79735111c794975d3e8a94c66be1b2d7af48c458_2_876x750.png 2x\" data-dominant-color=\"3B3E41\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1044\u00d7893 59.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>There is also <strong>CTRL+P</strong> for recent commands, as seen in the View menu.</p>\n<p>Other commands you might try using the command search bar for, using <strong>CTRL+L</strong></p>\n<p>Otherwise, you could always create a feature request on GitHub, though Pete should see it here too.</p>", "<p><a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> has already listed all the options.</p>\n<p>QuPath doesn\u2019t provide a menu because it already has</p>\n<ul>\n<li><em>View \u2192 Show command list</em></li>\n<li><em>View \u2192 Show recent commands</em></li>\n</ul>\n<p>These provide searchable windows of commands, which can remain open on screen \u2013 so I personally think they are more convenient than a menu (i.e. you don\u2019t need to move the mouse or click so often). They also provide access to help tooltip text and show any corresponding shortcuts.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/b/6be51b878d800aaad798939e7214c0140a982416.png\" data-download-href=\"/uploads/short-url/fotWTH2UVSuBRXkGVJuV5fJbcTY.png?dl=1\" title=\"Screenshot 2023-03-21 at 08.29.08\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6be51b878d800aaad798939e7214c0140a982416_2_689x491.png\" alt=\"Screenshot 2023-03-21 at 08.29.08\" data-base62-sha1=\"fotWTH2UVSuBRXkGVJuV5fJbcTY\" width=\"689\" height=\"491\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6be51b878d800aaad798939e7214c0140a982416_2_689x491.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6be51b878d800aaad798939e7214c0140a982416_2_1033x736.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/b/6be51b878d800aaad798939e7214c0140a982416.png 2x\" data-dominant-color=\"F6F7F7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-21 at 08.29.08</span><span class=\"informations\">1196\u00d7852 38.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78862": ["<p>Hello Everyone.</p>\n<p>Thank you for your good advice for my last article.<br>\nFinally, I fixed that problem and succeeded in separating individual images from the \u201c*_seg.npy\u201d file.<br>\nNow I am implementing the cell segmentation by using the following command.</p>\n<blockquote>\n<p>python -m cellpose --image_path \u201c/app/mainApi/app/static/6414f306d4e67a92627f2b78/image3/3-Channel Cells_Z010_458.75 nm.ome.tiff\u201d --pretrained_model <strong>tn3</strong> --chan 0 --chan2 0 --diameter 50 --stitch_threshold 0.0 --flow_threshold 0.4 --cellprob_threshold 0.7 --fast_mode  --save_png --save_outlines</p>\n</blockquote>\n<p>Whenever I use this command, the following error occurs.</p>\n<blockquote>\n<p><code>pretrained model has incorrect path</code></p>\n</blockquote>\n<p>Because of this error, the cellpose uses the \u201ccyto\u201d model automatically.</p>\n<p>I think that the cellpose 2.2 provides some standard pretrained models(like cyto or tissuenet) which can be used without specifying the path.<br>\nAccording to the cellpose 2.2 document, we can use some pretrained models and I think that \u201cTN3\u201d is one of them.<br>\nWhy do you think this error occurred?<br>\nHow can I fix this error?<br>\nWhich models can I use without specifying the path?</p>\n<p>Any help would be appreciated.</p>", "<blockquote>\n<p>\u2013pretrained_model <strong>tn3</strong></p>\n</blockquote>\n<p>Can you try using capital T and capital N?<br>\nHere\u2019s the relevant cellpose code I think:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20\" target=\"_blank\" rel=\"noopener nofollow ugc\">MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"19\" style=\"counter-reset: li-counter 18 ;\">\n          <li>MODEL_NAMES = ['cyto','nuclei','tissuenet','livecell', 'cyto2', 'general',</li>\n          <li>                'CP', 'CPx', 'TN1', 'TN2', 'TN3', 'LC1', 'LC2', 'LC3', 'LC4']</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "78863": ["<p>I would like to create the 3D display in a jupyter notebook using the k3d backend but when I run my example nothing shows up.</p>\n<p>I installed <code>vedo</code>, <code>notebook</code>, and <code>k3d</code> but then none of the examples create an interactive window. I suspect that I am missing a requirement, but it isn\u2019t clear.</p>\n<p>Thank you</p>", "<p>I found the documentation. I had to enable the jupyter notebook extension!</p>\n<p>First I installed: <code>vedo</code>, <code>notebook</code>, and <code>k3d</code> into a virtual environment.</p>\n<p>Then I ran:</p>\n<pre><code>./vedo-env/bin/jupyter nbextension install --py --sys-prefix k3d\n./vedo-env/bin/jupyter nbextension enable --py --sys-prefix k3d\n</code></pre>\n<p>I\u2019m using a virtual environment so I use <code>--sys-prefix</code></p>\n<p>I found the instructions here, <a href=\"https://k3d-jupyter.org/user/install.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Installation \u2014 K3D-jupyter documentation</a> which seems pretty obvious. They didn\u2019t show up on the github though.</p>\n<p>Here is an example.</p>\n<pre><code class=\"lang-auto\">from vedo import dataurl, Mesh, Plotter, Volume, settings\n\nsettings.default_backend = 'k3d'\n\nmsh = Mesh(dataurl+\"beethoven.ply\").c('gold').subdivide()\nplt = Plotter(bg='black')\nplt.show(msh)\n\n</code></pre>\n<p>Here is the output, you can see there are some quirks.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8.jpeg\" data-download-href=\"/uploads/short-url/q2TvTy9mBAS5miCiu3qwVzzXUo0.jpeg?dl=1\" title=\"resulting 3D image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_690x438.jpeg\" alt=\"resulting 3D image\" data-base62-sha1=\"q2TvTy9mBAS5miCiu3qwVzzXUo0\" width=\"690\" height=\"438\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_690x438.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_1035x657.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b68c4f9a975c9bbf04db469bf0b822f856c529e8_2_1380x876.jpeg 2x\" data-dominant-color=\"C4C4E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">resulting 3D image</span><span class=\"informations\">1703\u00d71082 103 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>what are the quirks in the image?</p>", "<p>The color is supposed to be gold, and the background is supposed to be black.</p>", "<p>Hopefully this is fixed in the dev version:</p>\n<pre><code class=\"lang-auto\">pip install -U git+https://github.com/marcomusy/vedo.git\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e.png\" data-download-href=\"/uploads/short-url/wWbgPZFHfOjDEcm4e4owVucryPA.png?dl=1\" title=\"Screenshot from 2023-03-22 15-14-50\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e_2_690x497.png\" alt=\"Screenshot from 2023-03-22 15-14-50\" data-base62-sha1=\"wWbgPZFHfOjDEcm4e4owVucryPA\" width=\"690\" height=\"497\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e_2_690x497.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e_2_1035x745.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6d93fb0bf616f2b55d1bd7703719d4c149d066e.png 2x\" data-dominant-color=\"84858A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-22 15-14-50</span><span class=\"informations\">1121\u00d7809 170 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>thanks for reporting the issue.</p>", "<p>Works great now!</p>\n<p>The first time I ran the notebook, nothing showed up, but when I re-run it everything seems great.</p>\n<p>Here is another question.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732.jpeg\" data-download-href=\"/uploads/short-url/lFXV0OVHX3S7Vdx2rzXxI7Ffp8C.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_690x433.jpeg\" alt=\"image\" data-base62-sha1=\"lFXV0OVHX3S7Vdx2rzXxI7Ffp8C\" width=\"690\" height=\"433\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_690x433.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_1035x649.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/7/97ec172a02082c863c161217c4dc9ff26ed03732_2_1380x866.jpeg 2x\" data-dominant-color=\"41583F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71205 184 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I\u2019m doing pretty much the same thing, except I am using a PLY file. In the vtk display the meshes are just solid colors.</p>\n<p>Do you know what is being shown here? It looks pretty cool.</p>", "<p>nice <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nthat looks like a bunch of cells, you can check if there is any vertex-associated data or celldata in your mesh with:<br>\n<code>msh</code><br>\nand / or<br>\n<code>msh.print()</code></p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3.jpeg\" data-download-href=\"/uploads/short-url/tVxTU9r4iRbSUqdctsiBOu1oY15.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3_2_602x500.jpeg\" alt=\"image\" data-base62-sha1=\"tVxTU9r4iRbSUqdctsiBOu1oY15\" width=\"602\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3_2_602x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3_2_903x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1c059e3349c37d378d95456e14e2dbf9c54e0c3.jpeg 2x\" data-dominant-color=\"E9EBEA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1192\u00d7989 155 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I don\u2019t see anything associated with the vertex. In the ply file it just has x,y,z,r,g,b The color is the same for each vertex.</p>", "<p>yes, the data array associated to the vertices is called \u201cRGB\u201d, but these are \u201cdirect\u201d colors (not associated to a scalar mapping), probably the result of a segmentation.<br>\nI\u2019ll try to implement that in the <code>k3d</code> backend so the two renderings will match.</p>", "<p>If you want I can start a github issue and give you a sample .ply file. I made the segmentations. I\u2019m mainly curious what the color patterns are from. Everything works ok if I use the mesh.c() command with the associated meshes.</p>", "<blockquote>\n<p>If you want I can start a github issue and give you a sample .ply file.</p>\n</blockquote>\n<p>yes sure!</p>"], "78864": ["<p>HI<br>\nI am failing to  install deeplabcut on ubuntu 20.04 in a computer with gpu.<br>\nCan you tell me where is a good tutorial for this case?</p>\n<p>There are so much information on the internet , it is not clear which one is the right one.</p>\n<p>thanks</p>", "<p>Please use the offical DLC Cookbook: <a href=\"https://deeplabcut.github.io/DeepLabCut/docs/recipes/installTips.html#installation-on-ubuntu-20-04-lts\" class=\"inline-onebox\">Installation Tips \u2014 DeepLabCut</a></p>", "<p>Thanks so much !! I will try.</p>", "<p>Hi Konrad</p>\n<p>Thanks for your reply.</p>\n<p>I didn\u2019t success to run deeplab cut with Ubuntu.</p>\n<p>I will try with a new computer in windows 10 with gpu nvidia quadro RTX 4000.</p>\n<p>Before I will begin which version of :</p>\n<p><strong>Python, tensorflow , cuda and cudnn should be installed.</strong></p>\n<p>Thanks so much</p>\n<p>Silvia</p>"], "78865": ["<p>I am now segmenting nuclei in 2D and 3D. I am using watershed, but it sometimes results in over segmentation.<br>\nIn the process, I would like to merge the blob with its neighboring label depending on the size of the blob itself and the way it touches the neighboring segment.<br>\nI know it happens often, but I can\u2019t find an easy way to implement such a thing.<br>\nCould someone please give me some advice?</p>", "<p>Hi <a class=\"mention\" href=\"/u/hiroalchem\">@hiroalchem</a> ,</p>\n<p><a href=\"https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification\">napari-apoc</a> is a <a href=\"https://napari.org\">napari</a>-plugin that allows to merge labels by training a random forest clasfifier.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19.jpeg\" data-download-href=\"/uploads/short-url/zpmNPdafIHzNirAIjv1Ww7iz47L.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_690x367.jpeg\" alt=\"image\" data-base62-sha1=\"zpmNPdafIHzNirAIjv1Ww7iz47L\" width=\"690\" height=\"367\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_690x367.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_1035x550.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f82a308ee9043a72518fdd8da176c26da2eb1c19_2_1380x734.jpeg 2x\" data-dominant-color=\"6B6A69\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1909\u00d71016 133 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The classifier can be trained on parameters such as:</p>\n<ul>\n<li>\n<code>touch_portion</code>: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of <code>1/6</code> to each other.</li>\n<li>\n<code>mean_touch_intensity</code>: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way.</li>\n<li>\n<code>standard_deviation_intensity_difference</code>: The absolute difference between the standard deviation of the two objects. This measurement allows to differentiate [in]homogeneous objects and [not] merge them.</li>\n<li>\n<code>area_difference</code>: The difference in area/volume/pixel-count allows differentiating small and large objects and [not] merging them.</li>\n</ul>\n<p>Note: pixel-count-based features are recommended to be used in isotropic images only.</p>\n<p>                    <a href=\"https://raw.githubusercontent.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/main/images/merge_objects.gif\" target=\"_blank\" rel=\"noopener\" class=\"onebox\">\n            <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/4/d486f5c3b3b2f4923400d133636aaed44490ae07.gif\" width=\"690\" height=\"348\">\n          </a>\n\n</p>\n<p>After training the <code>LabelMerger</code> interactively, you can call it from a Python code as demonstrated here:<br>\n<a href=\"https://github.com/haesleinhuepf/apoc/blob/main/demo/merge_objects.ipynb\">https://github.com/haesleinhuepf/apoc/blob/fcec43837445514eebe6897376465867bf450045/demo/merge_objects.ipynb</a></p>\n<p>Let me know if this works for you! The LabelMerger is still quite new and I\u2019m eager for feedback in order to improve it.</p>\n<p>Best,<br>\nRobert</p>", "<p>An interesting problem\u2026<br>\nApart from the size, what do you mean <em>\"the way it touches the neighboring segment</em>\"? Are you thinking along the lines of perimeter length or the geometry of the contact?</p>", "<p>Thank you! I will check it.</p>", "<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a> What I meant was the length of the touching borders.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/c/6cc8cd19c1f8af5b55f7a71d39f663b3e20cb221.png\" data-download-href=\"/uploads/short-url/fwlMrtgjR07fHHrgEu8lc37lwIh.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/c/6cc8cd19c1f8af5b55f7a71d39f663b3e20cb221.png\" alt=\"image\" data-base62-sha1=\"fwlMrtgjR07fHHrgEu8lc37lwIh\" width=\"531\" height=\"500\" data-dominant-color=\"D7C9C8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">566\u00d7532 3.75 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p><a class=\"mention\" href=\"/u/hiroalchem\">@hiroalchem</a> the plugins above might solve your problem \u201cout of the box\u201d, but if you want you can use scikit-image for this. The general data structure that you need is called a <a href=\"https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_rag.html#sphx-glr-auto-examples-segmentation-plot-rag-py\">region adjacency graph</a>! You can <a href=\"https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_boundary_merge.html#sphx-glr-auto-examples-segmentation-plot-boundary-merge-py\">merge hierarchically</a> or you can <a href=\"https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_rag_mean_color.html#sphx-glr-auto-examples-segmentation-plot-rag-mean-color-py\">threshold the graph</a>. The thresholding or merging is done by the \u201cweight\u201d attributes on the edges, but you change it to be the count easily:</p>\n<pre><code class=\"lang-python\">rag = graph.rag_mean_boundary(labels_image, edge_image)  # edge_image can be dummy e.g. np.ones_like\n\nfor u, v, data in rag.edges(data=True):\n    data['weight'] = data['count']\n</code></pre>\n<p><code>rag</code> is a subclass of a <a href=\"https://networkx.org/documentation/stable/reference/index.html\">networkx graph</a>, so you can look there for all the ways you can manipulate it! <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>I think the RAG code in scikit-image hasn\u2019t been very battle tested, so if the API is a bit clunky, please raise an issue so we can improve it!</p>"], "78866": ["<p>Hi everyone,</p>\n<p>I am new to image analysis, so apologies if this is a basic question! I\u2019ve been trying to figure it out for a few days now.</p>\n<p>I have .vsi images that I can open and visualize very nicely in QuPath with my four fluorescent channels (DAPI plus 3 specific stains). My end goal is to analyze individual channels in CellProfiler. Currently, I need to separate the fluorescent channels (without downsampling) so that I can export individual channels using the tile_exporter.</p>\n<p>I know you can split channels in ImageJ, but going from QuPath to ImageJ requires significant downsampling. I would like to avoid downsampling and instead use tile_exporter so that I can analyze in CellProfiler. Is it possible to split fluorescent channels in QuPath and export them individually?</p>\n<p>Thanks!</p>\n<p>Best,<br>\nSarah</p>", "<aside class=\"quote no-group\" data-username=\"svkremer\" data-post=\"1\" data-topic=\"78866\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/5fc32e/40.png\" class=\"avatar\"> Sarah:</div>\n<blockquote>\n<p>My end goal is to analyze individual channels in CellProfiler.</p>\n</blockquote>\n</aside>\n<p>Not sure about the channel handling, that might have changed recently, <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> or <a class=\"mention\" href=\"/u/finglis\">@finglis</a> might know more, but you shouldn\u2019t need to split the channels for CellProfiler, it should handle multichannel files. <a href=\"https://carpenter-singh-lab.broadinstitute.org/blog/input-modules-tutorial\" class=\"inline-onebox\">Input Modules Tutorial | Carpenter-Singh Lab</a><br>\nThe existence of the other channels doesn\u2019t prevent you from ignoring them.</p>"], "78869": ["<p>Thank you for taking time to read this question.</p>\n<p>My question is. Is it possible to share/collaborate between two OMERO instances? I am using one host by my university, but our co-work is from another university, they have OMERO instance too.</p>\n<p>I know the hard drive is one option for data transfer, but as both universities have OMERO instances, I felt it might be worthwhile to ask how to share/send/view data from one instance to another.</p>\n<p>Does anyone have any experience or ideas?</p>\n<p>Many thanks</p>", "<p>It sounds like <a href=\"https://github.com/ome/omero-cli-transfer\" rel=\"noopener nofollow ugc\">omero-cli-transfer</a> was made for you <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> You\u2019ll still need to transfer a file, but you should be able to recover all annotations, ROIs and so on on the destination server.</p>", "<p>Hi Erick, thank you very much for your reply. Recover all annotations, ROIs would be very helpful for us. I will have a look into it!</p>"], "78873": ["<p>Hi everyone,<br>\nI\u2019m writing a plugin that contains some \u2018print\u2019 statements mostly for debugging. Is there a way of choosing where these are printed? When I run Napari through the console they are printed in the console directly which is fine. However, when I run it from a jupyter notebook they are printed in the last executed cell which makes the analysis a bit messy once I run a couple of cells. So, is it possible to print directly in Napari\u2019s console or you recommend another way of doing this?</p>\n<p>Thanks!</p>", "<p>You could use <code>show_info</code> from <code>napari.utils.notifications</code> for showing info in popup in napari viewer.</p>\n<p>Another option is to use the logging module <a href=\"https://docs.python.org/3/library/logging.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">logging \u2014 Logging facility for Python \u2014 Python 3.11.2 documentation</a> which allows control if printed to the console.</p>"], "78878": ["<p>Is it possible to only detect spots in the cytoplasm with the function of subcellular detection in QuPath</p>", "<p>Not that I know of, but you could remove the ones in the nucleus after classifying them using a script. You would then need to recaclulate the estimated spot count through.</p>", "<aside class=\"quote\" data-post=\"5\" data-topic=\"75129\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-detecting-positive-cells-based-on-subcellular-area/75129/5\">QuPath detecting positive cells based on subcellular area</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Have you tried the existing script? It might need updating for 0.4.x <a href=\"https://forum.image.sc/t/need-help-quantifying-nuclear-and-cytoplasmic-rnascope-foci/51077/2\" class=\"inline-onebox\">Need help quantifying nuclear and cytoplasmic RNAscope foci - #2 by Research_Associate</a>\n  </blockquote>\n</aside>\n\n<p>Followed by something like <a href=\"https://forum.image.sc/t/clearing-selected-objects-without-clearing-my-previously-annotated-cell-populations/78497\" class=\"inline-onebox\">Clearing 'selected objects' without clearing my previously annotated cell populations?</a></p>"], "78879": ["<p>Hey,<br>\nI\u2019m trying to evaluate my deeplabcut model using a custom metric. I\u2019m trying to use the predictions in the snapshot h5 file in \u201cevaluation-results\u201d but from where i understand the data is shuffled. I tried to use the pickle file in \u201ctraining-datasets\u201d but with no success (I dont understand the variables and only contains information about the training data). Can someone tell me how to unshuffle the predictions or point to me where i can find the information necessary to do it ?<br>\nThank you so much in advance !</p>", "<p>There is a pickle in evaluation results with <code>_full</code> suffix that contains all the information from evaluation with prediction, groundtruth keyed by the tuple of video name and frame index and metada.</p>", "<p>Thank you Konard for the quick response ! I can\u2019t seem to find the file you\u2019re describing. Maybe there\u2019s parameters i can change to make deeplabcut generate the file you\u2019re talking about ?<br>\nThank you in advance.</p>", "<p>When you run evaluation in the evaluation results you get the folder for the iteration and inside that folder a <code>csv</code> file with combined results and a folder for a specific trainset/shuffle. Inside that folder there should be 5 files, 2 <code>csv</code> and 3 <code>pickle</code>. Default run of <code>deeplabcut.evaluate_network(config)</code> should create those</p>", "<p>Hey Konrad, All i get inside the file of the specific shuffle is h5 files that i talked about for every snapshot and a csv file for results on that specific shuffle. There\u2019s no pickle files. I used the gui at first maybe that has something to do with it.</p>", "<p>In the <code>project_folder\\evaluation-results\\iteration-#\\etc.</code>?</p>", "<p>yes Konrad.</p>", "<p>I just checked an older dlc project i had. There\u2019s no pickle files in that folder also.</p>", "<p>What if you run evaluation again? Which version of DLC are you on?</p>", "<p>I\u2019m using DLC 2.3.2. I tried to do run the evaluation again with no success.</p>", "<p>Can you show a screenshot of what is created inside the evaluation-results folder? Seems odd</p>", "<p>Here\u2019s what i get with plotting set to True.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/a/fa176acbc999beb4ec61fb17b4552684e7ad10ab.png\" data-download-href=\"/uploads/short-url/zGpxoECohP0bXNYUucjGjFY5TaH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/a/fa176acbc999beb4ec61fb17b4552684e7ad10ab.png\" alt=\"image\" data-base62-sha1=\"zGpxoECohP0bXNYUucjGjFY5TaH\" width=\"690\" height=\"245\" data-dominant-color=\"F4F4F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">981\u00d7349 20.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Oh, I get it now. The pickles are created for multianimal projects. In your case with single animal the <code>h5</code> file should contain the results for the model predictions indexed with video name and frame number (extracted frame file name from labeled-data). Like so:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048fa6e88c9f1d433c9e8365ff886349b2ede1c7.png\" data-download-href=\"/uploads/short-url/ElGkQpKsv23SlrPHTXZbaia1wP.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048fa6e88c9f1d433c9e8365ff886349b2ede1c7.png\" alt=\"image\" data-base62-sha1=\"ElGkQpKsv23SlrPHTXZbaia1wP\" width=\"690\" height=\"227\" data-dominant-color=\"353941\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1431\u00d7471 29.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>And to get the ground truths you can simply concat all the <code>CollectedData</code> files in your labeled-data and sort both with the same index</p>", "<p>Thank you so much for the help ! Can i ask how do you visualize the h5 file like that ?</p>", "<p>The problem I have that the h5 file contain the video names and the images names (all the videos have images named from 0001 to 0500) while in the dataset there\u2019s more than 8000 images. then i find the predictions with no indication to which image they belong.</p>", "<p>I\u2019m just reading it with <code>pandas</code> - <code>pd.read_hdf(path_to_file)</code></p>", "<p>I think I get it now ! Thank you so much for helping me <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I think you can use: <a href=\"https://github.com/DeepLabCut/DeepLabCut/blob/072c040e6119dd592d0b4fdb289c633e75a09325/deeplabcut/pose_estimation_tensorflow/core/evaluate.py#L217\" class=\"inline-onebox\">DeepLabCut/evaluate.py at 072c040e6119dd592d0b4fdb289c633e75a09325 \u00b7 DeepLabCut/DeepLabCut \u00b7 GitHub</a> with <code>returnjustfns=False</code> to get the data you need.</p>\n<p><a class=\"mention\" href=\"/u/tahayassine\">@tahayassine</a> Using this function and saving the results to a pickle file. This will give you the pickle that is saved for maDLC models with both predicitons and annotations matched to an image</p>", "<p>That\u2019s so helpful thank you <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> One more thing if you can help me ! I was trying to recreate the training and test errors in .csv file  but the prediction seems to be rescaled. I tried to rescale the groundtruth using the \u2018gobal_scale\u2019 attribute in pose_conf.yaml but it seems not to be enough as i get much higher error. How do i get the predictions and the groundtruth to the same scale ?</p>"], "78881": ["<p>Hey,</p>\n<p>we would like to use a camera synchronized with other hardware and I thought pycromanager will be a good idea to use. However, I still have some issue to understand the Acquistion function.</p>\n<ol>\n<li>We would like to use a DAQ card to send triggers to a stage, some hardware and maybe the camera (+ lasers). I have a function (<strong>fun</strong>) sending the correct, synchronized, sequence of signals (for one z-stack) and I would like to use micromanager to deal with the saving of the images. So, where do I put <strong>fun</strong>? Can I put it in the post_camera_hook_fn (after putting the camera into external triggering mode), as shown in the code below?</li>\n<li>Is the post_camera_hook_fun only called once before the Acquisition if any loaded device is in sequencing mode (such as the DemoStage) or camera in external trigger mode?</li>\n<li>Currently, the DAQ sequence only contains the signal for one z-stack, but we would like to acquire a time series of z-stacks. Can micromanager deal with the time sequencing using the order argument in the Aqcuisition \u201corder=\u2018tz\u2019\u201d vs \u201corder=\u2018tz\u2019\u201d? Is there a hook function which is called after each time point?</li>\n</ol>\n<p>Best wishes<br>\nFred</p>\n<pre><code class=\"lang-auto\">\ndef hookFn(event):\n    # function sending a sequence of triggers and AO via a DAQ card to other hardware (including cam and stage)\n    fun \n    return event\n\n\nev = multi_d_acquisition_events(z_start=0, z_end=0.9, z_step=0.1, num_time_points=5, time_interval_s=0, order='tz')\nwith Acquisition(directory=\"C:\\\\temp\", name=\"scan\", show_display=False, post_camera_hook_fn=hookFn) as acq:\n    acq.acquire(ev)\n    \n</code></pre>", "<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"1\" data-topic=\"78881\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>So, where do I put <strong>fun</strong>? Can I put it in the post_camera_hook_fn (after putting the camera into external triggering mode), as shown in the code below?</p>\n</blockquote>\n</aside>\n<p>Yes. Have you seen <a href=\"https://pycro-manager.readthedocs.io/en/latest/performance_guide.html#fast-acquisition-with-hardware-triggering\" rel=\"noopener nofollow ugc\">this page</a>?</p>\n<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"1\" data-topic=\"78881\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<ul>\n<li>Is the post_camera_hook_fun only called once before the Acquisition if any loaded device is in sequencing mode (such as the DemoStage) or camera in external trigger mode?</li>\n<li>Currently, the DAQ sequence only contains the signal for one z-stack, but we would like to acquire a time series of z-stacks. Can micromanager deal with the time sequencing using the order argument in the Aqcuisition \u201corder=\u2018tz\u2019\u201d vs \u201corder=\u2018tz\u2019\u201d? Is there a hook function which is called after each time point?</li>\n</ul>\n</blockquote>\n</aside>\n<p>It will be called once for each sequence of acquisition events. Whether events are sequenced is determined by 1) If they were all submitted at once in a single call to <code>acq.acquire(events)</code> and 2) whether the hardware (z stage etc) can support a sequence of that length. If you want everything to be sequenced in one go, submit everything in a single call to acquire. if you want to ensure that it gets broken up into multiple sequences (e.g. one for each z stack), make multiple calls to acquire.</p>\n<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"1\" data-topic=\"78881\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>Can micromanager deal with the time sequencing using the order argument in the Aqcuisition \u201corder=\u2018tz\u2019\u201d vs \u201corder=\u2018tz\u2019\u201d? Is there a hook function which is called after each time point?</p>\n</blockquote>\n</aside>\n<p>Do you mean in the <code>multi_dimensional_acq_events</code> function? I\u2019d encourage you to read a bit more about <a href=\"https://pycro-manager.readthedocs.io/en/latest/acq_events.html#customized-acquisition-events\" rel=\"noopener nofollow ugc\">what this function is doing under the hood</a> because understanding individual acquisition events should make this make more sense</p>", "<p>Thank you. With the sources you pointed out I could get the hardware trigering running in a nice simple way. I have a few follow up questions you might be able to help me with:</p>\n<ol>\n<li>Can I also use the Multi-Cam utility within Aquisition?</li>\n<li>Is there a way to report progress out of a running Aquisition in sequencing mode (at which image in the sequence it is) to use as a progress bar in the GUI? If not, I could use the DAQ device to do this, however that would not be as nice.</li>\n<li>Is there a way to interrupt/stop a running Acquistion in sequencing mode?</li>\n</ol>\n<p>Best wishes<br>\nFred</p>", "<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"3\" data-topic=\"78881\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>Can I also use the Multi-Cam utility within Aquisition?</p>\n</blockquote>\n</aside>\n<p>Yes</p>\n<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"3\" data-topic=\"78881\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>Is there a way to report progress out of a running Aquisition in sequencing mode (at which image in the sequence it is) to use as a progress bar in the GUI? If not, I could use the DAQ device to do this, however that would not be as nice.</p>\n</blockquote>\n</aside>\n<p>Only by monitoring images being pulled off the camera. The whole point of hardware triggering is that the computer is out of the loop.</p>\n<aside class=\"quote no-group\" data-username=\"f.goerlitz\" data-post=\"3\" data-topic=\"78881\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/f.goerlitz/40/68162_2.png\" class=\"avatar\"> Fred:</div>\n<blockquote>\n<p>Is there a way to interrupt/stop a running Acquistion in sequencing mode?</p>\n</blockquote>\n</aside>\n<p>Yes in theory you can abort it like any other acquisition but whether that works depends on the device adapters of the specific hardware you\u2019re using</p>"], "78882": ["<p>Hi everyone,</p>\n<p>I want to combine a large number of ROIs into groups corresponding to individual slices in a movie. The goal is to have only one grouped ROI per slice. The following macro works well when the total number of frames/slices is under 255:</p>\n<p>for (i=0; i&lt;roiManager(\u201ccount\u201d); i++) {<br>\nroiManager(\u201cSelect\u201d, i);<br>\nRoi.getPosition(channel, slice, frame)<br>\nRoiManager.setGroup(slice);<br>\n}<br>\nfor (i=1; i&lt;=nSlices; i++) {<br>\nRoiManager.selectGroup(i);<br>\nroiManager(\u201cCombine\u201d);<br>\nroiManager(\u201cAdd\u201d);<br>\nroiManager(\u201cdelete\u201d);<br>\n}<br>\nRoi.remove;<br>\nroiManager(\u201cShow All with labels\u201d);</p>\n<p>If the movie is longer than 255 frames, I get the following error:</p>\n<p>Error:\t\tGroup out of range in line 8:</p>\n<pre><code>\tRoiManager . setGroup ( slice &lt;)&gt; ; \n</code></pre>\n<p>Does anyone know how to overcome this limitation?</p>\n<p>Best</p>", "<aside class=\"quote no-group\" data-username=\"Maro\" data-post=\"1\" data-topic=\"78882\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/96bed5/40.png\" class=\"avatar\"> Maro:</div>\n<blockquote>\n<p>RoiManager.setGroup</p>\n</blockquote>\n</aside>\n<p>Quick search of the code make it look like setGroup takes an int that is hardcoded to be limited to less than 255</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868\" target=\"_blank\" rel=\"noopener\">imagej/ImageJ/blob/db7f6c4e8516ca6a678ffda1d1051ba8052d4cc3/ij/gui/Roi.java#L1868</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-java\">\n      <ol class=\"start lines\" start=\"1858\" style=\"counter-reset: li-counter 1857 ;\">\n          <li>\t\treturn groupNamesString;</li>\n          <li>\t}</li>\n          <li>\n          </li>\n<li>\t/** Sets the group names from a comma-delimeted string. */</li>\n          <li>\tpublic static void setGroupNames(String names) {</li>\n          <li>\t\tgroupNamesString = names;</li>\n          <li>\t\tgroupNames = null;</li>\n          <li>\t}</li>\n          <li>\n          </li>\n<li>\t/** Sets the group of this Roi, and updates stroke color accordingly. */</li>\n          <li class=\"selected\">\tpublic void setGroup(int group) {</li>\n          <li>\t\tif (group&lt;0 || group&gt;255)</li>\n          <li>\t\t\tthrow new IllegalArgumentException(\"Invalid group: \"+group);</li>\n          <li>\t\tif (group&gt;0)</li>\n          <li>\t\t\tsetStrokeColor(getGroupColor(group));</li>\n          <li>\t\tif (group==0 &amp;&amp; this.group&gt;0)</li>\n          <li>\t\t\tsetStrokeColor(null);\t\t\t</li>\n          <li>\t\tthis.group = group;</li>\n          <li>\t\tif (imp!=null) // Update Roi Color in the GUI</li>\n          <li>\t\t\timp.draw();</li>\n          <li>\t}</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nSo there may not be a way around this without editing the source code. <a class=\"mention\" href=\"/u/wayne\">@Wayne</a> ?</p>", "<p>Hi <a class=\"mention\" href=\"/u/maro\">@Maro</a>, <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>,</p>\n<p>There is no easy way to increase the ROI group limit of 255. Use the ROI Manager\u2019s More&gt;&gt;Save command to create a ZIP file of the ROIs, so I can reproduce the problem, and I will try to find a work around.</p>", "<p>Hi Wayne,</p>\n<p>Here is a RoiSet generated for a 300/512/200 frames/x/y time series.<br>\nThanks a lot for looking into this.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/zibvDjbAKlCruC5Vpsjn18bSSCw.zip\">Maro_RoiSet.zip</a> (7.8 MB)</p>\n<p>Maro</p>", "<p>Hi <a class=\"mention\" href=\"/u/maro\">@Maro</a>,</p>\n<p>The ImageJ 1.54d21 daily build adds a  RoiManager.selectPosition(c,z,t) macro function.</p>\n<p>Here is an updated version of your macro that uses this new function:</p>\n<pre><code class=\"lang-auto\">for (slice=1; slice&lt;=nSlices; slice++) {\n   showStatus(slice+\"/\"+nSlices);\n   RoiManager.selectPosition(0,slice,0);\n   roiManager(\"Combine\");\n   roiManager(\"Add\");\n   roiManager(\"delete\");\n}\nRoi.remove;\nroiManager(\"Show All with labels\");\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/wayne\">@Wayne</a>,</p>\n<p>This new function solves the issue.<br>\nThank you very much for your valuable support.</p>\n<p>Best regards,</p>\n<p>Maro</p>"], "78883": ["<p>Hi everyone,<br>\nI recently purchased a second hand Zeiss Axioimager 200M with a hamamatsu camera ORCA-ER, a led source Lumencore light engine AURA 5 and an Apotome 1 from Zeiss.<br>\nI need to purchase a computer to run the system and I\u2019m wondering how powerfull the computer needs to be for such equipment (quite outdated compared to most recent computers).<br>\nThis is for some prototyping applications.<br>\nThank you for your help</p>"], "42019": ["<p>Here is an ImageJ macro to measure distance between two user-drawn segmented lines (polylines) on an image. Shortest distances from points along the shorter line to the longer line are used to calculate the average distance between the two lines (their spline interpolations).</p>\n<p>The macro works in ImageJ 2.1.0/1.53c, and may be useful to someone.</p>\n<p>The macro prompts user to draw the two segmented lines (example below).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/14ea3ad3a22d11f69f917427cdd1ca496c22a766.jpeg\" data-download-href=\"/uploads/short-url/2Z1nPIRva6QjURnDvGj4WhWgBue.jpeg?dl=1\" title=\"Drawing lines\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14ea3ad3a22d11f69f917427cdd1ca496c22a766_2_315x250.jpeg\" alt=\"Drawing lines\" data-base62-sha1=\"2Z1nPIRva6QjURnDvGj4WhWgBue\" width=\"315\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14ea3ad3a22d11f69f917427cdd1ca496c22a766_2_315x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14ea3ad3a22d11f69f917427cdd1ca496c22a766_2_472x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14ea3ad3a22d11f69f917427cdd1ca496c22a766_2_630x500.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14ea3ad3a22d11f69f917427cdd1ca496c22a766_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Drawing lines</span><span class=\"informations\">818\u00d7648 78.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>The macro then makes the measurements and displays statistics on the image, and prompts the user to save the values in a user-selected file.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/6/b67b9971d5b8b30f87a7cfe2c17bd77c78a7175e.jpeg\" data-download-href=\"/uploads/short-url/q2jI1COBQGTgq6Mi05aMTXykcKG.jpeg?dl=1\" title=\"Measurements made\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b67b9971d5b8b30f87a7cfe2c17bd77c78a7175e_2_315x250.jpeg\" alt=\"Measurements made\" data-base62-sha1=\"q2jI1COBQGTgq6Mi05aMTXykcKG\" width=\"315\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b67b9971d5b8b30f87a7cfe2c17bd77c78a7175e_2_315x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b67b9971d5b8b30f87a7cfe2c17bd77c78a7175e_2_472x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b67b9971d5b8b30f87a7cfe2c17bd77c78a7175e_2_630x500.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/6/b67b9971d5b8b30f87a7cfe2c17bd77c78a7175e_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Measurements made</span><span class=\"informations\">816\u00d7654 86.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/pIOEPLOX3YtjTA8JyMOydMA3vKH.txt\">InteredgeDistance_v1.0.1_ImageJMacro.txt</a> (12.1 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/tGd6QuVkuem1iTPBUCVP5Op4CA2.pdf\">Instructions for InteredgeDistance macro.pdf</a> (789.1 KB)</p>", "<p>Awesome! Wondering if I could measure tight junction lengths within my samples? <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=9\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\"></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b90dba1d6cf92897c313d0308990fd87cc6bf80.jpeg\" data-download-href=\"/uploads/short-url/jUEIXg8aX8ajNhDWypu1ihYMbWE.jpeg?dl=1\" title=\"image\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b90dba1d6cf92897c313d0308990fd87cc6bf80_2_690x456.jpeg\" alt=\"image\" data-base62-sha1=\"jUEIXg8aX8ajNhDWypu1ihYMbWE\" width=\"690\" height=\"456\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b90dba1d6cf92897c313d0308990fd87cc6bf80_2_690x456.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b90dba1d6cf92897c313d0308990fd87cc6bf80.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b90dba1d6cf92897c313d0308990fd87cc6bf80.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b90dba1d6cf92897c313d0308990fd87cc6bf80_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">744\u00d7492 472 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>", "<p>See also this posts using the  Exact Signed Distance Euclidean plug-in or similar:</p>\n<aside class=\"quote quote-modified\" data-post=\"3\" data-topic=\"26065\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/3ab097/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/distance-between-polygons/26065/3\">Distance between polygons</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hello, \nThank you for your answer. \nHere is an image of my \u201cproblem\u201d. \nOn the left, the two membranes are shown by the red arrows. That\u2019s the space in between that I would like to measure. So far, only managed what is shown on the upper right side: drawing lines more or less regularly and measure the length. As I said, fastidious. \nHowever, I can \u201chighlight\u201d the membranes with polygons as shown on the left lower side. And that is the space separating both polygons that I would like to measure at\u2026\n  </blockquote>\n</aside>\n\n<p><a href=\"https://forum.image.sc/t/quantifying-color-distribution-in-birds/5021/2\" class=\"inline-onebox\">Quantifying color distribution in birds - #2 by oburri</a> (<a class=\"mention\" href=\"/u/oburri\">@oburri</a>: still a nice example and in my list of links!)</p>\n<aside class=\"quote\" data-post=\"10\" data-topic=\"36531\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/c/b9bd4f/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/average-distance-between-two-lines-and-between-a-line-and-particles/36531/10\">Average distance between two lines and between a line and particles</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    I\u2019m gonna try this, \nthank you!!!\n  </blockquote>\n</aside>\n", "<p>The newer version of the macro (1.1) should work when the lines are convoluted, as in your example.<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/072c5e37f899790859ccb2707c3542f765471080.jpeg\" data-download-href=\"/uploads/short-url/11soP4F10HKkRmX9WHRMOf5UNiM.jpeg?dl=1\" title=\"measuredImage\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/072c5e37f899790859ccb2707c3542f765471080_2_690x457.jpeg\" alt=\"measuredImage\" data-base62-sha1=\"11soP4F10HKkRmX9WHRMOf5UNiM\" width=\"690\" height=\"457\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/072c5e37f899790859ccb2707c3542f765471080_2_690x457.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/072c5e37f899790859ccb2707c3542f765471080_2_1035x685.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/072c5e37f899790859ccb2707c3542f765471080_2_1380x914.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/072c5e37f899790859ccb2707c3542f765471080_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">measuredImage</span><span class=\"informations\">1486\u00d7986 386 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>", "<p>Updated to version 1.1:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2343HoHY6BekV3oMKPagVt7fkns.txt\">InteredgeDistance_v1.1_ImageJMacro.txt</a> (16.3 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2G1EdvflSyOmS0S4LTlQGRd7RRq.pdf\">Instructions for InteredgeDistance macro 1.1.pdf</a> (786.9 KB)</p>", "<p><a class=\"mention\" href=\"/u/alpha2zee\">@alpha2zee</a><br>\nNice sharing.<br>\nThank you</p>", "<p>Hi,<br>\nThanks for sharing the macro. I started using it for scratch assay analysis. Is there a reference for the macro somewhere, in case we get to publish the analysis?<br>\nThanks,<br>\nCath</p>", "<p>As of now, there is no publication to cite as a reference for the macro. The macro logic is too simple to write about. Feel free to provide the macro code as supplementary material to accompany your publication.</p>\n<p>Let me know if you find any issue with the macro.</p>", "<p>Ok thanks! Let me know if you want to have your name in the acknowledgments (assuming we get to the manuscript-writing stage\u2026)</p>", "<p>I\u2019m okay with being named in acknowledgements. Also okay if you forget <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Hi Santosh. I tried your Macro to measure the distance between two semi-parallel lines. However, the result was different from the real distance (less) and I think it\u2019s because of the calculation method you are using (spline interpolation). In my case, I need to measure the thickness of a coating on a surface which ends up measuring the distance between two lines. Could you please let me know if I can modify this code to use in my problem?<br>\nI look forward to hearing from you.</p>", "<p>Hi Shiva, can you point to an example image or two? Yes, the macro I wrote requires poly/multisegment lines\u2026 their spline interpolations are used to identify a set of coordinates along a line to measure distance from/against.</p>", "<p>Hi Santosh,<br>\nthanks for your reply.<br>\nI am wondering if your <a class=\"attachment\" href=\"/uploads/short-url/4TcUweXoQr4fTUlH9Fkb00EOFag.tif\">canny.tif</a> (768.2 KB) macro can be modified to measure the perpendicular distance between two lines? I am trying to measure the thickness of a coating on a surface. I have used canny edge detection to find the edges (sending an image for your reference) and now I want to measure the average distance between the two lines.</p>\n<p>I greatly appreciate your help in this and I<a class=\"attachment\" href=\"/uploads/short-url/4TcUweXoQr4fTUlH9Fkb00EOFag.tif\">canny.tif</a> (768.2 KB)  look forward to hearing from you.<br>\nBest,</p>", "<p>Hi Shiva, I used the macro (v1.1) in Fiji/ImageJ 1.53c/2.1.0 and it seems to be working and measuring the distance accurately. No?<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/beca2340333b98e9f99b19bf6cc2883557a9b69a.jpeg\" data-download-href=\"/uploads/short-url/rdNNrelQCKbkzB4HvbyLLnLO0EG.jpeg?dl=1\" title=\"measured_canny.tif\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/beca2340333b98e9f99b19bf6cc2883557a9b69a.jpeg\" alt=\"measured_canny.tif\" data-base62-sha1=\"rdNNrelQCKbkzB4HvbyLLnLO0EG\" width=\"639\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/beca2340333b98e9f99b19bf6cc2883557a9b69a_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">measured_canny.tif</span><span class=\"informations\">648\u00d7507 50.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div></p>", "<p>Hi Santosh,<br>\nThanks for the early reply. actually, I am not sure if it is accurate. is there any way to get the measurement in micrometer instead of the pixel?</p>\n<p>I really appreciate your advice.</p>\n<p>looking forward to hearing from you.<br>\nShiva</p>", "<p>Hi Shiva, all image manipulation or measurement occurs at the level of pixel. To convert the macro\u2019s output in pixels to units of length like mm you will need image resolution (pixel per mm, etc.). The macro doesn\u2019t know and cannot identify image resolution.</p>", "<p><a class=\"mention\" href=\"/u/alpha2zee\">@alpha2zee</a><br>\nYou can retrieve the current image scale by using something like</p>\n<pre><code class=\"lang-auto\">length = 1;\ntoScaled(length);\nprint(\"\"+length);\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"17\" data-topic=\"42019\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">length = 1;\ntoScaled(length);\nprint(\"\"+length);\n</code></pre>\n</blockquote>\n</aside>\n<p>Thank you for the suggested command to get the image scale. Unfortunately, it will fail for images that don\u2019t have the correct metadata.</p>", "<p>Hi Santosh,<br>\nThanks again for your reply.<br>\nI have another question for you. As my measurement is to be very accurate, I am using an edge detector before using your provided Macro. The problem happens when I am drawing segmented lines on the detected edges. It is very time-consuming to select the accurate points one-by-one to draw the line and I have lots of images to analyze this way. Is there any way to modify this Macro somehow to draw the lines itself based on the edges I have already detected? I am sending the edge-detected image as a reference. <a class=\"attachment\" href=\"/uploads/short-url/eCIIqaNj7Bqdkqu8CLwukdAg7Uy.tif\">papre 20x.tif</a> (4.8 MB)</p>\n<p>Thanks for your help.<br>\nShiva</p>", "<p>The macro cannot auto-detect edges/lines \u2026 such ability is in a completely different domain (and I have no expertise for it).</p>\n<p>Looking at the example image, it seems that the \u201cprecision\u201d of the best measurement that can theoretically be achieved will still have at least an 1%-2% error, similar to what may be obtained with the macro by drawing polylines with may be 5-6 segments.</p>"], "72737": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0f5c12e06b02dd687c3d4485ef9f3508445007c.jpeg\" data-download-href=\"/uploads/short-url/w65tLTmKOXwHldP72HwIuv9s5pa.jpeg?dl=1\" title=\"104414921\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0f5c12e06b02dd687c3d4485ef9f3508445007c_2_333x250.jpeg\" alt=\"104414921\" data-base62-sha1=\"w65tLTmKOXwHldP72HwIuv9s5pa\" width=\"333\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0f5c12e06b02dd687c3d4485ef9f3508445007c_2_333x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0f5c12e06b02dd687c3d4485ef9f3508445007c_2_499x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0f5c12e06b02dd687c3d4485ef9f3508445007c_2_666x500.jpeg 2x\" data-dominant-color=\"9DB78D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">104414921</span><span class=\"informations\">945\u00d7708 151 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\noriginal image<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/d/9dde261a0c912695b7fd4255166bfe0e7a1fbcbf.jpeg\" data-download-href=\"/uploads/short-url/mwyUHvwYneJsqIFTsk6fIPDvTST.jpeg?dl=1\" title=\"segmentation preview\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dde261a0c912695b7fd4255166bfe0e7a1fbcbf_2_332x250.jpeg\" alt=\"segmentation preview\" data-base62-sha1=\"mwyUHvwYneJsqIFTsk6fIPDvTST\" width=\"332\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dde261a0c912695b7fd4255166bfe0e7a1fbcbf_2_332x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dde261a0c912695b7fd4255166bfe0e7a1fbcbf_2_498x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/d/9dde261a0c912695b7fd4255166bfe0e7a1fbcbf_2_664x500.jpeg 2x\" data-dominant-color=\"C59061\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">segmentation preview</span><span class=\"informations\">946\u00d7712 30.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nscreenshot of the segmentation preview with the three labels used shown (red=background, yellow=petal, green=anther)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/0/3019767e7d82faa43dfd8f3dc67caf0163df9065.jpeg\" data-download-href=\"/uploads/short-url/6RvtinDT0RnS8bNH4xxaAMG2K0t.jpeg?dl=1\" title=\"104414921_Simple Segmentation1,2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/3019767e7d82faa43dfd8f3dc67caf0163df9065_2_333x250.jpeg\" alt=\"104414921_Simple Segmentation1,2\" data-base62-sha1=\"6RvtinDT0RnS8bNH4xxaAMG2K0t\" width=\"333\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/3019767e7d82faa43dfd8f3dc67caf0163df9065_2_333x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/3019767e7d82faa43dfd8f3dc67caf0163df9065_2_499x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/0/3019767e7d82faa43dfd8f3dc67caf0163df9065_2_666x500.jpeg 2x\" data-dominant-color=\"999999\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">104414921_Simple Segmentation1,2</span><span class=\"informations\">945\u00d7708 21.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nexport of the segmentation of the \u201cpetal\u201d label after training when image export settings are set to renormalize [min.max]: 1,2 to 0,255</p>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>Training a dataset to segment two parts of flowers (anthers [male reproductive parts] and petals [colorful bits]) out from images that have varying backgrounds. In the above image the petals are the white parts with magenta stripes and the anthers are the purple parts surrounding the pink starfish looking part (pistil). We want to extract the petals in one segmentation and the anthers in another so we can analyze those parts of the image in a color analysis comparing thousands of images within and between species boundaries. As you can see we are able to extract the petal portion but not the anther portion.</p>\n<p>What renormalize setting would create a segmentation with the anther label set as black and the background and petal labels set as white?</p>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<p>We have tried the following export setting with these results:<br>\n0,1 creates an entirely white image (no picture uploaded)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/5/25025756ae110488b00bac2e3f3a0423cc1c63a7.jpeg\" data-download-href=\"/uploads/short-url/5hoGiSVADhCFy0Zm8KYMYiBYzRl.jpeg?dl=1\" title=\"104414921_Simple Segmentation0,2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/5/25025756ae110488b00bac2e3f3a0423cc1c63a7_2_333x250.jpeg\" alt=\"104414921_Simple Segmentation0,2\" data-base62-sha1=\"5hoGiSVADhCFy0Zm8KYMYiBYzRl\" width=\"333\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/5/25025756ae110488b00bac2e3f3a0423cc1c63a7_2_333x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/5/25025756ae110488b00bac2e3f3a0423cc1c63a7_2_499x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/5/25025756ae110488b00bac2e3f3a0423cc1c63a7_2_666x500.jpeg 2x\" data-dominant-color=\"808080\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">104414921_Simple Segmentation0,2</span><span class=\"informations\">945\u00d7708 5.93 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n0,2 is the closest but isn\u2019t black and white which is what we need<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2b0df6e4e6188800b9674edc284d7b0c2617f3fc.jpeg\" data-download-href=\"/uploads/short-url/68SrTlJousdqhpigdur3Vc9XCe8.jpeg?dl=1\" title=\"104414921_Simple Segmentation1,3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b0df6e4e6188800b9674edc284d7b0c2617f3fc_2_333x250.jpeg\" alt=\"104414921_Simple Segmentation1,3\" data-base62-sha1=\"68SrTlJousdqhpigdur3Vc9XCe8\" width=\"333\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b0df6e4e6188800b9674edc284d7b0c2617f3fc_2_333x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b0df6e4e6188800b9674edc284d7b0c2617f3fc_2_499x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b0df6e4e6188800b9674edc284d7b0c2617f3fc_2_666x500.jpeg 2x\" data-dominant-color=\"989898\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">104414921_Simple Segmentation1,3</span><span class=\"informations\">945\u00d7708 21.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n1,3</p>", "<p>Dear <a class=\"mention\" href=\"/u/mason_mcnair\">@Mason_McNair</a>,</p>\n<p>first of all it\u2019s cool to see some flowers in ilastik <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/hibiscus.png?v=12\" title=\":hibiscus:\" class=\"emoji\" alt=\":hibiscus:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>In general we advise to export the probabilities and threshold these rather than the simple segmentation. And I\u2019ll motivate it for you, too.</p>\n<p><strong>TLDR:</strong> We would recommend exporting the probabilties image and thresholding the channels of interest separately (e.g. in Fiji).</p>\n<p>Internally, what ilastik will produce as <strong>probabilities</strong> will be a multichannel image with as many channels as you had classes in training. In your example it would be 3 channels (background, petal, anther). Each of those channels give the probability for every pixel that it belongs to the respective class. So the <em>first</em> channel would give you the probability, that a pixel at position <code>(x, y)</code> is <em>background</em>, the <em>second</em> would be the probability for <em>petal</em> and the <em>third</em> channel holds the probability for <em>anther</em>. The numbers in these channels are floating point numbers between <code>0</code> and <code>1</code> (so numbers like <code>0.1</code>, <code>0.2345</code> and so on). For every pixel position, if you take the numbers of all channels and add them up, you will get a value of <code>1</code>.<br>\nIn order to get to the <em>simple segmentation</em>, you would go through every pixel position in the <em>probabilities</em> image and check which channel has the maximum and use the corresponding value. E.g. <code>1</code> if the max is in the first (background) channel, <code>2</code> if the max is in the <em>petal</em> channel and <code>3</code> if the max is in the third (anther) channel. This introduces a subtle problem: Consider the pixel probabilities <code>(0.33, 0.33, 0.34)</code> for your three classes - the simple segmentation will say that it\u2019s <em>anther</em>, as the last number is the highest. However, this pixel still only has a probability of <code>0.34</code> to be <em>anther</em>. When thresholding only the <em>anther</em> channel, one would probably choose a higher threshold (0.5 to start with). This would also be our recommendation - to export probabilities, and do the thresholding with the respective channels of interest.</p>\n<p>I hope this makes sense! If not, please don\u2019t hesitate to ask for more details!</p>\n<p>Cheers<br>\nD</p>", "<p><a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>,  why is the recommendation to export the probabilities and then threshold? His segmentation so clearly identifies his petals and his anther.  Is there not a way from the segmentation export to directly measure the area of each where the % area of the background, petals, and anther adds up to 100.  I am trying to something similar, following the tutorials I have exported the segmentation and loaded them back in for object classification but end up having to do \u201cresegment\u201d in step 3 Object Classification.  I basically use the same labels and come up with the same segmentation.<br>\nI think I am understanding that if you do you segmentation and have a background and a foreground and that puts the backgroud in channel 1 and the foreground in channel 2 how can I call up the foreground channel only and get an area for that.  Ideally, I could get the same for the background and the totals would be 100.  I don\u2019t want to have to reclassify after I have done the pixel classification. I have attached my segmentation and original image.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/4ifpeIOfQT2akxdjehBWP6JApyN.tiff\">1_2_L_x5_Simple Segmentation.tiff</a> (46.1 KB)</p>\n<p>Thanks for your help.<br>\nKristin<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg\" data-download-href=\"/uploads/short-url/Ef8Wlhap0RPA0mhugiuF1GmMQt.jpeg?dl=1\" title=\"1_2_L_x5\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285_2_666x499.jpeg\" alt=\"1_2_L_x5\" data-base62-sha1=\"Ef8Wlhap0RPA0mhugiuF1GmMQt\" width=\"666\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285_2_666x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg 2x\" data-dominant-color=\"ADADAD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">1_2_L_x5</span><span class=\"informations\">990\u00d7743 227 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Update:I tried to threshold on the probabilities map. 1) I don\u2019t exactly know how to best use the smooth feature. 2) I played around with the threshold and could not get it exactly how I want it\u2026which was already done in the pixel classification\u2026the blue is background and the yellow is foreground. In this image you can see that large areas of the yellow are not being captured, if I adjust the threshold to get all the yellow it gets too much of the blue\u2026 Without thresholding or going through featue selection is there a way to get total area of the foreground and background from the original segmentation?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44.jpeg\" data-download-href=\"/uploads/short-url/9Q3go8KJu0W0njNd5AZ3SWO8TEo.jpeg?dl=1\" title=\"probailitiesthreshold\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44_2_690x405.jpeg\" alt=\"probailitiesthreshold\" data-base62-sha1=\"9Q3go8KJu0W0njNd5AZ3SWO8TEo\" width=\"690\" height=\"405\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44_2_690x405.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44_2_1035x607.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/44f5e162b68ef65f7cb0285dc289074c697d9c44.jpeg 2x\" data-dominant-color=\"ACCE8D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">probailitiesthreshold</span><span class=\"informations\">1364\u00d7801 156 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"SpecialK\" data-post=\"3\" data-topic=\"72737\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/ec9cab/40.png\" class=\"avatar\"> Kristin:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>, why is the recommendation to export the probabilities and then threshold?</p>\n</blockquote>\n</aside>\n<p>Hi <a class=\"mention\" href=\"/u/specialk\">@SpecialK</a>,</p>\n<p>If you only have two classes, then exporting the \u201cSimple Segmentation\u201d, rather than Probabilities is ok(ish). With more classes, the above holds true, that you will get pixels classified to one class, despite all probabilities being low.</p>\n<p>If you\u2019re looking \u201conly\u201d for the ratio of two (or more) different classes, then I\u2019d probably recommend doing that in Fiji, rather then in ilastik object classification (this would make most sense if you were to train a classifier afterwards).</p>\n<p>From the images you posted it looks like you want to quantify \u201crough\u201d vs \u201csmooth\u201d areas. Challenging here is that the image is watermarked and also has other text on it. Ideally you\u2019d be working on the raw data without that (if you can get access to it). In any case, I would propose to treat this as a 3-class problem in Pixel Classification: 1) Smooth areas 2) Rough Areas 3) Background (anything that is not considered your sample).</p>\n<p>Then with the exported probablilities, I\u2019d suggest to go to either Fiji, or something like Python (would be my choice), and 1) Load the probabilities 2) Thresholding the \u201crough\u201d image class at 0.5 to get a binary image of the rough area 3) Threshold the sum of the \u201csmooth\u201d and \u201crough\u201d channel at 0.5, to get a binary image of the total sample area. Then you could compare the two areas.</p>\n<p>Does this make sense?</p>\n<p>Cheers<br>\nDominik</p>", "<p>Dominik,</p>\n<p>Thanks for your response. I\u2019m not familiar with FIJI. What I did try was using CellProfiler. I used the original images and the probabilities map. I split out the channels on the probabilities, using the red channel to convert to gray then I thresholded on this gray image (the rough part), using the IdentifyPrimaryObjects (see below). Based on your response I\u2019m wondering if I should try manual Threshold and set to 0.5? I also ran it again with the green split to gray to look at the smooth portion\u2026.those two portions don\u2019t add up to 100% which is why I was hoping to use the segmentation. I can\u2019t get the segmentation to work in CellProfiler.</p>\n<p>Do you have any other suggestions using CellProfiler. Btw, right now I\u2019m not worried about the background or the writing, this is just testing the feasibility, in the future yes, I would use images without the writing and crop the background out if needed.</p>\n<p>Thanks,</p>\n<p>Kristin</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6.png\" data-download-href=\"/uploads/short-url/hV4mf0CmHIKWetBFEwBNRA6yv7E.png?dl=1\" title=\"image001.png\" rel=\"noopener nofollow ugc\"><img width=\"551\" height=\"430\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_551x430.png\" data-base62-sha1=\"hV4mf0CmHIKWetBFEwBNRA6yv7E\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_551x430.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_826x645.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d9879738fe944038d21d2c488fedef6a5e1f5e6_2_1102x860.png 2x\" data-dominant-color=\"ECECEC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image001.png</span><span class=\"informations\">1172\u00d7913 106 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78891": ["<p>Receiving error for line widths &gt;1 when creating kymographs. Looking to fix this issue for wider image and longer movement record.</p>\n<p>Adding a new straight line to a .tif file and updating the line width to 30. Attempted plug-ins KymographBuilder and KymoResliceWide with both showing no more than 1 width errors. Attempted on multiple computers with separate Fiji downloads and have the same issue. How do I solve this?</p>", "<p>Found this related post for KymographBuilder:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/fiji/KymographBuilder/issues/13#issuecomment-399985738\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/fiji/KymographBuilder/issues/13#issuecomment-399985738\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/fiji/KymographBuilder</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/fiji/KymographBuilder/issues/13#issuecomment-399985738\" target=\"_blank\" rel=\"noopener nofollow ugc\">Doesn't work at all with new version of Fij</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2018-05-14\" data-time=\"14:48:59\" data-timezone=\"UTC\">02:48PM - 14 May 18 UTC</span>\n      </div>\n\n        <div class=\"date\">\n          closed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2018-06-25\" data-time=\"15:36:27\" data-timezone=\"UTC\">03:36PM - 25 Jun 18 UTC</span>\n        </div>\n\n      <div class=\"user\">\n        <a href=\"https://github.com/jochenkrattenmacher\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"jochenkrattenmacher\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d20e95943d4cec95fcc4ae5bf1dd50249ab4c18f.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          jochenkrattenmacher\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">There is a drop-down menu popping up asking me to select \"ImageDisplay\", and whe<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\">n I press OK it says \"'imagedisplay' is required but unset.\"</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\n(It seems this warning shouldn\u2019t affect the function.)<br>\nI also tested it on an image and did get a different output when changing the line width.</p>", "<p>Hi Cayla, unfortunately, it outputs the same image with a width of 1 no matter the line width I select.</p>", "<p>The output image should have the same dimensions regardless of line width\u2013it will just take the maximum value along the width. Could you share an example image that illustrates what you\u2019re trying to track, and where you\u2019re selecting your line?</p>\n<p>For comparison, I opened the Mitosis sample image (File &gt; Open samples &gt; Mitosis (5D stack), and selected a line on the image (left image). When I set the width to 1, I got the kymograph at the top right (which is pretty dim since I didn\u2019t place my line well), and when I set it to 15, I got the kymograph at the bottom right, which manages to capture more signal from the red dots which my thinner line didn\u2019t. They looked similar straight out of the plugin because they auto-contrasted, but when set to display the same values, you can see the max-projected one (width=15) is much brighter.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/694c42f7b41f48a9b470df5ade5c0fb60789ef22.png\" alt=\"image\" data-base62-sha1=\"f1vwuEQEgfjpMYHLttF8FwKh6a6\" width=\"361\" height=\"325\"><br>\nYou should have this example image too if you\u2019d like to test the plugin on this.</p>"], "78893": ["<p>Hi,</p>\n<p>Is there an implementation of the FFT Bandpass filter (or similar) that allows suppressing stripes of custom orientations (e.g. not vertical or horizontal)?</p>\n<p>Thanks a lot</p>", "<p>Hi <a class=\"mention\" href=\"/u/swa\">@Swa</a></p>\n<p>You can use the FFT and inverse FFT in Fiji but it is more manual than running a bandpass filter:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bdf395400d164ae4c033953b9b8684c755b56451.png\" alt=\"FFT-to-remove-regular-pattern\" data-base62-sha1=\"r6o7ciOzpW4U1iW0l15E2F7wU6J\" width=\"490\" height=\"176\"></p>\n<p>Original:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93.jpeg\" data-download-href=\"/uploads/short-url/quJBqrQTanEm85r5Ts6Kw4xAI5Z.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_499x499.jpeg\" alt=\"image\" data-base62-sha1=\"quJBqrQTanEm85r5Ts6Kw4xAI5Z\" width=\"499\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_499x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_748x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b1f50d7b31fc94604d82e0e92834b955cfed93_2_998x998.jpeg 2x\" data-dominant-color=\"848484\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1437\u00d71437 593 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n\u201cCleaned up\u201d image:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51.jpeg\" data-download-href=\"/uploads/short-url/6LqufhdHc0JpX3AHGw9A1QB5GQF.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_499x499.jpeg\" alt=\"image\" data-base62-sha1=\"6LqufhdHc0JpX3AHGw9A1QB5GQF\" width=\"499\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_499x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_748x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f6981ad0f715cf9eedfb8a269bf3d1b52ebcd51_2_998x998.jpeg 2x\" data-dominant-color=\"767676\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1437\u00d71437 371 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Does this make sense?</p>\n<p>Best wishes,<br>\nMarie</p>"], "78894": ["<p>Hi, as the subject says. We trained white rats (social behavior) on sufficient number of frames, but we have another experiment with black rats on the white floor. Shall I add such novel frames to the existing training set (and retrain with more iterations) or, should that be a separate model? Can\u2019t find info on that\u2026 Thanks | P</p>", "<p>It will be easiest to make a separate model, since the one you have is heavily biased towards white rats. But with more and more data you can think of combining it into one model (just keep the bodypart number and names the same between both projects).</p>\n<p>An interesting thing to try out might be color inversion on the video - it\u2019s something I alwyas wanted to try out, but don\u2019t have the data (since this could potentially become a good augmentation method for training models for both black and white rodents)</p>\n<p>There is also an approach like the one used in superanimal models, but I guess that requires way more data - and I haven\u2019t read the paper fully yet, so I\u2019m not sure about the process.</p>", "<p>Thank you, Konrad. I tried inverted video, but this did not work. Regards! | P</p>", "<p>On its own I didn\u2019t think it would, but grayscale inverted if the background is sufficiently average, should work nicely as an augmentation method - I don\u2019t think it\u2019s part of imgaug or tensorpack though, so there would be some actual augmented data creation before creating a training dataset</p>"], "52270": ["<p>Hello,</p>\n<p>I have a question about pgfocus board. We need a couple of zdrift correction module in our lab and Pgfocus sounds like a very good option. Is there any way that we can buy the pgfocus module? <a class=\"mention\" href=\"/u/kbellve\">@kbellve</a></p>", "<p>Hi Vahid,</p>\n<p>Yes, it is possible to buy a pgFocus module although I rather everyone make their own.</p>\n<p>I am more interested in finishing the pgFocus shield project rather than building the original pgFocus. Once I validate the shield, it would be simpler for others to build themselves rather the original pgFocus. Let me know your timeline (longer the better) and if you would be interested in a pgFocus shield.</p>\n<p>I would have to go back into the design of the pgFocus shield and confirm everything\u2026which would take longer, and order the boards\u2026and actually validate it works. This could take a few months from beginning to end, assuming everything goes well.</p>", "<p>Thank you for the response. We need the z-drift correction module as soon as possible, so most likely we cannot wait for a couple of months for the new module at this point. We have tried to make the module ourselves, however, the price gets quite expensive per board since we only need two or three at this point. That is why I was wondering if we can buy an already assembled module.</p>", "<p>The parts can be pricey. I wasn\u2019t going for inexpensive when I designed it because I didn\u2019t know if it would work\u2026so I went with higher quality and higher resolution DA, AD, and Op Amp chips\u2026 I do wish it was cheaper and simpler for people to build themselves.</p>\n<p>Just a few questions if you don\u2019t mind\u2026</p>\n<p>Does your lab/group have the expertise to handle the optics, which is to reflect a &gt;800 nm beam off of a coverslip? To do this, one needs a cover glass/water interface (refractive index change).</p>\n<p>Do you need the Analog to Digital part of pgFocus? This is the part that allows voltage pass-through to be sampled. It lets pgFocus adjust the focus point automatically.</p>\n<p>Do you need the Digital to Analog part of pgFocus? This is the part that allows controlling voltage pass-through to be modified. This lets pgFocus maintain focus position in response to the light sensor.</p>\n<p>The A/D and DA requires \u00b1 Voltage\u2026which means an internal power converter from 5V.</p>\n<p>Do you need the light sensor part of pgFocus? This is a 128 pixel sensor that determines focus location by a reflected beam.</p>\n<p>The reason why I am asking if it might be possible to build a pgFocus like board now using simple breakout boards + arduino (or FPGA or Raspberry Pi)</p>\n<p>For example, this board might work for sampling voltage, although its voltage range is limited but might be good enough: <a href=\"https://www.adafruit.com/product/1085\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ADS1115 16-Bit ADC - 4 Channel with Programmable Gain Amplifier : ID 1085 : $14.95 : Adafruit Industries, Unique &amp; fun DIY electronics and kits</a></p>\n<p>This board would have definitely have worked! <a href=\"https://www.digikey.com/en/products/detail/digilent-inc/410-309/4969950\" rel=\"noopener nofollow ugc\">https://www.digikey.com/en/products/detail/digilent-inc/410-309/4969950</a><br>\nBut it appears that it is no longer made. <img src=\"https://emoji.discourse-cdn.com/twitter/frowning.png?v=9\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\"></p>\n<p>I do have two pgFocus available and I can build more.</p>", "<p>We do have the expertise for handling the optics. Our lab develops microscopy techniques, that is why we need the z-drift correction module to be able to real time keep the sample in focus for live cell imaging. Therefore, we do need the AD and DA and the light sensor.<br>\nI was wondering how much the price is for the pgFocus board that you have available and how we can buy it.</p>", "<p>I couldn\u2019t find anything that was a suitable replacement given that you need the full functionality of pgFocus. I will check the status of the two I have and get in touch directly.</p>", "<p>Any news on this? Our lab is also interested in getting a pgFocus. Happy to buy one if possible but also happy to do it ourselves.</p>\n<p>Our piezo has closed loop control so might be able to just send the signal back from the light sensor through Arduino with an extra 16 bit ADC, but would like to have some of the pgFocus funcitonality in micromanager. Thoughts?</p>", "<p>Hi <a class=\"mention\" href=\"/u/ponjavic\">@ponjavic</a></p>\n<p>I sent him a pgFocus. I make them as needed for those who can\u2019t make one themselves. I have designed an Arduino Shield version, which I have never tested. <a href=\"https://github.com/kbellve/pgFocus-Shield\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - kbellve/pgFocus-Shield: This is the Arduino Shield version of pgFocus. It hasn't been validated yet.</a></p>\n<p>If I was going to build another pgFocus, I will probably do a shield instead of a full version. It would give me a reason to finally validate it\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/laughing.png?v=12\" title=\":laughing:\" class=\"emoji\" alt=\":laughing:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>We could potentially make one ourselves or through electronics workshop, but not very experienced with soldering on 10s-100s of components. If you\u2019d be interested in making a shield we\u2019d happily purchase one/the components. Or if you think it can be done with off-the-shelf DAC/ADC extensions using an Arduino (compatible with the MM plugin) that would also be very interesting to us.</p>", "<p>I did a quick search for high resolution ADC and DAC breakout boards and I didn\u2019t come up with anything useful.</p>\n<p>Adafruit does have a 16 bit ADC but I think it only works in the 0V to 5V range.</p>\n<p>Tons of 12 bit breakout boards out there, although most are limited to 0V to 5V range.</p>\n<p>pgFocus uses a 14 bit ADC and 14 bit DAC with a \u00b110V range, which are well suited for their intended task.</p>\n<p>I don\u2019t mind modifying the pgFocus shield if something can be found.</p>", "<p>How about this:<br>\n<a href=\"https://www.archiduino.com/product/snipcard-dac-16-bit-2-channels-0-10v-buffered-copy/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://www.archiduino.com/product/snipcard-dac-16-bit-2-channels-0-10v-buffered-copy/</a></p>\n<p>Would it be hard to modify pgFOCUS to run with 0-5V ADC and DAC (would be plenty for us)?<br>\nAlternatively could just half the sensor voltage with resistors to work with the 0-5V ADC?</p>", "<p>I think those are usable.</p>\n<p>From a quick read of the DAC documentation, it says range is 0V- VDD range, which is usually the supply voltage and if it is connected to an Arduino, that would be 5V. The manufacturer of SnipCard DAC says 0 to 10V, which is fine if it will do that range.  The Archiduino might take 10V, unlike an Arduino.</p>\n<p>For a 0-10V range, you will need to make 5V your center focus position to allow the pgFocus to correct focus in both directions.</p>\n<p>For a 0-5V range, you will need to make 2.5V as your center focus position.</p>\n<p>Having a voltage offset isn\u2019t a big deal, but if for some reason pgFocus is off, and then you turn it on, it will cause focus change. Not a problem always have pgFocus on, and not in focus control.</p>\n<p>It might also limit your up travel of your piezo if you are starting at 2.5V or 5V\u2026</p>\n<p>What you do lose is pgFocus ability to follow and control focus outside of those voltage ranges. This might not matter if you aren\u2019t doing 3D.</p>\n<p>You do have me thinking\u2026should I modify (and simplify) the shield to allow the use of these external DAC/ADCs\u2026</p>\n<p>The benefit of pgFocus is that it provides -10V/10V power, and can control focus -5V to 5V, and also allows pass-through of a piezo signal at its original fidelity. I could still make a pgFocus shield with all the basic circuitry + the power supply\u2026 (could be 10V or -10V/10V), allowing the use of DAC/ADC breakout boards\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>As far as pgFocus firmware changes to run these DAC/ADC? I don\u2019t think it will be much\u2026they are Analog Devices, the same manufacturer I use, and they also use SPI to communicate. They would need to be connected to the same pins Arduino pins that pgFocus uses.</p>", "<p>Thanks that\u2019s helpful.</p>\n<p>From our point of view we simply want one even if 0-5V, (possibly 4 in the future) so whatever is easiest for you, either if you\u2019re willing to modify the shield, finish it or make more hardware versions.</p>\n<p>I guess something that\u2019s as close to off the shelf as possible would be easiest for new users, but I understand you might not want to compromise on capability.</p>", "<p>After thinking about it, I will modify the pgFocus shield to accept external DAC/ADC boards. It could be as easy as putting the correct through-holes in the right places. I could also move pgFocus DAC and ADC onto external boards.</p>\n<p>The issue is I can\u2019t find what I need, the pcb layout of the SnipCards. Spacing of the pins, or what the pins even do.</p>\n<p>The SnipCards are supposed to be Open Source Hardware, which usually means that all files are provided to recreate the hardware.  I will reach out to them and ask about their PCB layout.</p>\n<p><a href=\"https://www.archiduino.com/dual-channel-dac-snipcard/\" rel=\"noopener nofollow ugc\">https://www.archiduino.com/dual-channel-dac-snipcard/</a></p>", "<p>FYI update, I contacted the author of the SnipCards and he was very helpful with providing the information I needed.</p>\n<p>No promises, but I will see how hard it is to modify the pgFocus shield to accept external DAC/ADCs. You will still need to do some SMD soldering (e.g. Op Amps, resistors) but it should be much easier for a novice to do. It would also be much easier for me to do\u2026and send to people\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/grin.png?v=12\" title=\":grin:\" class=\"emoji\" alt=\":grin:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>I might look at changing the resistors to PTH, and maybe look for suitable Op Amps that are also PTH\u2026It would be really cool for everything to be PTH\u2026actually, I am looking at the board design now\u2026might not be enough space to go to PTH for everything due to size of the BNC connectors\u2026could switch to SMB connectors.</p>", "<p>Hi <a class=\"mention\" href=\"/u/kbellve\">@kbellve</a></p>\n<p>I am interested in upgrading one or two of our microscopes with a pgfocus system and I am wondering, whether you made any progress with the arduino shield version? Also after a quick google search it seems that the tsl1401 is no longer produced. Are there any plans to update pgFocus with a newer line sensor?</p>\n<p>Thanks a lot for the information!</p>", "<p>Karl <a class=\"mention\" href=\"/u/kbellve\">@kbellve</a></p>\n<p>For the last 14 months, I\u2019ve been trying to get my Leica DMIRB/E up and running with Micromanager. It is no longer in its original confocal configuration, so this is just a stand-alone. Finding help had been nearly impossible. I thought Micromanager could operate the nose piece focus mechanism and possibly the motorized turret, although the turret isn\u2019t a priority. I noticed pgFocus was an option in the menu, but I wasn\u2019t sure where it could be downloaded nor how to install it, let alone if it would do what I needed. That being to focus my scope. Any help or suggestions would be very much appreciated.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pbase.com/smokedaddy/leica_dm_irbe&amp;page=all\">\n  <header class=\"source\">\n\n      <a href=\"https://pbase.com/smokedaddy/leica_dm_irbe&amp;page=all\" target=\"_blank\" rel=\"noopener nofollow ugc\">PBase</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:400/225;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/4/e47bad45987659b095260f0816898b69be93c507.png\" class=\"thumbnail\" width=\"400\" height=\"225\"></div>\n\n<h3><a href=\"https://pbase.com/smokedaddy/leica_dm_irbe&amp;page=all\" target=\"_blank\" rel=\"noopener nofollow ugc\">Leica DM IRBE by Squatting Dog</a></h3>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Regards,<br>\n-JWW</p>", "<p>HI <a class=\"mention\" href=\"/u/jww\">@JWW</a>, the pgfocus is a module for auofocussing, i.e. it does not include a focus drive.  Does you IRBE have a motorized Z drive?</p>", "<p>Yes. To my limited knowledge, the focus mechanism is an encoder. After I replied here, I just got the Z- drive focusing working with Micromanager, but not for automated stacking of images. From my understanding, Micromanager doesn\u2019t support DSLR camera control.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pbase.com/smokedaddy/leica_dm_irbe\">\n  <header class=\"source\">\n\n      <a href=\"https://pbase.com/smokedaddy/leica_dm_irbe\" target=\"_blank\" rel=\"noopener nofollow ugc\">PBase</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:400/225;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/1263f6e3a57c4f79ab8da461464f9d826788ae3b.png\" class=\"thumbnail\" width=\"400\" height=\"225\"></div>\n\n<h3><a href=\"https://pbase.com/smokedaddy/leica_dm_irbe\" target=\"_blank\" rel=\"noopener nofollow ugc\">Leica DM IRBE by Squatting Dog</a></h3>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>-JW:</p>", "<p>Yes, you would need to use a supported camera (plenty to choose from).</p>"], "78899": ["<p>Hi,<br>\nI am trying to create a lasso tool so I can create polygons in the shape layer by click and dragging the mouse. Here is the code I have so far:</p>\n<pre><code class=\"lang-auto\">import napari\n\nviewer = napari.Viewer()\n#label_layer=viewer.add_labels(remap)\n#shapes = None\nshape = viewer.add_shapes()#data=shapes,shape_type='polygon',edge_width=4,edge_color='coral', face_color='royalblue')\nshape.blending = 'additive'\n\ncontour=[]\n\n@shape.mouse_drag_callbacks.append\ndef update_selection(layer, event):\n    \n    if len(contour)&gt;1:\n        contour.clear()\n    \n    print(shape.data)\n    shapes_len = len(shape.data)\n    yield\n    \n    while 'Alt'in event.modifiers and event.button == 1: #'Alt'in event.modifiers and\n        \n        data_coordinates = layer.world_to_data(event.position)\n        cords = np.round(data_coordinates).astype(int)\n        contour.append(cords)\n    \n        #time.sleep(0.01)\n        shapes=shape.data\n        \n        if len(shapes)&gt;shapes_len:   \n            shapes.pop(-1)\n        \n        shapes.append(np.stack(contour))\n        shape.data=shapes\n        \n        yield \n    return\n</code></pre>\n<p>This works to create one polygon by click and dragging but for some reason when I want to create a second and I am triggering the mouse_drag_callback function my previous shape gets deleted (why I put the print statement there to figure out). I don\u2019t understand why, but then I guess I don\u2019t fully understand what the mouse decorator does.</p>\n<p>Also there seems to be a lag to get in the while loop to draw the polygon. I need to click twice to initiate it. Any idea why this is?</p>\n<p>Thanks in advance<br>\nYannick</p>", "<p>Hi <a class=\"mention\" href=\"/u/yannick_blum\">@Yannick_Blum</a>,</p>\n<p>I tested your script and it seems to work without problem for me. The only thing you have to fix is to add a condition to add the most recent shape. It should only be added if the shape has more than one vertex. This is why you have to click twice: it generates an error when you try to add a shape with a single vertex the first time you click in the viewer. When you click again, you have two vertices and it works. This fixes it:</p>\n<pre><code class=\"lang-python\">if len(contour) &gt; 1:\n    shapes.append(np.stack(contour))\n</code></pre>\n<p>I attach a short video of what I get. Is this what you expect? Maybe your napari is old? It worked for 0.4.15 and 0.4.17 for me.<br>\n</p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e0d945507cdbcb933d4db21ac390d5a629c009.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e0d945507cdbcb933d4db21ac390d5a629c009.mp4\" rel=\"noopener nofollow ugc\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c9e0d945507cdbcb933d4db21ac390d5a629c009.mp4</a>\n    </source></video>\n  </div><p></p>\n<p>Guillaume</p>", "<p>Hi <a class=\"mention\" href=\"/u/yannick_blum\">@Yannick_Blum</a>,</p>\n<p>Are you looking to add this as part of a plugin or to contribute to Napari? In any case I would like you to point to a PR that exists regarding adding draw polygon functionality to Napari <a href=\"https://github.com/napari/napari/pull/5555\" rel=\"noopener nofollow ugc\">PR #5555</a>. If you would like to go through the code sometime, please let me know. Also feel free to try out and give feedback on your user experience:)</p>\n<p>Wouter-Michiel</p>", "<p><a class=\"mention\" href=\"/u/guiwitz\">@guiwitz</a><br>\nthanks, this line of code indeed solves it, don\u2019t fully understand why yet <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> :- <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">.</p>\n<p><a class=\"mention\" href=\"/u/wmv1992\">@wmv1992</a><br>\nOh, I hadn\u2019t seen this yet, thanks. I am biologist, who is using python and napari for image analysis. My initial idea was to make a tool, which would allow me to select several labels in a label layer using shapes and I wanted a lasso tool, which would make it easier to precisely select labels, and then plot some graphs based on the selection. A bit like napari-clusters-plotter plugin but the other way around.</p>\n<p>I would love to make a plugin but I don\u2019t yet really have the programming skills and time to get into the plugin creation.<br>\nYannick</p>", "<p>In that case, I am aiming to wrap up the PR soon. We had a community meeting today where the polygon lasso tool was discussed and I will implement the things mentioned during the discussions there. The current branch does have the RDP algorithm enabled by default. If you would like to work from this branch for now with it disabled I can point you to how to do that.</p>"], "78900": ["<p>This error occur when I open the project after I saved it. I cannot open the image. May I know how can I fix this.</p>\n<p>Thanks.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/6/668d766d2c753e867d8a30bcf18937686c0c9431.png\" alt=\"QP error\" data-base62-sha1=\"eDdMCwyDP1vgBvtdq9ADrAVgW65\" width=\"527\" height=\"258\"></p>", "<p>Searching for other examples of this error message <a href=\"https://forum.image.sc/t/hierarchy-is-null-cannot-invoke-can-anyone-help/78488/4\" class=\"inline-onebox\">Hierarchy is null - Cannot invoke - Can anyone help? - #4 by petebankhead</a><br>\n<a href=\"https://forum.image.sc/t/i-cannot-open-the-qupath-file-due-to-the-error-note-hierarchy-is-null-what-can-i-do/68160/4\" class=\"inline-onebox\">I cannot open the QuPath file due to the error note \"hierarchy is null\". What can I do? - #4 by Research_Associate</a><br>\nEtc.<br>\nIf there is a backup file in the folders described above you can try and use that instead.</p>"], "78908": ["<p>Hi,</p>\n<p>I am using a windows. I have tried to set up a conda environment for stardist 0.83 where in this case I have a program with just a printing of \u201cHello World\u201d.</p>\n<p>from stardist.models import StarDist2D<br>\nfrom stardist.data import test_image_nuclei_2d</p>\n<p>print(\u201cHello World!!\u201d)</p>\n<p>However the executable  always fails. I have tried many different sets of environments with different versions of Tensorflow 2 (2.4, 2.10) and corresponding set of Cuda. I tried also some different versions of Python. Have anyone succeeded in creating a working executable with a contained stardist and which type of environment was then used (exact versions and their installation order would be appreciated) ?</p>\n<p>Kind regards<br>\nFredrik Olsson</p>"], "78909": ["<p>Hi all,</p>\n<p>We\u2019ve written a Python module for GPU-accelerated drift correction / image registration, that we call pyGPUreg. It is more than 40x faster than StackReg (the Python port of TurboReg) in our tests (using a Quadro P2200), and does not require a CUDA compatible GPU.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449.png\" data-download-href=\"/uploads/short-url/h3vtifcuWEyoA4q23Cbx4pJwZFT.png?dl=1\" title=\"pyPGUreg_vs_StackReg\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449_2_690x274.png\" alt=\"pyPGUreg_vs_StackReg\" data-base62-sha1=\"h3vtifcuWEyoA4q23Cbx4pJwZFT\" width=\"690\" height=\"274\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449_2_690x274.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449_2_1035x411.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/778a7646cbcc00c2ed72da8f05c459980db80449.png 2x\" data-dominant-color=\"F6F6F6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">pyPGUreg_vs_StackReg</span><span class=\"informations\">1048\u00d7417 51.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>We wrote it for our own use, but are wondering whether it may be useful for others as well; maybe it is worth integrating it as an ImageJ plugin or into CLIJ?</p>\n<p>Registration with pyGPUreg is based on phase correlation. Input images are uploaded to the GPU, and forward FFTs, the phase correlation, and a reverse FFT are computed on the GPU as well, after which the resulting image is transferred to the CPU where the image shift is detected. This shift is used to launch a second GPU process, which re-samples on of the the input images and returns the registered image to the CPU.</p>\n<p>It is available via pip (pip install pyGPUreg) and the code is available here: <a href=\"https://github.com/bionanopatterning/pyGPUreg\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - bionanopatterning/pyGPUreg</a><br>\nIt is still somewhat of a work in progress, so it comes with no guarantees that it really works. But it does work for us.</p>\n<p>We\u2019d be happy to be contacted by anyone who is interested!</p>", "<p>That is really cool <a class=\"mention\" href=\"/u/martgf\">@MartGF</a>! As someone who frequently rants against monopolies I particularly love that you implemented it in OpenGL and not CUDA! <img src=\"https://emoji.discourse-cdn.com/twitter/joy.png?v=12\" title=\":joy:\" class=\"emoji\" alt=\":joy:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/heart.png?v=12\" title=\":heart:\" class=\"emoji\" alt=\":heart:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>As a side note, I wonder how hard it would be to align two image layers in napari by grabbing the <em>textures</em> that are already on the GPU\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/exploding_head.png?v=12\" title=\":exploding_head:\" class=\"emoji\" alt=\":exploding_head:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>First think I thought when I saw this, given that it\u2019s not using OpenCL but OpenGL, just like napari/vispy.</p>"], "78910": ["<p>Hi everyone,</p>\n<p>I\u2019m currently trying to add a public user to our OMERO appliance prototype, to display public data. I tried to follow the official procedure (<a href=\"https://omero.readthedocs.io/en/stable/sysadmins/public.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Publishing data using OMERO.web \u2014 OMERO documentation</a>). It went OK (i use Docker containers with Ansible installation, and i added/adjusted the corresponding lines to get them compatible with 01-default-webapps.omero):</p>\n<p>\u201c\u201d\"</p>\n<h1>\n<a name=\"public-user-set-1\" class=\"anchor\" href=\"#public-user-set-1\"></a>Public user set</h1>\n<p>config set \u2013 omero.web.public.enabled True<br>\nconfig set \u2013 omero.web.public.user \u2018guest\u2019<br>\nconfig set \u2013 omero.web.public.password \u2018guest\u2019<br>\nconfig set \u2013 omero.web.public.get_only true<br>\n<a class=\"hashtag\" href=\"/tag/config\">#<span>config</span></a> set \u2013 omero.web.public.url_filter \u2018^/webgateway\u2019<br>\nconfig set \u2013 omero.web.public.url_filter \u2018^/(webadmin/myphoto/|webclient/(?!(script_ui|ome_tiff|figure_script))|webgateway/(?!(archived_files|download_as))|iviewer|api|3Dscript)\u2019<br>\nconfig set \u2013 omero.web.public.server_id 1<br>\n\u201c\u201d\"</p>\n<p>The line \u201cconfig set \u2013 omero.web.public.url_filter \u2018^/webgateway\u2019\u201d didn\u2019t changed anything to the main frontend, however, the line \u201cconfig set \u2013 omero.web.public.url_filter \u2018^/(webadmin/myphoto/|webclient/(?!(script_ui|ome_tiff|figure_script))|webgateway/(?!(archived_files|download_as))|iviewer|api|3Dscript)\u2019\u201d added an option to connect as a public user on the frontend:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e.png\" data-download-href=\"/uploads/short-url/eLKEC05o5xALjpEJVhUGTvJKdEa.png?dl=1\" title=\"Screenshot 2023-03-22 at 11-03-49 OMERO.web - Login\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_690x282.png\" alt=\"Screenshot 2023-03-22 at 11-03-49 OMERO.web - Login\" data-base62-sha1=\"eLKEC05o5xALjpEJVhUGTvJKdEa\" width=\"690\" height=\"282\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_690x282.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_1035x423.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/7/67844faf7656ccbefab3dcc93d8a49e055c5132e_2_1380x564.png 2x\" data-dominant-color=\"E0E4EA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-22 at 11-03-49 OMERO.web - Login</span><span class=\"informations\">1637\u00d7670 40.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>However, by clicking on it, i have an error screen:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a.png\" data-download-href=\"/uploads/short-url/jCg2lpsOOdi9Si8fQH2N0okKeM2.png?dl=1\" title=\"Screenshot 2023-03-22 at 11-10-19 OMERO.web - support\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_690x284.png\" alt=\"Screenshot 2023-03-22 at 11-10-19 OMERO.web - support\" data-base62-sha1=\"jCg2lpsOOdi9Si8fQH2N0okKeM2\" width=\"690\" height=\"284\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_690x284.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_1035x426.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/9/897c722e9e8f7d4513efc5d4e0603d185049c02a_2_1380x568.png 2x\" data-dominant-color=\"F2F2F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-22 at 11-10-19 OMERO.web - support</span><span class=\"informations\">1637\u00d7675 52.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>With this message:</p>\n<p>\u201c\u201d\"<br>\nTraceback (most recent call last):</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/django/core/handlers/exception.py\u201d, line 47, in inner<br>\nresponse = get_response(request)</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/django/core/handlers/base.py\u201d, line 181, in _get_response<br>\nresponse = wrapped_callback(request, *callback_args, **callback_kwargs)</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/decorators.py\u201d, line 538, in wrapped<br>\nretval = f(request, *args, **kwargs)</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/decorators.py\u201d, line 597, in wrapper<br>\ncontext = f(request, *args, **kwargs)</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/webclient/views.py\u201d, line 575, in load_template<br>\nreturn _load_template(request=request, menu=menu, conn=conn, url=url, **kwargs)</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omeroweb/webclient/views.py\u201d, line 495, in _load_template<br>\nleaders, members = conn.getObject(\u201cExperimenterGroup\u201d, active_group).groupSummary()</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py\u201d, line 3271, in getObject<br>\nresult = self.getQueryService().findByQuery(</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py\u201d, line 5102, in <strong>getattr</strong><br>\nobj = self._obj or self._getObj()</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py\u201d, line 5033, in _getObj<br>\nself._obj = self._create_func()</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omero/gateway/<strong>init</strong>.py\u201d, line 5009, in cf<br>\nobj = getattr(self._conn.c.sf, self._func_str)()</p>\n<p>File \u201c/opt/omero/web/venv3/lib/python3.8/site-packages/omero_API_ice.py\u201d, line 758, in getQueryService<br>\nreturn _M_omero.api.ServiceFactory._op_getQueryService.invoke(self, ((), _ctx))</p>\n<p>omero.SecurityViolation: exception :<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/b/eb1c03e7926625ea7fc12539c025467ea8166d82.png?v=12\" title=\":omero:\" class=\"emoji emoji-custom\" alt=\":omero:\" loading=\"lazy\" width=\"20\" height=\"20\">:SecurityViolation<br>\n{<br>\nserverStackTrace =<br>\nserverExceptionClass =<br>\nmessage = Access denied to guest user: omero.api.IQuery<br>\n}</p>\n<p>&lt;WSGIRequest: GET \u2018/webclient/\u2019&gt;<br>\n\u201c\u201d\"</p>\n<p>So, what may be the root cause?</p>\n<p>Also, i have two others questions, partially related to the problem:</p>\n<ul>\n<li>What is the purpose of the \u201cguest\u201d user? It seems to be different to a public user (i noticed that by looking in the various examples contained in <a href=\"https://github.com/orgs/ome/repositories\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Open Microscopy Environment \u00b7 GitHub</a>)</li>\n<li>How do enable the \u201cdefault\u201d group? It is normally created by default, but in my installation, it is disabled, and i can\u2019t figured how to create it (in fact, i\u2019m not even supposed to create it).</li>\n</ul>\n<p>Best regards and thanks by advance, Marc.</p>", "<aside class=\"quote no-group\" data-username=\"mmongy\" data-post=\"1\" data-topic=\"78910\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/51bf81/40.png\" class=\"avatar\"> Marc Mongy:</div>\n<blockquote>\n<p>What is the purpose of the \u201cguest\u201d user?</p>\n</blockquote>\n</aside>\n<p>The guest user is used internally to load non-sensitive information. It has special handling (and restrictions) so I wouldn\u2019t suggest using it as the public user. i.e. this is likely your root problem.</p>\n<p>Instead, create a new user (named \u201cpublic\u201d, \u201cPUBLIC\u201d, etc.) and configure it for public access.</p>\n<aside class=\"quote no-group\" data-username=\"mmongy\" data-post=\"1\" data-topic=\"78910\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/51bf81/40.png\" class=\"avatar\"> Marc Mongy:</div>\n<blockquote>\n<p>How do enable the \u201cdefault\u201d group?</p>\n</blockquote>\n</aside>\n<p>I\u2019m not exactly sure what you are referring to, but two possibilities:</p>\n<ul>\n<li>Each user has a <em>default</em> group, but it need not be named \u201cdefault\u201d and is whatever group they are a member of first.</li>\n<li>The \u201cdefault\u201d group I know of is related to LDAP based login. When a user gets created via LDAP, a choice must be made of which group to put them in, and one of the mechanisms by default puts them in the \u201cdefault\u201d group which is automatically created.</li>\n</ul>\n<p>~Josh</p>", "<p>Hi Josh, thanks for the answer, but:</p>\n<aside class=\"quote group-team\" data-username=\"joshmoore\" data-post=\"2\" data-topic=\"78910\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/joshmoore/40/1634_2.png\" class=\"avatar\"> Josh Moore:</div>\n<blockquote>\n<p>The guest user is used internally to load non-sensitive information. It has special handling (and restrictions) so I wouldn\u2019t suggest using it as the public user. i.e. this is likely your root problem.</p>\n<p>Instead, create a new user (named \u201cpublic\u201d, \u201cPUBLIC\u201d, etc.) and configure it for public access.</p>\n</blockquote>\n</aside>\n<p>Here is the content of my ansible playbook.yml for omero-web:</p>\n<pre><code class=\"lang-auto\">- hosts: localhost\n  roles:\n  - role: ome.omero_web\n    omero_web_config_set:\n      omero.web.public.enabled: true\n      omero.web.public.server_id: 1\n      omero.web.public.user: public\n      omero.web.public.password: \"{{ omero_web_public_password }}\"\n      omero.web.public.url_filter: \"^/(webadmin/myphoto/|webclient/\\\n      (?!(action|logout|annotate_(file|tags|comment|rating|map)|\\\n      script_ui|ome_tiff|figure_script))|\\\n      webgateway/(?!(archived_files|download_as)))\"\n  - role: ome.java\n\n\n  vars:\n    ice_version: \"3.6\"\n    ice_install_devel: False\n    ice_install_python: False\n    ice_python_wheel: https://github.com/ome/zeroc-ice-py-centos7/releases/download/0.1.0/zeroc_ice-3.6.4-cp27-cp27mu-linux_x86_64.whl\n    omero_web_systemd_setup: False\n    omero_web_setup_nginx: False\n    # These defaults can be overriden at runtime\n    omero_web_public_password: public\n    omero_web_config_set:\n      omero.web.application_server.host: 0.0.0.0\n      # Allow connecting to different minor releases\n      # When https://github.com/openmicroscopy/openmicroscopy/pull/5913 is\n      # released this can be removed\n      omero.web.check_version: \"false\"\n      omero.web.server_list: [[omero, 4064, omero]]\n      omero.web.secure: \"true\"\n\n</code></pre>\n<p>I didn\u2019t used \u201cguest\u201d for public user. I have still the same error.</p>\n<p>Marc.</p>", "<p>Hi <a class=\"mention\" href=\"/u/mmongy\">@mmongy</a><br>\nThat looks OK. I assume you\u2019ve restarted omero-web as part of that update?<br>\nDid you create the \u201cpublic\u201d user with correct password, and can you login as that user normally via CLI, Insight (and web)?<br>\nI wonder if your browser has cached a previous session. Can you try a different browser or \u201cincognito\u201d mode etc?</p>\n<p>Will</p>"], "78912": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17.jpeg\" data-download-href=\"/uploads/short-url/nCuMTdroB8zXhs90PuV09OJGVUz.jpeg?dl=1\" title=\"Intestinal tissue\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_690x271.jpeg\" alt=\"Intestinal tissue\" data-base62-sha1=\"nCuMTdroB8zXhs90PuV09OJGVUz\" width=\"690\" height=\"271\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_690x271.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_1035x406.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a58c06d9426c2175df36c4db8b9517113b0c8a17_2_1380x542.jpeg 2x\" data-dominant-color=\"CBBFBF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Intestinal tissue</span><span class=\"informations\">1900\u00d7747 206 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hello out there!</p>\n<p>I am about to begin a PhD project on intestinal health in piglets, and is therefore interested in using QuPath for image analysis as much as possible. I am currently using some control samples from our archive to train in using different features in QuPath. I have created a thresholder that identifies the tissue samples and created annotations named \u201cTissue\u201d. Then I have trained a few pixel classifiers to identify the different layers of tissue within the gut segments, but when running the classifiers on my samples, it takes quite a lot of machine power, and I was wondering if it is possible to tell QuPath to only run the classifier within the selected annotations and not on the entire image? - When I choose the region for loading my pixel classifier to \u201cAny annotation ROI\u201d it still runs the classifier across almost the entire image, including a large amount of irrelevant background. <img src=\"https://emoji.discourse-cdn.com/twitter/see_no_evil.png?v=12\" title=\":see_no_evil:\" class=\"emoji\" alt=\":see_no_evil:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d.jpeg\" data-download-href=\"/uploads/short-url/ys8BGymgcGtNgj7svA2n0qcjwGp.jpeg?dl=1\" title=\"Classifier\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d_2_690x392.jpeg\" alt=\"Classifier\" data-base62-sha1=\"ys8BGymgcGtNgj7svA2n0qcjwGp\" width=\"690\" height=\"392\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d_2_690x392.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d_2_1035x588.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1783469cbe89e95182404a2ec868ad143ba932d.jpeg 2x\" data-dominant-color=\"8B7FB7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Classifier</span><span class=\"informations\">1353\u00d7769 166 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Following the pixel classification process, I have been able to create objects only based on the annotations, so the system works, and therefore this request is mainly for time-optimizing purposes <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Any comments or suggestions will be highly appreciated! <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>What your image shows is not the pixel classifier running, that is a preview (and is not necessary at all, for visualization purposes only). It is also only being run in tiles where there is an annotation, the tiles are just quite large at the resolution you are zoomed out to.</p>\n<p>When you run the classifier, it will generally run on the selected annotations, and you can select annotations by class as shown in another recent post <a href=\"https://forum.image.sc/t/hes-cell-classification-using-measure-features/78931\" class=\"inline-onebox\">HES cell classification using measure features</a></p>"], "74820": ["<p>The core that I work at is moving into quantitative image analysis and we were wondering at the community\u2019s consensus on a few subjects. Assume that all the questions relate to a scanning confocal microscope being used at the limits of resolution.</p>\n<p><strong>1.</strong> General flat-field correction questions:<br>\n1.a. People\u2019s thoughts on using retrospective vs prospective flatfield correction. I didn\u2019t find anything in the literature, linked below, explicitly stating one is preferred over the other; my assumption is, if you have control over the image gathering process go for prospective, if not retrospective is better than nothing.</p>\n<p><em><strong>1.b.</strong></em> The use of background subtraction varies in some of the literature I\u2019ve read. For example, in Quantitative analysis of Digital Microscope Images, Wolf describes using a background image, obtained from taking an image in a region of no fluorescence, to correct both the flatfield and the sample image. In the book Microscope Image Processing Second Edition 2022, the same basic equation is used but background is replaced with dark current image, which is defined as an image where all light to the dector is blocked. Background subtraction in the book is viewed as a completely alternative approach to flatfield/dark current. I was wondering if dark current image or the background image should be used for flatfield correction where A[x,y] is the corrected image, I[x,y] is the image measured, and F[x,y] is the flatfield image. Also, would like to know people\u2019s arguments for and against using dark current or background.<br>\nA[x,y] = (I[x,y] \u2013 C[x,y]) / (F[x,y] \u2013 C[x,y])</p>\n<p><em><strong>1.c</strong></em> The book Microscope Image Processing Second Edition 2022, states that background subtraction can be used as an alternative to Flatfield/dark current correction and that it\u2019s a very effective method of correcting autofluorescence.10 Does this mean that background subtraction should be used instead of Flatfield/dark current correction? That feels inherently off to me which is why I ask, though this is of course assuming that the community\u2019s recommendation for 1.b isn\u2019t just to do Flatfield/background correction.</p>\n<p><em><strong>1.d.</strong></em> For retrospective flat-field correction is the BaSiC plugin from ImageJ the recommended tool?</p>\n<p><strong>2.</strong> Deconvolution seems to have a number of potential pitfalls, especially if used in conjunction with quantitative image analysis.5</p>\n<p><em><strong>2.a.</strong></em> I was wondering at the frequency that people measure their PSF for deconvolution. The book Microscope Image Processing Second Edition 2022, implies that experimental PSF for deconvolution should be tailored to and measured prior to the experiment that it will be used on.10 I was wondering if cores actually measure at this frequency for deconvolution or if they gather the PSF weekly, or monthly instead?</p>\n<p><em><strong>2.b.</strong></em> Assuming that a measured PSF is on hand, and a 3D image is collected what deconvolution algorithm would be recommended for these three situations:<br>\ni. high signal, low noise<br>\nii. high signal, high noise<br>\niii. low signal, low noise.</p>\n<p><em><strong>2.c.</strong></em> I wanted to double check that the deconvolution follows after flat-field correction/background subtraction. Are there any other common pre-processing steps people perform prior to deconvolution? I know that deconvolution can increase the impact of artifacts and noise, does anyone perform deconvolution post noise removal?</p>\n<p><em><strong>2.d.</strong></em> As a general workflow practice, should I check for whether a deconvolution technique is appropriate for the images I am analyzing?6,7 Are there any other methods to check whether the deconvolution algorithm is assigning intensity inappropriatly besides graphing the percent bins of pixel intensity in the post deconvolution to raw data and taking the R value?6,7</p>\n<p><strong>3.</strong> In the handful of quantifications I\u2019ve done, I\u2019ve redirected the segmented ROI to a copy of the raw image for obtaining intensity measurements, which after reading more about flatfield correction, I now assume is wrong. So, I\u2019m checking whether I should be applying the ROI to the deconvoluted image for intensity quantification or further up the chain to the flatfield/background corrected image.</p>\n<p><strong>4.</strong> Lastly, double checking that my ordering of Flatfield/background subtraction, Noise removal, and Deconvolution is correct. Does anyone perform any other standard quality control methodologies that limit batch effects between samples that I\u2019m missing?<br>\ni. Copy of image raw<br>\nii. Flatfield correction<br>\niii. Noise removal<br>\niv. Deconvolution<br>\nvi. Pre-processing</p>\n<p>Reference:</p>\n<ol>\n<li><a href=\"https://calm.ucsf.edu/how-acquire-flat-field-correction-images\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How to acquire flat-field correction images | Center for Advanced Light Microscopy</a></li>\n<li><a href=\"https://forum.image.sc/t/quantifying-microscopy-images-top-10-tips-for-image-acquisition/12343/2\" class=\"inline-onebox\">Quantifying microscopy images: top 10 tips for image acquisition - #2 by cfrick13</a></li>\n<li><a href=\"https://www.embopress.org/doi/full/10.15252/embj.2020105889\" rel=\"noopener nofollow ugc\">https://www.embopress.org/doi/full/10.15252/embj.2020105889</a></li>\n<li><a href=\"https://forum.image.sc/t/deconvolution-and-intensity-quantification/37887\" class=\"inline-onebox\">Deconvolution and intensity quantification</a></li>\n<li><a href=\"https://forum.image.sc/t/pro-and-cons-with-deconvolution-algorithm-comparison/58750/2\" class=\"inline-onebox\">Pro and cons with deconvolution algorithm (comparison) - #2 by Research_Associate</a></li>\n<li><a href=\"https://forum.image.sc/t/how-do-we-reasonably-show-a-restoration-algorithm-is-quantitative/48666/3\" class=\"inline-onebox\">How do we reasonably show a restoration algorithm is \"quantitative\"? - #3 by bnorthan</a></li>\n<li><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3942261/\" rel=\"noopener nofollow ugc\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3942261/</a></li>\n<li><a href=\"https://rupress.org/jcb/article-pdf/221/11/e202107093/1439851/jcb_202107093.pdf\" rel=\"noopener nofollow ugc\">https://rupress.org/jcb/article-pdf/221/11/e202107093/1439851/jcb_202107093.pdf</a></li>\n<li><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5472168/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">A BaSiC tool for background and shading correction of optical microscopy images - PMC</a></li>\n<li>Merchant, F.A., Castleman, K.R. Microscope Image Processing, Second Edition. Book</li>\n<li>Wolf, David. Quantitative analysis of digital microscope images. Methods in cell biology</li>\n</ol>", "<p>Hi <a class=\"mention\" href=\"/u/creativerror404\">@creativerror404</a></p>\n<p>I am not a microscopist, but an image/signal processing engineer.  I don\u2019t know the answer to many of your questions, but hopefully some experienced microscopists will give input, I am interested in the answers myself.</p>\n<p>I have done deconvolution as part of image processing workflows for many years so here are some thoughts on your deconvolution related questions based on my experience.</p>\n<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.a.</strong></em> I was wondering at the frequency that people measure their PSF for deconvolution.</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/jlacoste\">@jlacoste</a> (Judith Lacoste) may be able to answer this question.</p>\n<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.b.</strong></em> Assuming that a measured PSF is on hand, and a 3D image is collected what deconvolution algorithm would be recommended for these three situations:<br>\ni. high signal, low noise<br>\nii. high signal, high noise<br>\niii. low signal, low noise.</p>\n</blockquote>\n</aside>\n<p>You may be interested in playing around with <a href=\"http://bigwww.epfl.ch/deconvolution/deconvolutionlab2/\">deconvolution lab 2</a>.  It implements many different algorithms and is a great resource for discovering what algorithm and parameters work for your image type.</p>\n<p>In my experience the most important part of deconvolution is getting a good estimate of the PSF.  If you have a bad estimate of PSF no algorithm will give a good answer.  Once you have a good PSF the next thing to think about is how to handle noise.  This involves optimizing the number of iterations used and/or optimizing a smoothing or regularization parameter.</p>\n<p>I have used the non-circulant version of Richardson Lucy algorithm for many years.   For widefield images it is important to use non-circulant edge handling to avoid edge artifacts.  This is less important for confocal.  I use\u2026</p>\n<ol>\n<li>Richardson Lucy with \u201chundreds\u201d of iterations for high signal low noise.</li>\n<li>Richardson Lucy with \u201chundreds\u201d of iterations and total variation regularization for high signal high noise and low signal/low noise.</li>\n<li>RIchardson Lucy with total variation regularization and 10s of iterations for low signal, high noise.</li>\n</ol>\n<p>(Note that iterations are not always apples to apples, Richardson Lucy with high SNR takes hundreds of iterations to converge, however with low SNR noise may be an issue after 10s of iterations, some iterative algorithms converge in less iterations.  For example the Matlab version of Richardson Lucy is accelerated and converges is apr. 5X less iterations (with a trade-off of higher memory use and more complexity per iteration).</p>\n<p>I have some simulated images that I experiment with to approximate the parameterization (number of iterations and regularization factor).  I basically approximate the signal to noise and PSF of the image set to be deconvolved, then do a simulation with that PSF and SNR and estimate the parameterization of the algorithm.</p>\n<p>See <a href=\"https://github.com/True-North-Intelligent-Algorithms/tnia-python/blob/7bf8bb6b8d656e1c31799ea2cee7589e18928a1d/notebooks/Deconvolution/Nuclei_Deconvolution_Compare_to_Truth.ipynb\">this notebook</a> for an example simulation.</p>\n<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.c.</strong></em> I wanted to double check that the deconvolution follows after flat-field correction/background subtraction. Are there any other common pre-processing steps people perform prior to deconvolution? I know that deconvolution can increase the impact of artifacts and noise, does anyone perform deconvolution post noise removal?</p>\n</blockquote>\n</aside>\n<ol>\n<li>\n<p>If slices in a 3D image are mis-aligned you can perform slice to slice alignment before deconvolution.</p>\n</li>\n<li>\n<p>I would not perform deconvolution post-noise removal.  I do not have much experience combining separate noise removal and deconvolution algorithms.  Instead I have used deconvolution algorithms that have noise removal (regularization) build into the algorithm.</p>\n</li>\n</ol>\n<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.d.</strong></em> As a general workflow practice, should I check for whether a deconvolution technique is appropriate for the images I am analyzing?</p>\n</blockquote>\n</aside>\n<p>I generally use control samples and simulations.  For example a sample of densely packed beads of varying but known sizes and intensities is useful (I am not a microscopist, so don\u2019t know much about how to to create such a sample, but have found it useful to have something available when testing).  I segment and measure the bead image before and after deconvolution and verify that deconvoluton is leading to measurements that are closer to \u2018truth\u2019.</p>\n<p>The above is a good technique if you are interested in measuring properties of an image, but if you are trying to investigate the properties of small structures close to the resolution limit more care must be taken.  How do you know they are real?   Deconvolving control samples and verifying you are not getting unexpected structure or texture that could be mistaken for a real structure is useful.  In particular it is known that deconvolution can create \u2018knobby\u2019 artifacts.  (See this <a href=\"https://twitter.com/AndrewGYork/status/1513285510659117061\">twitter thread</a>).</p>\n<p>Let me know if you have any other deconvolution questions.</p>\n<p>Brian</p>", "<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.a.</strong></em> I was wondering at the frequency that people measure their PSF for deconvolution. The book Microscope Image Processing Second Edition 2022, implies that experimental PSF for deconvolution should be tailored to and measured prior to the experiment that it will be used on.10 I was wondering if cores actually measure at this frequency for deconvolution or if they gather the PSF weekly, or monthly instead?</p>\n</blockquote>\n</aside>\n<p>I cannot speak to most cores, but for us it was usually on a case by case basis, for those that needed or wanted to spend the extra time on deconvolution. At a core facility, there are almost as many experiments as users, and the PSF is not fixed for all depths, media, etc. If the processing was intended for a cover image or similar, the theoretical PSF was usually enough.</p>", "<p>Hi,</p>\n<p>My apologies for the late reply.  Performing a PSF on one particular objective once a year has been a realistic goal for general quality control for the core facility I work with.  However, there are a lot of factors contributing to the necessary frequency.  I suspect the range goes from once per month to never\u2026<br>\nBest regards,<br>\nJudith</p>", "<p>Hi,<br>\nif it is not too late, I have some comments about your questions. As a disclamer, I\u2019m more an astronomer than microscopist but a telescope and a microscope are not so different.</p>\n<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>1.b.</strong></em> The use of background subtraction varies in some of the literature I\u2019ve read. For example, in Quantitative analysis of Digital Microscope Images, Wolf describes using a background image, obtained from taking an image in a region of no fluorescence, to correct both the flatfield and the sample image. In the book Microscope Image Processing Second Edition 2022, the same basic equation is used but background is replaced with dark current image, which is defined as an image where all light to the dector is blocked. Background subtraction in the book is viewed as a completely alternative approach to flatfield/dark current. I was wondering if dark current image or the background image should be used for flatfield correction where A[x,y] is the corrected image, I[x,y] is the image measured, and F[x,y] is the flatfield image. Also, would like to know people\u2019s arguments for and against using dark current or background.<br>\nA[x,y] = (I[x,y] \u2013 C[x,y]) / (F[x,y] \u2013 C[x,y])</p>\n</blockquote>\n</aside>\n<p>About flat/dark/background correction you have:</p>\n<ul>\n<li>\n<code>D[x,y]</code> : the dark image where all light to the detector is blocked, it contains only dark current and thermal emission</li>\n<li>\n<code>B[x,y]</code>: the background that contain <code>D[x,y] + </code> background from rest of you setup up to your sample. It may contains some of the autofluorescence but if autofluorescence comes from your sample then it may no be present in the background. An idea to keep autofluorescence in the background in the use a very defocalized image as the backgroud</li>\n<li>\n<code>F[x,y]</code> is the flat field that gives information about differential throughput within the field of view (diffence in gain, dust on optics\u2026). , it has to be corrected from the dark. It depends how it has been taken but usually it should not contains any autofluorescence.<br>\nThe final equation is then:<br>\n<code>A[x,y] = (I[x,y] \u2013 B[x,y]) / (F[x,y] \u2013 D[x,y]) </code>\n</li>\n</ul>\n<p>keep in mind that this is true only if other parameters are equals (same filters, same integration time,\u2026)</p>", "<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><strong>2.</strong> Deconvolution seems to have a number of potential pitfalls, especially if used in conjunction with quantitative image analysis.5</p>\n<p><em><strong>2.a.</strong></em> I was wondering at the frequency that people measure their PSF for deconvolution. The book Microscope Image Processing Second Edition 2022, implies that experimental PSF for deconvolution should be tailored to and measured prior to the experiment that it will be used on.10 I was wondering if cores actually measure at this frequency for deconvolution or if they gather the PSF weekly, or monthly instead?</p>\n</blockquote>\n</aside>\n<p>First you can use a simulated PSF  such as the one created by PSFGenerator for wide field fluorescence.<br>\nThere is 2 main reasons why the actual PSF deviate from the theoretical one:</p>\n<ul>\n<li>static aberration due to you microscope/objective , that should evolve smoothly with time and in a quite well controlled environment and probalbly making a psf from time to time is sufficient to control this part</li>\n<li>aberration due to the oil/coverslip/sample. The main aberration is the spherical aberration due to refractive index mismatch between oil/coverslip/mounting medium. This can be mitigated using objective correcting collar but never perfectly. Keep in mind that the oil refractive index vary with temperature and as consequence the spherical aberration is vquite sensitive to the temperature and may vary during quite rapidly with time.</li>\n</ul>\n<p>If you need precise psf you will probably need to rely on a either beads in your medium or autocalibration procedure such as blind deconvolution (with EpiDemic) or PSF calibration as in [ Jizhou Li, Feng Xue, Fuyang Qu, Yi-Ping Ho, and Thierry Blu, \u201cOn-the-fly estimation of a microscopy point spread function,\u201d Opt. Express <strong>26</strong>, 26120-26133 (2018)]</p>\n<aside class=\"quote no-group\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.b.</strong></em> Assuming that a measured PSF is on hand, and a 3D image is collected what deconvolution algorithm would be recommended for these three situations:<br>\ni. high signal, low noise<br>\nii. high signal, high noise<br>\niii. low signal, low noise.</p>\n</blockquote>\n</aside>\n<p>I think that an algorithm that is efficient in the worse condition may be also efficient in more favorable solution so there must a single algorithm fits all for your situation. However in very favorable case (like case i) crude algorithm like  Richardson Lucy may work well so you can you whatever toolbox you want as most of them implement RL algorithm.</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"creativerror404\" data-post=\"1\" data-topic=\"74820\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/creativerror404/40/53436_2.png\" class=\"avatar\"> Kali Woods:</div>\n<blockquote>\n<p><em><strong>2.c.</strong></em> I wanted to double check that the deconvolution follows after flat-field correction/background subtraction. Are there any other common pre-processing steps people perform prior to deconvolution? I know that deconvolution can increase the impact of artifacts and noise, does anyone perform deconvolution post noise removal?<br>\n[\u2026]<br>\n<strong>4.</strong> Lastly, double checking that my ordering of Flatfield/background subtraction, Noise removal, and Deconvolution is correct. Does anyone perform any other standard quality control methodologies that limit batch effects between samples that I\u2019m missing?<br>\ni. Copy of image raw<br>\nii. Flatfield correction<br>\niii. Noise removal<br>\niv. Deconvolution<br>\nvi. Pre-processing</p>\n</blockquote>\n</aside>\n<p>Never never never, perform noise removal before deconvolution. Doing so you will introduce spatial correlation that will screw deconvolution algorithm.</p>"], "78917": ["<p>It must be taken into account that:</p>\n<p>The tubes can have different sizes, shapes and thicknesses</p>\n<p>The main objective is to automate the process</p>", "<p>Hi Tomas,</p>\n<p>Check here: <a href=\"https://imagej.net/software/fiji/downloads\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Fiji Downloads</a><br>\nHere some background &amp; tutorials: <a href=\"https://imagej.nih.gov/ij/docs/examples/index.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Tutorials and Examples</a></p>\n<p>Your question is somewhat broad still - check this video to get going: <a href=\"https://www.youtube.com/watch?v=FiFwxoxOmNo\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ImageJ Analysis: Length Measurement, Area Measurement and Thresholding - YouTube</a><br>\nAnd some more: <a href=\"https://www.youtube.com/@manchesterbioimaging/videos\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Manchester Bioimaging - YouTube</a></p>\n<p>Aside from ImageJ / Fiji there are quite some open source options (<a href=\"https://imagescience.org/software/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ImageScience.Org - Software</a>), though I think Fiji will be good for what you need. Both the image processing as well as automation.</p>", "<p>Hi Danielle</p>\n<p>Thanks!!!</p>\n<p>Im going to try with Fiji</p>"], "78919": ["<p>I am new to Elephant and Mastodon (but liking them very much already).</p>\n<p>One unexpected situation I\u2019d see is the output of the prediction (Elephant &gt; Detection &gt; Predict Spots) being visible at a smaller scale (but shape is preserved) compare to the BDV display. I have attached a screenshot of the BDV here.</p>\n<p>To reproduce this observation:</p>\n<ol>\n<li>Mastodon &gt; open h5/xml</li>\n<li>Plugins &gt; Elephant &gt; Detection &gt; Reset Detection Model</li>\n<li>Initialize the model with \u201cVersatile\u201d pre-trained model or default settings</li>\n<li>Plugins &gt; Elephant &gt; Detection &gt; Predict Spots</li>\n</ol>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548.png\" data-download-href=\"/uploads/short-url/rYLdWjSGAynXnNrGyBBonjwSdCM.png?dl=1\" title=\"Elephant_PredictSpots\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548_2_581x499.png\" alt=\"Elephant_PredictSpots\" data-base62-sha1=\"rYLdWjSGAynXnNrGyBBonjwSdCM\" width=\"581\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548_2_581x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c419095dcbf94ce1ea9901643cea91ce10c9f548.png 2x\" data-dominant-color=\"262626\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Elephant_PredictSpots</span><span class=\"informations\">656\u00d7564 81.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The image file used here is a 3D + time image stack saved in h5/xml pair. The xml file contains the XYZ size and unit. The Elephant docker was installed (on Linux) before Mastodon beta-26 was released. Elephant client is installed on another computer with the latest Fiji/ImageJ (versions below).</p>\n<p>May I know if I have missed a step here?</p>\n<p>Thank you very much, Catherine</p>\n<p>Versions:<br>\nJava 1.8.0_322 (64-bit)<br>\nMastodon beta-26<br>\nElephant (updated 0.4.2)</p>"], "78922": ["<p>Hi everyone,</p>\n<p>Does anybody knows a way to select multiple ROIs by clicking on them or by tracing a selection area around them on an image, and then get the indexes (in the ROI Manager) of the selected ROIs?<br>\nI forgot to precise that i would like to do it in a *.ijm macro.</p>\n<p>I found a way to do it on a single ROI, but I\u2019m stuck with the \u201cmultiple\u201d selection, which is actually essential for my problem.</p>\n<p>To complete my problem if it was necessary to understand the challenge: I use Cellpose to detect objects on images and get measurements on them, which are sent in an excel file. But I also detect false positives, which are in fact just background with noise in it. But that\u2019s not the problem, because I developed a little tool that loads the ROIs from Cellpose onto the image; then I can click on a false positive ROI, and the tool sends me the ROI index in a text document. Then I just have to retrieve the list of objects to neglect in the excel file. The real problem is that sometime I have to select a lot of small artefactual ROIs, and this is REALLY boring/complicated/time consuming, depending on the case. That\u2019s why it would be very useful to be able to delineate several ROIs and get all their indexes at the same time.<br>\nI hope someone could help me with this.</p>\n<p>Thank you!!</p>", "<p>Hi <a class=\"mention\" href=\"/u/jpolentes\">@JPolentes</a></p>\n<p>Just a thought as there is no ROI filtering according to criteria as far as I am aware of - happy to stand corrected.<br>\nConvert ROIs/instance mask to binary mask, use particle analyzer to filter by size and generate new ROIs.</p>\n<p>Also, see this post: <a href=\"https://forum.image.sc/t/fiji-roi-filtering/25973\" class=\"inline-onebox\">Fiji ROI filtering</a></p>\n<p>Does this help at all?</p>\n<p>Best wishes,<br>\nMarie</p>", "<p>hi <a class=\"mention\" href=\"/u/marie-nkaefer\">@Marie-nkaefer</a></p>\n<p>I thought of filtering the objects by size to eliminate small false positives, but in fact I also have small real objects sometimes. So this approach is not the right one for me. In fact, there is no point in filtering the objects, and what I would like to know is if there is any code or maybe a plugin that allows us to draw a boundary around multiple ROIs previously detected on an image. We can do this on a single ROI by double-clicking on it. This way, the ROI is selected on the image, and the corresponding ROI is also selected in the ROI manager. I would like to be able to do the same thing with several ROIs at the same time, and get their indexes in the ROI manager in one step.</p>", "<p>Hi <a class=\"mention\" href=\"/u/jpolentes\">@JPolentes</a></p>\n<p>try the solution marked in this thread: <a href=\"https://forum.image.sc/t/select-and-measure-multiple-rois-within-the-bigger-roi/40290/4\" class=\"inline-onebox\">Select and measure multiple ROIs within the bigger ROI - #4 by Research_Associate</a></p>\n<p>Best wishes,<br>\nMarie</p>"], "78924": ["<p>Hi,</p>\n<p>Thanks <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a> for bringing a super quick pixel classifier to napari!</p>\n<p>I have following issue: I have relatively big 3D image data ~(1100, 4090,4090) where I want to perform semantic pixel classification on. This is of course way to big to fit on any GPU. That\u2019s why I tried to use dask chunking to only load parts of the image, but I want to cover still a meaningful area, as I expect to have intensity differences along Z at from the sides to center. How would I approach such a task using<br>\n<span class=\"hashtag\">#napari-accelerated-pixel-and-object-classification</span>.<br>\nI already got a way around using relatively strong downsampling and a CPU based pixel classifier, but I am not quite happy with it, as the downsampling leads to resolution issues. Additionally, I will have to do bigger images in the future, so I think I need to figure something out.</p>\n<p>How does Dask interact with openCL? What would be the best way to chunk my data? Sorry, I am a little bit lost here.</p>\n<p>Best,</p>\n<p>Malte</p>", "<p>Hi <a class=\"mention\" href=\"/u/maltemederacke\">@MalteMederacke</a> ,</p>\n<p>soooo, without having seen your data, one thing I could think of would be the following: You say that you have your image data as a dask array, right? You could <a href=\"https://docs.dask.org/en/stable/generated/dask.array.rechunk.html\" rel=\"noopener nofollow ugc\">re-chunk</a>  the array into blocks of a given size (in MB) which you could set that it definitely fits into your GPU.</p>\n<p>As for training the classifier, you can probably load dask arrays into napari natively and create an annotation. You could then use the annotation to train the classifier from code. When it comes to applying the classifier, you could write yourself a little helper function that applies the classifier to each chunk in your array.</p>\n<p>I found an example on how to do this <a href=\"https://github.com/dask/dask/issues/7589\" rel=\"noopener nofollow ugc\">here</a>.</p>\n<p>Hope this helps!</p>\n<p>PS: As for how the GPU is used (According to the <a href=\"https://docs.dask.org/en/stable/gpu.html\" rel=\"noopener nofollow ugc\">dask documentation</a>):  <em>\u201cDask doesn\u2019t need to know that these functions use GPUs. It just runs Python functions. Whether or not those Python functions use a GPU is orthogonal to Dask. It will work regardless.\u201d</em> Fingers crossed <img src=\"https://emoji.discourse-cdn.com/twitter/+1.png?v=12\" title=\":+1:\" class=\"emoji\" alt=\":+1:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/maltemederacke\">@MalteMederacke</a> ,</p>\n<p>I\u2019m just adding some information to <a class=\"mention\" href=\"/u/el_pollo_diablo\">@EL_Pollo_Diablo</a>\u2019s excellent advice.</p>\n<p>I think rechunking is not necessary as you don\u2019t have chunks in the first place. I recommend processing the image tile-by-tile with overlap using dask\u2019s <code>map_overlap</code> (<a href=\"https://docs.dask.org/en/stable/generated/dask.array.map_overlap.html\">documentation</a>).</p>\n<aside class=\"quote no-group\" data-username=\"MalteMederacke\" data-post=\"1\" data-topic=\"78924\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/maltemederacke/40/52522_2.png\" class=\"avatar\"> Malte Mederacke:</div>\n<blockquote>\n<p>What would be the best way to chunk my data?</p>\n</blockquote>\n</aside>\n<p>Use a chunk size that fits into your GPU memory as many times as features you process (plus one, the result). Note: one voxel is four bytes large in this context. Consider testing it on <a href=\"https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification\">napari-apoc</a> interactively with a small image, which has a memory usage estimator (7):</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960.png\" data-download-href=\"/uploads/short-url/1UdPqffwZIAU04nIBLgZjNjsqPK.png?dl=1\" title=\"\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960_2_301x499.png\" alt=\"\" data-base62-sha1=\"1UdPqffwZIAU04nIBLgZjNjsqPK\" role=\"presentation\" width=\"301\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960_2_301x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d5d2c4b95f2a7504cf6901a4bf020d6e8d4b960.png 2x\" data-dominant-color=\"40454B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\"></span><span class=\"informations\">420\u00d7696 83.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<aside class=\"quote no-group\" data-username=\"MalteMederacke\" data-post=\"1\" data-topic=\"78924\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/maltemederacke/40/52522_2.png\" class=\"avatar\"> Malte Mederacke:</div>\n<blockquote>\n<p>How does Dask interact with openCL?</p>\n</blockquote>\n</aside>\n<p>Dask may not care, however, the GPU might have hickups. If you use <a href=\"https://github.com/haesleinhuepf/apoc\">APOC</a> which is based on <a href=\"https://github.com/clEsperanto/pyclesperanto_prototype\">clesperanto</a>, I recommend calling this code snippet to prevent issues with multi-threading (<a href=\"https://github.com/clEsperanto/pyclesperanto_prototype/issues/163\">read why</a>):</p>\n<pre><code class=\"lang-auto\">import pyclesperanto_prototype as cle\ncle.set_wait_for_kernel_finish(True)\n</code></pre>\n<p>You find a notebook demonstrating the entire procedure here:<br>\n<a href=\"https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiling_images_with_overlap.html\">https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiling_images_with_overlap.html</a></p>\n<p>Let us know if this works for you!</p>\n<p>Best,<br>\nRobert</p>", "<p>Hey <a class=\"mention\" href=\"/u/el_pollo_diablo\">@EL_Pollo_Diablo</a>, hey <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a>,</p>\n<p>thanks for replying so fast! That was exactly the advise I am looking for, so I will try now my best to make it work. I will come back to you if I made it work and let you know how it went. Thanks for giving me guidance!</p>\n<p>Best,</p>\n<p>Malte</p>"], "78925": ["<p>Hi,<br>\nI trained stardist model and it work wonders in napari and imageJ plugins but when I used in python script it does very bad job of segmenting cells. I believe its to do with normalisation but I have tried everything but can\u2019t fix the issue.</p>\n<p>Details: Model is trained on RGB images [\u2018YXC\u2019] . I use it to predict RBG images and tried both type of normalisation</p>\n<pre><code class=\"lang-auto\">def get_stardist_model(model_name='stardist_model',directory= './models/'):\n    model = StarDist2D(None, name=model_name, basedir=directory)\n    return model\n\ndef stardist_predict(img):       \n    model = get_stardist_model()\n     #axis_norm = (0,1) #normalise independently \n    axis_norm = (0,1,2) #normalise jointly\n    img_norm = normalize(img, 1,99.8, axis=axis_norm)\n    labels, details = model.predict_instances(img_norm)\n    return labels, details\n</code></pre>\n<p>Can anyone help me figure the issue please?</p>\n<p>regards<br>\nAtif</p>"], "78931": ["<p>Hi everyone</p>\n<p>I\u2019m working with HES images, I used cell detection to get all cells in an area (see image) . My next step is to subclassify all detections according some features i.e. Nucleus Circularity.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88.jpeg\" data-download-href=\"/uploads/short-url/ip31MVcE9Ak4SM8h0VZGC9DCgje.jpeg?dl=1\" title=\"Capture.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88_2_662x500.jpeg\" alt=\"Capture.PNG\" data-base62-sha1=\"ip31MVcE9Ak4SM8h0VZGC9DCgje\" width=\"662\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88_2_662x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88_2_993x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/80fc009ce088ab37fc256f23099d0f43d0dcda88.jpeg 2x\" data-dominant-color=\"8A5574\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture.PNG</span><span class=\"informations\">1118\u00d7844 284 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I create this piece of code</p>\n<pre><code class=\"lang-auto\">selectObjectsByClassification(\"area\");\nrunPlugin('qupath.imagej.detect.cells.WatershedCellDetection', '{\"detectionImageBrightfield\":\"Hematoxylin OD\",\"requestedPixelSizeMicrons\":0.5,\"backgroundRadiusMicrons\":8.0,\"backgroundByReconstruction\":true,\"medianRadiusMicrons\":0.0,\"sigmaMicrons\":1.5,\"minAreaMicrons\":10.0,\"maxAreaMicrons\":400.0,\"threshold\":0.1,\"maxBackground\":2.0,\"watershedPostProcess\":true,\"cellExpansionMicrons\":0.0,\"includeNuclei\":true,\"smoothBoundaries\":true,\"makeMeasurements\":true}')\nlymph = getPathClass('lymph') # the subclass that i want to create \nignore = getPathClass('ignore') # cells to ignore \nfor (cell in getCellObjects()) {\n    m1 = measurement(cell, 'Nucleus: Circularity')\n    if(m1 &gt; 0.85)\n        cell.setPathClass(linfo)\n    else\n        cell.setPathClass(ignore)\n    }\nfireHierarchyUpdate()\n</code></pre>\n<p>but I don\u2019t have any results\u2026 anyone could help me with that please\u2026</p>", "<p>It would help if you shared the error message, but one thing that should be an error is that <code>linfo</code> in your if statement is not defined. I suspect that should be <code>lymph</code></p>", "<p>hi <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> ,<br>\nThanks for your comment, I would like to share with you the error message , but I don\u2019t have one\u2026 the script run but I don\u2019t get results \u2026 and as you say is  a mistake at the moment to copy the lympho class the original was exactly the same to predefine variable</p>", "<aside class=\"quote no-group\" data-username=\"Luis_Cano\" data-post=\"3\" data-topic=\"78931\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/luis_cano/40/50413_2.png\" class=\"avatar\"> Luis Cano:</div>\n<blockquote>\n<p>don\u2019t get results</p>\n</blockquote>\n</aside>\n<p>What results are you expecting? It looks like you currently don\u2019t have any cell objects (just nuclei since you didn\u2019t use a cell expansion), so the second part of your script should not do anything.</p>\n<p>You could try getDetectionObjects() instead since you are not using cells (which need both a cytoplasm and nucleus).</p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> maybe it would be worth having cell detection create cell objects even when there is no cell expansion by using the same ROI for the cytoplasm and nucleus? Seems like this has come up before and confuses people. Alternatively, a warning within the Cell Detection dialog saying that 0 expansion will not create cell objects.</p>", "<p><a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> \u2026 you are a genius man! that was the answer, I changed and all work!<br>\nThank you so much!!!</p>\n<p>P.D: Do you have a extensive documentation or maybe manual about qupath methods and functions? I would like to explore in deep the scripting part</p>\n<p>best</p>\n<p>Luis</p>", "<p>Main resources aside from the primary readthedocs page and all of the forum post examples are probably<br>\nPete\u2019s gists as examples: <a href=\"https://gist.github.com/petebankhead\" class=\"inline-onebox\">petebankhead\u2019s gists \u00b7 GitHub</a><br>\nThe javadocs which I think are linked through the scripting interface now <a href=\"https://qupath.github.io/javadoc/docs/\" class=\"inline-onebox\">Overview (QuPath 0.4.0)</a><br>\nAnd a kind of guide/example thing here <a href=\"https://www.imagescientist.com/image-analysis#scripting\" class=\"inline-onebox\">Image Analysis Topics \u2014 Image Scientist</a></p>\n<p>Also some examples of older code here, but most of them are not as relevant these days <a href=\"https://gist.github.com/Svidro\" class=\"inline-onebox\">Svidro\u2019s gists \u00b7 GitHub</a></p>"], "78933": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad.png\" data-download-href=\"/uploads/short-url/gilyGCekyUbY6uq8Fnk1wiz3bK5.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad_2_690x362.png\" alt=\"image\" data-base62-sha1=\"gilyGCekyUbY6uq8Fnk1wiz3bK5\" width=\"690\" height=\"362\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad_2_690x362.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad_2_1035x543.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/7235bc4bb4a19dcfdc4fb7242e0bcd6c9d5e4aad.png 2x\" data-dominant-color=\"F6C2C3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1179\u00d7620 56.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nI wanted to point out to <a class=\"mention-group notify\" href=\"/groups/team\">@team</a> the linking of an unrelated OCR product from a freshly created account into a post about reading CZI images. It also makes use of text that is forum relevant like mentioning Fiji.</p>\n<p>Sigh. Might be chatbot supported to target specific forums\u2026</p>\n<p>Not so cheers,<br>\nMike</p>", "<p>We will fight and win <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThanks for the heads up. I have seen and banned similar things recently, and now that we are aware we will be more cautious.</p>\n<p>BTW look at the results: our forum is one of the cleanest and most respectful. On top of being super useful.</p>", "<aside class=\"quote group-team\" data-username=\"tinevez\" data-post=\"2\" data-topic=\"78933\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tinevez/40/21_2.png\" class=\"avatar\"> Jean-Yves Tinevez:</div>\n<blockquote>\n<p>We will fight and win <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n</blockquote>\n</aside>\n<p>I see an AI cold war coming\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/joy.png?v=12\" title=\":joy:\" class=\"emoji\" alt=\":joy:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Many bots seem to like replying to old threads with many views - this is a signal I will try to look out for ( <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> 's example above looks consistent with this ).</p>\n<p>We might consider more actively discouraging replies to very long topics and prefer starting new topics that link to related ones. This could remove the bot signal above, but would keep the forum a little more tidy. (Edit: this hasn\u2019t really been a big issue outside a few crazy long threads)</p>", "<aside class=\"quote group-team\" data-username=\"tinevez\" data-post=\"2\" data-topic=\"78933\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tinevez/40/21_2.png\" class=\"avatar\"> Jean-Yves Tinevez:</div>\n<blockquote>\n<p>BTW look at the results: our forum is one of the cleanest and most respectful. On top of being super useful.</p>\n</blockquote>\n</aside>\n<p>On that note, I have bumped a few more people up to Leader status when I see them answering a lot of questions in particular topics, so that they can do things like add tags, edit formatting (code especially, those dastardly curly quotes), split topics, etc. The last two yesterday were <a class=\"mention\" href=\"/u/psobolewskiphd\">@psobolewskiPhD</a> and <a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a> for Napari and DeepLabCut respectively.</p>\n<p>Not only does it give them a bit more utility when providing help, hopefully it makes them feel appreciated as well.</p>\n<p>Cheers,<br>\nMike</p>"], "78934": ["<p>Hi,</p>\n<p>When trying to add NeuroCyto-LUTs through the Fiji updater, I encounter an HTTP response code 403 for certain LUTs.</p>\n<p>If I open my browser and go to <a href=\"https://sites.imagej.net/NeuroCyto-LUTs/luts/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Index of /NeuroCyto-LUTs/luts</a>, I\u2019m able to manually download the file, but I didn\u2019t want to do this multiple times.</p>\n<p><strong>I\u2019ve found that a couple iterations of closing Fiji and attempting to update NeuroCyto-LUTs through the Fiji updater will eventually get them all.</strong></p>\n<p>I hope this helps someone who encounters a similar situation.</p>\n<p>See below for the first problematic file\u2019s error messages.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4279ac060e97f7428271c3fd7c4587cdd997a04b.png\" alt=\"Fiji-updater-403\" data-base62-sha1=\"9u4c1SAFbkeDAzWLwbM4D2pxrV9\" width=\"536\" height=\"135\"></p>\n<pre><code class=\"lang-auto\">[ERROR] java.io.IOException: Server returned HTTP response code: 403 for URL: https://sites.imagej.net/NeuroCyto-LUTs/luts/JDM_Grays%20g=0.25.lut-20220424170324\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat sun.net.www.protocol.http.HttpURLConnection$10.run(HttpURLConnection.java:1967)\n\tat sun.net.www.protocol.http.HttpURLConnection$10.run(HttpURLConnection.java:1962)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1961)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1521)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1505)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:268)\n\tat net.imagej.updater.util.Downloader.download(Downloader.java:117)\n\tat net.imagej.updater.util.Downloader.start(Downloader.java:98)\n\tat net.imagej.updater.Installer.start(Installer.java:154)\n\tat net.imagej.ui.swing.updater.UpdaterFrame.install(UpdaterFrame.java:627)\n\tat net.imagej.ui.swing.updater.UpdaterFrame$13.run(UpdaterFrame.java:564)\nCaused by: java.io.IOException: Server returned HTTP response code: 403 for URL: https://sites.imagej.net/NeuroCyto-LUTs/luts/JDM_Grays%20g=0.25.lut-20220424170324\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1917)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1505)\n\tat sun.net.www.protocol.http.HttpURLConnection.getHeaderField(HttpURLConnection.java:3078)\n\tat java.net.HttpURLConnection.getHeaderFieldDate(HttpURLConnection.java:552)\n\tat java.net.URLConnection.getLastModified(URLConnection.java:559)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getLastModified(HttpsURLConnectionImpl.java:436)\n\tat net.imagej.updater.util.Downloader.download(Downloader.java:108)\n\t... 4 more\n</code></pre>"], "78936": ["<p>Hello,<br>\nI have a question regarding how to open files in a macro, especially if the path and filename are concatenated from previously defined substrings.<br>\nSpecifically, my files are located in a folder with two subfolders called images_cropped and masks_cropped. I would like to open one file from images_cropped and automatically open the corresponding file with the same name from masks_cropped.<br>\nFirst, I am extracting the image name (without the file ending) and the path to my file in images_cropped. I believe I have succeeded in this. I am however stuck in the subsequent step. My idea is to concatenate several substrings to create the path to the file in masks_cropped. However, I receive the error:</p>\n<blockquote>\n<p>Error:\t\t\u2018)\u2019 expected in line 10:<br>\nopen ( \"folder_mask + \u201cmasks_cropped/\u201d + name + \". png \u201c\u201d ) ;</p>\n</blockquote>\n<p>Which I understand to mean that a parenthesis is not closed somewhere, but I can for the life of me not figure out where.<br>\nThe macro runs fine if I replace this line with the path to an exemplary picture.</p>\n<p>My code looks like this:</p>\n<blockquote>\n<p>//extract information about the file<br>\nfolder = getDirectory(\u201cimage\u201d);<br>\nfile = getTitle;<br>\nend = lengthOf(file) - 4;<br>\nname = substring(file, 0, end);</p>\n</blockquote>\n<p>-4 to remove the last 4 characters, i.e. the file ending</p>\n<blockquote>\n<p>//open image for masking<br>\nend_mask = lengthOf(folder) -15;<br>\nfolder_mask = substring(folder, 0, end_mask);<br>\nopen(\u201cfolder_mask + \u201cmasks_cropped/\u201d + name + \u201c.png\u201d\u201d);</p>\n</blockquote>\n<p>I remove the last 15 characters as my files are located in the folder images_cropped and I want to move one down in the directory structure. There may be more elegant solutions, but it works for me so far.</p>\n<blockquote>\n<p>//create mask<br>\nselectWindow(name + \u201c.png\u201d);<br>\nrun(\u201cCreate Selection\u201d);<br>\nrun(\u201cCreate Mask\u201d);</p>\n<p>//apply mask to other image<br>\nselectWindow(name + \u201c.jpg\u201d);<br>\nrun(\u201cRestore Selection\u201d);<br>\nrun(\u201cInvert\u201d);<br>\nsetBackgroundColor(0, 0, 0);<br>\nrun(\u201cClear\u201d, \u201cslice\u201d);<br>\nrun(\u201cSelect None\u201d);<br>\nsaveAs(\u201cBmp\u201d, folder + file + \u201cmasked.bmp\u201d);</p>\n</blockquote>", "<aside class=\"quote no-group\" data-username=\"Mareike_Daubert\" data-post=\"1\" data-topic=\"78936\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mareike_daubert/40/67083_2.png\" class=\"avatar\"> Mareike Daubert:</div>\n<blockquote>\n<p>open ( \"folder_mask + \u201cmasks_cropped/\u201d + name + \". png \u201c\u201d ) ;</p>\n</blockquote>\n</aside>\n<p>Despite the misleading error, it looks like it might actually be the quotes, not the parenthesis? Try:</p>\n<pre><code class=\"lang-auto\">open(folder_mask+\"masks_cropped/\"+name+\".png\"); \n</code></pre>", "<p>Hello,<br>\nthank you, that solved it <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I appreciate that this question is solved, but you may also find the <a href=\"https://imagej.nih.gov/ij/developer/macro/functions.html#replace\" rel=\"noopener nofollow ugc\"><code>replace</code> function</a> useful. Something like:</p>\n<pre><code class=\"lang-auto\">filePath=\"path/folder_mask/images_cropped/file.png\") ;\nprint(filePath);\nnewPath=replace(filePath,\"images_cropped\",\"masks_cropped\");\nprint(newPath);\n</code></pre>\n<p>The advantage here being that if your filenames in <code>images_cropped</code> and <code>masks_cropped</code> are the same, then you don\u2019t have to rebuild the string based on a hardcoded length in <code>substring</code>.</p>\n<p>Hope that helps!</p>"], "78938": ["<p>Hi all,<br>\nI am quite new to QuPath.<br>\nI would like to know wether exist an automated  way to get annotations from an image which has pixels values in range [0, 255]. At each different pixel values should correspond a different label.</p>", "<p>How large are the images? That sounds like a fairly standard import process (see some of the cellpose import scripts for cells where each object created is based on a set of pixels with a specific integer value) if the images are small, otherwise you might need something like</p>\n<aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"71579\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/smcardle/40/17938_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/script-for-generating-double-threshold-classifier/71579\">Script for generating double threshold classifier</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hi All, \nHere\u2019s a script I wrote that will hopefully be useful to some people. \nNormally, if I need to find double-positive regions, I run two pixel classifiers, then use Java geometry functions to find the object intersections. This works, but if there are many small detections, it can be quite slow. The script below uses ImageOps to apply a threshold to two different channels and writes it as a single pixel classifier. Then, the \u201ccolocalized\u201d objects can be created directly through normal QuPa\u2026\n  </blockquote>\n</aside>\n", "<p>This might also be relevant: <a href=\"https://qupath.readthedocs.io/en/0.4/docs/advanced/exporting_annotations.html#binary-labeled-images\" class=\"inline-onebox\">Exporting annotations \u2014 QuPath 0.4.3 documentation</a></p>"], "78939": ["<p>I\u2019m trying to run the following script which was barely modified from the one given to me in the QuPath documentation. I\u2019m getting the following error. I\u2019m new to image processing so please forgive me if there is an obvious answer.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8.png\" data-download-href=\"/uploads/short-url/r9x7aS3jsgXi54nl7wrD67a2bwI.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_690x291.png\" alt=\"image\" data-base62-sha1=\"r9x7aS3jsgXi54nl7wrD67a2bwI\" width=\"690\" height=\"291\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_690x291.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_1035x436.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be4e99324bdc2b6a5d63e3948106252c219c8bc8_2_1380x582.png 2x\" data-dominant-color=\"F9F9F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1755\u00d7741 53.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Recent post suggest the stardist script is working assuming you have the right versions of QuPath and the StarDist plugin. <a href=\"https://forum.image.sc/t/qupath-stardist-global-normalisation-for-deconvolved-channels-in-v-0-4-x/78814/6\" class=\"inline-onebox\">QuPath StarDist global normalisation for deconvolved channels in v.0.4.x - #6 by ym.lim</a><br>\nCan you confirm which versions you have?</p>\n<p>Also not sure if some of the error is missing as I don\u2019t see a line number listed. Always better to post the errors as text, not a screenshot since they can\u2019t be word searched or copied.</p>"], "78943": ["<p>I am looking to purchase a new spinning disk system to replace our aging CV7000. Does anyone have a favorite system to work with 384-well formats?</p>\n<p>Thanks</p>", "<p>Hi <a class=\"mention\" href=\"/u/ekatz\">@ekatz</a></p>\n<p>I cannot help you here, as I am not knowledgeable about hardware, however I am interested in learning who is knowledgeable about Spinning Disk systems, as I have long standing questions regarding the best way to calculate theoretical PSFs for Spinning disk, so just really following the question, as a side note you may also want to try the micro-forum <a href=\"https://forum.microlist.org/\">https://forum.microlist.org/</a> related forum.</p>\n<p>Brian</p>", "<p>Fantastic. I will post there, too.</p>", "<p>Additionally, the confocal listserv gets quite a lot of activity if you have not already tried there. And I think a wider variety of people tend to check it, though I\u2019d prefer the microlist forum got more action.<br>\n<a href=\"https://lists.umn.edu/cgi-bin/wa?SUBED1=CONFOCALMICROSCOPY&amp;A=1\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://lists.umn.edu/cgi-bin/wa?SUBED1=CONFOCALMICROSCOPY&amp;A=1</a></p>"], "78945": ["<p>Hi all,<br>\nI just bought a monochrome and color 5 Mp Mightex CMOS cameras. They are very inexpensive and I was hoping they would work with MM. So far I can\u2019t get them to work with the Mightex driver that is included with MM 1.4.22 (Win10 64-bit).</p>\n<p>Mightex ships a utility app that does successfully talk to the camera, so I know I have the Mightex drivers installed properly. If anyone knows anything more, I\u2019d be grateful!</p>\n<p>Cheers,<br>\nJeff Hardin<br>\nDept. of Integrative Biology<br>\nUniv. of Wisconsin-Madison</p>"], "78946": ["<p>I am trying to run the reaching-mackenzie example code given in deeplabcut repo. It worked fine for 10000 iterations, then changed the number of iterations to 300000 and tried rerunning it. The code I used is as follows :</p>\n<pre><code class=\"lang-auto\">path_config_file = r'D:\\000_DeepLabCut\\D1\\DeepLabCut\\examples\\Reaching-Mackenzie-2018-08-30\\config.yaml'\ndeeplabcut.create_training_dataset(path_config_file,Shuffles=[1])\n\n# initiate training\ndeeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)\n</code></pre>\n<p>Training continued for several thousand iterations but then got interrupted throwing error as follows :</p>\n<pre><code class=\"lang-auto\">InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values\n\t [[{{node train_op/CheckNumerics}}]]\n\nDuring handling of the above exception, another exception occurred:\n</code></pre>\n<p>And further also mentioned details as :</p>\n<pre><code class=\"lang-auto\">\nInvalidArgumentError: Graph execution error:\n\nDetected at node 'train_op/CheckNumerics' defined at (most recent call last):\n</code></pre>\n<p>I am adding an image of the complete error with this for further details. What I found confusing is that, it worked perfectly fine for the first time when I run it with 10000 iterations but then threw errors when I tried to rerun the same jupyter notebook.</p>\n<p>I am adding complete error output below  :</p>\n<pre><code class=\"lang-auto\">&gt; --------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378, in BaseSession._do_call(self, fn, *args)\n   1377 try:\n-&gt; 1378   return fn(*args)\n   1379 except errors.OpError as e:\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361, in BaseSession._do_run.&lt;locals&gt;._run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\n   1360 self._extend_graph()\n-&gt; 1361 return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n   1362                                 target_list, run_metadata)\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454, in BaseSession._call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\n   1452 def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n   1453                         run_metadata):\n-&gt; 1454   return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n   1455                                           fetch_list, target_list,\n   1456                                           run_metadata)\n\nInvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values\n\t [[{{node train_op/CheckNumerics}}]]\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidArgumentError                      Traceback (most recent call last)\nCell In[2], line 9\n      4 deeplabcut.create_training_dataset(path_config_file,Shuffles=[1])\n      6 # initiates training. \n      7 # you may edit the number of iterations using 'maxiters' parameter\n----&gt; 9 deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py:223, in train_network(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\n    212         train(\n    213             str(poseconfigfile),\n    214             displayiters,\n   (...)\n    219             allow_growth=allow_growth,\n    220         )  # pass on path and file name for pose_cfg.yaml!\n    222 except BaseException as e:\n--&gt; 223     raise e\n    224 finally:\n    225     os.chdir(str(start_path))\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py:212, in train_network(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\n    209         from deeplabcut.pose_estimation_tensorflow.core.train import train\n    211         print(\"Selecting single-animal trainer\")\n--&gt; 212         train(\n    213             str(poseconfigfile),\n    214             displayiters,\n    215             saveiters,\n    216             maxiters,\n    217             max_to_keep=max_snapshots_to_keep,\n    218             keepdeconvweights=keepdeconvweights,\n    219             allow_growth=allow_growth,\n    220         )  # pass on path and file name for pose_cfg.yaml!\n    222 except BaseException as e:\n    223     raise e\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py:283, in train(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\n    280     current_lr = lr_gen.get_lr(it - start_iter)\n    281     lr_dict = {learning_rate: current_lr}\n--&gt; 283 [_, loss_val, summary] = sess.run(\n    284     [train_op, total_loss, merged_summaries], feed_dict=lr_dict\n    285 )\n    286 cum_loss += loss_val\n    287 train_writer.add_summary(summary, it)\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968, in BaseSession.run(self, fetches, feed_dict, options, run_metadata)\n    965 run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None\n    967 try:\n--&gt; 968   result = self._run(None, fetches, feed_dict, options_ptr,\n    969                      run_metadata_ptr)\n    970   if run_metadata:\n    971     proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191, in BaseSession._run(self, handle, fetches, feed_dict, options, run_metadata)\n   1188 # We only want to really perform the run if fetches or targets are provided,\n   1189 # or if the call is a partial run that specifies feeds.\n   1190 if final_fetches or final_targets or (handle and feed_dict_tensor):\n-&gt; 1191   results = self._do_run(handle, final_targets, final_fetches,\n   1192                          feed_dict_tensor, options, run_metadata)\n   1193 else:\n   1194   results = []\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371, in BaseSession._do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1368   return self._call_tf_sessionprun(handle, feed_dict, fetch_list)\n   1370 if handle is None:\n-&gt; 1371   return self._do_call(_run_fn, feeds, fetches, targets, options,\n   1372                        run_metadata)\n   1373 else:\n   1374   return self._do_call(_prun_fn, handle, feeds, fetches)\n\nFile ~\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1397, in BaseSession._do_call(self, fn, *args)\n   1392 if 'only supports NHWC tensor format' in message:\n   1393   message += ('\\nA possible workaround: Try disabling Grappler optimizer'\n   1394               '\\nby modifying the config for creating the session eg.'\n   1395               '\\nsession_config.graph_options.rewrite_options.'\n   1396               'disable_meta_optimizer = True')\n-&gt; 1397 raise type(e)(node_def, op, message)\n\nInvalidArgumentError: Graph execution error:\n\nDetected at node 'train_op/CheckNumerics' defined at (most recent call last):\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in &lt;module&gt;\n      app.launch_new_instance()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\aselvite\\AppData\\Local\\Temp\\ipykernel_18272\\1865657941.py\", line 9, in &lt;module&gt;\n      deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 212, in train_network\n      train(\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 238, in train\n      learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 117, in get_optimizer\n      train_op = slim.learning.create_train_op(loss_op, optimizer)\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\learning.py\", line 436, in create_train_op\n      return training.create_train_op(\n    File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\training\\training.py\", line 468, in create_train_op\n      total_loss = array_ops.check_numerics(total_loss,\nNode: 'train_op/CheckNumerics'\nLossTensor is inf or nan : Tensor had NaN values\n\t [[{{node train_op/CheckNumerics}}]]\n\nOriginal stack trace for 'train_op/CheckNumerics':\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in &lt;module&gt;\n    app.launch_new_instance()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n    app.start()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n    self.io_loop.start()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n    reply_content = await reply_content\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n    res = shell.run_cell(\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n    result = runner(coro)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\aselvite\\AppData\\Local\\Temp\\ipykernel_18272\\1865657941.py\", line 9, in &lt;module&gt;\n    deeplabcut.train_network(path_config_file, shuffle=1, saveiters=5, displayiters=10, maxiters=300000)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 212, in train_network\n    train(\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 238, in train\n    learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 117, in get_optimizer\n    train_op = slim.learning.create_train_op(loss_op, optimizer)\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\learning.py\", line 436, in create_train_op\n    return training.create_train_op(\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\training\\training.py\", line 468, in create_train_op\n    total_loss = array_ops.check_numerics(total_loss,\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1220, in check_numerics\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\aselvite\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3798, in _create_op_internal\n    ret = Operation(\n</code></pre>\n<p>Could you <a class=\"mention\" href=\"/u/konrad_danielewski\">@Konrad_Danielewski</a> please help in this regard, like what is the probable cause of this error and how come it worked at first but threw error in second run.</p>", "<p>It can be corrupted data, wrong tensorflow version, optimizer problem etc.</p>\n<p>Best thing to do would be to run a small project of your own, with your own data. Especially if you\u2019re using  data annotated automatically with the testscript.</p>"], "78954": ["<p>Interested in gaining valuable insights on how to choose the right imaging solution for your Medical / Life science application?</p>\n<p>Join this webinar titled <strong>\u201cDigital Imaging Solutions for Life Science Applications\u201d</strong>.</p>\n<p><a href=\"https://www.e-consystems.com/webinars/digital-imaging-solutions-for-life-science-applications.asp\" rel=\"noopener nofollow ugc\"><strong>REGISTER FOR THE WEBINAR \u00bb</strong></a></p>\n<p>Tuesday, March 28, 2023, 10.00 am \u2013 11.00 am PST</p>\n<p><strong>Key Takeaways</strong></p>\n<ul>\n<li>Role of cameras in life science applications</li>\n<li>5 key factors involved in choosing an imaging solution</li>\n<li>Updates on e-con Systems\u2019 latest camera solutions</li>\n</ul>\n<p>Don\u2019t miss this exclusive opportunity to listen to thought leaders in the embedded vision space!</p>\n<p><a href=\"https://www.e-consystems.com/webinars/digital-imaging-solutions-for-life-science-applications.asp\" rel=\"noopener nofollow ugc\"><strong>SAVE MY SEAT \u00bb</strong></a></p>\n<p><em>Not available on March 28? Don\u2019t worry - you can still register,</em> <em>as they will share the complete webinar recording with all registrants\u2026</em></p>"], "76908": ["<p>We are looking for a research software engineer (100%) to join our interdisciplinary team of imaging scientists from EPFL and clinical practitioners from the CHUV.</p>\n<p><a href=\"https://recruiting.epfl.ch/Vacancies/2804/Description/2\" rel=\"noopener nofollow ugc\">https://recruiting.epfl.ch/Vacancies/2804/Description/2</a></p>\n<p>In this role, you will be responsible for building software tools that enable pathologists to easily but robustly use image-analysis algorithms produced at EPFL for the routine analysis of their patients\u2019 images, and to combine these tool with QuPath, a popular software framework for digital pathology used by thousands of researchers worldwide (<a href=\"https://qupath.github.io\" rel=\"noopener nofollow ugc\">https://qupath.github.io</a>). In collaboration with QuPath\u2019s core software development team, you will also be in charge of building new software extensions to improve the interoperability between QuPath and a selection of clinically-oriented software platforms for digital pathology.</p>\n<p>The position is funded by a Swiss translational grant for personalised medicine (PHRT), for which the EPFL Center for Imaging is leading a multi-partner consortium that aims at accelerating the transfer of advanced imaging software for digital pathology to the clinics.</p>", "<p>Hi,</p>\n<p>The link provided in the job description is not working for me.</p>\n<p>Would you please guide me, how to apply for this position?</p>\n<p>Thank you!</p>"], "78960": ["<p>Dear All</p>\n<p>I am trying to segment objects (vacuoles) fluorescently marked at their periphery and imaged as Z-stacks.</p>\n<p>I\u2019m currently doing this slice-by-slice, but this yields poor results (there is a lot of noise within slices + slice-to-slice variability).</p>\n<p>// Example processing for one slice<br>\nrun(\u201cEnhance Contrast\u201d, \u201csaturated=0.35\u201d);<br>\nrun(\u201cInvert\u201d);<br>\nrun(\u201cSubtract Background\u2026\u201d, \u201crolling=1000 light sliding\u201d);<br>\nmakeRectangle(0, 0, 2048, 2048);<br>\nrun(\u201cSharpen\u201d);<br>\nrun(\u201cGaussian Blur\u2026\u201d, \u201csigma=0.7\u201d);<br>\nrun(\u201cEnhance Contrast\u2026\u201d, \u201csaturated=0.20 normalize\u201d);<br>\nrun(\u201c8-bit\u201d);<br>\nrun(\u201cAuto Local Threshold\u201d, \u201cmethod=Phansalkar radius=15 parameter_1=0.08 parameter_2=0 white\u201d);<br>\nrun(\u201cInvert\u201d);</p>\n<p>I\u2019m wondering about 3D segmentation packages / functions / protocols you would recommend to improve this, or even recommendations that could help me improve the 2D processing.</p>\n<p>I copy below a link to the original data (green channel) + current segmentation (red channel) in a stack.</p>\n<p>Cropped version (512 x 490): <a href=\"https://drive.google.com/file/d/1AlSyynf_GAQ_gfpOzhGjpXxQeWfsqtNv/view?usp=share_link\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">composite_masks-red_original-green_crop.tif - Google Drive</a></p>\n<p>Original stack (2k x 2k): <a href=\"https://drive.google.com/file/d/1Khge0NMWaPDbcWPpFwicb5oc_VzMZ2DU/view?usp=sharing\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">vacuole_s1-100_original_1.tif - Google Drive</a></p>\n<p>Thank you for your help and input</p>\n<p>Sincerely,<br>\nSaurabh<br>\nPhD student,<br>\nWeizmann Institute</p>", "<p>Hi Saurabh,</p>\n<p>I would recommend to <a href=\"https://imagej.net/software/fiji/\">install Fiji</a> and test the <a href=\"https://biovoxxel.github.io/bv3dbox/\">BioVoxxel 3D Box</a>.</p>\n<p>One thing I realized\u2026 could it be that your z-scaling (voxel size in z) is incorrect? It seems to be 312-times smaller than the x/y pixel scaling. If that is incorrectly recorded by your imaging device, any 3D segemtnation will be problematic.<br>\n[image]</p>\n<p>Furthermore, it is in cm which seems odd.</p>\n<p>Here a macro which gave me the objects in 3D while not perfectly separated due to the scaling problem. You will first need to follow all the installation instructions on the pages above to b able to run this.</p>\n<pre><code class=\"lang-auto\">run(\"Enhance Contrast...\", \"saturated=0.01 normalize process_all use\");\nrun(\"Voronoi Threshold Labler (2D/3D)\", \"filtermethod=Gaussian filterradius=3.0 backgroundsubtractionmethod=None backgroundradius=1.0 histogramusage=full thresholdmethod=Li fillholes=2D separationmethod=[Maxima Spheres] spotsigma=7.0 maximaradius=7.0 volumerange=0-Infinity excludeonedges=false outputtype=Labels stackslice=10 applyoncompleteimage=false processonthefly=false\");\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150.jpeg\" data-download-href=\"/uploads/short-url/y24U8k59pSHiYEdGON9DYtV2d7a.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_690x264.jpeg\" alt=\"image\" data-base62-sha1=\"y24U8k59pSHiYEdGON9DYtV2d7a\" width=\"690\" height=\"264\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_690x264.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_1035x396.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/ee8614c2bd4ee1a11e3a11e64879f4caa94e0150_2_1380x528.jpeg 2x\" data-dominant-color=\"2F2B2A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1626\u00d7624 173 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Attached also a GIF showing the objects in 3D with a guessed z-dimension size to enable proper display.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/1447fcd2e90d64aa54c27449da1ab618969e21af.gif\" alt=\"VTL_vacuole_s1-100_original-1-1.tif-1\" data-base62-sha1=\"2TpMusbqgmX566FEg9zHF4vFOFh\" width=\"342\" height=\"342\" class=\"animated\"></p>\n<p>Hope this get\u2019s you kick-started.</p>", "<p>Hi Jan</p>\n<p>Thank you for your suggestions.</p>\n<p>I added the correct image dimensions in image properties as you pointed out. I am not sure why they were not recorded properly in imagej.</p>\n<p>I installed BioVoxxel but when i try to run \u201cVoroni Threshold\u201d, I get the following error.</p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Linux 4.15.0-109-generic; 473MB of 42199MB (1%)</p>\n<p>java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception<br>\nat net.imagej.legacy.LegacyService.runLegacyCompatibleCommand(LegacyService.java:308)<br>\nat net.imagej.legacy.DefaultLegacyHooks.interceptRunPlugIn(DefaultLegacyHooks.java:166)<br>\nat ij.IJ.runPlugIn(IJ.java)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat ij.IJ.run(IJ.java:319)<br>\nat ij.IJ.run(IJ.java:330)<br>\nat ij.macro.Functions.doRun(Functions.java:703)<br>\nat ij.macro.Functions.doFunction(Functions.java:99)<br>\nat ij.macro.Interpreter.doStatement(Interpreter.java:281)<br>\nat ij.macro.Interpreter.doStatements(Interpreter.java:267)<br>\nat ij.macro.Interpreter.run(Interpreter.java:163)<br>\nat ij.macro.Interpreter.run(Interpreter.java:93)<br>\nat ij.macro.Interpreter.run(Interpreter.java:107)<br>\nat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)<br>\nat ij.IJ.runMacro(IJ.java:158)<br>\nat ij.IJ.runMacro(IJ.java:147)<br>\nat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)<br>\nat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)<br>\nat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)<br>\nat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)<br>\nat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)<br>\nat org.scijava.script.ScriptModule.run(ScriptModule.java:164)<br>\nat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)<br>\nat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)<br>\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>\nat java.lang.Thread.run(Thread.java:750)<br>\nCaused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Module threw exception<br>\nat java.util.concurrent.FutureTask.report(FutureTask.java:122)<br>\nat java.util.concurrent.FutureTask.get(FutureTask.java:192)<br>\nat net.imagej.legacy.LegacyService.runLegacyCompatibleCommand(LegacyService.java:304)<br>\n\u2026 30 more<br>\nCaused by: java.lang.RuntimeException: Module threw exception<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:127)<br>\n\u2026 6 more<br>\nCaused by: java.lang.NullPointerException<br>\nat de.biovoxxel.bv3dbox.gui.BV_VoronoiThresholdLabelingGUI.cancel(BV_VoronoiThresholdLabelingGUI.java:409)<br>\nat org.scijava.module.ModuleRunner.cleanupAndBroadcastCancelation(ModuleRunner.java:185)<br>\nat org.scijava.module.ModuleRunner.run(ModuleRunner.java:157)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)<br>\n\u2026 6 more</p>\n<p>I tried with different imagej versions, but the error still persists. I also cannot use any of the other functions from the biovoxxel toolbar from the plugin menu.</p>\n<p>I have not put up your suggestion or the error on the list, wasn\u2019t sure how to, but we can now correspond on the old list.</p>\n<p>Sincerely<br>\nSaurabh</p>", "<p>Hi <a class=\"mention\" href=\"/u/saurabhm\">@saurabhm</a>,</p>\n<p>Hm, strange. Which update sites did you switch on in Fiji?</p>\n<p>You will minimally need to activate the following ones for this to work.</p>\n<ul>\n<li>bv3dbox</li>\n<li>clij</li>\n<li>clij2</li>\n<li>clijx-assistant</li>\n<li>clijx-assistant-extensions</li>\n<li>3D ImageJ Suite</li>\n</ul>\n<p>And standard-wise</p>\n<ul>\n<li>Fiji</li>\n<li>ImageJ</li>\n<li>Java 8</li>\n</ul>\n<p>are also active</p>\n<p>However, the error message does not really point to any of those as far as I can see. You also run exactly the same Fiji version I have. Potentially try to reinstall Fiji again and add those update sites only. Then try to run one of the BioVoxxel functions or the Voronoi Threshold Labeler directly again.</p>\n<p>I tried a fresh Fiji installation with the update and the functions work on my end.</p>"], "78961": ["<p>Hi,</p>\n<p>I\u2019m trying to record some metadata for a multidimensional aquisition to an IJ results table. I have created a beanshell script by hacking together various code I have found online, but it is not working. Can someone please help me figure out what I am doing wrong? Here is my script so far:</p>\n<pre><code class=\"lang-auto\">// Load the Micro-Manager core\nimport org.micromanager.MMStudio;\nimport org.micromanager.acquisition.Acquisition;\nimport org.micromanager.acquisition.MDA;\nimport org.micromanager.utils.ReportingUtils;\nimport ij.measure.ResultsTable;\n\n// Create an instance of the Micro-Manager core\nMMStudio mm = MMStudio.getInstance();\n\n// Set up the acquisition settings\nMDA mda = new MDA();\nmda.numFrames = 25;\nmda.numChannels = 1;\nmda.numSlices = 1;\nmda.intervalMs = 0;\nmda.save = true;\nmda.rootName = \"my_image_stack_\";\nmda.channelNames = new String[] {\"Channel1\"};\n\n// Create a results table to store the time tag data\nResultsTable rt = new ResultsTable();\nrt.setPrecision(0);\n\n// Start the acquisition\nmm.acquisitions().runAcquisition(mda, true);\n\n// Loop through each frame and add the time tag to the results table\nfor (int i = 0; i &lt; mda.numFrames; i++) {\n    double timeTag = mm.acquisitions().getTimeStamp(i);\n    rt.incrementCounter();\n    rt.addValue(\"Frame\", i+1);\n    rt.addValue(\"TimeTag\", timeTag);\n}\n\n// Show the results table\nrt.show(\"Time Tags\");\n\n// Report the completion of the acquisition\nReportingUtils.showMessage(\"Acquisition complete!\");\n</code></pre>", "<p>After looking at this problem some more I realize that part of the problem is that when you decide to save the images as a stack you loose almost all the metadata. But if you instead choose to save as individual images you get a ton of metadata both embedded in each image as well as saved as a separate txt file.</p>\n<p>I guess a possible workaround is to make a script that uses MDA to record individual images, and then have ImageJ post-hoc in the same script create an image stack and save the metadata to a results table.</p>"], "78962": ["<p><a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a></p>\n<p>I see that in BIOP Operetta Importer plugin, in the GUI there are now options to flip images horizontally, vertically or both. Many many thanks for these!  I seem to need the vertical flip for our data from the Opera Phenix.<br>\nMy question is can these options be used in the groovy scripts?  If so, how?  I tried to look at the javadoc of OperettaManager, but I\u2019m getting a 404. I don\u2019t know any groovy or java scripting so a short example would be super helpful.</p>\n<p>Many thanks for any help.<br>\nHuw</p>", "<p>\u2026attempting to solve my own problem here, I tried adding .flipVertical(true) to the opm builder, which appears to have worked:</p>\n<pre><code class=\"lang-auto\">#@File id (label=\"Selected File\")\n#@File save_dir (label=\"Save Location\", style=\"directory\")\n#@Integer downsample (label=\"Downsample Factor\", value=1)\n\nimport ch.epfl.biop.operetta.OperettaManager\nimport ij.gui.Roi\nimport ij.IJ\n\ndef opm = new OperettaManager.Builder()\n\t\t\t\t\t\t\t\t\t.setId( id )\n\t\t\t\t\t\t\t\t\t.setSaveFolder( save_dir )\n\t\t\t\t\t\t\t\t\t.flipVertical(true)\n\t\t\t\t\t\t\t\t\t.build();\n</code></pre>\n<p>I\u2019ll try stitching the tiles that has produced and see if it\u2019s right.</p>", "<p>This now looks right!!  Hopefully that\u2019s all I\u2019ll need <img src=\"https://emoji.discourse-cdn.com/twitter/crossed_fingers.png?v=12\" title=\":crossed_fingers:\" class=\"emoji\" alt=\":crossed_fingers:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c.jpeg\" data-download-href=\"/uploads/short-url/rxbdxqRjNTfIMTKuFpUiCtZskWM.jpeg?dl=1\" title=\"bigstitcher result\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_564x500.jpeg\" alt=\"bigstitcher result\" data-base62-sha1=\"rxbdxqRjNTfIMTKuFpUiCtZskWM\" width=\"564\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_564x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_846x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/0/c0fae598c757becf3d6eb62185d9d55a44d0a45c_2_1128x1000.jpeg 2x\" data-dominant-color=\"11160A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">bigstitcher result</span><span class=\"informations\">1214\u00d71075 62.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Huw</p>"], "78963": ["<p>Hi, I would like to download Cellpose plugin and try it for my images.<br>\nHowever when I click on the download link it refers me to Github repository.<br>\nI am not into programming so much, I would like to have this very basic question that how I can add it to my cell profiler?<br>\nthanks a lot</p>", "<h2>\n<a name=\"sara-i-found-it-easier-to-create-a-small-python-script-python-at-least-399-installation-install-cuda-i-am-using-cuda-116-httpsdocsnvidiacomcudacuda-installation-guide-microsoft-windowsindexhtml-pip-install-cellpose-pip-uninstall-torch-pip-install-torch-f-httpsdownloadpytorchorgwhltorch_stablehtml-1\" class=\"anchor\" href=\"#sara-i-found-it-easier-to-create-a-small-python-script-python-at-least-399-installation-install-cuda-i-am-using-cuda-116-httpsdocsnvidiacomcudacuda-installation-guide-microsoft-windowsindexhtml-pip-install-cellpose-pip-uninstall-torch-pip-install-torch-f-httpsdownloadpytorchorgwhltorch_stablehtml-1\"></a>Sara,<br>\nI found it easier to create a small Python script.<br>\npython at least 3.9.9<br>\nInstallation:<br>\nInstall CUDA (I am using Cuda 11.6)<br>\n<a href=\"https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">cuda-installation-guide-microsoft-windows 12.1 documentation</a><br>\npip install cellpose<br>\npip uninstall torch<br>\npip install torch -f <a href=\"https://download.pytorch.org/whl/torch_stable.html\" rel=\"noopener nofollow ugc\">https://download.pytorch.org/whl/torch_stable.html</a>\n</h2>\n<p>import cv2  # pip install opencv-python if needed<br>\nimport skimage  # pip install scikit-image if needed<br>\nfrom skimage.color import rgb2gray, label2rgb<br>\nfrom skimage.measure import label, regionprops, regionprops_table<br>\nimport cellpose  # pip install cellpose<br>\nimport cellpose.models<br>\nimport matplotlib.pyplot as plt  # pip install matplotlib<br>\nimport matplotlib.patches as mpatches<br>\nfrom matplotlib.patches import Rectangle<br>\nimport torch</p>\n<p>image = cv2.imread(filename)<br>\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br>\nchannels = [0, 0]  # IF YOU HAVE GRAYSCALE<br>\nlabels, flows, style, diams = model_cyto.eval(gray, diameter=average_diameter_pixs, channels=channels)<br>\n\u2026</p>"], "40051": ["<p>Hi all,</p>\n<p>since <a href=\"https://forum.image.sc/t/imagej-wiki-is-down/39672\">imagej.net is still down</a> and since it will change it\u2019s nature from mediawiki to github pages, the mechanism to create update sites by creating a wiki user cannot be applied anymore.</p>\n<p>All existing update site credentials should still work.</p>\n<p>Until a new mechanism is in place, please answer to this thread in case\u2026</p>\n<ul>\n<li>you want a new update site associated with an existing user</li>\n<li>you want a new user name with a new update site</li>\n</ul>\n<p>\u2026 and someone with server access will take care of it.</p>", "<p>Thank you frauzfall!<br>\nI would like a new user name and a new update site.<br>\nThe update site should be \u201cObjectColocalizationPlugins\u201d, and the user name can be my name I guess: Anders Lunde. Or anders_lunde.<br>\nTanks</p>", "<p>Hey <a class=\"mention\" href=\"/u/frauzufall\">@frauzufall</a>,</p>\n<p>thanks for taking care of this / us <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>I would like to request for a new upate site called \u201cclincubator\u201d associated to my user account. But don\u2019t tell anyone <img src=\"https://emoji.discourse-cdn.com/twitter/laughing.png?v=9\" title=\":laughing:\" class=\"emoji\" alt=\":laughing:\"></p>\n<p>Thanks!</p>\n<p>Cheers,<br>\nRobert</p>", "<p><a class=\"mention\" href=\"/u/frauzufall\">@frauzufall</a><br>\nthanks so much for taking care of this!<br>\nWe are (cc <a class=\"mention\" href=\"/u/nornil\">@nornil</a>) also in the process of submitting a publication about an application with a Fiji Update Site. How long do you think it will take until everything works again? I would feel better submitting the publication only after we are back to a normal state\u2026</p>", "<p><a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a> AFAIK, Fiji update sites are back to normal.</p>", "<p>But I was also wondering about the wiki, because the link to the wiki will be in the publication\u2026</p>", "<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"6\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian_Tischer:</div>\n<blockquote>\n<p>I was also wondering about the wiki,</p>\n</blockquote>\n</aside>\n<p>Check out Curtis last update on the wiki:</p><aside class=\"quote quote-modified\" data-post=\"46\" data-topic=\"39672\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/imagej-wiki-is-down/39672/46\">ImageJ wiki is read-only</a> <a class=\"badge-wrapper  bullet\" href=\"/c/web-site/3\"><span class=\"badge-category-bg\" style=\"background-color: #808281;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"Discussion about the websites of the software partners of the Image.sc forum, their organization, how they work, and how they can be improved.\">Websites</span></a>\n  </div>\n  <blockquote>\n    Status update for yesterday and today: \nAs mentioned in my last update, several ImageJ team members met yesterday to plan the migration of <a href=\"http://imagej.net\">imagej.net</a> from MediaWiki to GitHub Pages. The effort is already very far along; now we are working on mass migrating all the content. You can visit the <a href=\"https://imagej.github.io/\">live preview</a> if curious (still highly incomplete of course), and ongoing technical discussion can be followed more closely on the <a href=\"http://gitter.im/imagej/imagej.github.io\">imagej.github.io Gitter channel</a> if you desire. \nToday we spent time troublesho\u2026\n  </blockquote>\n</aside>\n", "<p><a class=\"mention\" href=\"/u/etadobson\">@etadobson</a> Could you please field these update site requests? Happy to advise if you need guidance. Thank you!</p>\n<aside class=\"quote no-group\" data-username=\"Christian_Tischer\" data-post=\"4\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christian_tischer/40/1180_2.png\" class=\"avatar\"> Christian_Tischer:</div>\n<blockquote>\n<p>How long do you think it will take until everything works again?</p>\n</blockquote>\n</aside>\n<p>Not sure yet. I will post a projected timeline to the <a href=\"https://forum.image.sc/t/imagej-wiki-is-down/39672\">ImageJ wiki is down</a> thread before week\u2019s end.</p>", "<p>Sorry <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> - was away on a short holiday\u2026</p>\n<p>Do I still need to take care of any of these?  If so - who needs assistance?</p>", "<p>The following update sites now exist:</p>\n<ul>\n<li>\n<code>ObjectColocalizationPlugins</code> with authorized user <code>AndersLunde</code> (<a class=\"mention\" href=\"/u/anders_lunde\">@Anders_Lunde</a>)</li>\n<li>\n<code>clincubator</code> with authorized user <code>Haesleinhuepf</code> (<a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a>)</li>\n</ul>", "<p>Fantastic, thanks <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> <img src=\"https://emoji.discourse-cdn.com/twitter/vulcan_salute.png?v=9\" title=\":vulcan_salute:\" class=\"emoji\" alt=\":vulcan_salute:\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a>, <a class=\"mention\" href=\"/u/frauzufall\">@frauzufall</a></p>\n<p>I am very grateful to you for the <a href=\"http://imagej.net\" rel=\"nofollow noopener\">imagej.net</a> migration process.<br>\nIt\u2019s difficult to ask in this situation, I would like to request new my update site.</p>\n<p>update site : SheetMeshProjection<br>\nuser account : WadaH or hwada</p>\n<p>Thanks for your help,</p>\n<p>hwada</p>", "<aside class=\"quote no-group\" data-username=\"hwada\" data-post=\"12\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/h/4491bb/40.png\" class=\"avatar\"> hwada:</div>\n<blockquote>\n<p>update site : SheetMeshProjection<br>\nuser account : WadaH or hwada</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/hwada\">@hwada</a> Done! Since your username did not already exist, I have PMed your upload password to you.</p>", "<p>Dear <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a>! I have two questions regarding update sites:</p>\n<ol>\n<li>Should I still use \u201c<a href=\"https://sites.imagej.net/ObjectColocalizationPlugins/\" rel=\"nofollow noopener\">https://sites.imagej.net/ObjectColocalizationPlugins/</a>\u201d as the update site for my plugin as a long term solution? I need to know because I will publish this link in a journal soon. In other words, will this plugin repository continue to work long term?</li>\n<li>I have forgotten my password to my ImageJ wiki account, and cannot upload my jars. Can you help me reset my password (AndersLunde)?</li>\n</ol>", "<aside class=\"quote no-group\" data-username=\"Anders_Lunde\" data-post=\"14\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/anders_lunde/40/32086_2.png\" class=\"avatar\"> Anders_Lunde:</div>\n<blockquote>\n<p>Should I still use \u201c<a href=\"https://sites.imagej.net/ObjectColocalizationPlugins/%E2%80%9D\">https://sites.imagej.net/ObjectColocalizationPlugins/\u201d</a> as the update site for my plugin as a long term solution?</p>\n</blockquote>\n</aside>\n<p>Sure. We are committed to maintaining these update sites for the long term. However, I don\u2019t think you actually need to publish that link directly in the journal. Why not just publish a link like <a href=\"https://imagej.net/Object_Colocalization_Plugins\">https://imagej.net/Object_Colocalization_Plugins</a>? (The only issue right now is: if you haven\u2019t already created a wiki page for your project, you can\u2019t edit right now because the wiki is temporarily read-only! <img src=\"https://emoji.discourse-cdn.com/twitter/confounded.png?v=9\" title=\":confounded:\" class=\"emoji\" alt=\":confounded:\"> But we\u2019re working as fast as we can to get it back up and running.) Links to pages of <a href=\"http://imagej.net\">imagej.net</a> will definitely be maintained very long term\u2014even if we eventually restructure the site, we\u2019ll always add redirects to new locations as appropriate.</p>\n<aside class=\"quote no-group\" data-username=\"Anders_Lunde\" data-post=\"14\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/anders_lunde/40/32086_2.png\" class=\"avatar\"> Anders_Lunde:</div>\n<blockquote>\n<p>Can you help me reset my password (AndersLunde)?</p>\n</blockquote>\n</aside>\n<p>Yep. I sent you a PM with the new credentials.</p>", "<p>Hi, <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a>,</p>\n<p>Thank you for your immediate support!<br>\nI would like to try uploading now.<br>\nAnd, I have a question.<br>\nIs it ok to include the dependent jar files created by others?</p>\n<p>hwada</p>", "<p>Hi again, <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a>,</p>\n<p>I tried uploading my file, and I put the password you provided in the password input field of fiji, but it seems that it cannot be authenticated.<br>\nThe documentation says that it\u2019s different from the wiki account password, is there something else?</p>\n<p>hwada</p>", "<aside class=\"quote no-group\" data-username=\"hwada\" data-post=\"16\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/h/4491bb/40.png\" class=\"avatar\"> hwada:</div>\n<blockquote>\n<p>Is it ok to include the dependent jar files created by others?</p>\n</blockquote>\n</aside>\n<p>Yep\u2014as long as their license allows redistribution. Presumably all your dependencies are open source? See also the <a href=\"https://imagej.net/Personal_Update_Site_Terms_of_Service\">Personal Update Site Terms of Service</a>.</p>\n<aside class=\"quote no-group\" data-username=\"hwada\" data-post=\"17\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/h/4491bb/40.png\" class=\"avatar\"> hwada:</div>\n<blockquote>\n<p>I put the password you provided in the password input field of fiji, but it seems that it cannot be authenticated.</p>\n</blockquote>\n</aside>\n<p>I just tested it, and it works. Here are the steps I followed:</p>\n<ol>\n<li>Choose <em>Help \u203a Update\u2026</em> from the menu</li>\n<li>Click \u201cManage Update Sites\u201d button</li>\n<li>Click \u201cAdd update site\u201d button (<em>not</em> \u201cAdd my site\u201d\u2026 I would like to remove that button because it\u2019s confusing)</li>\n<li>For URL enter <code>https://sites.imagej.net/SheetMeshProjection/</code>\n</li>\n<li>For Name enter <code>SheetMeshProjection</code>\n</li>\n<li>For Host enter <code>webdav:WadaH</code>\n</li>\n<li>Leave <code>Directory on Host</code> blank</li>\n</ol>\n<p>It will look like this:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7e2d2df1503bcf9831f3260eb935a0133de96e8.png\" data-download-href=\"/uploads/short-url/qeJlmSWMGTSSepHCJOzXiU9px6E.png?dl=1\" title=\"SheetMeshProjection-update-site\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e2d2df1503bcf9831f3260eb935a0133de96e8_2_690x270.png\" alt=\"SheetMeshProjection-update-site\" data-base62-sha1=\"qeJlmSWMGTSSepHCJOzXiU9px6E\" width=\"690\" height=\"270\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e2d2df1503bcf9831f3260eb935a0133de96e8_2_690x270.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e2d2df1503bcf9831f3260eb935a0133de96e8_2_1035x405.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e2d2df1503bcf9831f3260eb935a0133de96e8_2_1380x540.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7e2d2df1503bcf9831f3260eb935a0133de96e8_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">SheetMeshProjection-update-site</span><span class=\"informations\">1624\u00d7636 167 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<ol start=\"7\">\n<li>Then click Close.</li>\n<li>Then click \u201cAdvanced mode\u201d and in the View Options choose \u201cView local-only files\u201d. You should see your plugin JAR(s) and dependencies listed. Select them all with shift+click, then right-click and there should be a \u201cUpload to SheetMeshProjection\u201d option available. Click that.</li>\n<li>Click \u201cApply changes (upload)\u201d button.</li>\n<li>When the password prompt comes up, paste the password I shared with you via PM.</li>\n</ol>\n<p>I was able to upload a file <code>jars/test-0.0.0.jar</code> to your update site using these credentials, so I know they work.</p>", "<p>There was one difference from my procedure.</p>\n<aside class=\"quote no-group\" data-username=\"ctrueden\" data-post=\"18\" data-topic=\"40051\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png\" class=\"avatar\"> ctrueden:</div>\n<blockquote>\n<ol start=\"6\">\n<li>For Host enter <code>webdav:WadaH</code>\n</li>\n</ol>\n</blockquote>\n</aside>\n<p>I changed this point(webdav:SheetMeshProjection -&gt; webdave:WadaH), then I could upload my files.<br>\nThank you for your very kind help.</p>\n<p>hwada</p>", "<p>I would like to request an update site: CellularImaging<br>\nMy imagej username was schmiedc</p>\n<p>Thank you!</p>"], "78966": ["<p>Hi,</p>\n<p>I am having issues clustering cell sub-populations using Cytomap. I have IHC images which I segment using Histocat and then carry out the spatial analysis in Cytomap. For my analysis, I am trying to identify different CD8+ T cell subsets in the lymph node. Some subsets (e.g. CXCR5+ CD8+ T cells) are rarely found in my IHC images, and when I run the cell clustering in cytomap this and other rare subsets are being underrepresented when I compare the cytomap output to the original IHC image.</p>\n<p>I have tried running the clustering repeatedly using different normalisation and clustering parameters. Normally I first cluster my cells into either CD8+ and CD8- cells as a means of excluding unwanted cell populations. This first step is able to accurately detect all CD8+ T cells. I then proceed to cluster the CD8+ T cells into different sub-populations. However, the clustering detects fewer CD8+ T cell subsets than are visibly present in the original IHC image.</p>\n<p>Is there a way I can improve the clustering so that the cell detection in Cytomap is representative of the original IHC staining. I am trying to ensure my analysis is accurate and robust enough for me to submit my work for publication.</p>\n<p>Thanks,</p>\n<p>Leonard</p>", "<aside class=\"quote no-group\" data-username=\"Leo_Mv\" data-post=\"1\" data-topic=\"78966\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/leo_mv/40/22830_2.png\" class=\"avatar\"> Leonard:</div>\n<blockquote>\n<p>However, the clustering detects fewer CD8+ T cell subsets than are visibly present in the original IHC image.</p>\n</blockquote>\n</aside>\n<p>I am not certain what the problem is (and have no real experience with Histocat), but I do want to emphasize that CytoMAP clustering is based purely off of numerical values passed to it, which tend not to represent the stain distribution that we see very well. The fact that most analyses are 2D makes matters even worse. Depending on the software, there is often a \u201ccytoplasmic expansion\u201d stage which is not necessarily context dependent which will poll a larger area than the tight cytoplasms of lymphocytes, leading to them being visible to the eye, but not showing up well as \u201caverage intensities\u201d for a given ROI in a spreadsheet.</p>\n<p>Cheers,<br>\nMike</p>", "<p>If you are looking to capture a specific population of cells you can clearly see in the image you can manually gate on the channel intensity of the cells as you would with flow cytometry data. I go over this workflow a little in one of the examples: <a href=\"https://cstoltzfus.com/posts/2021/06/CytoMAP%20Demo/\" class=\"inline-onebox\">Simplified CytoMAP Workflow - Dr. Caleb R. Stoltzfus</a></p>\n<p>When I do this with my samples I like to have the original image open, a plot of the position of the cells in X,Y, and the plot with my cell gate. The I will adjust my cell gate, save the population, refresh or re-plot the cell positions, then compare their distribution to the original image to see if I am roughly capturing the cells I need to quantify.</p>\n<p>One thing to note when doing this is that most image display software flips the images when they display them. In the cytomap plot in the bottom left there is an options button, inside that sub menu you can flip the Y axis so the X-Y display of the positions of the cells will match the actual image.</p>", "<aside class=\"quote no-group\" data-username=\"cstoltzfus\" data-post=\"3\" data-topic=\"78966\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cstoltzfus/40/32335_2.png\" class=\"avatar\"> Dr. Caleb Stoltzfus:</div>\n<blockquote>\n<p>In the cytomap plot in the bottom left there is an options button, inside that sub menu you can flip the Y axis so the X-Y display of the positions of the cells will match the actual image.</p>\n</blockquote>\n</aside>\n<p>Not entirely related to the thread, but I didn\u2019t know this and I\u2019m glad I do now!</p>"], "54392": ["<p>Dear OME team and <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a> ,</p>\n<p>I two quick questions about the maximum intensity projection feature in Omero web. I\u2019ve found two old threads related to my questions:</p>\n<p><a href=\"https://forum.image.sc/t/omero-figure-max-projections-problem/30542\">https://forum.image.sc/t/omero-figure-max-projections-problem/30542</a><br>\n<a href=\"https://www.openmicroscopy.org/community/viewtopic.php?p=9679\" rel=\"noopener nofollow ugc\">https://www.openmicroscopy.org/community/viewtopic.php?p=9679</a></p>\n<p>But I\u2019m not fully satisfied so far <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">  So here are my questions:</p>\n<ol>\n<li>how is actually the limitation defined when the Projection feature is disabled in iviewer and figure? and can this limitation somehow be manipulated by an Omero admin?</li>\n<li>if the manipulation is not possible or recommended, can I actually use the \u201cCombine Images\u201d server-side-script to create MIP in batch? for example for a complete dataset? I tried it, but struggled with the parameters and in the end did not get a MIP image, but maybe I just didn\u2019t use it correctly.</li>\n</ol>\n<p>Thanks in advance<br>\nAnna</p>", "<p>Hi!</p>\n<p>For your first point, I think this <a href=\"https://github.com/ome/omero-iviewer/issues/341#issuecomment-677507226\" rel=\"noopener nofollow ugc\">issue</a> may contain relevant information: there is a parameter to define the threshold above which projections are not generated, but increasing it too much may be costly for the server. There is also a limit in the iviewer.</p>\n<p>Edit: Actually, this <a href=\"https://github.com/ome/omero-iviewer/pull/349\" rel=\"noopener nofollow ugc\">PR</a> may be clearer.</p>", "<p>Yes, thanks Pierre. There was also another recent question about this at <a href=\"https://forum.image.sc/t/z-projection-disabled-in-viewers-for-big-images/54181/2\" class=\"inline-onebox\">Z-projection disabled in viewers for \"big\" images - #2 by will-moore</a></p>\n<p>The <code>Combine_Images.py</code> script doesn\u2019t do projection I\u2019m afraid. It combines many single-plane images into a multi-dimensional stack across Z, C or T.<br>\nHowever, it wouldn\u2019t be too hard to create a script to do that if you need it. OMERO.insight also offers the ability to save a Z-projection as a new Image, although I don\u2019t know what the size limits are for that.</p>\n<p>Regards,<br>\nWill.</p>\n<p>Will</p>", "<p>Thanks a lot for your quick response and sorry for creating a double post, I must have overseen that recent post. These questions arose because we handle many 2160x2160px images and tiled images. So for the big single images increasing the mentioned parameter should help to get the feature working in iviewer I guess.</p>\n<p>I thought it would be an additional selling point to our users if there was something like a server-side-script creating MIPs also for tiled images without additional effort for them. But I\u2019m unsure if our server system can handle such loads. So maybe we will stick for those images to creating MIPs in external software and uploading them if needed to Omero, that\u2019s how it\u2019s done right now anyway.</p>\n<p>Thanks and have a nice weekend!<br>\nAnna</p>", "<p><a class=\"mention\" href=\"/u/will-moore\">@will-moore</a> I have recevied users demands for Max Intensity Projection and Autocontrast features on OMERO. I understand that due to the size of the images, it will be quite impossible to do it on the fly. Is there a possibility to do it during server-sdie CLI import and save the Projection somewhere. Similarly is it possible to do Autocontrast and save it as default Rendering setting for the iviewer?</p>", "<p>Hi Ken,<br>\nI just created <a href=\"https://gist.github.com/will-moore/4eb2fe61cd35cabd4682083b1a45e0e9\" class=\"inline-onebox\">Python script to create a Max Intensity Projection for an OMERO Image \u00b7 GitHub</a>.<br>\nThat gives an example of how to do projection and save as new Image in OMERO, preserving the metadata etc. You can save that and run in via the command-line with the Image ID:</p>\n<pre><code class=\"lang-auto\">$ python max_intensity_projection.py 123\n</code></pre>\n<p>I guess you could run this as a cron job at off-peak time to do projections on newly imported Images (instead of run directly at import time)?</p>\n<p>For Auto-contrast, you can set the current rendering settings for the logged-in user with <a href=\"https://github.com/ome/omero-py/blob/b34a8aab77bac105e27dd820a193a51ffe318182/src/omero/gateway/__init__.py#L8792\">image.set_active_channels()</a>. When that user opens iviewer, they will see those settings. How you calculate the start/end contrast limits is kinda up to you. By default, OMERO picks the brightest and darkest pixels for each channel in the Image (checks every Z/T plane), but you may want a different strategy?</p>\n<p>Will</p>", "<p>Hi Will,</p>\n<p>Thanks for that! I shall definitely try to implement that and see whether it satisfy our users!</p>\n<p>Best regards<br>\nKen</p>", "<p>Hi <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>,</p>\n<p>Quik question and reply on Auto-contrast, it seems that the images that we deal with, they didn\u2019t use the default, \u201cBy default, OMERO picks the brightest and darkest pixels for each channel\u201d. i.e. they always open with 0-65535 in each channel. Is it because of the size of the images, i.e. similar to the strategy used in Max Intensity Projection, that it won\u2019t generate it on the fly due to the size?</p>\n<p>And having a quick look at image.set_active_channels, it doesn\u2019t seem to save the rendering on the thumbnails, or does it?</p>\n<p>My original thought on Auto-contrast above was to pre-calculate the range and save that as default for thumbnails and opening with iviewer/pathviewer. Would that be possible?</p>\n<p>Ken</p>", "<p>Hi <a class=\"mention\" href=\"/u/ken.ho\">@ken.ho</a><br>\nThe size of the images shouldn\u2019t affect the auto-contrast, but it is possible that OMERO hasn\u2019t calculated the min/max range for some images. E.g. if you import on the command-line you can use <code>--skip minmax</code> to improve import speed, useful for large imports.<br>\nIf you do a fresh import (without skipping minmax) and find that you still have that issue then something\u2019s not right.</p>\n<p><code>set_active_channels()</code> won\u2019t cause a new thumbnail to be generated, but simply refreshing the webclient should do it.</p>\n<p>There\u2019s also a cli tool for setting rendering settings <a href=\"https://github.com/ome/omero-cli-render\" class=\"inline-onebox\">GitHub - ome/omero-cli-render: OMERO command-line tool for rendering</a>, e.g. apply settings from a yml file to all images in a Dataset <code>$ omero render set Dataset:1 settings.yml</code>, which should regenerate thumbnails.</p>\n<p>Sounds good: As above, if the minmax is not being calculated already, it should be.<br>\nYou can tell if an image doesn\u2019t have min/max set as the Min/Max and Full Range buttons behave he same e.g. <a href=\"http://idr.openmicroscopy.org/webclient/img_detail/9836841/\" class=\"inline-onebox\">OMERO.iviewer</a></p>\n<p>Will</p>", "<p>Hello <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a>,</p>\n<p>I wanted to test the script you\u2019ve created to make a Maximum intensity projection of an image.<br>\nI rewrite it a bit to be able to upload it on our server but I get stuck with this error that always happen.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/edSmE56P7wevUSj8YFdbA1yZ3MU.zip\">mip.zip</a> (1.6 KB)</p>\n<p>Do you have an idea on where it is coming from ?<br>\nThank you,</p>\n<p>R\u00e9my.</p>\n<pre><code class=\"lang-auto\">WARNING:omero.gateway:ValueError on &lt;class 'omero.gateway.OmeroGatewaySafeCallWrapper'&gt; to &lt;bd82e55b-8a75-4979-acfb-742b82fed7feomero.api.IPixels&gt; copyAndResizeImage(('37505', object #0 (::omero::RInt)\n{\n    _val = 2912\n}, object #0 (::omero::RInt)\n{\n    _val = 2912\n}, object #0 (::omero::RInt)\n{\n    _val = 1\n}, object #0 (::omero::RInt)\n{\n    _val = 1\n}, range(0, 1), None, False, &lt;ServiceOptsDict: {'omero.client.uuid': 'bd82e55b-8a75-4979-acfb-742b82fed7fe', 'omero.session.uuid': '55f7839e-9caf-4aac-9497-facd9beac5ac', 'omero.group': '253'}&gt;), {})\nTraceback (most recent call last):\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 4856, in __call__\n    return self.f(*args, **kwargs)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero_api_IPixels_ice.py\", line 893, in copyAndResizeImage\n    return _M_omero.api.IPixels._op_copyAndResizeImage.invoke(self, ((imageId, sizeX, sizeY, sizeZ, sizeT, channelList, methodology, copyStats), _ctx))\nValueError: invalid value for argument 1 in operation `copyAndResizeImage'\nERROR:omero.gateway:Failed to setPlane() on rawPixelsStore while creating Image\nTraceback (most recent call last):\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 3864, in createImageFromNumpySeq\n    image, dtype = createImage(plane, channelList)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 3790, in createImage\n    rint(sizeT), channelList, None, False, self.SERVICE_OPTS)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 4859, in __call__\n    return self.handle_exception(e, *args, **kwargs)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 4856, in __call__\n    return self.f(*args, **kwargs)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero_api_IPixels_ice.py\", line 893, in copyAndResizeImage\n    return _M_omero.api.IPixels._op_copyAndResizeImage.invoke(self, ((imageId, sizeX, sizeY, sizeZ, sizeT, channelList, methodology, copyStats), _ctx))\nValueError: invalid value for argument 1 in operation `copyAndResizeImage'\nTraceback (most recent call last):\n  File \"./script\", line 113, in &lt;module&gt;\n    run_script()\n  File \"./script\", line 105, in run_script\n    message = do_max_intensity_projection(conn, script_params)\n  File \"./script\", line 65, in do_max_intensity_projection\n    sourceImageId=image_id, channelList=clist, dataset=dataset)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 3893, in createImageFromNumpySeq\n    raise exc\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 3864, in createImageFromNumpySeq\n    image, dtype = createImage(plane, channelList)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 3790, in createImage\n    rint(sizeT), channelList, None, False, self.SERVICE_OPTS)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 4859, in __call__\n    return self.handle_exception(e, *args, **kwargs)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero/gateway/__init__.py\", line 4856, in __call__\n    return self.f(*args, **kwargs)\n  File \"/opt/omero/server/venv3/lib64/python3.6/site-packages/omero_api_IPixels_ice.py\", line 893, in copyAndResizeImage\n    return _M_omero.api.IPixels._op_copyAndResizeImage.invoke(self, ((imageId, sizeX, sizeY, sizeZ, sizeT, channelList, methodology, copyStats), _ctx))\nValueError: invalid value for argument 1 in operation `copyAndResizeImage'\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/rdornier\">@Rdornier</a></p>\n<p>This small fix you need is to use <code>rlong</code> instead of <code>rstring</code> for the <code>IDs</code> input:</p>\n<pre><code class=\"lang-auto\">description=\"Image ID.\").ofType(rlong(0)),\n</code></pre>\n<p>You can also improve the usability of the script by returning the new Image so that users can find it:</p>\n<pre><code class=\"lang-auto\">from omero.rtypes import rstring, rlong, robject\n...\n\ndef do_max_intensity_projection(conn, script_params):\n\n  # return the image...\n  return new_image\n\n...\n\n        image = do_max_intensity_projection(conn, script_params)\n        client.setOutput(\"Message\", rstring(\"Created Image:\" + image.name))\n        client.setOutput(\"Image\", robject(image._obj))\n\n</code></pre>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/5936c82a225f1111abc7caddf7268dba5c3b1717.png\" alt=\"Screenshot 2023-03-23 at 12.24.30\" data-base62-sha1=\"cJdRBgIoO8jRV0oI9TBTmxT2jeD\" width=\"403\" height=\"97\"></p>", "<p>Also, if you use the same Type and IDs inputs as in e.g. <a href=\"https://github.com/ome/omero-scripts/blob/88219a3323685a575f92e689222b91b4e590da3c/omero/util_scripts/Channel_Offsets.py#L268\" class=\"inline-onebox\">omero-scripts/Channel_Offsets.py at 88219a3323685a575f92e689222b91b4e590da3c \u00b7 ome/omero-scripts \u00b7 GitHub</a> then the currently selected Image(s) will be filled-in when the script dialog is shown.</p>\n<p>You could also add a loop in to process all selected Images, if you want to allow that?</p>", "<p>Thanks <a class=\"mention\" href=\"/u/will-moore\">@will-moore</a> !<br>\nIndeed, it is a small fix ; now, it perfectly works.</p>\n<blockquote>\n<p>You could also add a loop in to process all selected Images, if you want to allow that?</p>\n</blockquote>\n<p>This is what I planed to do</p>"], "78970": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p>A confocal stack was segmented in 3D. A mesh was created. When cells are selected using the \u201cselect connected area tool\u201d the wrong cell is often selected instead. It is not a neighbour cell.</p>\n<h3>\n<a name=\"analysis-goals-2\" class=\"anchor\" href=\"#analysis-goals-2\"></a>Analysis goals</h3>\n<p>Accurate segmentation</p>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<p>We have tried 3 different machines and multiple different versions of MGX. The bug exists on all new machines purchased in the last year. Only some cells are affected, these cells are always affected. They can be selected if we use an old laptop. We tried with and without CUDA, the windows and linux version of MGX.</p>", "<p>Selection in MorphoGraphX is done by rendering all the triangles off screen with unique colors without shading, and then querying what color is under the mouse pointer when you click.</p>\n<p>If selection gives random results there are a couple possibilities.</p>\n<ol>\n<li>\n<p>The mesh is too large. If the number of faces is greater than 2^24 (8bit rgb), then some triangles will get the same color. The number of triangles will be roughly 2x the number of vertices.</p>\n</li>\n<li>\n<p>Multisampling. If the graphics driver has multisampling turned on, or anything else that could affect the unique color, it could cause problems.</p>\n</li>\n</ol>\n<p>If it is 1) I would expect the problem to occur on any machine, although there is no guarantee you would have the problem with the same cells. If it is 2) then all meshes on that machine would be affected, and potentially all selection tools, although again there is no guarantee that it would manifest the same way for different meshes/tools.</p>"], "78972": ["<p>I am looking to run a simple positive cells selection on 1000s of DAB-stained images in Qupath. I have figured out how to do one image but cannot seem to apply this to batch processing. I would like to count the entire image but can\u2019t seem to do so without it asking for an annotation/area, and then I cant seem to apply the same annotation automatically across multiple images.</p>\n<p>Anyone have any ideas?</p>", "<p>Hi Paul,</p>\n<p>Welcome to the forum!<br>\nIt sounds like you need to create a script for your process that can then be run across all the image in the project. Check out <a href=\"https://qupath.readthedocs.io/en/0.4/docs/scripting/index.html\">workflows and scripts</a> section for an explanation of how to do this.</p>\n<p>As for the initial annotation are you looking to have it:</p>\n<ol>\n<li>Over the whole image</li>\n<li>Around the tissue</li>\n<li>In a specific place the same across each image</li>\n<li>A variable position and size for each image</li>\n</ol>", "<p>Hi Fiona, thanks for replying. I am looking to count over the whole image.</p>", "<p>Hi Paul,<br>\nTo batch-process images requires you to write a script. Most of the things you do to an image will appear in the workflow tab and you can use this to generate a script.</p>\n<p>Your workflow will look something like this:</p>\n<p>Correct colour vectors<br>\nAnnotate tissue area<br>\nRun positive cell detection</p>\n<p>There are plugins for all of these so it should be simple to script. Once you have done all of this on one image you can generate the script and run for project. Assuming all slides were stained and scanned at the same time the variables should work across all images.</p>", "<p>Is your image of the whole slide with whitespace around it or of a close-up of the tissue so the whole image has cells?</p>", "<p>Its a close-up of the image (with the less important bit, such as glands) removed. I have uploaded an example for you. We have 1000s of these types of images.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/xGshjjrNDFCWPWoHkBH4AQV8GWU.tif\">Tester image DAB.tif</a> (10.4 MB)</p>", "<p>Something like:</p>\n<pre><code class=\"lang-auto\">setImageType('BRIGHTFIELD_H_DAB')\nsetColorDeconvolutionStains('{\"Name\" : \"H-DAB estimated\", ' + \n          '\"Stain 1\" : \"Hematoxylin\", \"Values 1\" : \"0.8228 0.52632 0.21444\", ' +\n          '\"Stain 2\" : \"DAB\", \"Values 2\" : \"0.32346 0.46972 0.82142\", ' +\n          '\"Background\" : \" 232 232 231\"}')\n                            \ncreateFullImageAnnotation(true)\n\nrunPlugin('qupath.imagej.detect.cells.PositiveCellDetection', \n            '{\"detectionImageBrightfield\": \"Hematoxylin OD\", ' +\n            ' \"requestedPixelSizeMicrons\": 0.5, ' +\n            ' \"backgroundRadiusMicrons\": 8.0, ' +\n            ' \"medianRadiusMicrons\": 0.0, ' +\n            ' \"sigmaMicrons\": 1.5, ' +\n            ' \"minAreaMicrons\": 10.0, ' +\n            ' \"maxAreaMicrons\": 400.0, ' +\n            ' \"threshold\": 0.1, ' +\n            ' \"maxBackground\": 2.0, ' +\n            ' \"watershedPostProcess\": true, ' +\n            ' \"excludeDAB\": false, ' +\n            ' \"cellExpansionMicrons\": 8.0, ' +\n            ' \"includeNuclei\": true, ' +\n            ' \"smoothBoundaries\": true, ' +\n            ' \"makeMeasurements\": true, ' +\n            ' \"thresholdCompartment\": \"Cytoplasm: DAB OD mean\", ' +\n            ' \"thresholdPositive1\": 0.2, ' +\n            ' \"thresholdPositive2\": 0.4, ' +\n            ' \"thresholdPositive3\": 0.6, ' +\n            '\"singleThreshold\": true}')\n            \nprintln(\"Done!\")\n\n</code></pre>\n<p>should work. You will need to check <em>all</em> the variables are correct as I have not tested it on your image. Try working out the variables using one image through the GUI then plug them in the script where needed.<br>\nThe colour vectors are something that can be worked out using the preprocessing options and then play around with cell detection until you are getting good results.</p>\n<p>(Edited to correct script)</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10.jpeg\" data-download-href=\"/uploads/short-url/jqyvkpYjHAx0ZoeS1B24L8RUmXu.jpeg?dl=1\" title=\"Screenshot 2023-03-23 130124\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_517x293.jpeg\" alt=\"Screenshot 2023-03-23 130124\" data-base62-sha1=\"jqyvkpYjHAx0ZoeS1B24L8RUmXu\" width=\"517\" height=\"293\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_517x293.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_775x439.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/8829ce61e988661bd9105b337b872e2aed4bdf10_2_1034x586.jpeg 2x\" data-dominant-color=\"BFBFDB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-23 130124</span><span class=\"informations\">1246\u00d7707 178 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThis is the result for that script so you might want to make <code> \"thresholdPositive1\"</code> something like 0.1 rather than 0.2\u2026</p>", "<p>Brilliant Chris, thankyou. I will let you know how I get on.</p>"], "78974": ["<p>I\u2019m creating a macro to do batch processing using MOSAIC Particle Tracker but I haven\u2019t been able to set the intensity threshold to absolute in the macro,  it always assumes that is the percentile.  I was wondering if anyone knew the correct command to  do so.<br>\nThank you for your time <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi, here are two examples of macro commands that run Particle Tracker in percentile and absolute mode:</p>\n<pre><code class=\"lang-auto\">// Percentile\nrun(\"Particle Tracker 2D/3D\", \"radius=3 cutoff=0.001 per/abs=0.501 link=2 displacement=10 dynamics=Brownian\");\n\n// Absolute\nrun(\"Particle Tracker 2D/3D\", \"radius=3 cutoff=0.001 per/abs=50 absolute link=2 displacement=10 dynamics=Brownian\");\n</code></pre>\n<p>As you may notice, the only difference is adding \u2018absolute\u2019 keyword to command (and this is exactly what macro recorder is creating in my case).</p>", "<p>Thank you so much! I don\u2019t why but my macro recorder didn\u2019t show that command.</p>"], "78980": ["<p>Hello everyone,<br>\nI installed OMERO.web and everything worked fine. Then I installed  OMERO.iviewer from the omero.web virtual environment and installation successfully completed.</p>\n<p>I followed this instructions:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pypi.org/project/omero-iviewer/\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6ef85bcf60dcca703f2a4b057ca0fefbdf0668c6.png\" class=\"site-icon\" width=\"32\" height=\"30\">\n\n      <a href=\"https://pypi.org/project/omero-iviewer/\" target=\"_blank\" rel=\"noopener nofollow ugc\">PyPI</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f97026709e67b2111b465be6427519ead928642.webp\" class=\"thumbnail onebox-avatar\" width=\"300\" height=\"300\">\n\n<h3><a href=\"https://pypi.org/project/omero-iviewer/\" target=\"_blank\" rel=\"noopener nofollow ugc\">omero-iviewer</a></h3>\n\n  <p>A Python plugin for OMERO.web</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>When I tried to load OMERO.web again from my browser It did not work anymore.</p>\n<p>(base) [omero-web@bigbang root]$ pip freeze<br>\nappdirs==1.4.4<br>\nasgiref==3.4.1<br>\nasync-timeout==4.0.2<br>\ncertifi==2022.12.7<br>\ncharset-normalizer==2.0.12<br>\nconcurrent-log-handler==0.9.20<br>\nDjango==3.2.18<br>\ndjango-cors-headers==3.7.0<br>\ndjango-pipeline==2.0.7<br>\ndjango-redis==4.8.0<br>\nfuture==0.18.3<br>\ngunicorn==20.1.0<br>\nidna==3.4<br>\nimportlib-metadata==4.8.3<br>\nnumpy==1.19.5<br>\nomero-iviewer==0.12.0<br>\nomero-marshal==0.8.0<br>\nomero-py==5.13.1<br>\nomero-web==5.19.0<br>\npackaging==21.3<br>\nPillow==7.1.1<br>\nportalocker==2.7.0<br>\npyparsing==3.0.9<br>\npytz==2022.7.1<br>\nPyYAML==6.0<br>\nredis==3.4.1<br>\nrequests==2.27.1<br>\nsqlparse==0.4.3<br>\ntyping_extensions==4.1.1<br>\nurllib3==1.26.14<br>\nwhitenoise==5.3.0<br>\nzeroc-ice==3.6.5<br>\nzipp==3.6.0</p>\n<p>This is my config</p>\n<p>(base) [omero-web@bigbang root]$ /opt/omero/web/venv3/bin/omero config get<br>\nomero.glacier2.IceSSL.ProtocolVersionMax=TLS1_2<br>\nomero.glacier2.IceSSL.Protocols=TLS1_0,TLS1_1,TLS1_2<br>\nomero.web.application_server=wsgi-tcp<br>\nomero.web.apps=[\u201comero_iviewer\u201d]<br>\nomero.web.caches={\u201cdefault\u201d: {\u201cBACKEND\u201d: \u201cdjango_redis.cache.RedisCache\u201d,\u201cLOCATION\u201d: \u201credis://127.0.0.1:6379/0\u201d}}<br>\nomero.web.middleware=[{\u201cindex\u201d: 1, \u201cclass\u201d: \u201cdjango.middleware.common.BrokenLinkEmailsMiddleware\u201d}, {\u201cindex\u201d: 2, \u201cclass\u201d: \u201cdjango.middleware.common.CommonMiddleware\u201d}, {\u201cindex\u201d: 3, \u201cclass\u201d: \u201cdjango.contrib.sessions.middleware.SessionMiddleware\u201d}, {\u201cindex\u201d: 4, \u201cclass\u201d: \u201cdjango.middleware.csrf.CsrfViewMiddleware\u201d}, {\u201cindex\u201d: 5, \u201cclass\u201d: \u201cdjango.contrib.messages.middleware.MessageMiddleware\u201d}, {\u201cindex\u201d: 6, \u201cclass\u201d: \u201cdjango.middleware.clickjacking.XFrameOptionsMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}, {\u201cindex\u201d: 0, \u201cclass\u201d: \u201cwhitenoise.middleware.WhiteNoiseMiddleware\u201d}]<br>\nomero.web.open_with=[[\u201cImage viewer\u201d, \u201cwebgateway\u201d, {\u201csupported_objects\u201d: [\u201cimage\u201d], \u201cscript_url\u201d: \u201cwebclient/javascript/ome.openwith_viewer.js\u201d}], [\u201comero_iviewer\u201d, \u201comero_iviewer_index\u201d, {\u201csupported_objects\u201d: [\u201cimages\u201d, \u201cdataset\u201d, \u201cwell\u201d], \u201cscript_url\u201d: \u201comero_iviewer/openwith.js\u201d, \u201clabel\u201d: \u201cOMERO.iviewer\u201d}]]<br>\nomero.web.server_list=[[\u201clocalhost\u201d, 4064, \u201comero\u201d]]<br>\nomero.web.session_engine=django.contrib.sessions.backends.cache<br>\nomero.web.viewer.view=omero_iviewer.views.index</p>\n<p>(base) [omero-web@bigbang root]$ /opt/omero/web/venv3/bin/omero web start</p>\n<p>0 static files copied to \u2018/opt/omero/web/omero-web/var/static\u2019, 593 unmodified, 2 post-processed.<br>\nClearing expired sessions. This may take some time\u2026 Traceback (most recent call last):<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/cache/<strong>init</strong>.py\u201d, line 39, in create_connection<br>\nbackend_cls = import_string(backend)<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/utils/module_loading.py\u201d, line 17, in import_string<br>\nmodule = import_module(module_path)<br>\nFile \u201c/usr/lib64/python3.6/importlib/<strong>init</strong>.py\u201d, line 126, in import_module<br>\nreturn _bootstrap._gcd_import(name[level:], package, level)<br>\nFile \u201c\u201d, line 994, in _gcd_import<br>\nFile \u201c\u201d, line 971, in _find_and_load<br>\nFile \u201c\u201d, line 955, in _find_and_load_unlocked<br>\nFile \u201c\u201d, line 665, in _load_unlocked<br>\nFile \u201c\u201d, line 678, in exec_module<br>\nFile \u201c\u201d, line 219, in _call_with_frames_removed<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django_redis/cache.py\u201d, line 7, in <br>\nfrom .util import load_class<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django_redis/util.py\u201d, line 7, in <br>\nfrom django.utils.encoding import smart_text, python_2_unicode_compatible<br>\nImportError: cannot import name \u2018python_2_unicode_compatible\u2019</p>\n<p>The above exception was the direct cause of the following exception:</p>\n<p>Traceback (most recent call last):<br>\nFile \u201cmanage.py\u201d, line 75, in <br>\nexecute_from_command_line(sys.argv)<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/<strong>init</strong>.py\u201d, line 419, in execute_from_command_line<br>\nutility.execute()<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/<strong>init</strong>.py\u201d, line 413, in execute<br>\nself.fetch_command(subcommand).run_from_argv(self.argv)<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/base.py\u201d, line 354, in run_from_argv<br>\nself.execute(*args, **cmd_options)<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/base.py\u201d, line 393, in execute<br>\nself.check()<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/management/base.py\u201d, line 423, in check<br>\ndatabases=databases,<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/checks/registry.py\u201d, line 76, in run_checks<br>\nnew_errors = check(app_configs=app_configs, databases=databases)<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/checks/caches.py\u201d, line 63, in check_file_based_cache_is_absolute<br>\ncache = caches[alias]<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/utils/connection.py\u201d, line 62, in <strong>getitem</strong><br>\nconn = self.create_connection(alias)<br>\nFile \u201c/opt/omero/web/venv3/lib64/python3.6/site-packages/django/core/cache/<strong>init</strong>.py\u201d, line 43, in create_connection<br>\n) from e<br>\ndjango.core.cache.backends.base.InvalidCacheBackendError: Could not find backend \u2018django_redis.cache.RedisCache\u2019: cannot import name \u2018python_2_unicode_compatible\u2019</p>\n<p>Can anypone help me to solve this issue?</p>\n<p>Thank you very much in advance</p>", "<p>That line in <code>django-redis 4.8</code> that\u2019s causing the error <a href=\"https://github.com/jazzband/django-redis/blob/2b4a21620c6a7127e0d0800f3e05a010ff7b4db4/django_redis/util.py#L7\" class=\"inline-onebox\">django-redis/util.py at 2b4a21620c6a7127e0d0800f3e05a010ff7b4db4 \u00b7 jazzband/django-redis \u00b7 GitHub</a></p>\n<p>I think you need to upgrade to <code>django-redis==5.0.0</code> which is the current requirement for omero-web:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68\" target=\"_blank\" rel=\"noopener\">ome/omero-web/blob/52bfbb40341d3355bc0c9f9d38ce016d3c6baba6/setup.py#L68</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"58\" style=\"counter-reset: li-counter 57 ;\">\n          <li>        \"django-pipeline==2.0.7\",</li>\n          <li>        \"django-cors-headers==3.7.0\",</li>\n          <li>        \"whitenoise&gt;=5.3.0\",</li>\n          <li>        \"gunicorn&gt;=19.3\",</li>\n          <li>        \"omero-marshal&gt;=0.7.0\",</li>\n          <li>        \"Pillow\",</li>\n          <li>    ],</li>\n          <li>    include_package_data=True,</li>\n          <li>    tests_require=[\"pytest\"],</li>\n          <li>    extras_require={</li>\n          <li class=\"selected\">        \"redis\": [\"django-redis==5.0.0\"],</li>\n          <li>    },</li>\n          <li>)</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "44167": ["<p>Hello,</p>\n<p>On imagej or fiji what\u2019s the simplest way to extract a single slice (image) out or a stack? Is there a command to extract, let\u2019s say the 5th slice from a 30 images stack?<br>\nThanks in advance.</p>", "<p>You can use duplicate:<br>\n<em><strong>Image &gt; Duplicate\u2026</strong></em><br>\nThen specify a range</p>\n<p>Or you can use the substack maker:<br>\n<em><strong>Image &gt; Stacks &gt; Tools &gt; Make substack\u2026</strong></em></p>", "<p>Great! Many thanks!<br>\nBest,</p>", "<p>Thanks, I wanted to take the first frame from a stack and this worked</p>"], "78989": ["<p>Hi, I have a project with MIF images and I would like to use Qupath, however after that I created my project I\u2019m not able to see the right channels in the \u201cbrightness and contrast\u201d section. Do you know how to fix this? thank you<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403.jpeg\" data-download-href=\"/uploads/short-url/2ZEujHjfWoQlEHPm92DhoSlhGMP.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_690x440.jpeg\" alt=\"image\" data-base62-sha1=\"2ZEujHjfWoQlEHPm92DhoSlhGMP\" width=\"690\" height=\"440\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_690x440.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_1035x660.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/4/14fc7b51e27c304911d3719aaf51a56681fdb403_2_1380x880.jpeg 2x\" data-dominant-color=\"131618\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1642\u00d71048 129 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hey Antonio-</p>\n<ol>\n<li>Check your Image tab to make sure QuPath is reading this as a fluorescence image</li>\n<li>Check your input data to make sure the file has all the channels and hasn\u2019t been converted to a flattened RGB</li>\n</ol>", "<p>Thank you Sara! I think you are right and they have been converted to RGB, even if at the beginning I save them as fluorescence\u2026 do you know if there is a way to convert back ?</p>", "<p>If your original image had exactly 3 channels and they were pseudo-colored red, green, and blue, you can work with the data exactly as is. Unfortunately, if your image had more channels than that, once it has been converted down to a 3 channel RGB, there is no way to undo it. Information has been lost and QuPath cannot recover it. <img src=\"https://emoji.discourse-cdn.com/twitter/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>It would help to know what the original file type is, and what the Server type listed in the Image tab. Some images, like 4 channel images from MRXS files, can only be rendered as RGB because that is the only option due to how difficult 3DHISTECH has made working with their files. You may want to look up one of the many posts on the forum about dealing with or converting such files into a usable format if that is the case.</p>\n<p>Otherwise, as <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> says, once a 4 channel or more image is saved as RGB, much of the data is gone, and you will need to go back to the original data.</p>", "<p>Sorry, I should mention, an RGB image is 8 bit. So, you can only trust if it your original data was also 3 channel, 8 bit.</p>", "<p>Hi Antonio,<br>\nWhat software did you use to generate the RGB? What is the source of the mIF file?</p>"], "78991": ["<p>Hello,</p>\n<p>I am currently trying to manually seed and segment a stack taken of an Arabidopsis leaf, and I\u2019m having trouble getting clearly defined cell borders. When I project the signal onto the mesh, it doesn\u2019t look like the clean lines given in the example images from the manual. It looks like this:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c.png\" data-download-href=\"/uploads/short-url/6m8i1lYmf3hyqlqohUtyYrRAkxe.png?dl=1\" title=\"Screenshot 2023-03-23 115757\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_297x375.png\" alt=\"Screenshot 2023-03-23 115757\" data-base62-sha1=\"6m8i1lYmf3hyqlqohUtyYrRAkxe\" width=\"297\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_297x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_445x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/c/2c8d8e4b1f802b686ab4cd08a4828d77c093622c_2_594x750.png 2x\" data-dominant-color=\"606060\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-23 115757</span><span class=\"informations\">646\u00d7814 242 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I can still see where the cell borders are, but when I place seeds onto the mesh the watershed segmentation doesn\u2019t seem to be able to accurately find them:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e.png\" data-download-href=\"/uploads/short-url/3831JItPSjHHOwDpQgJvYfkthoO.png?dl=1\" title=\"Screenshot 2023-03-23 115704\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_201x250.png\" alt=\"Screenshot 2023-03-23 115704\" data-base62-sha1=\"3831JItPSjHHOwDpQgJvYfkthoO\" width=\"201\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_201x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_301x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/15ef71090ca3ef42a7bdbd8cf0e0d00ef8b6db3e_2_402x500.png 2x\" data-dominant-color=\"58550F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-23 115704</span><span class=\"informations\">653\u00d7809 282 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If I draw around each cell with the seeds, the borders are still very jagged and often don\u2019t match up with the cell outlines:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6.png\" data-download-href=\"/uploads/short-url/2PoHrbHlsc1uOJTwnnEySePjUvI.png?dl=1\" title=\"Screenshot 2023-03-23 120127\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_195x250.png\" alt=\"Screenshot 2023-03-23 120127\" data-base62-sha1=\"2PoHrbHlsc1uOJTwnnEySePjUvI\" width=\"195\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_195x250.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_292x375.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13d3baca5698aac62e09e2eea4e2bc4d34098ce6_2_390x500.png 2x\" data-dominant-color=\"342E4F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-23 120127</span><span class=\"informations\">635\u00d7811 287 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I have tried subdividing the mesh multiple times before seeding, along with subdividing adaptive near borders after segmentation, and I still can\u2019t get the borders to be clearly defined like in the manual. I\u2019m guessing the problem has something to do with the original projected signal but I\u2019m not sure what to do about it.</p>\n<p>Does anyone have any suggestions?<br>\nThank you!</p>", "<p>Those look like the mesophyll cells underneath the surface. What projection distance did you use?</p>"], "78992": ["<p>Hi, I am trying to extract stage coordinates from a .vsi file using Bio-formats Macro Extensions.<br>\nI get \u201cMacro error\u201d when trying to read the stage positions. Any ideas what could be wrong?</p>\n<p>file = File.openDialog(\u201cSelect a File\u201d);<br>\nrun(\u201cBio-Formats Macro Extensions\u201d);<br>\nExt.setId(file);<br>\nExt.getPixelsPhysicalSizeZ(sizeZ); print(sizeZ); - this wprks fine<br>\nExt.getPlanePositionX(stage_x , 0); print(stage_x)<br>\nExt.close();</p>", "<p>I don\u2019t know these functions, but one thing that I can see is that there is a ; missing after print(stage_x).</p>", "<p>That\u2019s not the issue. Ignore that part, was a misprint here.</p>"], "78997": ["<p>Hi,</p>\n<p>Recently I found a scale bar that I just saved did not look right, and this issue kept happening a few times, so I am here to see if anyone knows I did something wrong.</p>\n<p>The example image I am using is this:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5.jpeg\" data-download-href=\"/uploads/short-url/vKMFGyuDAlpmBCEij3cJTEcVlyt.jpeg?dl=1\" title=\"NoScaling\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_690x439.jpeg\" alt=\"NoScaling\" data-base62-sha1=\"vKMFGyuDAlpmBCEij3cJTEcVlyt\" width=\"690\" height=\"439\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_690x439.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_1035x658.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/e/de8d45a38c40d4bf18c6db1a188171b37c3a21e5_2_1380x878.jpeg 2x\" data-dominant-color=\"C0C0C0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">NoScaling</span><span class=\"informations\">1460\u00d7930 135 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIt is 1460*930 on my laptop.<br>\nAnd I use Analyze&gt;Set Scale as 232 pixel / 1 cm as this:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/3/b3206a40f113bad1ba9e10011715ce40443bd7f6.jpeg\" alt=\"SS1\" data-base62-sha1=\"pyCUNEiWLGPLOoMwouUg7IuwG0K\" width=\"311\" height=\"362\"><br>\nThen, I use Analyze&gt;Tools&gt;Scale bar:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/41798f69e04af3af1d048177c24b169070b243c0.jpeg\" alt=\"SB1\" data-base62-sha1=\"9ldtA4AM7EblOpiRLX3qfXtpCA8\" width=\"301\" height=\"459\"><br>\nThe scale bar looks like this:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0.jpeg\" data-download-href=\"/uploads/short-url/dJTnI0CB0D7312olAwjCskaC2cg.jpeg?dl=1\" title=\"WithScaleBar0\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0_2_690x481.jpeg\" alt=\"WithScaleBar0\" data-base62-sha1=\"dJTnI0CB0D7312olAwjCskaC2cg\" width=\"690\" height=\"481\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0_2_690x481.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/0/604c558da790eaf91172686dad793f45a4ba9ef0.jpeg 2x\" data-dominant-color=\"C4C4C4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">WithScaleBar0</span><span class=\"informations\">927\u00d7647 61.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIf you measure the scale bar in this figure it already does not live up to my expectation that the scale bar should reflect 1 cm. When I measure it using shortcut M, I am told this is actually 0.69 cm, and when I save it as jpg and reopen it, it measures 160 px which is exactly 232*0.69.</p>\n<p>I wonder if there is anything I might have messed up or if this is some weird bug. My ImageJ seems to be ImageJ 1.53t. Any comment or help is appreciated!</p>\n<p>Best,<br>\nZhengyu</p>", "<p>Hi <a class=\"mention\" href=\"/u/zy112\">@zy112</a> . How did you determine the scaling of your image?</p>", "<p>With the downloaded image and scaled inputs I get<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717.jpeg\" data-download-href=\"/uploads/short-url/77KcoHSMeKNMVhI0xeBZ0gQPJOv.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_690x466.jpeg\" alt=\"image\" data-base62-sha1=\"77KcoHSMeKNMVhI0xeBZ0gQPJOv\" width=\"690\" height=\"466\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_690x466.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_1035x699.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31ef597861c764da87b81bf30d8a6979f2104717_2_1380x932.jpeg 2x\" data-dominant-color=\"C3C3C3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1557\u00d71053 90.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nalso in 1.53t</p>\n<p>Your font size and the \u201ccm\u201d in the first window do look a bit odd though. Not sure why.</p>", "<p>Hi Johanna, I calibrated it on another photo and added it using \u201cset scale\u201d.</p>", "<p>Thanks! It is really puzzling for me as well.</p>", "<p>Hi, I tried to use the latest version of FIJI-ImageJ, and it works well.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2uv5oWJvj4n0XZsRZlSb5FUszr7.tif\">test.tif</a> (1.3 MB)</p>"], "78999": ["<p>I am trying control the z stage of a Nikon Ti with Micro manager 2.<br>\nHowever, whenever I tried to move the z stage, it crashed the software. I was able to control the z stage with the TIControl software from Nikon, though. The<br>\nnikon driver version is 2.5.0.1</p>\n<p>Also, I see on the micro manager page saying that I shouldn\u2019t use a USB 3 ports. However, all USB ports on my computer are USB 3. Am I suppose to find a USB to PCI card just for this?</p>\n<p>The error log is attached.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hWFKUFOF14teuqkWBMYt6Gt1qSJ.txt\">hs_err_pid10604.txt</a> (48.2 KB)</p>"], "79001": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p>I\u2019ve uploaded a sample image of an OCT image I took. As you can tell, there are some spheres that are of interest to me. Also to note, the substrate is on a tilt (this is one image in a 3D stack). Since we can readily see with our own eyes the air-substrate interface (the continuous surface near the top of the image - its higher on the left and lower on the right), how could I get ImageJ or FIJI to identify the plane at which the surface is?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/lIcbaIjeloGqWXTReYn7tXWSwSC.tif\">gethelp2.tif</a> (163.9 KB)</p>\n            <div class=\"onebox imgur-album\">\n              <a href=\"https://imgur.com/a/y1QKYcu\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n                <span class=\"outer-box\" style=\"width:600px\">\n                  <span class=\"inner-box\">\n                    <span class=\"album-title\">[Album] Imgur: The magic of the Internet</span>\n                  </span>\n                </span>\n                <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/2/b26226bb00127689188e7770557360abe99487d7.jpeg\" title=\"Imgur: The magic of the Internet\" height=\"315\" width=\"600\">\n              </a>\n            </div>\n\n<p>Here is my image.</p>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>This is a sample of silver particles in 1% agarose gel imaged with OCT.</p>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<p>I\u2019d like to measure and identify the plane of the surface, so I can map an equation for it. My analysis requires knowing the depth of the particles from the surface.</p>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<p>I\u2019ve been identifying particles on the surface and then using them to calculate the equation for the surface. Is there something easier that I\u2019m overlooking? There is also a lot of noise in the image, so the interface is not as readily apparently if I plot profile of one line\u2026can there be an average that I can make?</p>", "<p>Have you tried either <code>Ridge detection</code> or filters for edge detection <a href=\"https://forum.image.sc/t/image-denoise-after-edge-detection/49285\" class=\"inline-onebox\">image denoise after edge detection</a></p>", "<p>Hi,</p>\n<p>With your image, try this:</p>\n<pre><code class=\"lang-auto\">//This macro requires the BAR plugin\nrun(\"Set Measurements...\", \"area centroid perimeter bounding stack display redirect=None decimal=3\");\nTitle=getTitle();\ngetDimensions(width, height, channels, slices, frames);\nrun(\"Duplicate...\", \"title=Blurred\");\nrun(\"Spectrum\");\nrun(\"Gaussian Blur...\", \"sigma=2\");\nrun(\"Find Edges\");\n\nrun(\"Subtract...\", \"value=25\");\n\nCoordinateX=newArray();\nCoordinateY=newArray();\n\nfor (i = 10; i &lt; width; i=i+width/10) {\n\tselectWindow(\"Blurred\");\n\tmakeLine(i, 0, i, height, 5);\n\trun(\"Plot Profile\");\n\trun(\"Find Peaks\", \"min._peak_amplitude=5 min._peak_distance=0 min._value=[] max._value=[] list\");\n\tYList=newArray();\n\tfor (j = 0; j &lt; height; j++) {\n\t\tY=getResult(\"X1\", j, \"Plot Values\");\n\t\tYList=Array.concat(YList,Y);\n\t}\n\tYList=Array.deleteValue(YList, NaN);\n\tArray.print(YList);\n\tif(YList.length==2){\n\t\tCoordinateX=Array.concat(CoordinateX,i);\n\t\tCoordinateY=Array.concat(CoordinateY,YList[1]);\n\t}\n\tclose(\"Plot Values\");\n\tclose(\"Plot of Blurred\");\n\tclose(\"Peaks in Plot of Blurred\");\n\n}\nLast=CoordinateX.length-1;\nselectWindow(\"Blurred\");\n\nmakeLine(CoordinateX[0], CoordinateY[0], CoordinateX[Last], CoordinateY[Last]);\n\nrun(\"Measure\");\n\n\n</code></pre>\n<p>You should get this image<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/d/2dcee1ab8fb9dc6411a799672c75d01d324e3745.png\" alt=\"image\" data-base62-sha1=\"6xeJ7D33t6o3pv5osTVjzkgcn6l\" width=\"657\" height=\"301\"><br>\nand a table of result with the angle of the line and its coordinates</p>\n<p>M.</p>", "<p>A possible change\u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7.png\" data-download-href=\"/uploads/short-url/qw8399HYdgVVyGhWIcMAxxhKL0H.png?dl=1\" title=\"How to detect the surface\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_345x242.png\" alt=\"How to detect the surface\" data-base62-sha1=\"qw8399HYdgVVyGhWIcMAxxhKL0H\" width=\"345\" height=\"242\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_345x242.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_517x363.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9da4e5ed4a4800a00cb051df124e359a9e1e1a7_2_690x484.png 2x\" data-dominant-color=\"F8F2F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">How to detect the surface</span><span class=\"informations\">965\u00d7679 63.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e.png\" data-download-href=\"/uploads/short-url/2f7S2kL1ngkvVdDPXRszYpAwdKS.png?dl=1\" title=\"Profil de surface\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e_2_690x316.png\" alt=\"Profil de surface\" data-base62-sha1=\"2f7S2kL1ngkvVdDPXRszYpAwdKS\" width=\"690\" height=\"316\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e_2_690x316.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/f/0fba18a82bdf95ba37a67cb1828dd3fc1080e42e.png 2x\" data-dominant-color=\"5D5C5C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Profil de surface</span><span class=\"informations\">909\u00d7417 270 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<pre><code class=\"lang-auto\">macro \"How to detect the surface\"\n{\nrequires(\"1.54b\");\nsetBackgroundColor(0,0,0);\nsetOption(\"BlackBackground\",true);\n//--------------------------------\n// Start batch mode\nsetBatchMode(true);\n//Copy and select\norig=getImageID();\nrun(\"Duplicate...\", \"title=1\");\nclose(\"\\\\Others\");\nrun(\"Duplicate...\", \"title=2\");\nh=getHeight();\nw=getWidth();\n//--------------------------------\n// Start processing\nrun(\"Invert\");\nimageCalculator(\"Subtract create\", \"2\",\"1\");\nselectWindow(\"Result of 2\");\nrun(\"Convert to Mask\");\nrun(\"Invert\");\ndoWand(0, 0, 11.0, \"Legacy\");\nsetBackgroundColor(255,255,255);\nrun(\"Clear\", \"slice\");\nsetBackgroundColor(0,0,0);\nrun(\"Clear Outside\");\ndoWand(0,h/2, 11.0, \"Legacy\");\n//run(\"Fill Holes\");\n//run(\"Select None\");\n//run(\"Open\");\nrun(\"Median\", \"radius=2\");\n//--------------------------------\nx=newArray();\ny=newArray();\nfind_scale=h/255;\nfor(i=0;i&lt;w;i++){\nmakeLine(i,0,i,h);\nprofile=getProfile();\nArray.getStatistics(profile, min, max, mean, stdDev) ;\nx[i]=i;\ny[i]=find_scale*mean;\n}\n//--------------------------------\nFit.doFit(\"Straight Line\", x, y);\na=d2s(Fit.p(0),6);\nb=d2s(Fit.p(1),6);\n//--------------------------------\nx1=0;\ny1=Fit.f(x1);\nx2=getWidth();\ny2=Fit.f(x2);\nselectImage(\"1\");\nmakeLine(x1,y1,x2,y2);\n//--------------------------------\n// End of batch mode\nsetBatchMode(false);\n//--------------------------------\nFit.plot;\n//--------------------------------\n// End of processing\n//--------------------------------\n\n//--------------------------------\nexit(\"It's over!\");\n}\n\n</code></pre>", "<p>That worked perfect on the image! One quick question - can this work on a stack of images (3D stack) to generate a plane?</p>\n<p>Thank you thank you!</p>\n<aside class=\"onebox googledrive\" data-onebox-src=\"https://drive.google.com/file/d/1YbHxfkkiZX50ngwadYaNad3KycWxiugY/view?usp=sharing\">\n  <header class=\"source\">\n\n      <a href=\"https://drive.google.com/file/d/1YbHxfkkiZX50ngwadYaNad3KycWxiugY/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\">drive.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n<div class=\"aspect-image\" style=\"--aspect-ratio:690/362;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae_2_690x362.jpeg\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae_2_690x362.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae_2_1035x543.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5df60023b80e63691985fca301c0aba51c76b6ae.jpeg 2x\" data-dominant-color=\"101010\"></div>\n\n<h3><a href=\"https://drive.google.com/file/d/1YbHxfkkiZX50ngwadYaNad3KycWxiugY/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\">getHelp3d.tif</a></h3>\n\n<p>Google Drive file.</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>This would the be the stack I\u2019m referring to.</p>"], "79002": ["<p><a>Uploading: \u5c4f\u5e55\u622a\u56fe 2023-03-24 102325.png\u2026</a><br>\n<a>Uploading: \u5c4f\u5e55\u622a\u56fe 2023-03-24 102226.png\u2026</a></p>\n<p>run(\u201cSplit Channels\u201d);<br>\nselectWindow(\u201ctitle.jpg (red)\u201d);<br>\nsetAutoThreshold(\u201cDefault dark\u201d);<br>\n//run(\u201cThreshold\u2026\u201d);<br>\n//setThreshold(70, 255);<br>\nsetOption(\u201cBlackBackground\u201d, false);<br>\nrun(\u201cConvert to Mask\u201d);</p>\n<p>Can you help me see what\u2019s wrong with my script? Why always report an error:<br>\nNo window with the title \"title.jpg(red)\"found.</p>", "<p>Hi <a class=\"mention\" href=\"/u/cary\">@Cary</a>, welcome to the forum!</p>\n<p>The error message is quite informative. In order to select a window you need to know its name. There is a separate function that allows you to find the name of the currently active window: <code>getTitle()</code>. Here is a short example of how you might use it:</p>\n<pre><code class=\"lang-auto\">//-- Open a new image\nnewImage(\"example\", \"RGB noise\", 100, 100, 1);\n//-- find the name of the window and record it into a variable\ntitle=getTitle();\n//print(title);\nrun(\"Split Channels\");\n//-- Select the Red window\nselectWindow(title + \" (red)\");\n</code></pre>\n<p>Note how you can concatenate the original name with a string (in this case <code>\" (red)\"</code>) using a plus sign (<code>+</code>).</p>\n<p>It also helps others when you add code to a post, if you can use the toolbar button to format your code (or use triple back ticks <code>```</code> as above), and this really helps with readability.</p>\n<p>Hope that helps!</p>", "<p><strong>Special thanks to you, the issue has been resolved.</strong></p>"], "79005": ["<p>Hi CellProfiler community,</p>\n<p>I am trying to count the number of co-positive nuclei. However, some speckles in my DAPI channel are being detected as nuclei, which is skewing my co-positive percentages.<br>\nThere is a stark difference in intensity (but not size) between nuclei and speckles. I have tried setting pixel intensity thresholds within the \u2018IdentifyPrimaryObject\u2019 function, but this does not filter out the DAPI speckles.<br>\nHowever, I have found that the \u2018FindMaxima\u2019 function is able to pick up these speckles very well (blue particles in image). Is there any way to filter out all the particles that are detected using \u2018FindMaxima\u2019?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d.png\" data-download-href=\"/uploads/short-url/2l2DhxTkMVAnpDhOYqgstblX4Ul.png?dl=1\" title=\"image3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_234x500.png\" alt=\"image3\" data-base62-sha1=\"2l2DhxTkMVAnpDhOYqgstblX4Ul\" width=\"234\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_234x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_351x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d.png 2x\" data-dominant-color=\"191919\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image3</span><span class=\"informations\">385\u00d7820 90.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If not, any other suggestions for removing speckles based on their higher intensity are very much welcome.</p>\n<p>Thank you in advance.</p>"], "79006": ["<h3>\n<a name=\"display-range-calculation-1\" class=\"anchor\" href=\"#display-range-calculation-1\"></a>Display Range Calculation</h3>\n<p>I\u2019m trying to understand how the default \u201cDisplay Range\u201d that is shown in the Brightness &amp; Contrast window or under Image &gt; Show Info\u2026 are calculated.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b.png\" data-download-href=\"/uploads/short-url/zrrWRjB3qwQGTlJ3QXZz3vsjZ7J.png?dl=1\" title=\"ImageJ_DisplayRange\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b_2_690x371.png\" alt=\"ImageJ_DisplayRange\" data-base62-sha1=\"zrrWRjB3qwQGTlJ3QXZz3vsjZ7J\" width=\"690\" height=\"371\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b_2_690x371.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b_2_1035x556.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f86677ceffd7c3203e8069f87b0c4fff3895165b.png 2x\" data-dominant-color=\"3B3B50\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ImageJ_DisplayRange</span><span class=\"informations\">1093\u00d7588 202 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h4>\n<a name=\"imagej-documentation-2\" class=\"anchor\" href=\"#imagej-documentation-2\"></a>ImageJ Documentation</h4>\n<p>The ImageJ documentation says the following, but this is related to the scaling / mapping. It is unclear to me how these minimum and maximum display values are actually obtained.</p>\n<blockquote>\n<p>The two numbers under the plot are the minimum and maximum displayed pixel values. These two values define the display range, or \u2018window\u2019. ImageJ displays images by linearly mapping pixel values in the display range to display values in the range 0\u2013255.</p>\n</blockquote>", "<p>Hi <a class=\"mention\" href=\"/u/rlprice410\">@rlprice410</a>,</p>\n<p>when I open the B&amp;C-tool, the min. and max. display values correspond to the min. and max. values in the histogram.<br>\nAre you sure that the shown B&amp;C tool actually corresponds to the image. When you have the tool open, it will not update what it displays, when the image changes. You need to activate the tool window to get the display for the current image.<br>\nThe min. and max. values are actually the minimum and maximum intensity values in the image.</p>\n<p>Best,<br>\nVolker</p>", "<p>I just checked this with a different channel and am getting the same window effect on the Display Range as above. I also opened the file in Python and the min / max intensity values are the ones displayed in the histogram, so there is a default window or scaling being applied to the Display Range.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb.jpeg\" data-download-href=\"/uploads/short-url/mSzdWsfL82AcujoxpHHYWskgYST.jpeg?dl=1\" title=\"ImageJ_DisplayRange_2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb_2_542x500.jpeg\" alt=\"ImageJ_DisplayRange_2\" data-base62-sha1=\"mSzdWsfL82AcujoxpHHYWskgYST\" width=\"542\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb_2_542x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb_2_813x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a05aefa1b4e8684f948186415e654f14d4e630fb.jpeg 2x\" data-dominant-color=\"4D4D31\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ImageJ_DisplayRange_2</span><span class=\"informations\">900\u00d7830 70.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The input image is 16-bit, so maybe this has something to do with it?</p>", "<p>Hi <a class=\"mention\" href=\"/u/rlprice410\">@rlprice410</a> . If you Reset your Brightness/Contrast settings, the full display range should correspond to the histogram.</p>", "<p>I understand that it can be reset, but I\u2019m trying to understand how the default display range window is calculated. It\u2019s not the same as the autocontrast either, because if I do B&amp;C &gt; Reset &gt; Auto after opening the file, the Auto Contrast Display Range is 551 - 7423 for the blue image above.</p>", "<p><a class=\"mention\" href=\"/u/rlprice410\">@rlprice410</a></p>\n<p>For 16-bit images:</p>\n<blockquote>\n<p>When displayed, the intensity of each pixel that is written in the image file is converted into the <em>grayness</em> of that pixel on the screen. How these intensities are interpreted is specified by the image type. From the <a href=\"https://imagej.nih.gov/ij/docs/concepts.html\">Basic concepts page</a>:</p>\n<blockquote>\n<p>16-bit and 32-bit grayscale images are not directly displayable on computer monitors, which typically can show only display 256 shades of gray. Therefore, the data are mapped to 8-bit by windowing. The window defines the range of gray values that are displayed: values below the window are made black, while values above the window are white. The window is defined by minimum and maximum values that can be modified using Image\u25b7Adjust\u25b7<a href=\"https://imagej.nih.gov/ij/docs/guide/146-28.html#sub:Brightness/Contrast...%5BC%5D\">Brightness/Contrast\u2026 [C]\u2191</a>.<br>\nIt may happen that the initial windowing performed by ImageJ on these high bit\u2013depth (or HDR<a href=\"https://imagej.nih.gov/ij/docs/guide/146-Nomenclature.html#nom-hdr\">[?]</a>) images is suboptimal. Please note that windowing does not affect image data</p>\n</blockquote>\n</blockquote>"], "79010": ["<p>My university clusters rely on Singularity instead of Docker, so if any of this could be fixed with the helper package <code>deeplabcut-docker</code>, please let me know.</p>\n<p>My local cluster relies for every new Jupyter notebook on a freshly created token to access the server.<br>\nUnfortunately the docker container does not seem to create such a token. This means I get to the login page of the Jupyter server, but cannot go any further.</p>\n<p>Is there a way to force the token generation?<br>\nI already contacted my IT department a few days ago, but so far I haven\u2019t heard back.</p>\n<pre><code class=\"lang-auto\">$singularity run deeplabcut-2.2.1.1.sif\n[I 09:26:45.496 NotebookApp] Serving notebooks from local directory: /home/&lt;university&gt;/&lt;department&gt;/&lt;userid&gt;\n[I 09:26:45.496 NotebookApp] Jupyter Notebook 6.4.12 is running at:\n[I 09:26:45.496 NotebookApp] http://&lt;node&gt;:8888/\n[I 09:26:45.496 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\n</code></pre>"], "79011": ["<p>Hi everyone,</p>\n<p>This is my first time using the new maDLC 2.3.0 and my task is too see whether the new version is suitable for our laboratory to analyze our social direct behaviour videos. I have to analyze videos where there are two rats in an open field arena and one of them is marked on the fur. I am using the GUI interface of the program.</p>\n<p>The training and the evaluation of the training run smoothly without any errors, my problems arise after I try to analyze a new video. Since this is a test project for now, I wanted to analyze only a 3 minute long video. I checked the boxes that I would like to create new labelled videos, plot the rajectories and save the csv file.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6.png\" data-download-href=\"/uploads/short-url/2FPpmxmOCs9pIY7JEExSgiFzq5g.png?dl=1\" title=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 100710\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_690x351.png\" alt=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 100710\" data-base62-sha1=\"2FPpmxmOCs9pIY7JEExSgiFzq5g\" width=\"690\" height=\"351\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_690x351.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_1035x526.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/12bed0162e2d7db0aad20778d8191427126471e6_2_1380x702.png 2x\" data-dominant-color=\"242E39\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">K\u00e9perny\u0151k\u00e9p 2023-03-21 100710</span><span class=\"informations\">1920\u00d7979 37.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The analyzation runs and finishes, I did not see any major error massages, just some smaller ones, but I have attached an image about it. However, after it finishes succesfully, there are no plots, no video, no h5 nor csv file created, only two pickle files.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73dc8eb0cd3c81ffb626fc9ad89f2a0c86742935.png\" data-download-href=\"/uploads/short-url/gwXrY6AA1bG1ckQGAKoQ8Lc94vb.png?dl=1\" title=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 110909\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73dc8eb0cd3c81ffb626fc9ad89f2a0c86742935.png\" alt=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 110909\" data-base62-sha1=\"gwXrY6AA1bG1ckQGAKoQ8Lc94vb\" width=\"690\" height=\"333\" data-dominant-color=\"202020\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">K\u00e9perny\u0151k\u00e9p 2023-03-21 110909</span><span class=\"informations\">1917\u00d7927 71.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/8/d88fbfe8f329bb21552c4560cb91785db1f7c4b4.png\" data-download-href=\"/uploads/short-url/uTN7bcGje4vl8PTWuEJprml4Kyg.png?dl=1\" title=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 110946\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/8/d88fbfe8f329bb21552c4560cb91785db1f7c4b4.png\" alt=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 110946\" data-base62-sha1=\"uTN7bcGje4vl8PTWuEJprml4Kyg\" width=\"690\" height=\"97\" data-dominant-color=\"1D1D1D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">K\u00e9perny\u0151k\u00e9p 2023-03-21 110946</span><span class=\"informations\">1917\u00d7272 20 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Now I have read in other forum questions, that in the case of the new maDLC the csv and h5 files are created after the \u2018refine tracklets\u2019 step. That is why I decided to try to go to the next step. Since in the GUI of the 2.3.0 the \u2018Create video\u2019 is the next step, I tried that first, however neither the skeleton building nor the create video option works. In the case of the video creation I tried with filtered and unfiltered version, as well, but same error comes up, that it cannot find the file. After that I tried to jump to the refine tracklets option, but it does not work either, as it cannot find h5 file for it.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/6/96072a54aae5255a224c830331e77aa549677ba9.png\" data-download-href=\"/uploads/short-url/lpcYhoMINYsVGsCbp0ECT6STmjf.png?dl=1\" title=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 111024\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/6/96072a54aae5255a224c830331e77aa549677ba9.png\" alt=\"K\u00e9perny\u0151k\u00e9p 2023-03-21 111024\" data-base62-sha1=\"lpcYhoMINYsVGsCbp0ECT6STmjf\" width=\"690\" height=\"277\" data-dominant-color=\"1C1C1C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">K\u00e9perny\u0151k\u00e9p 2023-03-21 111024</span><span class=\"informations\">1920\u00d7773 58.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It seems to me that there are major output files missing and I do not know why. I tried to run the analyzation again and again, also with different video, but it is the same everytime.</p>\n<p>I have limited programming knowledge, thus I may have missed something important, so if you have any further questions, I will try my best to answer it. <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thank you for your help in advance, I am rather desperate to find the solution.</p>"], "79013": ["<p>Hi everyone,</p>\n<p>I\u2019m developing a program to read czi files with pylibCZIrw and libczi. I try to build my program in flask with multiprocessing. However, if I open the multithreading in flask, the pylibCZIrw cannot read czi file correctly even if I add a lock when calling the pylibczirw.czi.read() function. Like the following image.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/48d4da512ff1c852151151077faa5bd5087d818b.png\" alt=\"Error Info\" data-base62-sha1=\"aoipBaFgivOP448qsJv7y5wXbV9\" width=\"681\" height=\"91\"></p>\n<p>Does the pylibczirw support multithreading? The doc of libczi told me that this library can be used in multithreading, so I think this may be caused by the pylibczirw or _pylibczirw.pyd?</p>\n<p>Thanks all.</p>", "<p>Hi <a class=\"mention\" href=\"/u/suica46\">@Suica46</a></p>\n<p>I checked with one of our experts and it is had to tell what is going on here.<br>\nHere is his response:</p>\n<hr>\n<p>The error seems to come from here:</p>\n<p><a href=\"https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FZEISS%2Flibczi%2Fblob%2F9a716c2122eae58e69bad28bf1b51919b0e84769%2FSrc%2FlibCZI%2Fdecoder_wic.cpp%23L235&amp;data=05%7C01%7C%7C1af84ee4eabb4de27b0d08db2c795baa%7C28042244bb514cd680347776fa3703e8%7C0%7C0%7C638152671952735072%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=NIM%2BEebE7m6GsDRAgyWouch9czmxVgHl811Wrn77Xk4%3D&amp;reserved=0\" rel=\"noopener nofollow ugc\">https://github.com/ZEISS/libczi/blob/9a716c2122eae58e69bad28bf1b51919b0e84769/Src/libCZI/decoder_wic.cpp#L235</a></p>\n<p>and it is a problem with constructing the WIC-decoder for JPGXR-compressed data (which is a Windows-component).</p>\n<p>Reasons could be\u2026 just guessing\u2026 either some configuration-issue or COM-apartment-intricacy. Or the data is bogus.</p>\n<p>Ideas/Question:</p>\n<ul>\n<li>Does the same operation work ok if running single-threaded? Or \u2013 how sure are we that the problem is related to flask/concurrency at all?</li>\n<li>libCZI could be configured to not use WIC but its own JPGXR-decoder. On Windows and by default, it uses the Windows-codec, but on Linux it uses its own implementation. One could configure things to use the \u201cbundled\u201d decoder on Windows instead, and this would rule out \u201cconfiguration- and COM-issues\u201d.</li>\n</ul>"], "62632": ["<p>Hi <a class=\"mention\" href=\"/u/oburri\">@oburri</a>, <a class=\"mention\" href=\"/u/romainguiet\">@romainGuiet</a>,</p>\n<p>I\u2019m trying to start working with cellpose in <span class=\"hashtag\">#Fiji</span>, with the same parameters I tried in cellpose.<br>\nCellpose doesn\u2019t work for other reasons (I think), and I think I\u2019m missing a dependency or something, and therefore it doesn\u2019t work.</p>\n<p>See the error log:</p>\n<pre><code class=\"lang-auto\">[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose, &amp;, python, -m, cellpose, --help]\n[INFO] true\ncyto_channel:1:nuclei_channel:2\nC:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp\\SRFG_D8_73_X60_SCAN1-t1.tif\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose, &amp;, python, -m, cellpose, --dir, C:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp, --pretrained_model, cyto2, --chan, 1, --chan2, 2, --diameter, 55, --flow_threshold, 0.4, --cellprob_threshold, 0.0, --save_tif, --no_npy, --use_gpu]\njava.lang.NullPointerException\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:224)\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)\n\tat org.scijava.command.CommandModule.run(CommandModule.java:196)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:165)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</code></pre>\n<p>Any clue for what I\u2019m doing wrong?</p>\n<p>Cheers,<br>\nDaniel</p>", "<p>Hi <a class=\"mention\" href=\"/u/daniel_waiger\">@Daniel_Waiger</a> ,</p>\n<p>I\u2019m sorry you experienced issues with the plugin, let\u2019s try to identify what\u2019s going wrong!</p>\n<p>A-From the log, I see that your cellpose conda environment is located in <code>D:\\anaconda3\\envs\\cellpose</code> , is that correct ? Otherwise you need to specify the path in <code>Plugins&gt;BIOP&gt;Cellpose&gt; Define Env. &amp; prefs.</code></p>\n<p>B- Moreover, after a recent update, in the <code>Plugins&gt;BIOP&gt;Cellpose&gt; Define Env. &amp; prefs.</code> you now have to specify which version of cellpose you are using 0.6 or 0.7  (we don\u2019t support yet the 1.0.0 but it should arrive soon <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> )</p>\n<p>C-If it\u2019s still not working, can you share the image you are trying to process ? or the size of the image ?</p>\n<p>Best,</p>\n<p>Romain</p>", "<p>Hi <a class=\"mention\" href=\"/u/romainguiet\">@romainGuiet</a>,<br>\nSure!</p>\n<p>Just a quick note, we are still working on 0.4 scale in XY.<br>\nThe full-res image is much larger, and can\u2019t be uploaded to the forum as a file.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/1BOlI7yPF3LSJZfDji0cZvVsVX4.tif\">SRFG_D8_73_X60_SCAN1-1.tif</a> (15.3 MB)</p>\n<p><strong>So, if I\u2019m relating to my other post:</strong><br>\n<a href=\"https://forum.image.sc/t/cellpose-error-index-1001-is-out-of-bounds-for-axis-0-with-size-1001/62628/4\">Cellpose: ERROR: index 1001 is out of bounds for axis 0 with size 1001</a></p>\n<p>Does that mean I have to create a separate env. for the wrapper to work? <img src=\"https://emoji.discourse-cdn.com/twitter/face_with_peeking_eye.png?v=12\" title=\":face_with_peeking_eye:\" class=\"emoji\" alt=\":face_with_peeking_eye:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>If so, how do I install an older version?</p>\n<p>Cheers,<br>\nDaniel</p>", "<p>Hi <a class=\"mention\" href=\"/u/daniel_waiger\">@Daniel_Waiger</a> ,</p>\n<p>when installing in conda using pip you can specify a version ````pip install cellpose==0.7.2```</p>\n<p>I just tested \u201cquickly\u201d the version 1.0.0 and it\u2019s \u201cworking\u201d BUT you need to <strong>uncheck</strong> <em>UseResample</em> in <code>Plugins&gt;BIOP&gt;Cellpose&gt; Define Env. &amp; prefs.</code></p>\n<p>Cheers,</p>\n<p>R</p>", "<p>Hi again,</p>\n<p>Are you sure?<br>\nI still get the same error, same for 0.7.2.</p>\n<p>Maybe I\u2019m selecting a wrong path for the .env?</p>\n<pre><code class=\"lang-auto\">[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose, &amp;, python, -m, cellpose, --help]\n[INFO] true\ncyto_channel:1:nuclei_channel:0\nC:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp\\SRFG_D8_73_X60_SCAN1-1-t1.tif\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose, &amp;, python, -m, cellpose, --dir, C:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp, --pretrained_model, cyto2, --chan, 1, --diameter, 55, --flow_threshold, 0.4, --mask_threshold, 0.0, --omni, --save_tif, --no_npy, --use_gpu]\njava.lang.NullPointerException\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:224)\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)\n\tat org.scijava.command.CommandModule.run(CommandModule.java:196)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:165)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose, &amp;, python, -m, cellpose, --help]\n[INFO] true\n\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/2/b2e4724d5d4f413cecbe5062dd7b945f3195249c.png\" data-download-href=\"/uploads/short-url/pwyqUEIOP3mH0ymDo42sUvitmAQ.png?dl=1\" title=\"2022-01-31_12h30_20\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/2/b2e4724d5d4f413cecbe5062dd7b945f3195249c_2_517x222.png\" alt=\"2022-01-31_12h30_20\" data-base62-sha1=\"pwyqUEIOP3mH0ymDo42sUvitmAQ\" width=\"517\" height=\"222\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/2/b2e4724d5d4f413cecbe5062dd7b945f3195249c_2_517x222.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/2/b2e4724d5d4f413cecbe5062dd7b945f3195249c_2_775x333.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/2/b2e4724d5d4f413cecbe5062dd7b945f3195249c_2_1034x444.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/2/b2e4724d5d4f413cecbe5062dd7b945f3195249c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">2022-01-31_12h30_20</span><span class=\"informations\">1205\u00d7519 140 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thx,<br>\nDaniel</p>", "<p>The path seems right.</p>\n<p>Did you enable conda outside of conda prompt ? ie \u201cactivate\u201d conda so it can be used from any command prompt (and not only from conda prompt)?<br>\nYou can find the steps <a href=\"https://github.com/BIOP/ijl-utilities-wrappers/blob/master/README.md#-enable-conda-command-outside-conda-prompt-\">in our documentation</a>.</p>\n<p>Cheers</p>\n<p>R</p>", "<p>Still,<br>\nSame error.<br>\nI fixed the PowerShell issue,could it be the CUDA toolkit? can you know from the error?</p>\n<pre><code class=\"lang-auto\">cyto_channel:1:nuclei_channel:0\nC:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp\\SRFG_D21_90_X60_SCAN1-t1.tif\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose_07, &amp;, python, -m, cellpose, --dir, C:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp, --pretrained_model, cyto2, --chan, 1, --diameter, 55, --flow_threshold, 0.4, --mask_threshold, 0.0, --omni, --save_tif, --no_npy, --use_gpu]\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose_07, &amp;, python, -m, cellpose, --help]\n[INFO] true\ncyto_channel:1:nuclei_channel:2\nC:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp\\SRFG_D21_90_X60_SCAN1-t1.tif\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose_07, &amp;, python, -m, cellpose, --dir, C:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp, --pretrained_model, cyto2, --chan, 1, --chan2, 2, --diameter, 60, --flow_threshold, 0.4, --mask_threshold, 0.0, --save_tif, --no_npy, --use_gpu]\njava.lang.NullPointerException\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:224)\n\tat org.scijava.command.CommandModule.run(CommandModule.java:196)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:165)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n</code></pre>\n<p>Thx,<br>\nDaniel</p>", "<p>Hi <a class=\"mention\" href=\"/u/daniel_waiger\">@Daniel_Waiger</a> ,</p>\n<p>I can reproduce an error if I use the <code>--omni</code> but it works fine without it.</p>\n<p>I tried to reproduce your/similar line with the conda env and I get an error :</p>\n<pre><code class=\"lang-auto\">(D:\\conda_envs\\cellposeGPU100_113-821) C:\\Users\\guiet&gt;python -m cellpose --dir C:\\Users\\guiet\\AppData\\Local\\Temp\\cellposeTemp --pretrained_model cyto2_omni --chan 1 --chan2 2 --diameter 60 --flow_threshold 0.4 --mask_threshold 0.0 --omni --save_tif --no_npy --use_gpu --verbose\n2022-01-31 16:53:01,133 [INFO] WRITING LOG OUTPUT TO C:\\Users\\guiet\\.cellpose\\run.log\n2022-01-31 16:53:01,409 [INFO] ** TORCH CUDA version installed and working. **\n2022-01-31 16:53:01,410 [INFO] &gt;&gt;&gt;&gt; using GPU\n&gt;&gt;&gt;&gt; Omnipose enabled. See https://raw.githubusercontent.com/MouseLand/cellpose/master/cellpose/omnipose/license.txt for licensing details.\n2022-01-31 16:53:01,416 [INFO] &gt;&gt;&gt;&gt; running cellpose on 1 images using chan_to_seg RED and chan (opt) GREEN\nTraceback (most recent call last):\n  File \"D:\\conda_envs\\cellposeGPU100_113-821\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"D:\\conda_envs\\cellposeGPU100_113-821\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"D:\\conda_envs\\cellposeGPU100_113-821\\lib\\site-packages\\cellpose\\__main__.py\", line 458, in &lt;module&gt;\n    main()\n  File \"D:\\conda_envs\\cellposeGPU100_113-821\\lib\\site-packages\\cellpose\\__main__.py\", line 240, in main\n    logger.info('&gt;&gt;&gt;&gt; omni is ON, cluster is %d'%(args.omni,args.cluster))\nTypeError: not all arguments converted during string formatting\n</code></pre>\n<p>So the issue might come from the omni model/method <img src=\"https://emoji.discourse-cdn.com/twitter/smiling_face_with_tear.png?v=12\" title=\":smiling_face_with_tear:\" class=\"emoji\" alt=\":smiling_face_with_tear:\"></p>\n<p>Can you give another try without selection --omni?</p>\n<p>Best</p>\n<p>R</p>", "<aside class=\"quote no-group\" data-username=\"romainGuiet\" data-post=\"8\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/romainguiet/40/81_2.png\" class=\"avatar\"> romainGuiet:</div>\n<blockquote>\n<p><code>    logger.info('&gt;&gt;&gt;&gt; omni is ON, cluster is %d'%(args.omni,args.cluster))</code></p>\n</blockquote>\n</aside>\n<p>I referenced an issue and offered a PR to fix it on the cellpose github</p><aside class=\"onebox githubpullrequest\" data-onebox-src=\"https://github.com/MouseLand/cellpose/pull/434\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/pull/434\" target=\"_blank\" rel=\"noopener\">github.com/MouseLand/cellpose</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Pull Request\">\n    <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 12 16\" aria-hidden=\"true\"><path d=\"M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/MouseLand/cellpose/pull/434\" target=\"_blank\" rel=\"noopener\">Update __main__.py</a>\n    </h4>\n\n    <div class=\"branches\">\n      <code>MouseLand:master</code> \u2190 <code>lacan:patch-1</code>\n    </div>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-02-01\" data-time=\"09:07:41\" data-timezone=\"UTC\">09:07AM - 01 Feb 22 UTC</span>\n      </div>\n\n      <div class=\"user\">\n        <a href=\"https://github.com/lacan\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"lacan\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/a/fa442d9041c2e00af96a3c1eb73d3ed8e8199552.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          lacan\n        </a>\n      </div>\n\n      <div class=\"lines\" title=\"1 commits changed 1 files with 1 additions and 1 deletions\">\n        <a href=\"https://github.com/MouseLand/cellpose/pull/434/files\" target=\"_blank\" rel=\"noopener\">\n          <span class=\"added\">+1</span>\n          <span class=\"removed\">-1</span>\n        </a>\n      </div>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Error in omni log text that was throwing an error...</p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p><a class=\"mention\" href=\"/u/romainguiet\">@romainGuiet</a>,<br>\nI had high hopes, but\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/weary.png?v=12\" title=\":weary:\" class=\"emoji\" alt=\":weary:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/weary.png?v=12\" title=\":weary:\" class=\"emoji\" alt=\":weary:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/weary.png?v=12\" title=\":weary:\" class=\"emoji\" alt=\":weary:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/weary.png?v=12\" title=\":weary:\" class=\"emoji\" alt=\":weary:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/weary.png?v=12\" title=\":weary:\" class=\"emoji\" alt=\":weary:\"></p>\n<p>Nope,<br>\nSame issue.</p>\n<pre><code class=\"lang-auto\">cyto_channel:1:nuclei_channel:0\nC:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp\\SRFG_D8_73_X60_SCAN1-1-t1.tif\n[cmd.exe, /C, conda, activate, D:\\anaconda3\\envs\\cellpose_07, &amp;, python, -m, cellpose, --dir, C:\\Users\\owner\\AppData\\Local\\Temp\\cellposeTemp, --pretrained_model, cyto2, --chan, 1, --diameter, 55, --flow_threshold, 0.4, --mask_threshold, 0.0, --save_tif, --no_npy, --use_gpu]\njava.lang.NullPointerException\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:224)\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)\n\tat org.scijava.command.CommandModule.run(CommandModule.java:196)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:165)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n</code></pre>\n<p>Any other leads to follow?</p>\n<p>Thx,<br>\nDaniel</p>", "<p><img src=\"https://emoji.discourse-cdn.com/twitter/face_with_spiral_eyes.png?v=12\" title=\":face_with_spiral_eyes:\" class=\"emoji only-emoji\" alt=\":face_with_spiral_eyes:\"></p>\n<p>I just want to make sure that if we run the same line we get the same result, can you start your conda env and run the line below (just replace <code>$DirectoryWithImage$</code> ), you should get the output image in the same folder</p>\n<p><code>python -m cellpose --dir $DirectoryWithImage$ --pretrained_model cyto2 --chan 1 --diameter 55 --flow_threshold 0.4 --mask_threshold 0.0 --save_tif --no_npy --use_gpu --resample</code></p>\n<p>Cheers,</p>\n<p>R</p>", "<p><img src=\"https://emoji.discourse-cdn.com/twitter/ok_hand.png?v=12\" title=\":ok_hand:\" class=\"emoji only-emoji\" alt=\":ok_hand:\"></p>\n<aside class=\"quote no-group\" data-username=\"romainGuiet\" data-post=\"11\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/romainguiet/40/81_2.png\" class=\"avatar\"> romainGuiet:</div>\n<blockquote>\n<p>DirectoryWithImage</p>\n</blockquote>\n</aside>\n<p>Should I just replace it with a folder path?<br>\nJust making sure that I understand your request.</p>\n<p>I\u2019ll do it first thing in the morning!</p>\n<p>Thanks,<br>\nDaniel</p>", "<p>Hi <a class=\"mention\" href=\"/u/romainguiet\">@romainGuiet</a>,</p>\n<p>Tomorrow finally arrived (facility life <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">).</p>\n<pre><code class=\"lang-auto\">(cellpose) C:\\Users\\owner&gt;python -m cellpose --dir $C:\\Users\\owner\\Desktop\\original_tif$ --pretrained_model cyto2 --chan 1 --diameter 55 --flow_threshold 0.4 --mask_threshold 0.0 --save_tif --no_npy --use_gpu --resample\nusage: __main__.py [-h] [--use_gpu] [--check_mkl] [--mkldnn] [--dir DIR] [--look_one_level_down] [--mxnet]\n                   [--img_filter IMG_FILTER] [--channel_axis CHANNEL_AXIS] [--z_axis Z_AXIS] [--chan CHAN]\n                   [--chan2 CHAN2] [--invert] [--all_channels] [--pretrained_model PRETRAINED_MODEL] [--unet UNET]\n                   [--nclasses NCLASSES] [--omni] [--cluster] [--fast_mode] [--no_resample] [--no_net_avg]\n                   [--no_interp] [--do_3D] [--diameter DIAMETER] [--stitch_threshold STITCH_THRESHOLD]\n                   [--flow_threshold FLOW_THRESHOLD] [--mask_threshold MASK_THRESHOLD] [--anisotropy ANISOTROPY]\n                   [--diam_threshold DIAM_THRESHOLD] [--exclude_on_edges] [--save_png] [--save_tif] [--no_npy]\n                   [--savedir SAVEDIR] [--dir_above] [--in_folders] [--save_flows] [--save_outlines] [--save_ncolor]\n                   [--save_txt] [--train] [--train_size] [--mask_filter MASK_FILTER] [--test_dir TEST_DIR]\n                   [--learning_rate LEARNING_RATE] [--n_epochs N_EPOCHS] [--batch_size BATCH_SIZE]\n                   [--min_train_masks MIN_TRAIN_MASKS] [--residual_on RESIDUAL_ON] [--style_on STYLE_ON]\n                   [--concatenation CONCATENATION] [--save_every SAVE_EVERY] [--save_each] [--verbose] [--testing]\n__main__.py: error: unrecognized arguments: --resample\n</code></pre>\n<p>Still an error!</p>\n<p>What\u2019s next?</p>\n<p>D</p>", "<p>When trying to run in a cellpose 0.7 env,<br>\nit claims that there are no images in the folder.</p>\n<pre><code class=\"lang-auto\">(cellpose_07) C:\\Users\\owner&gt;python -m cellpose --dir $C:\\Users\\owner\\Desktop\\original_tif$ --pretrained_model cyto2 --chan 1 --diameter 55 --flow_threshold 0.4 --mask_threshold 0.0 --save_tif --no_npy --use_gpu --resampl\n2022-03-14 10:38:14,662 [INFO] WRITING LOG OUTPUT TO C:\\Users\\owner\\.cellpose\\run.log\n2022-03-14 10:38:15,020 [INFO] ** TORCH CUDA version installed and working. **\n2022-03-14 10:38:15,020 [INFO] &gt;&gt;&gt;&gt; using GPU\nTraceback (most recent call last):\n  File \"D:\\anaconda3\\envs\\cellpose_07\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"D:\\anaconda3\\envs\\cellpose_07\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"D:\\anaconda3\\envs\\cellpose_07\\lib\\site-packages\\cellpose\\__main__.py\", line 436, in &lt;module&gt;\n    main()\n  File \"D:\\anaconda3\\envs\\cellpose_07\\lib\\site-packages\\cellpose\\__main__.py\", line 214, in main\n    image_names = io.get_image_files(args.dir,\n  File \"D:\\anaconda3\\envs\\cellpose_07\\lib\\site-packages\\cellpose\\io.py\", line 123, in get_image_files\n    raise ValueError('ERROR: no images in --dir folder')\nValueError: ERROR: no images in --dir folder\n</code></pre>", "<p>Hi,</p>\n<aside class=\"quote no-group\" data-username=\"Daniel_Waiger\" data-post=\"13\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/daniel_waiger/40/24019_2.png\" class=\"avatar\"> Daniel_Waiger:</div>\n<blockquote>\n<p><code>__main__.py: error: unrecognized arguments: --resample</code></p>\n</blockquote>\n</aside>\n<p>I guess you are using the cellpose 1.0 and resample is by default and should not be added to the line</p>\n<aside class=\"quote no-group\" data-username=\"Daniel_Waiger\" data-post=\"14\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/daniel_waiger/40/24019_2.png\" class=\"avatar\"> Daniel_Waiger:</div>\n<blockquote>\n<p>it claims that there are no images in the folder.</p>\n</blockquote>\n</aside>\n<p>Not sure about the use of <code>$</code> , did you tried without the <code>$</code> ?</p>\n<p>Cheers,</p>\n<p>R</p>", "<p>Hi all,</p>\n<p>Just to add to this, I\u2019m getting a similar error when trying to use Cellpose in Fiji as well using Cellpose 0.7</p>\n<pre data-code-wrap=\"log\"><code class=\"lang-nohighlight\">[INFO] true\nNOTE : Can't use 3D mode, on 2D image\nC:\\Users\\guerard\\AppData\\Local\\Temp\\cellposeTemp\\MAX_TEST_01.vsi-C405,C488,C561,C640-1-1-t1.tif\n[cmd.exe, /C, conda, activate, Z:\\Melvin\\cellpose, &amp;, python, -m, cellpose, --dir, C:\\Users\\guerard\\AppData\\Local\\Temp\\cellposeTemp, --pretrained_model, nuclei, --chan, 1, --diameter, 17, --flow_threshold, 0.4, --cellprob_threshold, 0.0, --save_tif, --no_npy]\njava.lang.NullPointerException\n\tch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:224)\n\tch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)\n\tch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentNucleiImgPlusAdvanced.run(Cellpose_SegmentNucleiImgPlusAdvanced.java:57)\n\tch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentNucleiImgPlusBasic.run(Cellpose_SegmentNucleiImgPlusBasic.java:32)\n\torg.scijava.command.CommandModule.run(CommandModule.java:196)\n\torg.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\torg.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\torg.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\torg.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tjava.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tjava.lang.Thread.run(Thread.java:748)\n\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusOwnModelAdvanced.run(Cellpose_SegmentImgPlusOwnModelAdvanced.java:224)\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentImgPlusAdvanced.run(Cellpose_SegmentImgPlusAdvanced.java:110)\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentNucleiImgPlusAdvanced.run(Cellpose_SegmentNucleiImgPlusAdvanced.java:57)\n\tat ch.epfl.biop.wrappers.cellpose.ij2commands.Cellpose_SegmentNucleiImgPlusBasic.run(Cellpose_SegmentNucleiImgPlusBasic.java:32)\n\tat org.scijava.command.CommandModule.run(CommandModule.java:196)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/lguerard\">@lguerard</a> ,<br>\ncould you share your image ?<br>\nDid you check that it works if you\u2019re using the command line in your cellpose env ?<br>\nCheers,<br>\nR</p>", "<p>Not yet, I was trying to apply it on an image that wasn\u2019t saved on disk, is that not possible at all ? I will try and save it on disk and see if the command line works and keep you posted.</p>", "<aside class=\"quote no-group\" data-username=\"lguerard\" data-post=\"18\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/lguerard/40/14201_2.png\" class=\"avatar\"> lguerard:</div>\n<blockquote>\n<p>Not yet, I was trying to apply it on an image that wasn\u2019t saved on disk, is that not possible at all ?</p>\n</blockquote>\n</aside>\n<p>Yes and no <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\">  , the wrapper will save the active image in a \u201ctemporary\u201d folder (in your case : <code>C:\\Users\\guerard\\AppData\\Local\\Temp\\cellposeTemp</code> ), run a cellpose command on it and re-open the cellpose results in ImageJ/Fiji.</p>\n<p>What are the dimension of the image ? Can you try to duplicate only the nuclei channel before runnning?</p>\n<p>Cheers,</p>\n<p>R</p>", "<aside class=\"quote no-group\" data-username=\"romainGuiet\" data-post=\"19\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/romainguiet/40/81_2.png\" class=\"avatar\"> romainGuiet:</div>\n<blockquote>\n<p>Yes and no <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"> , the wrapper will save the active image in a \u201ctemporary\u201d folder (in your case : <code>C:\\Users\\guerard\\AppData\\Local\\Temp\\cellposeTemp</code> ), run a cellpose command on it and re-open the cellpose results in ImageJ/Fiji.</p>\n</blockquote>\n</aside>\n<p>Ok cool, so it shouldn\u2019t change anything if I save it myself or not <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> . I will still try in order to run the command line.</p>\n<aside class=\"quote no-group\" data-username=\"romainGuiet\" data-post=\"19\" data-topic=\"62632\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/romainguiet/40/81_2.png\" class=\"avatar\"> romainGuiet:</div>\n<blockquote>\n<p>What are the dimension of the image ? Can you try to duplicate only the nuclei channel before runnning?</p>\n</blockquote>\n</aside>\n<p>This image is 2304x2304 and it was only the nuclei channel <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>EDIT : Just tested and it complained about <code>unrecognized argument --cellprob_threshold</code> Removing it worked fine.</p>"], "79017": ["<p>Hi There,</p>\n<p>I am measuring numbers of nuclei per spot in one H&amp;E image of 10X visium datasets. As I have nearly 4000 different annotation objects, is there any way I can select and focus on the object I want via searching for its \u201cName\u201d or \u201cObject ID\u201d?</p>\n<p>And this is my [quote=\u201cYuxuan Xie, post:1, topic:79017, full:true, username:YuxuanXie\u201d]<br>\nHi There,</p>\n<p>I am measuring numbers of nuclei per spot in one H&amp;E image of 10X visium datasets. As I have nearly 4000 different annotation objects, is there any way I can select and focus on the object I want via searching for its \u201cName\u201d or \u201cObject ID\u201d?</p>\n<p>This is my attempt for the task.</p>\n<pre><code class=\"lang-auto\">import qupath.lib.objects.PathObject;\nimport qupath.lib.objects.PathAnnotationObject;\n\n// Get the current project\nvar currentProject = getProject();\n\n// Get all annotation objects in the project\nvar annotationObjects = currentProject.getAllObjects().findAll { obj -&gt;\n  obj instanceof PathAnnotationObject\n}.collect { obj -&gt;\n  obj.getPathObject()\n}.findAll { obj -&gt;\n  obj instanceof PathObject\n};\n\n// Find the annotation object with the specified name\nvar annotationObject = annotationObjects.find { obj -&gt;\n  obj.getName() == \"ATACCAGGTGAGCGAT-1\"\n};\n\n// Select the annotation object in the view\nif (annotationObject) {\n  getViewer().selectObject(annotationObject);\n} else {\n  println(\"Annotation object not found\");\n}\n</code></pre>\n<p>But it comes the errors as such.</p>\n<pre><code class=\"lang-auto\">ERROR: It looks like you've tried to access a method that doesn't exist.\n\n\nERROR: No signature of method: qupath.lib.projects.DefaultProject.getAllObjects() is applicable for argument types: () values: [] in QuPathScript at line number 8\n\n</code></pre>\n<p>Any suggestions? Many thanks!</p>\n<p>Yuxuan</p>", "<p>This is your problem:</p>\n<aside class=\"quote no-group\" data-username=\"YuxuanXie\" data-post=\"1\" data-topic=\"79017\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/977dab/40.png\" class=\"avatar\"> Yuxuan Xie:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">var annotationObjects = currentProject.getAllObjects().findAll { obj -&gt;\n  obj instanceof PathAnnotationObject\n</code></pre>\n</blockquote>\n</aside>\n<p>Try something like:</p>\n<pre><code class=\"lang-auto\">var annotationObjects = getAnnotationObjects()\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"ChrisStarling\" data-post=\"2\" data-topic=\"79017\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/chrisstarling/40/63517_2.png\" class=\"avatar\"> ChrisStarling:</div>\n<blockquote>\n<p>This is your problem:</p>\n</blockquote>\n</aside>\n<p>Agreed, I don\u2019t think there is a way to get all of the objects in a whole project, just within an image hierarchy. getAnnotationObjects will work for the current image. You would need to iterate through subsequent images to process the whole project, which is usually done by \u201cRun for project\u201d  <a href=\"https://qupath.readthedocs.io/en/stable/docs/scripting/workflows_to_scripts.html#running-a-script-for-multiple-images\" class=\"inline-onebox\">Workflows to scripts \u2014 QuPath 0.4.3 documentation</a></p>\n<p>I also do not think there is a selectObject() function at all, when checking the javadocs. How was this script generated?</p>", "<p>Hi, there!</p>\n<p>After Chris\u2019 suggestion, my first error is gone. But like you suggested, there is no selectObject() method indeed.</p>\n<pre><code class=\"lang-auto\">ERROR: It looks like you've tried to access a method that doesn't exist.\n\n\nERROR: No signature of method: org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getViewer() is applicable for argument types: () values: [] in QuPathScript at line number 17\n\n</code></pre>\n<p>which points to this line</p>\n<pre><code class=\"lang-auto\">// Select the annotation object in the view\nif (annotationObject) {\n  getViewer().selectObject(annotationObject);\n}\n</code></pre>\n<p>Best,<br>\nYuxuan</p>", "<p>Do the annotations have a class, or could the name be set as the class?</p>\n<p>Or is the listed name actually the class?</p>", "<p>Currently the class of annotation is set to be \u201cSpot\u201d as shown below:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/7368c54a969b2c614564d0bff47536e7d70975ce.png\" alt=\"image\" data-base62-sha1=\"gsXnvK3WRYcXItOcVfQEh2zAnme\" width=\"589\" height=\"338\"></p>\n<p>The Name and Class are different here.</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"3\" data-topic=\"79017\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>I don\u2019t think there is a way to get all of the objects in a whole project, just within an image hierarchy</p>\n</blockquote>\n</aside>\n<p>You technically can loop through all entries in a project, use <code>hierarchy.getAnnotationObjects()</code> to get the objects from each image hierarchy, and add them to a list to collect all objects from each entry\u2026</p>\n<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"3\" data-topic=\"79017\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>I also do not think there is a selectObject() function at all</p>\n</blockquote>\n</aside>\n<p>There is <code>selectObjects()</code> which should work in place of <code>selectObject()</code>.</p>", "<aside class=\"quote no-group\" data-username=\"ym.lim\" data-post=\"7\" data-topic=\"79017\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png\" class=\"avatar\"> Yau Mun Lim:</div>\n<blockquote>\n<p>You technically can loop through all entries in a project, use <code>hierarchy.getAnnotationObjects()</code> to get the objects from each image hierarchy, and add them to a list to collect all objects from each entry\u2026</p>\n</blockquote>\n</aside>\n<p>Yep, exactly. That\u2019s one way of iterating through next images.</p>\n<p>And selectObjects should work though I don\u2019t think it is part of getViewer()</p>", "<p>Thank you Yau Mun, Chris and Kind MicroscopyRA!</p>\n<p>The code now runs very smoothly.<br>\nHere is it:</p>\n<pre><code class=\"lang-auto\">import qupath.lib.objects.PathObject;\nimport qupath.lib.objects.PathAnnotationObject;\n\n// Get the current project\nvar currentProject = getProject();\n\n// Get all annotation objects in the project\nvar annotationObjects = getAnnotationObjects()\n\n// Find the annotation object with the specified name\nvar annotationObject = annotationObjects.find { obj -&gt;\n  obj.getName() == \"ATACCAGGTGAGCGAT-1\"\n};\n\n// Select the annotation object in the view\nif (annotationObject) {\n  selectObjects(annotationObject);\n} else {\n  println(\"Annotation object not found\");\n}\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/5/f5dca33ee4cc5d945495be49140222f064fa3fc1.jpeg\" data-download-href=\"/uploads/short-url/z4ZH2UxDO81qKMbhsPveaapAhAl.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/5/f5dca33ee4cc5d945495be49140222f064fa3fc1.jpeg\" alt=\"image\" data-base62-sha1=\"z4ZH2UxDO81qKMbhsPveaapAhAl\" width=\"514\" height=\"500\" data-dominant-color=\"C78D9F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">621\u00d7604 41.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you all again for the swift replies! Good luck in your research.</p>\n<p>Best,</p>\n<p>Yuxuan</p>", "<p>One note, you are currently only finding one example of an object with that name, even if more then one exists. Is that expected?</p>\n<p>The whole thing could also be written as<br>\n<code>selectObjects( getAnnotationObjects().findAll{ it.getName() == \"ATACCAGGTGAGCGAT-1\" } )</code><br>\nWithout any imports or other variable definitions. As an option.</p>\n<p>It would also select all objects with that particular name string.</p>", "<aside class=\"quote no-group quote-modified\" data-username=\"Research_Associate\" data-post=\"10\" data-topic=\"79017\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>selectObjects( getAnnotationObjects().findAll{ it.getName() == \u201cATACCAGGTGAGCGAT-1\u201d } )</p>\n</blockquote>\n</aside>\n<p>Indeed, as each spot is annotated with an unique barcode, I won\u2019t have repetitive object names on my slide.</p>\n<p>Thank you very much for the shortened code. Elegant!</p>\n<p>Best,</p>\n<p>Yuxuan</p>", "<p>Sounds good. I\u2019ll just leave this here too in case it is relevant. <a href=\"https://forum.image.sc/t/qupath-for-spatial-trans/78657/3\" class=\"inline-onebox\">QuPath for spatial trans - #3 by Mark_Zaidi</a></p>", "<p>In that case, I\u2019ll leave my response to Mark\u2019s video as well <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> <a href=\"https://forum.image.sc/t/contruct-voronoi-polygons-from-a-set-of-points-groovy-script/70998/13\" class=\"inline-onebox\">Contruct Voronoi polygons from a set of points (groovy script) - #13 by petebankhead</a></p>", "<p>If your goal is to measure the number of nuclei per spot in an H&amp;E image of a Visium-processed sample, Squidpy can already do this through their default tools. The <a href=\"https://squidpy.readthedocs.io/en/stable/auto_examples/image/compute_segmentation_features.html\">Extract Segmentation Features</a> demo shows how you can use <a href=\"https://squidpy.readthedocs.io/en/stable/api/squidpy.im.segment.html#squidpy.im.segment\">squidpy.im.segment</a> to run a segmentation model of your choice (watershed, but I\u2019ve also got it to work with StarDist)</p>\n<p><code>segmentation_label </code> will show the number of detected nuclei per spot. Other exported features include average  and std channel (R, G, B) intensity.</p>\n<p>Unlike QuPath, Squidpy has yet to implement stain deconvolution. Meaning, segmentation will undoubtably be poorer in Squidpy as your only options for the built-in watershed segmentation are to pass in just one channel, or the RGB mean. Calling a pre-trained StarDist H&amp;E model in Squidpy might yield better results.</p>"], "79018": ["<p>Hi <a class=\"mention\" href=\"/u/wayne\">@Wayne</a>,</p>\n<p>I am confused about the differences between the outputs of the 3 different ways of filtering the FFT:</p>\n<pre><code class=\"lang-auto\">run(\"Comparing Lengths\");\nrun(\"FFT\");\n\n//clear central vertical line\nmakeRectangle(254, 0, 5, 512);\nsetBackgroundColor(0, 0, 0);\nrun(\"Clear\", \"slice\");\n\nrun(\"Create Mask\");\nrun(\"Invert\");\n\n//Inverse FFT\nselectWindow(\"FFT of Comparing Lengths-1\");\nrun(\"FFT\");\n\n// FFT using filter\nselectWindow(\"Comparing Lengths-1\");\nrun(\"Duplicate...\", \" \");\nrun(\"Custom Filter...\", \"filter=Mask\");\nrename(\"FFT using mask as filter\");\nrun(\"Tile\");\n\n// FFT using filter on 8-bit\nselectWindow(\"Comparing Lengths-1\");\nrun(\"Duplicate...\", \" \");\nrun(\"8-bit\");\nrun(\"Custom Filter...\", \"filter=Mask\");\nrename(\"FFT using mask as filter on 8-bit input\");\nrun(\"Tile\");\n</code></pre>\n<p>Outputs:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea.jpeg\" data-download-href=\"/uploads/short-url/cqGqsIUEPnQAbgiiOKTz2facKfM.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_690x149.jpeg\" alt=\"image\" data-base62-sha1=\"cqGqsIUEPnQAbgiiOKTz2facKfM\" width=\"690\" height=\"149\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_690x149.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_1035x223.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/571e49263ea3e9032138263064cb9fb141b086ea_2_1380x298.jpeg 2x\" data-dominant-color=\"7F7F7F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2057\u00d7447 126 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is this working as intended?<br>\nI am confused by the outputs of the <code>run(\"Custom Filter...\", \"filter=Mask\");</code> commands.</p>\n<p>Thanks a lot</p>"], "79020": ["<p>Dear Qupath user, I try to convert my detections (by cellpose) into cells . Is there any script to do it ? I need probably to erode my detections before as Cellpose detect the cytoplasmic (cyto2 model) of my cells. My cell are quite round and little so the cytoplasmic is really around the nucleus (T and  B cells).Thanks fro your help.</p>", "<p>I think that was covered here <a href=\"https://forum.image.sc/t/qupath-cellpose-cell-objects/65691\" class=\"inline-onebox\">QuPath - Cellpose - Cell objects</a><br>\nand here <a href=\"https://forum.image.sc/t/combining-cellpose-and-stardist-detections-into-cells/78225/5\" class=\"inline-onebox\">Combining Cellpose and StarDist detections into cells - #5 by ChrisStarling</a><br>\nYou can use the same ROI for the nucleus and cytoplasm if you only have one ROI, or maybe use <code>null</code>.</p>", "<p>yes true it is perfect !</p>"], "79021": ["<p>Hello guys.<br>\nCan you help me to calculet the mean density values from a  specific area of a CT image?<br>\nThanks a lot</p>", "<p>Hopefully someone can help you, but I think they will need more information.</p>", "<p>You can do this in <a href=\"https://www.slicer.org\" rel=\"noopener nofollow ugc\">3D Slicer</a> by <a href=\"https://slicer.readthedocs.io/en/latest/user_guide/modules/dicom.html#read-dicom-files-into-the-scene\" rel=\"noopener nofollow ugc\">loading the DICOM image</a>, segmenting the region using <a href=\"https://slicer.readthedocs.io/en/latest/user_guide/modules/segmenteditor.html\" rel=\"noopener nofollow ugc\">Segment Editor module</a>, and computing average intensity using <a href=\"https://slicer.readthedocs.io/en/latest/user_guide/modules/segmentstatistics.html\" rel=\"noopener nofollow ugc\">Segment Statistics module</a>.</p>", "<p>Sorry for bothering you sir. Can you tell me if the surface from the table is the surface area of the tissue thresholded for -29 + 150 HU? And that the average intesity is thea mean from that table. Thanks a lot sir!</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c.png\" data-download-href=\"/uploads/short-url/eh0uGUYyYNa6KB8xa3VZ72wBJjC.png?dl=1\" title=\"3 D slicer segmentation\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c_2_690x333.png\" alt=\"3 D slicer segmentation\" data-base62-sha1=\"eh0uGUYyYNa6KB8xa3VZ72wBJjC\" width=\"690\" height=\"333\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c_2_690x333.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c_2_1035x499.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/4/640a9e179cd726c269ac710459185c5bafd5164c.png 2x\" data-dominant-color=\"353338\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">3 D slicer segmentation</span><span class=\"informations\">1336\u00d7646 70.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"Radu_Vrabie\" data-post=\"4\" data-topic=\"79021\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/radu_vrabie/40/20171_2.png\" class=\"avatar\"> Radu Vrabie:</div>\n<blockquote>\n<p>Can you tell me if the surface from the table is the surface area of the tissue thresholded for -29 + 150 HU? And that the average intesity is thea mean from that table.</p>\n</blockquote>\n</aside>\n<p>Correct.</p>\n<p>In the segmented region the minimum voxel intensity is -29 HU, maximum is 150 HU and the average is 30.5 HU.</p>\n<p>The surface area of the segmented region is a 3D surface area (includes the top and bottom surface and side). If you want to get the cross-section area of a single slice then you need to divide the value by 2, or for a simpler and more accurate computation, you can multiply the number of voxels by the pixel size. For example, if pixel size in the image slice is 0.2mm x 0.2mm and number of voxels is 21400 then the cross-sectional area is 21400 x 0.8mm x 0.8mm =13696mm2. You can see the pixel size in the Volume Information section in Volumes module.</p>\n<p>Note that you can get cross-sectional area along each slice in a 3D segmentation using \u201cSegment Cross-Section Area\u201d module (provided by Sandbox extension).</p>", "<aside class=\"quote no-group\" data-username=\"Radu_Vrabie\" data-post=\"4\" data-topic=\"79021\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/radu_vrabie/40/20171_2.png\" class=\"avatar\"> Radu Vrabie:</div>\n<blockquote>\n<p>Can you tell me if the surface from the table is the surface area of the tissue thresholded for -29 + 150 HU? And that the average intesity is thea mean from that table.</p>\n</blockquote>\n</aside>\n<p>Correct.</p>\n<p>In the segmented region the minimum voxel intensity is -29 HU, maximum is 150 HU and the average is 30.5 HU.</p>\n<p>The surface area of the segmented region is a 3D surface area (includes the top and bottom surface and side). If you want to get the cross-section area of a single slice then you need to divide the value by 2, or for a simpler and more accurate computation, you can multiply the number of voxels by the pixel size. For example, if pixel size in the image slice is 0.2mm x 0.2mm and number of voxels is 21400 then the cross-sectional area is 21400 x 0.8mm x 0.8mm =13696mm2. You can see the pixel size in the Volume Information section in Volumes module.</p>\n<p>Note that you can get cross-sectional area along each slice in a 3D segmentation using \u201cSegment Cross-Section Area\u201d module (provided by Sandbox extension).</p>"], "79024": ["<p>What is the difference between classes with or without a ***** after the class names\uff1fThanks\uff01<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18bbeac08f1d89dd0bd90762fcd41084afc51331.png\" alt=\"image\" data-base62-sha1=\"3wO4jzx8RocEYHmXgggRuHv7wyt\" width=\"269\" height=\"424\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/surgdocc\">@Surgdocc</a> there\u2019s some info on this at <a href=\"https://qupath.readthedocs.io/en/0.4/docs/concepts/classifications.html#ignored-classifications\" class=\"inline-onebox\">Classifications \u2014 QuPath 0.4.3 documentation</a></p>", "<p>Thank you so much, Pete. <img src=\"https://emoji.discourse-cdn.com/twitter/grinning.png?v=12\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "79027": ["<p>Hi,</p>\n<p>I have been playing with the Micro-Magellan recently. The performance of the NDTiff is very impressive, and the Dask array was pretty handy. But so far, I could only use it for storing the raw data. I am trying to do batch processing and normalization to the images (no changes to the dimension or dtype) and possibly save them back as NDTiff. Before I do too much hacking, is there already an easy way to make this happen? I tried to directly change the tiff files and save them back will modify the header files. Rewriting the metadata didn\u2019t help. Excuse me if I didn\u2019t read the documents carefully enough.<br>\nThank you!</p>\n<p>Yehe</p>", "<p>The only official way to write NDTiff right now is through the Java library. This is doable through python, but will be somewhat slow (100-200 MB/s). If you only want to modify pixels and keep the dtype the same and not make any modifications to metadata, you could probably do this is a somewhat hacky way where you overwrote the pixel data at each place it is stored in the file. If you look through the <code>dataset.index</code> field, you should be able to find out where each image is stored and then use python file IO to overwrite the pixels at each location</p>", "<aside class=\"quote no-group\" data-username=\"henrypinkard\" data-post=\"2\" data-topic=\"79027\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/henrypinkard/40/30258_2.png\" class=\"avatar\"> Henry Pinkard:</div>\n<blockquote>\n<p>If you only want to modify pixels and keep the dtype the same and not make any modifications to metadata, you could probably do this is a somewhat hacky way where you overwrote the pixel data at each place it is stored in the file</p>\n</blockquote>\n</aside>\n<p>That is possible with the <a href=\"https://pypi.org/project/tifffile/\" rel=\"noopener nofollow ugc\">tifffile</a> library, e.g.:</p>\n<pre><code class=\"lang-python\">import tifffile\nimport zarr\n\nstore = tifffile.imread('NDTiff3.2_multichannel_NDTiffStack.tif', mode='r+', aszarr=True)\nz = zarr.open(store, mode='r+')\nz[:, 1, :, 16:32, 24:42] = 100\nstore.close()\n</code></pre>\n<p>Support for NDTiffStorage in tifffile is relatively new. In any case, make a copy of the folder containing the ndtiff TIFF and index files before. Some metadata, like ranges and precision, might get out of sync with the data. Tifffile does not handle pyramids, tiled frames, or overlapping frames as the <a href=\"https://pypi.org/project/ndtiff/\" rel=\"noopener nofollow ugc\">ndtiff</a> library.</p>", "<p>Thank you for the suggestions. I\u2019ll give it a try! The Zarr hint was really helpful!</p>"], "79028": ["<p>Hi gugs,<br>\nWhen facing such a WSI, how can I make batch annotation rather than annotating these small tumor areas one by one? Could you please tell me if there are some efficient methods? Thanks!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b.jpeg\" data-download-href=\"/uploads/short-url/o8YsJqWCcAMG6hsf8X27jb31jlx.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b_2_535x500.jpeg\" alt=\"image\" data-base62-sha1=\"o8YsJqWCcAMG6hsf8X27jb31jlx\" width=\"535\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b_2_535x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b_2_802x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a937e715a4605c9bfff079837a96fdedcdf09d8b.jpeg 2x\" data-dominant-color=\"AE67AB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">899\u00d7839 228 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>I\u2019d recommend looking through this section of the docs <a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/pixel_classification.html\" class=\"inline-onebox\">Pixel classification \u2014 QuPath 0.4.3 documentation</a></p>", "<p>Thank you\uff01<br>\nI have trained a pixel classifier and detected tumor areas. But it seems that they  can only guide me to annotate. I can not keep them and still can not make batch annotations.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5.jpeg\" data-download-href=\"/uploads/short-url/22YcP00fiDXT5rZERMiZXqQMOxv.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_690x301.jpeg\" alt=\"image\" data-base62-sha1=\"22YcP00fiDXT5rZERMiZXqQMOxv\" width=\"690\" height=\"301\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e5a536a8d49223f5e03b956c059417ecbbff3a5_2_1380x602.jpeg 2x\" data-dominant-color=\"855983\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1797\u00d7785 259 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>The link and video shows you how to create annotations from a trained pixel classifier. In your image you haven\u2019t saved the classifier. Maybe you didn\u2019t create a project either? Not sure.</p>"], "79029": ["<p>Hi All,</p>\n<p>I have a list of coordinates from which I want to do polyline in the roiManager. It does not work on all the list.</p>\n<p>I reproduce the problem in the following macro where the first arrays works but the 2nd arrays does\u2019t.</p>\n<p>I can\u2019t figure out  why .</p>\n<p>Any ideas ?</p>\n<pre><code class=\"lang-auto\">//Macro starts\n\nnewImage(\"Untitled\", \"8-bit black\", 200, 200, 1);\n\nxpoints=newArray(12,12);\nypoints=newArray(51.733,51.986);\n\n\nmakeSelection( \"polyline\", xpoints, ypoints );\n        roiManager(\"Add\");\n\n\nxpoints2=newArray(11.572,11.594);\nypoints2=newArray(51.733,51.986);\n\nmakeSelection( \"polyline\", xpoints2, ypoints2 );\n        roiManager(\"Add\");\n\n\n// Macro End\n</code></pre>\n<p>\u2013</p>\n<p>Eric Denarier<br>\nGrenoble Institut des Neurosciences<br>\nInserm U1216<br>\nChemin Fortun\u00e9 Ferrini<br>\n38700 La Tronche<br>\nFrance</p>\n<p>T\u00e9l :33 (0)4 565 205 23</p>", "<p>I think the script is fine, the values are wrong.</p>\n<pre><code class=\"lang-auto\">newImage(\"Untitled\", \"8-bit white\", 200, 200, 1);\n\nxpoints=newArray(12,22);\nypoints=newArray(51,62);\n\n\nmakeSelection( \"polyline\", xpoints, ypoints );\n        roiManager(\"Add\");\n\n\nxpoints2=newArray(21,11);\nypoints2=newArray(51,11);\n\nmakeSelection( \"polyline\", xpoints2, ypoints2 );\n        roiManager(\"Add\");\n\n\n</code></pre>\n<p>works for me. And is easier to see since the lines have length. The sub-pixel values seem to be the issue. Your second line is much less than a pixel.</p>", "<p>Interesting.</p>\n<blockquote>\n<p>Your second line is much less than a pixel.</p>\n</blockquote>\n<p>The first line is even shorter, but it works.</p>\n<p>Some kind of rounding up error?<br>\nAlso,</p>\n<pre><code class=\"lang-auto\">\nmakeSelection( \"polygon\", xpoints2, ypoints2 );\n</code></pre>\n<p>seems to work fine.</p>", "<p>Hi Eugene,<br>\nDo you mean that with the value in the macro below it is working for you ?<br>\nI set the decimal number to 9 and it did not change anything</p>\n<p>xpoints2=newArray(11.572,11.594);<br>\nypoints2=newArray(51.733,51.986);</p>\n<p>makeSelection( \u201cpolyline\u201d, xpoints2, ypoints2 );<br>\nroiManager(\u201cAdd\u201d);</p>", "<p>Hi MRA,<br>\nThanks for your answer but I have shorter distance working</p>", "<p>Hi Eric,</p>\n<p>I\u2019ve changed Roi type to polygon, not polyline and it seems fine with it.<br>\nThe best way would be to debug whole ImageJ, but I do not have access to it right now.</p>\n<p>Cheers,<br>\nEugene</p>", "<p>Actually the Error message is : The active image does not have a selection</p>", "<p>Yes, I saw that with the original code, because it isn\u2019t making a selection for you to \u201cAdd\u201d in the next line.</p>", "<p>Cool, I will try to see if it is a problem later !!<br>\nThank you Eugene<br>\nHave a good week end</p>", "<p>It looks like the polyline specifically won\u2019t work for some subpixel angles. For example, you can break the first line by making the xpoints 11.5, 11.5.</p>\n<p>Haven\u2019t figured it out entirely, but it seems fine for pixel values.</p>", "<p>From Wayne on ImageJ list :<br>\nThis bug is fixed in the ImageJ 1.54d22 daily build.</p>\n<p>-wayne</p>"], "79034": ["<p>I\u2019m getting the following error whenever I try to run a CliJ function on my GPU:<br>\nCLIJ2 Warning: You\u2019re creating an image with size 4.4 gigabytes, which exceeds your GPUs capabilities (max 3.9 gigabytes).<br>\nCLIJ Error: Creating an image or kernel failed because your device ran out of memory.<br>\nYou can check memory consumption in CLIJ2 by calling these methods from time to time and see which images live in memory at specific points in your workflow:  Ext.CLIJ2_reportMemory(); // ImageJ Macro  print(clij2.reportMemory()); // Java/groovy/jythonFor support please contact the CLIJ2 developers via the forum on <a href=\"https://image.sc\">https://image.sc</a> .<br>\nTherefore, please report the complete error message, the code snippet or workflow you were running, an example image if possible and details about your graphics hardware.</p>\n<p>I in fact have a 16 GB NVIDIA GeForce RTX 3080 laptop GPU. Why does the plugin only detect 3.9 GB fand how do I get the plugin to detect the full 16 GB? Thank you for your help.</p>", "<p>Hi <a class=\"mention\" href=\"/u/gopherconfocal\">@GopherConfocal</a> ,</p>\n<aside class=\"quote no-group\" data-username=\"GopherConfocal\" data-post=\"1\" data-topic=\"79034\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/gopherconfocal/40/51778_2.png\" class=\"avatar\"> Mary E Brown:</div>\n<blockquote>\n<p>I in fact have a 16 GB NVIDIA GeForce RTX 3080 laptop GPU. Why does the plugin only detect 3.9 GB fand how do I get the plugin to detect the full 16 GB? Thank you for your help.</p>\n</blockquote>\n</aside>\n<p>The reason is a limitation of the GPU and/or its driver. I presume it is related to how chips are soldered on the board of the GPU. But anway, if you were able to load a 16 GB image into your GPU, you couldn\u2019t do anything with it because it has no memory left for a processing result. Thus, the 4 GB limitation isn\u2019t too bad practically.</p>\n<p>You can find out the limitations of your GPU using this little macro:</p>\n<pre><code class=\"lang-auto\">run(\"CLIJ2 Macro Extensions\", \"cl_device=\");\nExt.CLIJ2_clInfo();\n</code></pre>\n<p>In case of my GPU, global memory size is 4 GB and the maximum image size is 1 GB:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/6/46711cf1f5277a56c37e83d6eb2c918afbd58a2e.png\" data-download-href=\"/uploads/short-url/a39LwEECcfS0KjgGN5Z1IsUANVY.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/6/46711cf1f5277a56c37e83d6eb2c918afbd58a2e.png\" alt=\"image\" data-base62-sha1=\"a39LwEECcfS0KjgGN5Z1IsUANVY\" width=\"690\" height=\"447\" data-dominant-color=\"EFEFEE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">791\u00d7513 7.97 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Let me know if this answers your question!</p>\n<p>Best,<br>\nRobert</p>", "<p>So CliJ cannot detect my full GPU even though I have the latest GeForce driver and all the money I spent was for naught?!?</p>", "<aside class=\"quote no-group\" data-username=\"GopherConfocal\" data-post=\"3\" data-topic=\"79034\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/gopherconfocal/40/51778_2.png\" class=\"avatar\"> Mary E Brown:</div>\n<blockquote>\n<p>all the money I spent was for naught?!?</p>\n</blockquote>\n</aside>\n<p>Your full GPU is detected. You can fill all 16 GB with image data, which is necessary anyway to process the image. Just as an example: For applying a Gaussian blur to a 4GB 32-bit image, a total of 16 GB is necessary: 4 GB for the input image, 2x 4GB for temporary intermediate results, and 4GB for the result.</p>\n<p>Addendum: You can debug memory consumption using the <code>GPU memory display</code> in the menu <code>Plugins &gt; CLIJ2 (ImageJ on GPU)</code>:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0b89cd044ac30993239dd856ff22564b8c837bde.gif\" alt=\"14\" data-base62-sha1=\"1E4ukcLLnq82iGl4cPKiL3Qcy0m\" width=\"690\" height=\"350\" class=\"animated\"></p>", "<p>Hi, Robert<br>\nI have a similar related question:<br>\nIn case of large images that exceeds the capacity of GPU memory.<br>\nAre there any functions in CLIJ can allow lazy- loading/processing?<br>\n(Something like \u201cUsing virtual stack\u201d in the Bioformat importer)</p>", "<p>Hi <a class=\"mention\" href=\"/u/weichen\">@weichen</a> ,</p>\n<p>we had a discussion earlier about potential strategies in these threads:</p><aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"41816\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/haesleinhuepf/40/16931_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/clij2-blockwise-analysis/41816/2\">CLIJ2 Blockwise analysis</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hey <a class=\"mention\" href=\"/u/rfrs\">@rfrs</a>, \nwhat kind of denoising / analysis where you thinking of? I\u2019m also working with light sheet data a lot and we use <a href=\"https://clij.github.io/clij2-docs/reference_downsampleSliceBySliceHalfMedian\">downsampleSliceBySliceHalfMedian</a> in routine to reduce our amount of data while imaging. \nTo fit your data into GPU memory of your computer, you have several strategies: \n\nProcess it slice by slice using <a href=\"https://clij.github.io/clij2-docs/reference_pushCurrentSlice\">pushCurrentZSlice</a>. There is also an <a href=\"https://github.com/clij/clij2-docs/blob/335fb08749225be3b522f4c20fcd78cc06bd2496/src/main/macro/push_pull_slices.ijm\">example macro</a> for this.\nProcess tiles using the methods <a href=\"https://clij.github.io/clij2-docs/reference_pushTile\">pushTile</a> and <a href=\"https://clij.github.io/clij2-docs/reference_pullTile\">pullTile</a> as also discussed in <a href=\"https://forum.image.sc/t/segmentation-of-islets-in-fluorescence-image/35358/6\">this thread</a>. Please note that pushTil\u2026\n  </blockquote>\n</aside>\n\n<aside class=\"quote quote-modified\" data-post=\"6\" data-topic=\"35358\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/haesleinhuepf/40/16931_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/segmentation-of-islets-in-fluorescence-image/35358/6\">Segmentation of islets in fluorescence image</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Awesome! Let me know how it goes! For working with hugh images, there are two new commands in CLIJx: pushTile and pullTile which allow you to process a huge image piece wise. \nHere is some example code: \n<a href=\"https://github.com/clij/clij2-docs/blob/master/src/main/macro/processTiles3D.ijm\">https://github.com/clij/clij2-docs/blob/master/src/main/macro/processTiles3D.ijm</a> \nfor (x = 0; x &lt; numTilesX; x++) {\n\tfor (y = 0; y &lt; numTilesY; y++) {\n\t\tfor (z = 0; z &lt; numTilesZ; z++) {\n\t\t\tExt.CLIJx_pushTile(original, x, y, z, tileWidth, tileHeight, tileDepth, margin, margin, margin);\n\t\t\t\n\t\t\tEx\u2026\n  </blockquote>\n</aside>\n\n<p>If you work with Python, you could also use <a href=\"https://www.dask.org/\">dask</a> for processing the data tile-by-tile:<br>\n<a href=\"https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiled_nuclei_counting_quick.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/32_tiled_image_processing/tiled_nuclei_counting_quick.html</a></p>\n<p>Furthermore, cropping and downsampling are often alternatives for answering a scientific question without processing every single pixel.</p>\n<p>Let us know if this helps!</p>\n<p>Best,<br>\nRobert</p>"], "79036": ["<p>Hi all,</p>\n<p>I tried to download icy for windows and it told me it is forbidden-failed each time I try to download. Please help.</p>\n<p>Thx</p>", "<p>Same here, I tried on two different browsers (edge and Opera), and I got the same result, error 403.</p>\n<p>I hope the team can fix the link.</p>\n<p>Best regards,</p>\n<p>Tiago</p>"], "79042": ["<p>Hello, masters, I want to use [deeplabcut.benchmark.metrics.calc_map_from_obj(<em>eval_results_obj</em> , <em>h5_file</em> , <em>metadata_file</em> , <em>oks_sigma=0.1</em> , <em>margin=0</em> , <em>symmetric_kpts=None</em> , <em>drop_kpts=None</em> )]to obtain mAP, but I don\u2019t understand eval very well <em>eval_results_obj</em> , <em>h5_file</em> , <em>metadata_file</em> ,  What do the three parameters \u201cfile\u201d specifically refer to? Where should I find these three files or how do I prepare them.</p>"], "79044": ["<p>Hi everyone,</p>\n<p>Does the pyimageJ package (<a href=\"https://github.com/imagej/pyimagej\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - imagej/pyimagej: Use ImageJ from Python</a>) have the functionality that is described here (<a href=\"https://imagej.net/plugins/diameterj\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DiameterJ</a>) in the diameterJ plugin (particularly the fiber diameter calculation functionality). If so, please can someone point me to a tutorial for this? Thanks so much!</p>"], "79047": ["<p>Good morning . How do I delete a QuPath project?</p>", "<p>You would delete the folder wherever it is in the operating system you are using. There is nothing special about deleting a project folder. It gets deleted like any other file/folder. If you forgot where you created the project, you can find that by right clicking on the images in the project and navigating to the project folder.</p>", "<p>Thanks . I got it here, I had some folders on the external HD and other desktops on the computer. I was wondering if in addition there was somewhere in QuPath to delete as well, but no need.</p>"], "79048": ["<p>I am working on a project doing pose estimation for dogs, however I am wondering, what is the best way to generalize the model to work on all dogs? Not just the dogs that I am training on?</p>\n<p>Is there any documentation on this or best practices I can follow?</p>\n<p>Thanks!</p>"], "79051": ["<p>Hi everyone,</p>\n<p>I am trying to build a macro that will allow me to subdivide an ROI into smaller ROIs to do a more in-depth spatial analysis of signal intensity and the number of stained cells within a brain region. I managed to build a macro that allows me to draw a rectangular ROI and subdivide it into smaller rectangle ROI:</p>\n<p><strong>Script</strong></p>\n<p>// Clear the ROI Manager<br>\nroiManager(\u201creset\u201d);</p>\n<p>// Get the current selection coordinates<br>\ngetSelectionCoordinates(x, y);</p>\n<p>// Draw a rectangle around the current selection<br>\nmakeRectangle(x[0], y[0], x[2] - x[0], y[2] - y[0]);<br>\nroiManager(\u201cadd\u201d);</p>\n<p>// Prompt the user to enter the number of subdivisions<br>\nnumDivisions = getNumber(\u201cEnter the number of subdivisions:\u201d, 2);</p>\n<p>// Calculate the size of each subdivision<br>\nsubWidth = (x[2] - x[0]) / numDivisions;<br>\nsubHeight = (y[2] - y[0]) / numDivisions;</p>\n<p>// Create sub-ROIs and add them to the ROI Manager<br>\nfor (i = 0; i &lt; numDivisions; i++) {<br>\nfor (j = 0; j &lt; numDivisions; j++) {<br>\nsubX = x[0] + i * subWidth;<br>\nsubY = y[0] + j * subHeight;<br>\nmakeRectangle(subX, subY, subWidth, subHeight);<br>\nroiManager(\u201cadd\u201d);<br>\n}<br>\n}</p>\n<p><strong>However</strong>, I have not been able to build a script that allows me to subdivide an irregular ROI into smaller ROIs of the equal area by dividing the original ROI into a different shape with smaller ROIs. I came up with this script, but it is giving me an error with the ROI manager command\u2026 Does anyone have an idea to pass this issue?</p>\n<p><strong>Script</strong></p>\n<p>// Get user-defined input for a number of sub-ROIs<br>\nn = getNumber(\u201cEnter number of sub-ROIs:\u201d, 2);</p>\n<p>// Get original ROI<br>\nroiManager(\u201cShow All\u201d);<br>\nwaitForUser(\u201cSelect the ROI to subdivide.\u201d);<br>\n<em><strong>original = roiManager(\u201cgetCurrentRoi\u201d);</strong></em></p>\n<p>// Calculate the area of the original ROI<br>\noriginalArea = original.getStatistics().area;</p>\n<p>// Calculate area of each sub-ROI<br>\nsubROIarea = originalArea / n;</p>\n<p>// Initialize variables<br>\nstartX = original.getBounds().x;<br>\nstartY = original.getBounds().y;<br>\nwidth = original.getBounds().width;<br>\nheight = original.getBounds().height;</p>\n<p>// Determine dimensions of each sub-ROI based on the desired area<br>\nsubROIsideLength = sqrt(subROIarea);<br>\nsubROInumSides = 6; // Change this to specify number of sides in polygon</p>\n<p>// Subdivide original ROI into smaller ROIs<br>\nfor (i = 0; i &lt; n; i++) {<br>\nsubROICenterX = startX + (i % (width / subROIsideLength)) * subROIsideLength + subROIsideLength / 2;<br>\nsubROICenterY = startY + (Math.floor(i / (width / subROIsideLength))) * subROIsideLength + subROIsideLength / 2;<br>\nsubROIvertices = <span class=\"chcklst-box fa fa-square-o fa-fw\"></span>;<br>\nfor (j = 0; j &lt; subROInumSides; j++) {<br>\nsubROIvertices[2 * j] = subROICenterX + subROIsideLength / 2 * Math.cos(2 * Math.PI * j / subROInumSides);<br>\nsubROIvertices[2 * j + 1] = subROICenterY + subROIsideLength / 2 * Math.sin(2 * Math.PI * j / subROInumSides);<br>\n}<br>\nsubROI = new PolygonRoi(subROIvertices, Roi.POLYGON);<br>\nroiManager(\u201cadd\u201d, subROI);<br>\n}</p>\n<p>// Remove original ROI from ROI Manager<br>\nroiManager(\u201cselect\u201d, 0);<br>\nroiManager(\u201cdelete\u201d);</p>\n<p>Thank you in advance for any help that you all can give me</p>"], "79054": ["<p>I am trying to use the ImageJ plugin TrackMate using a script.</p>\n<p>First, I tried to run TrackMate using ImageJ-MATLAB following the example in \u201c<a href=\"https://imagej.net/plugins/trackmate/scripting/using-from-matlab\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Using TrackMate from MATLAB</a>\u201d, but the script failed to import \u201cSparseLAPTrackerFactory\u201d. It seems that the other parts were imported.</p>\n<pre><code class=\"lang-auto\">%% The import lines, like in Python and Java\nimport java.lang.Integer\nimport ij.IJ\nimport fiji.plugin.trackmate.TrackMate\nimport fiji.plugin.trackmate.Model\nimport fiji.plugin.trackmate.Settings\nimport fiji.plugin.trackmate.SelectionModel\nimport fiji.plugin.trackmate.Logger\nimport fiji.plugin.trackmate.features.FeatureFilter\nimport fiji.plugin.trackmate.detection.LogDetectorFactory\nimport fiji.plugin.trackmate.tracking.sparselap.SparseLAPTrackerFactory\nimport fiji.plugin.trackmate.tracking.LAPUtils\nimport fiji.plugin.trackmate.gui.displaysettings.DisplaySettingsIO\nimport fiji.plugin.trackmate.gui.displaysettings.DisplaySettings\nimport fiji.plugin.trackmate.visualization.hyperstack.HyperStackDisplayer\n</code></pre>\n<p>Then, I try to use ImageJ macro using Jython. I saved the script below (language: python). But the script makes the same error for \u201cSparseLAPTrackerFactory\u201d</p>\n<pre><code class=\"lang-auto\"># from ij import IJ\nfrom ij import IJ, ImagePlus, ImageStack\nfrom fiji.plugin.trackmate import Model\nfrom fiji.plugin.trackmate import Settings\nfrom fiji.plugin.trackmate import TrackMate\nfrom fiji.plugin.trackmate.detection import LogDetectorFactory\nfrom fiji.plugin.trackmate.detection import DogDetectorFactory\nfrom fiji.plugin.trackmate.tracking.sparselap import SparseLAPTrackerFactory\n</code></pre>\n<p>I have reinstalled Fiji and MATLAB, but it was not resolved. Could you please comment on this issue or provide an example that works in the latest version?</p>\n<blockquote>\n<p>OS: Windows 11<br>\nImageJ - Version 1.54c 6 March 2023<br>\nMATLAB 2023a</p>\n</blockquote>", "<p>It seems that the path changed a bit ago.<br>\n\u201cfrom fiji.plugin.trackmate.tracking.jaqaman import SparseLAPTrackerFactory\u201d fixed the error.<br>\nI learned that I should refer to \u201c<a href=\"https://imagej.net/plugins/trackmate/scripting/scripting\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Scripting TrackMate</a>\u201d, and many other examples are outdated.</p>\n<p>I am not sure if I can use the Trackmate functions using ImageJ-MATLAB without the compatibility issue. I appreciate all the effort of the Trackmate team. Thank you!</p>", "<p>Hello.</p>\n<p>Yes the import paths of trackers haven changed woth v7.10. This is what causes the error on your script.</p>\n<p>But the examples in the scripting page linked above have been updated. You can refer to them for the correct paths.</p>\n<p>If they don\u2019t work it\u2019s a bug.</p>", "<p>Doh!<br>\nThe MATLAB + TrackMate script hasn\u2019t been updated you are right! I will fix it today</p>", "<p>Done! I have updated the script.<br>\nI tested it with my MATLAB installation and it now works as expected:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m\" target=\"_blank\" rel=\"noopener\">trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m</a></h4>\n\n\n      <pre><code class=\"lang-m\">%% TrackMate in MATLAB example.\n% We run a full analysis in MATLAB, calling the Java classes.\n\n%% Some remarks.\n% For this script to work you need to prepare a bit your Fiji installation\n% and its connection to MATLAB.\n%\n% 1.\n% In your Fiji, please install the 'ImageJ-MATLAB' site. This is explained\n% here: https://imagej.net/scripting/matlab. Then restart Fiji.\n%\n% 2.\n% Add the /path/to/your/Fiji.app/scripts to the MATLAB path. Either use the\n% path tool in MATLAB or use the command:\n% &gt;&gt; addpath( '/path/to/your/Fiji.app/scripts' )\n%\n% 3.\n% In MATLAB, first launch ImageJ-MATLAB:\n% &gt;&gt; ImageJ\n%\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/trackmate-sc/TrackMate/blob/9290204630cb0bebef9e6449fc5175f21e9f0b40/scripts/MATLABExampleScript_1.m\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "79055": ["<p>Hi guys! I am currently performing nuclei segmentation using DAPI channel of multiplex immunofluorescence tumor biopsy images. My goal is to perform the segmentation with various open source software to compare and contrast them.</p>\n<p>I have a working pipeline with semi-optimized parameters in QuPath but have one issue: For standardization across platforms, I require there to be a space between detected objects (nuclei). Is there a way to do this in QuPath ?</p>\n<p>For example in the screenshot below, it can be seen that the objects are overlapping<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0b0c95024be35a1437cd6fea316e98469aaeaf.png\" data-download-href=\"/uploads/short-url/aHRtWyBvoqMeu1WKtbpZj4ySCkf.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0b0c95024be35a1437cd6fea316e98469aaeaf.png\" alt=\"image\" data-base62-sha1=\"aHRtWyBvoqMeu1WKtbpZj4ySCkf\" width=\"498\" height=\"500\" data-dominant-color=\"545454\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">608\u00d7610 16.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>There is a module that does this object shrinking in CellProfiler and was wondering if there was an equivalent in QuPath, or if someone has any ideas if I can export this to another program to achieve my desired results?</p>\n<p>Thanks!</p>", "<p>If you export the masks from QuPath, you can choose to create a border region, and rather than the usual choice of making that border region a different, third value, you could choose to make it the same as the background value.</p>\n<p>Alternatively you could take the ROI for all of the cells and erode it one pixel, but that seems like way more processing power and time, and still requires exporting the masks.</p>"], "79056": ["<p>Hi!</p>\n<p>I have a plugin carrying a bunch of different acquistions. In particular some acquisitions can be stopped if certain criterion are reached (e.g. device property value corresponding to an activation laser in localization microscopy). The acquisition are started with <code>runAcquisitionWithSettings</code>:</p>\n<pre data-code-wrap=\"Java\"><code class=\"lang-plaintext\">AcquisitionManager acqManager = studio.acquisitions();\nDatastore store = acqManager.runAcquisitionWithSettings(seqBuilder.build(), false);\n</code></pre>\n<p>In the past (MM 2.0.0), I kind of hack my way around stopping the acquisition using the following call:</p>\n<pre data-code-wrap=\"Java\"><code class=\"lang-plaintext\">((DefaultAcquisitionManager) studio.acquisitions()).getAcquisitionEngine().stop(true);\n</code></pre>\n<p>Since <a href=\"https://github.com/micro-manager/micro-manager/commit/b126fcf0adacda9970b90fb161faa384fce50d4f\" rel=\"noopener nofollow ugc\">b126fcf0</a> the method <code>getAcquisitionEngine</code> is private and the snippet breaks.</p>\n<p>The other exposed method, <a href=\"https://github.com/micro-manager/micro-manager/blob/9eca07ec58cac6d3f76b221f1e75e040b5b751d2/mmstudio/src/main/java/org/micromanager/acquisition/internal/DefaultAcquisitionManager.java#L173\" rel=\"noopener nofollow ugc\"><code>haltAcquisition</code></a>, opens a dialog and thus requires user intervention. This is not suitable for automated acquisitions such as ours.</p>\n<p>Is there any way to stop an on-going acquisition without dialog?</p>\n<p>Thanks!<br>\ntagging <a class=\"mention\" href=\"/u/nicost\">@nicost</a> <a class=\"mention\" href=\"/u/henrypinkard\">@henrypinkard</a> <a class=\"mention\" href=\"/u/marktsuchida\">@marktsuchida</a></p>"], "79058": ["<p>Hi,<br>\nI can\u2019t load QuPath-0.4.3-MAC.pkg file on my MacBook Pro Intel. \u201cCannot open because it is damaged\u201d note appears on my screen. Can you help me solving this problem. And also I want to open bif files on my Mac. Can I open them?</p>", "<p>If you search the forum you will find a few posts all pointing to the docs where the error message is explained <a href=\"https://forum.image.sc/t/qupath-damaged-after-ventura-mac-update/73517/2\" class=\"inline-onebox\">QuPath damaged after Ventura Mac Update - #2 by petebankhead</a></p>", "<p>Thanks a lot.</p>", "<p>Hi, below the download button on the homepage there is a link to the <a href=\"https://qupath.readthedocs.io/en/stable/docs/intro/installation.html\">installation notes</a> in case there are problems installing QuPath:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9.png\" data-download-href=\"/uploads/short-url/usRxiUEFZVsuyFu58iZthYvnpsl.png?dl=1\" title=\"Screenshot 2023-03-26 at 09.31.34\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_516x375.png\" alt=\"Screenshot 2023-03-26 at 09.31.34\" data-base62-sha1=\"usRxiUEFZVsuyFu58iZthYvnpsl\" width=\"516\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_516x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_774x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/5/d5847abf136a4f7bac23b096d440fc16b6dc9bb9_2_1032x750.png 2x\" data-dominant-color=\"EBEAE5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-03-26 at 09.31.34</span><span class=\"informations\">1300\u00d7944 191 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "79061": ["<p>The following macro is meant to convert images from a .lif file into tiffs, ask for brightness adjustments and apply those, it\u2019s meant to save the raw (unadjusted) tiff, the adjusted tiff, and an adjusted montage of the channels within each image.</p>\n<p>It does all of that except applying the brightness adjustments to the adjusted tiff. Oddly, it does correctly apply it to the montage\u2026</p>\n<pre><code class=\"lang-auto\">run(\"Bio-Formats Macro Extensions\");\nGetTime();\nsetBatchMode(true); \ninput = getDirectory(\"Input directory, folder where your .lif file is and nothing else\");\noutput = getDirectory(\"Output directory, where you'd like your adjusted .tiff files to go\");\n\nrun(\"Input/Output...\", \"jpeg=100 gif=-1 file=.xls use_file copy_row save_column save_row\");\nDialog.create(\"File type\");\nDialog.addString(\"File suffix: \", \".tif\", 5);\nDialog.addNumber(\"Ch1:\", 0);\nDialog.addNumber(\"Ch1:\", 65535);\nDialog.addNumber(\"Ch2:\", 0);\nDialog.addNumber(\"Ch2:\", 65535);\nDialog.addNumber(\"Ch3:\", 0);\nDialog.addNumber(\"Ch3:\", 65535);\nDialog.addNumber(\"Ch4:\", 0);\nDialog.addNumber(\"Ch4:\", 65535);\nDialog.addNumber(\"Ch5:\", 0);\nDialog.addNumber(\"Ch5:\", 65535);\nDialog.show();\nsuffix = Dialog.getString();\nRange1 = Dialog.getNumber();\nRange2 = Dialog.getNumber();\nRange3 = Dialog.getNumber();\nRange4 = Dialog.getNumber();\nRange5 = Dialog.getNumber();\nRange6 = Dialog.getNumber();\nRange7 = Dialog.getNumber();\nRange8 = Dialog.getNumber();\nRange9 = Dialog.getNumber();\nRange10 = Dialog.getNumber();\n\nBC_range = newArray(Range1,Range2,Range3,Range4,Range5,Range6,Range7,Range8,Range9,Range10);\n\nprint(\"Brightness and contrast range is: \");\nArray.print(BC_range);\nprint(\"Blue\",\"Green\",\"Red\",\"Grays\",\"Yellow\");\n\nsuffix = \".lif\";\n\nprocessFolder(input);\n\nfunction processFolder(input) {\n\tlist = getFileList(input);\n\tfor (i = 0; i &lt; list.length; i++) {\n\t\tif(File.isDirectory(input + list[i]))\n\t\t\tprocessFolder(\"\" + input + list[i]);\n\t\tif(endsWith(list[i], suffix))\n\t\t\tprocessFile(input, output, list[i]);\n\t}\n}\n\nfunction processFile(input, output, file) {\n\tExt.setId(input + file);\n\tExt.getSeriesCount(count);\n\tprint(\"Processing: \" + input + file+ \" - \"+d2s(count,0)+\" images\");\n\n\tfor (f=0;f&lt;count;f++) {\n\t\topenLif(input+file,f);\n\t\tprint(\"Saving to: \" + output);\n\t\tlistImages();\n\t}\n}\n\nfunction openLif(input,f){\n\tExt.setSeries(f);\n\trun(\"Bio-Formats Importer\", \"open=[\" +input +\"] color_mode=Default view=[Standard ImageJ] stack_order=XYZCT series_\" + d2s(f+1,0));\n}\n\n//Create string \"image_1 image_2 image_3 image_4....\"\nfunction seriesN(num){\n\tstr = \"\";\n\tfor (i=0; i&lt;num; i++){\n\t\tii = i+1;\n        str = str+\"image_\"+ii+\" \";\n     }\n     return str;\n}\n\n//Save\nfunction listImages(){\n\timageList = getList(\"image.titles\");\n\tif (imageList.length==0)\n\t     print(\"No image windows are open\");\n\telse {\n    \tprint(\"Image windows:\");\n     \tfor (i=0; i&lt;imageList.length; i++){\n        \tprint(\"   \"+imageList[i]);\n        \t// TODO: Add condition for saving unadjusted files\n                saveTiff(imageList[i]);\n                \n                // Process the files as per the second script\n                processTiff(output, output, imageList[i]);\n     \t}\n\t}\n\tprint(\"\");\n}\n\n\nfunction saveTiff(winName){\n\tselectWindow(winName); \n\tsaveAs(\"Tiff\", output + winName);\n\tclose();\n}\n\nfunction processTiff(input, output, file) {\n\n\tprint(\"Processing: \" + input + file);\n\n\topen(input+file+\".tif\");\n\trenderColor(file);\n\tbrightnessNcontrast(file);\n\tdeleteSlices(file);\n\n\tsaveAs(\"Tiff\", output + file + \"_BCadj\");\n\trename(file);\n\t\n\tRGBmerge(file);\n\tscaleBar();\n\tmakeMontage(file);\n\n\t//selectWindow(\"Montage1to5\");\n\t//saveAs(\"Jpeg\", output + file + \"_montage1to5\");\n\tselectWindow(\"MontageHorizontal\");\n\tsaveAs(\"Jpeg\", output + file + \"_BCmont\");\n\n\tprint(\"Saving to: \" + output);\n\trun(\"Close All\");\n}\n\n\n\n// Give colors for each slice\nfunction renderColor(file){\n\tcolor = newArray(\"Blue\",\"Green\",\"Red\",\"Grays\",\"Yellow\");\n\t//color = newArray(\"Blue\",\"Green\",\"Red\");\n\n\trun(\"Make Composite\", \"display=Color\");\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\trun(color[i]);\n\t}\n}\n// Change brightness and contrast\nfunction brightnessNcontrast(file){\n\t//BC_range = newArray(0, 100, 00, 100, 0, 255, 0, 255);\n\tfor (i = 0; i &lt; nSlices; i++) {\n\t\tsetSlice(i+1);\n\t\tsetMinAndMax(BC_range[2*i],BC_range[2*i+1]);\n\t}\n\t\n}\n\n// Delete slices. You want to delete slice a, b\nfunction deleteSlices(file){\n\tdeleteA = 0;\n\tdeleteB = 0;\n\t//deleteB &gt; deleteA. deleteB should be larger than deleteA\n\n\tprint(\"deleting the slice \"+deleteA+\" and \"+deleteB);\n\tselectWindow(file+\".tif\");\n\tif(deleteB&gt;0){\n\t\tsetSlice(deleteB);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t\t}\n\tif(deleteA&gt;0){\n\t\tsetSlice(deleteA);\n\t\trun(\"Delete Slice\", \"delete=channel\");\n\t}\t\n}\n\n// Make an RGB file with 5 slices; 1-4 are each channels, and 5th are merge.\nfunction RGBmerge(file){\n\trun(\"Duplicate...\", \"title=4channels duplicate\");\n\trun(\"RGB Color\");\n\t\t\n\tselectWindow(file);\n\trun(\"Duplicate...\", \"title=Merge duplicate\");\n\tStack.setDisplayMode(\"composite\");\n\trun(\"RGB Color\");\n\trun(\"Copy\");\n\n\tselectWindow(\"4channels (RGB)\");\n\tsetSlice(nSlices);\n\trun(\"Add Slice\"); \n\trun(\"Paste\"); \n\n\tclose(\"4channels\");\n\tclose(\"Merge\");\n\tclose(\"Merge (RGB)\");\n\t\n\t// \"4channels (RGB)\" is made.\n}\n\n// Add scale bar\nfunction scaleBar(){\n\tsetSlice(nSlices);\n\trun(\"Set Scale...\", \"distance=311.0016 known=100 pixel=1 unit=\u00b5m\");\n\t//run(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] hide\");\n\trun(\"Scale Bar...\", \"width=100 height=4 font=14 color=White background=None location=[Lower Right] bold\");\n}\n\nfunction makeMontage(input){\n\t//selectWindow(\"4channels (RGB)\");\n\t//run(\"Make Montage...\", \"columns=1 rows=5 scale=0.25 border=2\");\n\t//rename(\"Montage1to5\");\n\tselectWindow(\"4channels (RGB)\");\n\trun(\"Make Montage...\", \"columns=\"+nSlices+\" rows=1 scale=0.5 border=2\");\n\trename(\"MontageHorizontal\");\n}\nfunction GetTime(){\n     MonthNames = newArray(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\");\n     DayNames = newArray(\"Sun\", \"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\");\n     getDateAndTime(year, month, dayOfWeek, dayOfMonth, hour, minute, second, msec);\n     TimeString =\"Date: \"+DayNames[dayOfWeek]+\" \";\n     if (dayOfMonth&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+dayOfMonth+\"-\"+MonthNames[month]+\"-\"+year+\"\\nTime: \";\n     if (hour&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+hour+\":\";\n     if (minute&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+minute+\":\";\n     if (second&lt;10) {TimeString = TimeString+\"0\";}\n     TimeString = TimeString+second;\n     print(TimeString);\n}\n</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/overcook6402\">@Overcook6402</a>,</p>\n<p>My guess is that you need to apply the contrast change. <code>setMinAndMax(min, max)</code> only makes a visual contrast adjustment on screen.<br>\nIt might work if you add <code>run(\"Apply LUT\");</code> after that.<br>\nBut I could not currently test this.</p>"], "21719": ["<p>Hi I ve a problem with the data produced during image analysis. When I trying to load that one in Excel the data appear in a single column. Many thanks in advance for all suggestions.<br>\nRegards</p>", "<p>Maybe you need to specify what is the field separator when opening those files. XLS files are not really Excel files. You can open them with any text editor.</p>", "<p>Hi Gabriel, pls find the file attached such as example. The file not permit to open as text and so I don\u2019t have the possibility to use te text import option. Regards</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/72a76ce88603763142d272e771462198f676a49d.jpeg\" data-download-href=\"/uploads/short-url/gmh8C6E9vjgzXZwnRdHc9gIvopD.jpeg?dl=1\" title=\"data%20da%20imageJ\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72a76ce88603763142d272e771462198f676a49d_2_690x408.jpeg\" alt=\"data%20da%20imageJ\" data-base62-sha1=\"gmh8C6E9vjgzXZwnRdHc9gIvopD\" width=\"690\" height=\"408\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72a76ce88603763142d272e771462198f676a49d_2_690x408.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72a76ce88603763142d272e771462198f676a49d_2_1035x612.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72a76ce88603763142d272e771462198f676a49d_2_1380x816.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72a76ce88603763142d272e771462198f676a49d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">data%20da%20imageJ</span><span class=\"informations\">1680\u00d7994 781 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>This is not an image-related question so not sure if it belongs to this forum. Anyway, since I am here:</p>\n<p>This is a CSV file (i.e. Comma/Character-Separated Values file. To import into Excel, use the text import wizard under the Data tab &gt; From Text/CSV</p>", "<p>Hi,<br>\nI know the procedure but the problem is that the file is opening in automatically in Excel and not permit to me to open it as txt or csv file. The result is the file attached shown above.<br>\nRegard</p>", "<p>Two other suggestions:</p>\n<ol>\n<li>Rename it to CSV then it will probably ask what are the field delimeters.</li>\n<li>Use something else. Libreoffice Calc works fine and it is free.</li>\n</ol>", "<p>Hi,<br>\nOn the \u201cData\u201d tab, there\u2019s a \u201cText to Column\u201d button that should do what you want.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4653fa8aaa2152a2c9e30cf0a943a0cf742b484.png\" data-download-href=\"/uploads/short-url/s1ovwApVlqysJGXk3bt9G9Vj3vu.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4653fa8aaa2152a2c9e30cf0a943a0cf742b484_2_690x314.png\" alt=\"image\" data-base62-sha1=\"s1ovwApVlqysJGXk3bt9G9Vj3vu\" width=\"690\" height=\"314\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4653fa8aaa2152a2c9e30cf0a943a0cf742b484_2_690x314.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4653fa8aaa2152a2c9e30cf0a943a0cf742b484_2_1035x471.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4653fa8aaa2152a2c9e30cf0a943a0cf742b484.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4653fa8aaa2152a2c9e30cf0a943a0cf742b484_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1254\u00d7572 137 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello marco,<br>\nJust save your data as a TEXT IMAGE in ImageJ, then Excell will open it as intended.<br>\nBob</p>", "<p>No, you cannot save a Results Table as \u201cText image\u201d. That option is to save images as txt files.</p>", "<p>dear all,<br>\nmany thanks for your suggestion but it isn\u2019t possible to rename the file (as Gabriel wrote above). I can try to download the Libreoffice. Anyway I\u2019ve attached the file if someone would to test and post here the solution I\u2019ll be gratefull. Regard and Best Wishes.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/jqC5ot35KPor93hVsYPcA9ToqZR.csv\">Results box 11.csv</a> (68.3 KB)</p>", "<p>I doubt very much that you could not rename a file\u2026<br>\nThere is even an IJ macro function to do that:<br>\n<strong>File.rename(path1, path2)</strong> - Renames, or moves, a file or directory. Returns \u201c1\u201d (true) if successful.</p>", "<p>Did you try the \u201cText to Columns\u201d option in Excel I posted above?</p>", "<p>There is also an  <strong>I/O Option</strong>  to select the \u201c<strong>file extension of tables</strong>\u201d. Set it to \u201c<strong>.csv</strong>\u201d</p>", "<p>Hi <span class=\"mention\">@macro</span>, try Beth\u2019s solution, also on this YouTube video as we are in YouTube times\u2026 <a href=\"https://www.youtube.com/watch?v=Z8hH-PSRDOw\" rel=\"nofollow noopener\">https://www.youtube.com/watch?v=Z8hH-PSRDOw</a></p>", "<p>Hi I ll try, the last one proposed seems ok for me.  I ll send u the feedback. Many Thanks all of u and Best Wishes.</p>"], "79065": ["<p>Hello there,<br>\nReading image as RGB and converting them to CMYK resulted in a better color definition to identify markers in an actuator. As you can see them in red, in the image attached.</p>\n<p>Now, they must be sterilized, as in:<br>\n\u2018\u2019\u2018\u2019<br>\nstereo = cv.StereoBM_create(numDisparities=16, blockSize=15)<br>\ndisparity = stereo.compute(CMYK_L, CMYK_R)<br>\n\u2018\u2019\u2018\u2019</p>\n<p>However , the following error occurs, because CMYK has a fourth dimension (i.e. K), causing calculations to break.</p>\n<p>error: OpenCV(4.7.0) /io/opencv/modules/calib3d/src/stereobm.cpp:1173: error: (-210:Unsupported format or combination of formats) Both input images must have CV_8UC1 in function \u2018compute\u2019</p>\n<p>How can images be converted from CMYK to grayscale directly?<br>\nShould I use another color system?<br>\nor even better, does stereo.compute()  support CMYK system?</p>\n<p>Converting them back to RGB is not am option/solution. Because markers get lost when the image is converted from RGB to grayscale.</p>\n<p>Markers get similar gray as the scenario, making it hard, to any openCV lib or segmentation algorithm such as Cany, to track marker\u2019s coordinates (x,y) accurately.</p>\n<p>Do I need to convert them to another system? which one?</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/04b983e6ca1217c9907990b9717820589eee7464.png\" alt=\"Screen Shot 2023-03-25 at 11.08.44 PM\" data-base62-sha1=\"FNndob3PC7fsEoDWIis6Xx3y3a\" width=\"351\" height=\"349\"></p>", "<p><a class=\"mention\" href=\"/u/iuri\">@iuri</a>, have you tried passing in the RGB image? I don\u2019t know what the <code>StereBM_create</code> function does internally, but if it accepts RGB images, it probably uses the color information to find marker correspondences.</p>\n<p>I don\u2019t have any experience with this kind of data but this example in the scikit-image gallery might help:</p>\n<p><a href=\"https://scikit-image.org/docs/stable/auto_examples/transform/plot_fundamental_matrix.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://scikit-image.org/docs/stable/auto_examples/transform/plot_fundamental_matrix.html</a></p>\n<p>As you can see in the example, the approach is divided into finding the corresponding markers, and then matching them to find the projections. You can therefore use whatever approach you want to find your markers, including e.g. thresholding the red channel only. However, you\u2019ll need some kind of feature to describe the markers so that the <code>match_descriptors</code> function can find matches.</p>", "<p>Hi <a class=\"mention\" href=\"/u/jni\">@jni</a>,</p>\n<p>In fact I did try already.<br>\nHowever, stereo.compute() does not recognize markers properly, using RGB system directly. I believe it\u2019s because markers get a very similar pixel\u2019s gradient than its neighborhood, as the other objects and the scenario, once they are converted from RGB to grayscale.</p>\n<p>That\u2019s the reason I thought that converting them to CMYK and  running images in stereo.compute(CMYK_L,CMYK_R) would return a better mapping, identifying markers and their (x,y) coordinates accurately.</p>\n<p>an example of grayscale has been attached.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912.png\" data-download-href=\"/uploads/short-url/c6PLLWtIllF9QrIrHD7LLfK1xyq.png?dl=1\" title=\"Screen Shot 2023-03-26 at 11.38.56 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912_2_433x500.png\" alt=\"Screen Shot 2023-03-26 at 11.38.56 AM\" data-base62-sha1=\"c6PLLWtIllF9QrIrHD7LLfK1xyq\" width=\"433\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912_2_433x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/4/54dfe18efced7757e4605fc1be78f1870caf3912.png 2x\" data-dominant-color=\"B6CEC7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-26 at 11.38.56 AM</span><span class=\"informations\">642\u00d7740 105 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>another example of an image with the results of stereo.compute() has been attached.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e.png\" data-download-href=\"/uploads/short-url/hnBx0o0SSeAQjeAdXss79ESOnDg.png?dl=1\" title=\"Screen Shot 2023-03-26 at 11.27.07 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e_2_431x500.png\" alt=\"Screen Shot 2023-03-26 at 11.27.07 AM\" data-base62-sha1=\"hnBx0o0SSeAQjeAdXss79ESOnDg\" width=\"431\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e_2_431x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79d00e65a0236ed66495e94ea17404a0f682ac6e.png 2x\" data-dominant-color=\"676767\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-26 at 11.27.07 AM</span><span class=\"informations\">482\u00d7558 47.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Anyway, for that reason, I converted from RGB to CMYK previously. So that, I could get a better contrast between markers, arm, and scenario (i.e. markers in magenta).<br>\nAs you can see in this image below</p>\n<p>Image in the CMYK system<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/04b983e6ca1217c9907990b9717820589eee7464.png\" alt=\"Screen Shot 2023-03-25 at 11.08.44 PM\" data-base62-sha1=\"FNndob3PC7fsEoDWIis6Xx3y3a\" width=\"351\" height=\"349\"></p>\n<p>However, converting from CMYK to grayscale system then running stereo.compute(CMYK_L, CMYK_R) returns an error as indicated.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48.png\" data-download-href=\"/uploads/short-url/nHfP3LVL3hvfTazlJ1BH0r5CWSc.png?dl=1\" title=\"Screen Shot 2023-03-26 at 11.43.18 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_690x184.png\" alt=\"Screen Shot 2023-03-26 at 11.43.18 AM\" data-base62-sha1=\"nHfP3LVL3hvfTazlJ1BH0r5CWSc\" width=\"690\" height=\"184\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_690x184.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_1035x276.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/6/a615bbbc36f7789e7c7ca5feb308efcd31c06f48_2_1380x368.png 2x\" data-dominant-color=\"F7F7F7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-26 at 11.43.18 AM</span><span class=\"informations\">2732\u00d7730 91.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The error is related to dimension, as CMYK has one more dimension such as (640,480),4, while RGB has only the first two, as in (640,480).</p>\n<p>How do we calculate disparity when the input is in the CMYK system?</p>\n<p>Best wishes,</p>"], "66778": ["<p>Following on from <a href=\"https://forum.image.sc/t/german-bioimaging-gerbi-would-like-to-join-as-a-community-partner/66302\">GerBI\u2019s</a> post, the Quality Assessment and Reproducibility<br>\nfor Instruments &amp; Images in Light Microscopy (<a href=\"https://quarep.org\">QUAREP-LiMi</a>) organization would like to join image.sc. Similarly, there are no specific software projects that are being added, but QL too intends to to point all <em>users</em> of existing projects (and perhaps some new developers) to image.sc rather than hosting those conversations anywhere else.</p>\n<p>cc: <a class=\"mention\" href=\"/u/nitschro\">@nitschro</a> (other WG leads to be added)</p>\n<h3>\n<a name=\"icons-1\" class=\"anchor\" href=\"#icons-1\"></a>Icons</h3>\n<p>Files can be downloaded from: <a href=\"https://quarep.org/wp-content/uploads/Logo-Files.zip\">https://quarep.org/wp-content/uploads/Logo-Files.zip</a> The versions below are from <code>PNGs-raster/Icons/</code></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/9/391e1234de0602af93220e9ec1469b4cb57fb23f.png\" data-download-href=\"/uploads/short-url/89hDRh0gy84opouAQpA1cDm1GLZ.png?dl=1\" title=\"QUAREP_logo_icon@1800px\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/391e1234de0602af93220e9ec1469b4cb57fb23f_2_124x124.png\" alt=\"QUAREP_logo_icon@1800px\" data-base62-sha1=\"89hDRh0gy84opouAQpA1cDm1GLZ\" width=\"124\" height=\"124\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/391e1234de0602af93220e9ec1469b4cb57fb23f_2_124x124.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/391e1234de0602af93220e9ec1469b4cb57fb23f_2_186x186.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/9/391e1234de0602af93220e9ec1469b4cb57fb23f_2_248x248.png 2x\" data-dominant-color=\"3A6359\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">QUAREP_logo_icon@1800px</span><span class=\"informations\">1800\u00d71801 152 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/c/6cb408902cfb86aabaec89c4153051683205d7c2.png\" data-download-href=\"/uploads/short-url/fvDhM3HIJhIB9jxnPUGKeWJ1NZM.png?dl=1\" title=\"QUAREP_logo_icon_white@1800px\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/c/6cb408902cfb86aabaec89c4153051683205d7c2_2_124x124.png\" alt=\"QUAREP_logo_icon_white@1800px\" data-base62-sha1=\"fvDhM3HIJhIB9jxnPUGKeWJ1NZM\" width=\"124\" height=\"124\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/c/6cb408902cfb86aabaec89c4153051683205d7c2_2_124x124.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/c/6cb408902cfb86aabaec89c4153051683205d7c2_2_186x186.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/c/6cb408902cfb86aabaec89c4153051683205d7c2_2_248x248.png 2x\" data-dominant-color=\"B3D9B9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">QUAREP_logo_icon_white@1800px</span><span class=\"informations\">1800\u00d71801 163 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/8/b8293bbb5eb4535206ae0c0ee949310d03696e17.png\" data-download-href=\"/uploads/short-url/qhacbzKmfIycU7yRQ33XwCKKKSr.png?dl=1\" title=\"QUAREP_logo_icon_white2@1800px\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b8293bbb5eb4535206ae0c0ee949310d03696e17_2_124x124.png\" alt=\"QUAREP_logo_icon_white2@1800px\" data-base62-sha1=\"qhacbzKmfIycU7yRQ33XwCKKKSr\" width=\"124\" height=\"124\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b8293bbb5eb4535206ae0c0ee949310d03696e17_2_124x124.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b8293bbb5eb4535206ae0c0ee949310d03696e17_2_186x186.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b8293bbb5eb4535206ae0c0ee949310d03696e17_2_248x248.png 2x\" data-dominant-color=\"B9BDC4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">QUAREP_logo_icon_white2@1800px</span><span class=\"informations\">1800\u00d71801 156 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/1/4172f8db9ddb637c92295787cb11d16c296886a8.png\" data-download-href=\"/uploads/short-url/9kZmsbboaXPJA3sAlO6B4v44A5y.png?dl=1\" title=\"QUAREP_logo_icon_gray@1800px\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/4172f8db9ddb637c92295787cb11d16c296886a8_2_124x124.png\" alt=\"QUAREP_logo_icon_gray@1800px\" data-base62-sha1=\"9kZmsbboaXPJA3sAlO6B4v44A5y\" width=\"124\" height=\"124\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/4172f8db9ddb637c92295787cb11d16c296886a8_2_124x124.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/4172f8db9ddb637c92295787cb11d16c296886a8_2_186x186.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/1/4172f8db9ddb637c92295787cb11d16c296886a8_2_248x248.png 2x\" data-dominant-color=\"787878\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">QUAREP_logo_icon_gray@1800px</span><span class=\"informations\">1800\u00d71801 125 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dcab26ebd56b4be46e3f2c096ae628b64205eb8e.png\" data-download-href=\"/uploads/short-url/vu7JzvLtbxFB3Sf4qK7Xf6QyZZA.png?dl=1\" title=\"QUAREP_logo_icon_blue@1800px\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dcab26ebd56b4be46e3f2c096ae628b64205eb8e_2_124x124.png\" alt=\"QUAREP_logo_icon_blue@1800px\" data-base62-sha1=\"vu7JzvLtbxFB3Sf4qK7Xf6QyZZA\" width=\"124\" height=\"124\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dcab26ebd56b4be46e3f2c096ae628b64205eb8e_2_124x124.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dcab26ebd56b4be46e3f2c096ae628b64205eb8e_2_186x186.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dcab26ebd56b4be46e3f2c096ae628b64205eb8e_2_248x248.png 2x\" data-dominant-color=\"3C879A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">QUAREP_logo_icon_blue@1800px</span><span class=\"informations\">1800\u00d71801 175 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "", "", "", "", "", "", "<p>After a vote of <a href=\"https://quarep.org/working-groups/wg-9-over-all-planning-funding/\">WG9</a> including the other WG chairs, I\u2019ve now made this topic public.</p>\n<p>If there are no questions/comments from the image.sc <a class=\"mention-group notify\" href=\"/groups/team\">@team</a>, I\u2019ll move forward with adding the appropriate logos from above.</p>\n<p>~Josh</p>", "<p>Added light and dark themes:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18aa2e685b654b9ee9d8b804dcdfd78c930bfa9e.png\" alt=\"image\" data-base62-sha1=\"3wc4oVBcXDAd9VMhZ2AmPZ9DWSG\" width=\"280\" height=\"138\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/27bed5b533923e1aa9c68851dc69bd1d63a9ebf7.png\" alt=\"Screenshot 2023-03-13 at 12.33.32\" data-base62-sha1=\"5FBtWHr9rf6CiXgCJsxf0gxO4Vp\" width=\"243\" height=\"132\"></p>\n<p>and created sidebar topic:</p>\n<aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"78499\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/joshmoore/40/1634_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/tag-sidebard-quarep/78499\">Tag sidebard: quarep</a> <a class=\"badge-wrapper  bullet\" href=\"/c/community-partners/62\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category contains topics describing the Community Partners being discussed on this forum. A  Community Partner  is an open-source software project or community organization that uses this forum as a primary recommended discussion channel.\">Community Partners</span></a>\n  </div>\n  <blockquote>\n    <a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/56fd6ba62c028f8d0f2273585d1e83c086647503.jpeg\" data-download-href=\"/uploads/short-url/cpy0Ps2y4BNWIgg7eM2PetbwFCr.jpeg?dl=1\" title=\"image\">[image]</a> \nThe <a href=\"https://quarep.org/\">QUAREP-LiMi</a> (\u201cQuality Assessment and Reproducibility for Instruments &amp; Images in Light Microscopy\u201d) is a group of enthusiastic light microscopists from Academia and Industry all interested in improving quality assessment (QA) and quality control (QC) in light microscopy.\n  </blockquote>\n</aside>\n", "<p>Thanks for setting this up, now we have to collect some people from QUAREP-LiMi to give this activity</p>", "<p>Good to see! <img src=\"https://emoji.discourse-cdn.com/twitter/fireworks.png?v=12\" title=\":fireworks:\" class=\"emoji\" alt=\":fireworks:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/fireworks.png?v=12\" title=\":fireworks:\" class=\"emoji\" alt=\":fireworks:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Thank you <a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a> for setting this up. It will be great to see community interchange about Quality Control, Reproducibility, and Data Management for Light Microscopy on Image.sc.</p>", "<p>Nice to see this is starting to roll <img src=\"https://emoji.discourse-cdn.com/twitter/sunglasses.png?v=12\" title=\":sunglasses:\" class=\"emoji\" alt=\":sunglasses:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/grinning.png?v=12\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThank you Josh Moore!</p>"], "79067": ["<p>Hi <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> and fellow scripters,<br>\nI\u2019m trying to get the path of an image to use later with another plugin in my macro.<br>\n<strong>The issue is that it prints the path as \u20180\u2019 instead of the expected string</strong><br>\nDoes anyone have an idea/experience with that bug?<br>\nI tried different locations on my PC<br>\n<a class=\"attachment\" href=\"/uploads/short-url/jAYJEJcyiBejcmVZRHTItuSAoSn.tif\">IDA-OCS 10-4.tif</a> (4.1 MB)<br>\nSince the image\u2019s original location is a google drive folder, a local folder didn\u2019t solve the issue.</p>\n<hr>\n<p>Snippet:</p>\n<pre><code class=\"lang-auto\">run(\"Open...\");\nrun(\"8-bit\");\nrun(\"Set Scale...\", \"distance=267.5 known=100 unit=microns global\");\nimgName = getTitle();\norgName = File.getNameWithoutExtension(imgName);\nprint(imgName);\nprint(orgName);\npath = File.getParent(orgName);\nprint(path);\n\n\n\n/*\nPre-processing of the image: Cropping the image's central region, renaming it, and enhancing the contrast for the segmentation part.\n */\nmakeRectangle(311, 148, 712, 672);\nrun(\"Duplicate...\", \" \");\nrename(orgName + \"-CLAHE\");\nnewName = getTitle();\nprint(newName);\n\nEtc... etc...\n</code></pre>\n<p><strong>Log:</strong><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f91ef66afe11e216b703c668a41cee63c8f35e10.png\" alt=\"image\" data-base62-sha1=\"zxPe58mjP0r8iL5giLgZtuQspCo\" width=\"206\" height=\"197\"></p>\n<p><strong>Original Image:</strong><br>\n<a class=\"attachment\" href=\"/uploads/short-url/jAYJEJcyiBejcmVZRHTItuSAoSn.tif\">IDA-OCS 10-4.tif</a> (4.1 MB)</p>\n<p>Thanks and cheers for any help,<br>\nDaniel</p>", "<p>Hi <a class=\"mention\" href=\"/u/daniel_waiger\">@Daniel_Waiger</a>,</p>\n<p>The problem is that you\u2019re not passing a full path string to <code>File.getParent</code>, only a name without extension.</p>\n<p>The function you need is <code>File.directory</code> which will then return the path to the most recently opened file. If you want to step up to the parent, you can then use <code>File.getParent</code> as in your original code.</p>\n<p>Hope that helps!</p>", "<p>Nice,</p>\n<p>Thanks <a class=\"mention\" href=\"/u/dnmason\">@dnmason</a>!</p>\n<p>Not sure why I remembered that\u2019s how this line works\u2026<br>\nAnyway, great advice!</p>\n<p>Cheers,<br>\nDaniel</p>"], "79073": ["<p>Hello</p>\n<p>I am currently scanning slides using a Panoramic 3dhistech slide scanner with 4 channels and Z stack. I was able to convert the file from MRXS to ome.tiff format, which allows me to analyze it using Qupath.<br>\nI am now in the process of ordering a new computer and considering the Apple Mac Mini M2 because of its impressive features. However, I have never used a Mac system before and was wondering if you could offer any advice or if anyone has experience analyzing large files on a Mac.</p>", "<p>Hi <a class=\"mention\" href=\"/u/nadeem\">@Nadeem</a> I develop QuPath using an M1 Mac\u2026 so it\u2019s especially important to me that QuPath has good support for Macs <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>There is an Apple Silicon version of QuPath, but there are a few limitations and complications described at <a href=\"https://qupath.readthedocs.io/en/0.4/docs/intro/installation.html#apple-silicon\" class=\"inline-onebox\">Installation \u2014 QuPath 0.4.3 documentation</a></p>\n<p>However I believe everything should just work if you download the Intel version of QuPath instead. That\u2019s the simplest solution, since it means you don\u2019t really have to think about compatible problems. It runs a bit more slowly than you have the Apple Silicon version because it needs to run through <a href=\"https://support.apple.com/en-gb/HT211861\">Rosetta 2</a>, but I think it isn\u2019t very noticeable \u2013 I only really know there is a difference between I did a speed comparison once.</p>\n<p>Note that you might not need to convert MRXS images to ome.tiff, because QuPath supports OpenSlide and OpenSlide supports <em>some</em> .mrxs files. There is more info at <a href=\"https://qupath.readthedocs.io/en/0.4/docs/intro/formats.html#mrxs-3d-histech\" class=\"inline-onebox\">Supported image formats \u2014 QuPath 0.4.3 documentation</a></p>"], "79076": ["<p>I am running CellProfiler on a recently updated Windows 11 computer. The program no longer opens and flashes a fast error before closing the console. It was working fine before a recent batch of Windows updates.</p>\n<p>\u201c[process exited with code 3221226356 (0xc0000374)]\u201d</p>\n<p>This has happened on a couple of computers after a recent windows update. I have tried installing and reinstalling CellProfiler 4.2.1 and 4.2.5. I have also insallted the Visual C++ Redistributable linked on the downloads page. Any help will be appreciated!</p>"], "79081": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c.png\" data-download-href=\"/uploads/short-url/4KsalnNguCoC8jheuVdrAzchU1C.png?dl=1\" title=\"Captura de Tela (17)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c_2_690x387.png\" alt=\"Captura de Tela (17)\" data-base62-sha1=\"4KsalnNguCoC8jheuVdrAzchU1C\" width=\"690\" height=\"387\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c_2_690x387.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c_2_1035x580.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/21490105865e40c9f7e45591eefcf9aeeda6129c.png 2x\" data-dominant-color=\"454545\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Captura de Tela (17)</span><span class=\"informations\">1366\u00d7768 78 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d.png\" data-download-href=\"/uploads/short-url/eezNKgutRNGJE1ql9kerK1kRPcx.png?dl=1\" title=\"Captura de Tela (16)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d_2_690x387.png\" alt=\"Captura de Tela (16)\" data-base62-sha1=\"eezNKgutRNGJE1ql9kerK1kRPcx\" width=\"690\" height=\"387\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d_2_690x387.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d_2_1035x580.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/3/63c448419fb9757e0c85b3d7a8c508670828c34d.png 2x\" data-dominant-color=\"373636\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Captura de Tela (16)</span><span class=\"informations\">1366\u00d7768 76.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db.jpeg\" data-download-href=\"/uploads/short-url/voXtULHxQbJhujyqgWHMvXykkjh.jpeg?dl=1\" title=\"Captura de Tela (15)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db_2_690x387.jpeg\" alt=\"Captura de Tela (15)\" data-base62-sha1=\"voXtULHxQbJhujyqgWHMvXykkjh\" width=\"690\" height=\"387\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db_2_690x387.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db_2_1035x580.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dc15acdd42b996b5616d86275323e9e293d7f9db.jpeg 2x\" data-dominant-color=\"81716D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Captura de Tela (15)</span><span class=\"informations\">1366\u00d7768 96.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "79089": ["<p>Hi,</p>\n<p>I am running DLC live using the GUI. My camera has 60 fps; however, the saved video file is only at 10fps (A 1 minute recording in real time is only 10s when played through a media player,) My question is about the saved video only (inference rate is fine). Is it possible to record the video at a higher frame rate?</p>"], "34044": ["<aside class=\"quote no-group\" data-username=\"j.burel\" data-post=\"30\" data-topic=\"26651\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/j.burel/40/15935_2.png\" class=\"avatar\"><a href=\"https://forum.image.sc/t/best-way-imagej-to-open-an-imageplus-from-omero-using-bioformats/26651/30\">Best way imageJ to open an ImagePlus from OMERO using Bioformats</a>\n</div>\n<blockquote>\n<p>The ImageJ-OMERO extension will also have to be updated.</p>\n</blockquote>\n</aside>\n<p>As I explained <a href=\"https://forum.image.sc/t/omero-update-site-proposal/29863/4\">here</a>, I did update ImageJ-OMERO to use OMERO 5.5 a while ago. The work was waiting on the release of pom-scijava 28.0.0, which is now complete. So I\u2019ve now gone ahead and pushed the OMERO 5.5 update to the master branch:</p>\n<aside class=\"onebox githubcommit\">\n  <header class=\"source\">\n      <a href=\"https://github.com/imagej/imagej-omero/commit/f1846f570906b30e39d2546ccc660c6a69a08cc0\" target=\"_blank\">github.com/imagej/imagej-omero</a>\n  </header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Commit\">\n    <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/imagej/imagej-omero/commit/f1846f570906b30e39d2546ccc660c6a69a08cc0\" target=\"_blank\">Update to OMERO 5.5</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        committed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2019-09-06\" data-time=\"00:08:16\" data-timezone=\"UTC\">12:08AM - 06 Sep 19 UTC</span>\n      </div>\n\n      <div class=\"user\">\n        <a href=\"https://github.com/ctrueden\" target=\"_blank\">\n          <img alt=\"ctrueden\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/2013c251d937c5e6e4b25c9508384fc65e320a6a.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          ctrueden\n        </a>\n        \n      </div>\n\n      <div class=\"lines\" title=\"changed 2 files with 48 additions and 34 deletions\">\n        <a href=\"https://github.com/imagej/imagej-omero/commit/f1846f570906b30e39d2546ccc660c6a69a08cc0\" target=\"_blank\">\n          <span class=\"added\">+48</span>\n          <span class=\"removed\">-34</span>\n        </a>\n      </div>\n    </div>\n\n  </div>\n</div>\n\n\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I have also promoted the imagej-omero dev version number to 1.0.0-5.5-SNAPSHOT. Reasons:</p>\n<ol>\n<li>To my knowledge, no major new development is currently planned for imagej-omero. We have the functionality we have, and the plan is to maintain it as-is\u2014updating to version 1.x conveys that intent to preserve backwards compatibility.</li>\n<li>The -5.5 suffix clarifies to which version of OMERO a given release of ImageJ-OMERO is bound.</li>\n</ol>\n<p>If a future release of OMERO breaks imagej-omero\u2019s API compatibility, we can bump the imagej-omero major version digit.</p>\n<p>If bug-fixes are needed to imagej-omero for a particular release of OMERO, we can bump the patch or minor digit of imagej-omero while preserving the OMERO version suffix\u2014e.g., 1.0.0-5.5 <img src=\"https://emoji.discourse-cdn.com/twitter/arrow_right.png?v=9\" title=\":arrow_right:\" class=\"emoji\" alt=\":arrow_right:\"> 1.0.1-5.5 for bug-fixes, or 1.0.0-5.5 <img src=\"https://emoji.discourse-cdn.com/twitter/arrow_right.png?v=9\" title=\":arrow_right:\" class=\"emoji\" alt=\":arrow_right:\"> 1.1.0-5.5 for backwards-compatible new features.</p>\n<p>I did not yet cut the 1.0.0-5.5 release of imagej-omero, because the Travis CI build is failing:</p>\n<p><a href=\"https://travis-ci.org/imagej/imagej-omero/builds/650861636#L694\" class=\"onebox\" target=\"_blank\">https://travis-ci.org/imagej/imagej-omero/builds/650861636#L694</a></p>\n<p>The failure is caused by an issue with <a href=\"http://test.imagej.net\">test.imagej.net</a>. I\u2019ll investigate soon. Next steps:</p>\n<ul>\n<li>Restore the <code>omero_test_infra_backup.zip</code> resource</li>\n<li>Ensure the Travis CI build passes, fixing any remaining issues</li>\n<li>Cut the 1.0.0-5.5 release of imagej-omero</li>\n<li>Update imagej-omero-legacy in a similar fashion and release it as well</li>\n<li>Upload the releases to the OMERO-5.5 update site (functionally they are unlikely to be different than what\u2019s already uploaded there, but good to have the official releases being served)</li>\n<li>Add the OMERO-5.5 update site to the central list of ImageJ update sites</li>\n<li>Try upgrading to OMERO 5.6.</li>\n</ul>\n<p><a class=\"mention\" href=\"/u/j.burel\">@j.burel</a> Is OMERO 5.6 backwards compatible with 5.5? I\u2019m assuming not since it\u2019s not a patch release? Should I go ahead and create a separate OMERO-5.6 update site once I get to that point, then?</p>", "<p><a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a><br>\nOMERO.insight version is 5.5.9 and it is compatible with 5.5 and 5.6<br>\nSo we should be fine with 5.5 update site.</p>\n<p>Jmarie</p>", "<p>Hi <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a><br>\nsorry to revive this thread!<br>\nI wanted to ask, is there is an update concerning to the release of ImageJ-OMERO 5.5 and 5.6 ?<br>\nWe are in the process of upgrading our OMERO server (currently on 5.4), hence the question <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nThank you!</p>", "<p><a class=\"mention\" href=\"/u/cellkai\">@CellKai</a> I\u2019m sorry, I don\u2019t have an update on ImageJ-OMERO yet. I\u2019ve been hammering on the new ImageJ wiki, which is nearing completion. The issue to watch is <a href=\"https://github.com/imagej/imagej-omero/pull/107\">imagej/imagej-omero#107</a>. I have added it to my list for next week, and hope to have news for you soon.</p>", "<p>Hi <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> , thank you very much for the prompt reply,  the link to the issue, also for adding it your list!<br>\nLooking forward to the new ImageJ wiki as well then <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>This work is now <a href=\"https://forum.image.sc/t/imagej-omero-5-5-5-6-update-site-ready-for-testing/69394\">ready for testing</a>.</p>", "<p>A post was split to a new topic: <a href=\"/t/imagej-omero-null-pointer-exception-in-omerosession/78333\">ImageJ-OMERO, Null pointer exception in OMEROSession</a></p>"], "50430": ["<p>Hi All,<br>\nIt seems that there is an issue with writing to files on Macros. I tried both \u201cprint\u201d and \u201cFile.append\u201d, but neither of them behave as expected when using \u201c\\t\u201d. Instead of a tab delimiter, one gets a new line behaviour.</p>\n<p>Currently, I go around the issue by printing out single lines from within a double loop, but this is not ideal.</p>\n<p>I noted that the issue was brought up before, but not resolved:</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"3103\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/o/85e7bf/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/saving-values-to-text-file/3103\">Saving values to text file</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Hi All, \nI just started working with ImageJ and have trouble saving information to a text file. \nI\u2019m running a threshold where I set the threshold manually. Then I want to save the lower and upper threshold in an text file. \nHowever it does not give an error, but it is also not saving anything into the textfile. \nHopefully someone can help my to find the mistake in my code. \nCode: \nrun(\"Threshold...\"); \ngetThreshold(lower,upper)      \nprint(\"your thresholds are; \"+lower, \"to \"+ upper);      \nfil\u2026\n  </blockquote>\n</aside>\n\n<p>best<br>\norkun</p>", "<p>Hi <a class=\"mention\" href=\"/u/orkun\">@orkun</a>,<br>\nfor me the tab-escape sequence works fine.<br>\nFor example:</p>\n<pre><code class=\"lang-auto\">path = \"/media/baecker/DONNEES1/mri/in/work/test.txt\"\nFile.append(\"Hello\\tWorld!\", path);\n</code></pre>\n<p>results in</p>\n<pre><code class=\"lang-auto\">Hello\tWorld!\n</code></pre>\n<p>being appended to the file.</p>\n<p>I\u2019m running FIJI (ImageJ 1.53h) on Ubuntu 20.04.</p>\n<p>Best,<br>\nVolker</p>", "<p>Just found this thread and I\u2019m wondering if there\u2019s an issue when using the tab escape sequence with a macro print command:</p>\n<p>print(\u201cabcd\\t1234\u201d);</p>\n<p>produces</p>\n<p>abcd1234</p>", "<p>Hi <a class=\"mention\" href=\"/u/quatreulls\">@quatreulls</a>,</p>\n<p>the tab is actually there, but the log-window is not displaying it. To check, copy the text from the log window and paste it into an editor.</p>\n<p>Best,<br>\nVolker</p>"], "79107": ["<p><strong>Hello <a class=\"mention\" href=\"/u/tinevez\">@tinevez</a> ,</strong></p>\n<p><strong>My name is Marina Herrero and I\u2019m currently doing my master\u2019s thesis using Mamut. I\u2019ve been trying to open a new Mamut annotation and the error message I am getting is as follows:</strong></p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 54MB of 12057MB (&lt;1%)</p>\n<p>ch.systemsx.cisd.hdf5.exceptions.HDF5FileNotFoundException: Path does not exit. (C:\\Users\\lab185\\Desktop\\MASTER TFM\\Pr\u00e1cticas.\\2021-12-08_16.49.16_ATHMP41-NLS-3xYFP_x_membraneRed_Movie2-track-track-crop.h5)<br>\nat ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:221)<br>\nat ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:177)<br>\nat ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:155)<br>\nat ch.systemsx.cisd.hdf5.HDF5ReaderConfigurator.reader(HDF5ReaderConfigurator.java:81)<br>\nat ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.openForReading(HDF5FactoryProvider.java:55)<br>\nat ch.systemsx.cisd.hdf5.HDF5Factory.openForReading(HDF5Factory.java:54)<br>\nat bdv.img.hdf5.Hdf5ImageLoader.open(Hdf5ImageLoader.java:183)<br>\nat bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:389)<br>\nat bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:79)<br>\nat bdv.BigDataViewer.initSetupNumericType(BigDataViewer.java:279)<br>\nat bdv.BigDataViewer.initSetups(BigDataViewer.java:311)<br>\nat fiji.plugin.mamut.SourceSettings.(SourceSettings.java:101)<br>\nat fiji.plugin.mamut.NewMamutAnnotationPlugin.run(NewMamutAnnotationPlugin.java:103)<br>\nat ij.IJ.runUserPlugIn(IJ.java:237)<br>\nat ij.IJ.runPlugIn(IJ.java:203)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.lang.Thread.run(Thread.java:750)</p>\n<p><strong>I\u2019ve tried updating, reinstalling the software and plugins from start in another pc and the same error appears. Don\u2019t know what to do, a few months ago it worked perfeclty fine.</strong></p>\n<p><strong>I\u2019ve tried oppening an old annotation and same thing:</strong></p>\n<p>(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 97MB of 12057MB (&lt;1%)</p>\n<p>hdf.hdf5lib.exceptions.HDF5FileInterfaceException: Unable to open file [\"f:\\jhdf5\\c\\build\\cmake-hdf5-1.10.5\\hdf5-1.10.5\\src\\h5fdsec2.c line 346 in H5FD_sec2_open(): unable to open file: name = \u2018C:\\Users\\lab185\\Desktop\\MASTER TFM\\Pr\u00c3\u00a1cticas\\DR4.\\DC_CROPPED.h5\u2019, errno = 2, error message = \u2018No such file or directory\u2019, flags = 0, o_flags = 0<br>\n\"]<br>\nat hdf.hdf5lib.H5.H5Fis_hdf5(Native Method)<br>\nat ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.isHDF5File(HDF5FactoryProvider.java:61)<br>\nat ch.systemsx.cisd.hdf5.HDF5Factory.isHDF5File(HDF5Factory.java:106)<br>\nat ch.systemsx.cisd.hdf5.HDF5BaseReader.openFile(HDF5BaseReader.java:231)<br>\nat ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:177)<br>\nat ch.systemsx.cisd.hdf5.HDF5BaseReader.(HDF5BaseReader.java:155)<br>\nat ch.systemsx.cisd.hdf5.HDF5ReaderConfigurator.reader(HDF5ReaderConfigurator.java:81)<br>\nat ch.systemsx.cisd.hdf5.HDF5FactoryProvider$HDF5Factory.openForReading(HDF5FactoryProvider.java:55)<br>\nat ch.systemsx.cisd.hdf5.HDF5Factory.openForReading(HDF5Factory.java:54)<br>\nat bdv.img.hdf5.Hdf5ImageLoader.open(Hdf5ImageLoader.java:183)<br>\nat bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:389)<br>\nat bdv.img.hdf5.Hdf5ImageLoader.getSetupImgLoader(Hdf5ImageLoader.java:79)<br>\nat bdv.BigDataViewer.initSetupNumericType(BigDataViewer.java:279)<br>\nat bdv.BigDataViewer.initSetups(BigDataViewer.java:311)<br>\nat fiji.plugin.mamut.SourceSettings.(SourceSettings.java:101)<br>\nat fiji.plugin.mamut.io.MamutXmlReader.readSourceSettings(MamutXmlReader.java:172)<br>\nat fiji.plugin.mamut.LoadMamutAnnotationPlugin.load(LoadMamutAnnotationPlugin.java:103)<br>\nat fiji.plugin.mamut.LoadMamutAnnotationPlugin.run(LoadMamutAnnotationPlugin.java:84)<br>\nat ij.IJ.runUserPlugIn(IJ.java:237)<br>\nat ij.IJ.runPlugIn(IJ.java:203)<br>\nat ij.Executer.runCommand(Executer.java:152)<br>\nat ij.Executer.run(Executer.java:70)<br>\nat java.lang.Thread.run(Thread.java:750)</p>\n<p><strong>Any help at all would be greatly appreciated.</strong></p>\n<p><strong>Thank you very much,</strong></p>\n<p><strong>Marina Herrero</strong></p>"], "77073": ["<p>Just checking if there have been updates to Zeiss CAN29 adapter to be able to recognize a Zeiss Axio Observer 7. Currently I cannot get a config file to find the scope.</p>\n<p>Thanks!</p>", "<p>Since your device is Zeiss I would rather use the MTB API provided by Zeiss especially if you know C#. I have created a project on Github which uses the MTB API <a href=\"https://github.com/BiologyTools/BioImager\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - BiologyTools/BioImager: A .NET microscopy imaging application based on Bio library. Supports various microscopes by using imported libraries &amp; GUI automation. Supports XInput game controllers to move stage, take images, run ImageJ macros on images or Bio C# scripts.</a></p>"], "75049": ["<p>Hello dear Omero users and developers,</p>\n<p>My university has set up an Omero server so that we can easily store, view, etc. our clinical data. Thanks to bioformats, a number of file formats are already available. Unfortunately not for our optical coherence tomography (OCT) prototype. The file format used there is developed by the manufacturer Thorlabs and is therefore a little proprietary and is not yet supported by Omero/bioformats.</p>\n<p>Is there a possibility to create a request here or somewhere else?</p>\n<p>I could provide one or more files. I have also written python code that can open such an oct file and export it to any other format. I would also make this available. However, since I have no idea about the API of omero/bio-formats and have not programmed with Java for a very long time, I would be happy to get some support from the community.</p>\n<p>About the file format:</p>\n<p>The OCT creates 3D scans (also in depth) of objects with the help of an infrared laser. This depth information is obtained via inferrometry. The data is either stored as spectral data, I have developed a python function to decode the spectral data, or as a plain tif stack in a (kind of) zip archive.<br>\nThe task now is to open this archive and extract the data.<br>\nAs I said, I already have ready-made functions in python that do this. I just don\u2019t know how to implement this new file format in bioformats so that we can store the OCT data directly on the Omero server and also view it.</p>\n<p>In the saved file there is also an XML header file with interesting meta data about the scan and about resolution and so on, which we can use to display the image.</p>\n<p>Can anyone help me write the code and add it to the project? Have any of you had experience adding a new file format to bio-formats/omero and would be willing to share your knowledge with me? Or are there people who could do the work for me?</p>\n<p>Many thanks in advance.</p>\n<p>Tobias</p>", "<p>Hi,</p>\n<p>Please read this blog post on new formats policy\u2026 <a href=\"https://www.openmicroscopy.org/2019/06/25/formats.html\" class=\"inline-onebox\">OME's position regarding file formats | Open Microscopy Environment (OME)</a></p>\n<p>Also our position on Next-Gen-File-Formats NGFF: <a href=\"https://www.openmicroscopy.org/2021/12/16/ome-ngff.html\" class=\"inline-onebox\">OME-NGFF 2021 updates | Open Microscopy Environment (OME)</a></p>\n<p>I would suggest that since you already have python code that can read your data, you focus on converting the data to OME-TIFF or OME-NGFF. Both formats can be imported into OMERO. NGFF is also a cloud-friendly format, suitable for remote access e.g. from s3 data storage.<br>\nSee the spec at <a href=\"https://ngff.openmicroscopy.org/latest/\" class=\"inline-onebox\">Next-generation file formats (NGFF)</a> and <a href=\"https://github.com/ome/ome-zarr-py\" class=\"inline-onebox\">GitHub - ome/ome-zarr-py: Implementation of next-generation file format (NGFF) specifications for storing bioimaging data in the cloud.</a> for help writing OME-NGFF.</p>\n<p>We\u2019d be happy to help with that effort,</p>\n<p>Regards,<br>\nWill</p>", "<p>Thank you Will for your reply.</p>\n<p>I\u2019ll look into both formats and see how I can implement this into our clinical workflow. It\u2019s a bit unfortunate that there are no more formats included, but after reading your linked blog posts I can understand why that is.<br>\nI think I can manage the file conversion, and if I have trouble in some aspects, I know where I can find help.</p>\n<p>Thanks again and best regards</p>\n<p>Tobias</p>", "<p>Dear Tobias,</p>\n<p>I am the developer of <a href=\"https://github.com/MedVisBonn/eyepy\" rel=\"noopener nofollow ugc\">eyepy</a> a Python package for working with OCT data. I would be happy to include your code for reading the ThorLabs .oct format into the package, to make it available to the community. If you are interested in contributing your work you are welcome to join our <a href=\"https://github.com/MedVisBonn/eyepy/discussions/11\" rel=\"noopener nofollow ugc\">discussion</a> on supporting additional file formats.</p>\n<p>Best Olivier</p>"], "38198": ["<p>I am trying to apply U-Net multi-class segmentation procedure on whole slide histopathology brightfield images that were annotated in QuPath by several tissue region categories such as tumor, normal, stroma etc.</p>\n<p>See example of multi class annotations (showing only a small subset of a slide):<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/d/8d6cfa8b8431f2d8c1acd98022ed5c9224e9c585.png\" alt=\"Screen Shot 2020-05-22 at 7.25.29 PM\" data-base62-sha1=\"kb6OabPwolKAKkivlfipyqnsF2l\" width=\"442\" height=\"458\"></p>\n<p>I need to bring the QuPath annotations into a single whole slide binary multi-channel PNG image where each tissue region category has its mask coded (for all the regions/elements in the slide) in the corresponding channel, in addition to a background channel annotating all the non annotated pixels. Alternatively, I would have a Python Numpy array coding the same annotation, but these two formats are exchangeable. Alternatively, XML output of the annotations should work too.</p>\n<p>The problem is that Python access to QuPath project files is <a href=\"https://qupath.readthedocs.io/en/latest/docs/reference/faqs.html?highlight=python#how-do-i-read-a-qpdata-file-in-python-c-r\" rel=\"nofollow noopener\">not possible</a>, and <a href=\"https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_annotations.html#binary-labeled-images\" rel=\"nofollow noopener\">QuPath groovy scripts</a> focus on tiles or individual ROIs instead of the whole image.</p>\n<p>Is there an existing script performing such task or can you advise how to arrive at one that does so?</p>\n<p>Thanks</p>", "<p>The reason the Groovy scripts focus on tiles is that a whole slide image is typically just far too big to export as a PNG \u2013 at least at full resolution.</p>\n<p>This page describes how to export an <code>ImageServer</code> in different ways, at different resolutions: <a href=\"https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_images.html\" class=\"inline-onebox\">Exporting images \u2014 QuPath 0.3.0 documentation</a></p>\n<p>The exact same scripting approaches can be applied to export the full image for the <code>LabeledImageServer</code> created in <a href=\"https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_annotations.html#labeled-tiles\">exporting annotations section</a>.</p>\n<p>But if your image is too large, the export will either fail (because of memory/array length issues) or be totally impractical (because a PNG is not a <a href=\"https://qupath.readthedocs.io/en/latest/docs/intro/formats.html\">tiled, pyramidal image</a>).</p>\n<p>The only export format that should work for whole slide images in general through QuPath is currently <code>.ome.tif</code> \u2013 because it does support writing tiled, pyramidal images.</p>\n<aside class=\"quote no-group\" data-username=\"asmagen\" data-post=\"1\" data-topic=\"38198\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/asmagen/40/30709_2.png\" class=\"avatar\"> asmagen:</div>\n<blockquote>\n<p>Alternatively, XML output of the annotations should work too.</p>\n</blockquote>\n</aside>\n<p>XML is a bit too vague\u2026 the exporting annotations page says why it is not supported. But GeoJSON is well-defined, and you can export as that.</p>", "<p>Thanks but I don\u2019t think the resolution is the problem - the output would be just a binary image. Anyway I could use a lower resolution representation if resolution is the only problem. Can you clarify how the exporting tiles script from the link you provided can be used to export the whole image in intermediate resolution? Or how can I export the <code>.ome.tif</code> you mentioned?</p>\n<p>Thanks</p>", "<p>I\u2019ve updated the documentation now to provide an example of this:<br>\n<a href=\"https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_annotations.html#full-labeled-image\" class=\"onebox\" target=\"_blank\">https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_annotations.html#full-labeled-image</a></p>", "<p>Thank you, that works well for RGB downsampled image coding the regions with different colors. However, I get the following error when I set <code>.multichannelOutput(true)</code> to get a multi-channel (channel per annotation group) image:</p>\n<pre><code class=\"lang-auto\">ERROR: IOException at line 28: Unable to write /Users/Assaf/Dropbox/shared_imaging/Tissue_Segmentation/export/Li63NDCLAMP-labels.png!  No compatible writer found.\n\nERROR: Script error (IOException)\n    at qupath.lib.images.writers.ImageWriterTools.writeImage(ImageWriterTools.java:153)\n    at qupath.lib.scripting.QP.writeImage(QP.java:1860)\n    at qupath.lib.scripting.QP$writeImage$0.callStatic(Unknown Source)\n    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55)\n    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:196)\n    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:216)\n    at Script9.run(Script9.groovy:29)\n    at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:317)\n    at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:155)\n    at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:800)\n    at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:734)\n    at qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:714)\n    at qupath.lib.gui.scripting.DefaultScriptEditor$2.run(DefaultScriptEditor.java:1130)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    at java.base/java.lang.Thread.run(Unknown Source)\n</code></pre>\n<p>How can I resolve this?</p>\n<p>Thanks</p>", "<p>The PNG writer can\u2019t support an arbitrary number of channels.<br>\nChange the file extension to either <code>.tif</code> or <code>.ome.tif</code>.</p>", "<p>Thank you <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>, I\u2019ve been using it and it works well!<br>\nI just wanted to clarify the export order consequence. I have multiple people annotating the images and they\u2019re probably using different strategies and order to annotate different regions, but the overall guideline is starting from the non tissue area annotation, then smallest to largest annotation (small structures first then tumor/normal/stroma etc). So I have two questions:</p>\n<ol>\n<li>If I use the following with <code>multichannelOutput(true)</code> option, am I correct that the labels coming later in the list are going to take over the pixels annotated by ones earlier in the list? (e.g. if a pixel is annotated with both normal and stroma, that pixel will have a value of 255 only in the third channel corresponding to stroma and not in the first channel corresponding to normal?)</li>\n</ol>\n<pre><code class=\"lang-auto\">  .addLabel('Normal', 0)\n  .addLabel('Tumor', 1)\n  .addLabel('Stroma', 2)\n  .addLabel('Bile Ducts', 3)\n  .addLabel('Lymphoid Aggregate', 4)\n  .addLabel('Tissue Fold', 5)\n  .addLabel('Background', 6)\n</code></pre>\n<ol start=\"2\">\n<li>The order of annotating the regions doesn\u2019t really matter since I\u2019m selecting the export order here, right?</li>\n<li>Is there a way to annotate smaller region inside a bigger region in QuPath? It seems that once we annotate a large area, we can\u2019t annotate the substructures within it, so we proceeded with the smallest to largest strategy. But it complicates our work in case we need to re-annotate a structure.</li>\n</ol>\n<p>Thank you!</p>", "<aside class=\"quote no-group\" data-username=\"asmagen\" data-post=\"7\" data-topic=\"38198\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/asmagen/40/30709_2.png\" class=\"avatar\"> asmagen:</div>\n<blockquote>\n<p>If I use the following with <code>multichannelOutput(true)</code> option, am I correct that the labels coming later in the list are going to take over the pixels annotated by ones earlier in the list? (e.g. if a pixel is annotated with both normal and stroma, that pixel will have a value of 255 only in the third channel corresponding to stroma and not in the first channel corresponding to normal?)</p>\n</blockquote>\n</aside>\n<p>No \u2013 pixels only take over if <code>multichannelOutput(false)</code>. With multichannel output, pixels with value 255 can overlap.</p>\n<aside class=\"quote no-group\" data-username=\"asmagen\" data-post=\"7\" data-topic=\"38198\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/asmagen/40/30709_2.png\" class=\"avatar\"> asmagen:</div>\n<blockquote>\n<p>The order of annotating the regions doesn\u2019t really matter since I\u2019m selecting the export order here, right?</p>\n</blockquote>\n</aside>\n<p>Mostly yes \u2013 the order in which you annotate doesn\u2019t matter, it is only the export order that really matters. However, if I use <code>multichannelOutput(false)</code> then the order can help me determine which annotations I need to make more carefully. See the tip at the end of <a href=\"https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_annotations.html#individual-annotations\">https://qupath.readthedocs.io/en/latest/docs/advanced/exporting_annotations.html#individual-annotations</a></p>\n<aside class=\"quote no-group\" data-username=\"asmagen\" data-post=\"7\" data-topic=\"38198\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/asmagen/40/30709_2.png\" class=\"avatar\"> asmagen:</div>\n<blockquote>\n<p>Is there a way to annotate smaller region inside a bigger region in QuPath? It seems that once we annotate a large area, we can\u2019t annotate the substructures within it, so we proceeded with the smallest to largest strategy. But it complicates our work in case we need to re-annotate a structure.</p>\n</blockquote>\n</aside>\n<p>Yes, see <a href=\"https://qupath.readthedocs.io/en/latest/docs/reference/faqs.html#how-do-i-create-a-new-annotation-inside-an-existing-one-using-the-brush-or-wand\">https://qupath.readthedocs.io/en/latest/docs/reference/faqs.html#how-do-i-create-a-new-annotation-inside-an-existing-one-using-the-brush-or-wand</a> and <a href=\"https://qupath.readthedocs.io/en/latest/docs/starting/annotating.html#locking-unlocking\">https://qupath.readthedocs.io/en/latest/docs/starting/annotating.html#locking-unlocking</a></p>", "<p>I see, thanks <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a></p>\n<p>So when I open the tif export file in Python and inspect the mask values in each channel, I see that most of the values are either 0 or 255, but oddly there are mask values in between?<br>\n[  0   4  12  20  28  32  36  56  60  84  95  96 100 103 115 120 135 139 151 155 159 171 195 199 219 223 227 235 243 251 255]<br>\nWhy is that? Note that I don\u2019t see any difference between the image generated for this mask after discretizing all of the non zero values to 255 (top) or alternatively considering only the 255 values as positive and therefore assigning all of the 0&lt;values&lt;255 to 0 (bottom).<br>\nWhy is that? I just need to use the pixels annotated by the annotators so if the software potentially adds a rim to the edges then I should exclude that.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/07ceaf060a61048bae518cd082fc7539a5f1024a.png\" data-download-href=\"/uploads/short-url/1749VdUFOCIUI9bBW9Vo8uf9Cbw.png?dl=1\" title=\"Screen Shot 2020-06-10 at 8.33.04 AM\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/07ceaf060a61048bae518cd082fc7539a5f1024a.png\" alt=\"Screen Shot 2020-06-10 at 8.33.04 AM\" data-base62-sha1=\"1749VdUFOCIUI9bBW9Vo8uf9Cbw\" width=\"316\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/07ceaf060a61048bae518cd082fc7539a5f1024a_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2020-06-10 at 8.33.04 AM</span><span class=\"informations\">318\u00d7502 20.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg></div></a></div><br>\nThank you</p>", "<p>I would expect values other than 0 and 255 to occur if</p>\n<ul>\n<li>compression is applied (e.g. the masks are saved as JPEGs at some point), or</li>\n<li>the masks are resized with interpolation</li>\n</ul>\n<p>I don\u2019t know enough about your exact steps to know if this is the explanation and, if so, exactly where this happens.</p>\n<p>Assuming you export the masks as TIFF, I would suggest opening these TIFF images in ImageJ and checking the values there. If they are not all 0 and 255 something must be going wrong in the export; if they are, then my expectation is that it is interpolation happening within Python.</p>", "<p>I found that it\u2019s indeed the interpolation. Thanks!</p>", "<p>Hi Pete. I\u2019m also deep into Qupath and like it. However, I\u2019ve got a case where I have to export tiles with their binary masks. So the problem is that the colors on these masks are not matching with those I set up.</p>\n<pre><code class=\"lang-auto\">// Create an ImageServer where the pixels are derived from annotations\ndef labelServer = new LabeledImageServer.Builder(imageData)\n        .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n        .downsample(downsample)\n        //  Integer RED = packRGB(255, 0, 0);\n        .addLabel(\"Non-small cell carcinoma undefined\", 1,  ColorTools.RED)\n        //  Integer GREEN = packRGB(0, 255, 0);\n        .addLabel(\"Artefacts\", 2,  ColorTools.GREEN)\n        //  Integer BLUE = packRGB(0, 0, 255);\n        .addLabel('Necrosis/debris etc.', 3,  ColorTools.BLUE)\n        //Integer MAGENTA = packRGB(255, 0, 255);\n        .addLabel(\"Stroma\", 4,  ColorTools.MAGENTA)\n        // Integer CYAN = packRGB(0, 255, 255);\n        .addLabel(\"Normal_epithelium\", 5,  ColorTools.CYAN)\n        //Integer YELLOW = packRGB(255, 255, 0);\n        .addLabel(\"Other malignancies/suspicious areas\", 6,  ColorTools.YELLOW)\n        .addLabel('Benign stroma', 7,  ColorTools.packRGB(255, 50, 50))\n        .addLabel(\"Adenocarcinoma\", 8,  ColorTools.packRGB(255, 150, 50))\n        .addLabel('NORMA', 9,  ColorTools.packRGB(255, 150, 150))\n        .addLabel(\"Benign epithelium/parenchyma\", 10,  ColorTools.packRGB(0, 50, 50))\n        .addLabel(\"I dont know\", 11,  ColorTools.packRGB(50, 50, 50))\n        .addLabel(\"Squamous-cell carcinoma\", 12,  ColorTools.packRGB(50, 150, 50))\n        .addLabel(\"Small-cell lung cancer\", 13,  ColorTools.packRGB(50, 150, 150))\n        .multichannelOutput(true)\n        .build()\n\n// Create an exporter that requests corresponding tiles from the original &amp; labeled image servers\nnew TileExporter(imageData)\n        .downsample(downsample)     // Define export resolution\n        .imageExtension('.png')     // Define file extension for original pixels (often .tif, .jpg, '.png' or '.ome.tif')\n        .tileSize(tileSize)              // Define size of each tile, in pixels\n        .labeledServer(labelServer) // Define the labeled image server to use (i.e. the one we just built)\n        .annotatedTilesOnly(true)  // If true, only export tiles if there is a (labeled) annotation present\n        .overlap(overlap)                // Define overlap, in pixel units at the export resolution\n        .writeTiles(pathOutput)     // Write tiles to the specified directory\n\n</code></pre>\n<p>But on tiles with masks colors are not from the code above. Do you mind to point how to solve this issue please?</p>", "<p>Hello! my output is the same exactly as yours, how did you end up viewing the original colors? Thanks!</p>", "<p>Just came across this now, but most programs will read the pixel values you assigned, which are 1-13, and will be a grayscale image. The exporter does not generate a color image, but is intended for use as labels.</p>\n<p>In general, you end up with small pixel values in a single channel image, unless the program can somehow read what the lookup tables are.</p>"], "75074": ["<p>Dear all,</p>\n<p>Following the much awaited and awesome arrival of QuPath 0.4.0, we\u2019ve updated the QuPath Cellpose extension as well</p>\n<h2>\n<a name=\"notable-changes-1\" class=\"anchor\" href=\"#notable-changes-1\"></a>Notable changes</h2>\n<h3>\n<a name=\"global-normalization-2\" class=\"anchor\" href=\"#global-normalization-2\"></a>Global Normalization</h3>\n<p>Thanks to the new StarDist Extension, we can now use some from of <strong>global normalization</strong> by either feeding it a fancy <code>preprocess()</code> command.</p>\n<pre><code class=\"lang-auto\"> .preprocess( Cellpose2D.imageNormalizationBuilder()\n\t.percentiles(0, 99.8)\n\t.perChannel(false)\n\t.downsample(10)\n\t.build()\n)\n</code></pre>\n<p>or using a shortcut</p>\n<pre><code class=\"lang-auto\">.normalizePercentilesGlobal(0, 99.8, 10)\n</code></pre>\n<p>Note that the shortcut is the equivalent to:</p>\n<pre><code class=\"lang-auto\"> .preprocess( Cellpose2D.imageNormalizationBuilder()\n    .percentiles(0, 99.8, 10)\n    .perChannel(true)\n    .downsample(10)\n    .useMask(true)\n    .build()\n)\n</code></pre>\n<p>The code from the Stardist Extension was copied to the Cellpose one.</p>\n<p>This is possible because cellpose now offers a <code>no_norm</code> parameter to turn off auto normalization.</p>\n<h3>\n<a name=\"all-the-cellpose-parameters-3\" class=\"anchor\" href=\"#all-the-cellpose-parameters-3\"></a>All the Cellpose parameters</h3>\n<p>You can now add <a href=\"https://cellpose.readthedocs.io/en/latest/command.html#options\">any command line options accepted by cellpose</a> by using the CellposeBuilders\u2019s <code>addParameter()</code> method.</p>\n<p>This allows you to configure cellpose as much as you want.</p>\n<p><strong>On the downside, this means that some options that were available on the Builder before have been removed.</strong> In case you see errors from your previously working scripts, that\u2019s the first place to look.</p>\n<h3>\n<a name=\"limited-cellpose-backwards-compatibility-4\" class=\"anchor\" href=\"#limited-cellpose-backwards-compatibility-4\"></a>Limited Cellpose backwards compatibility</h3>\n<p>Cellpose changed some CLI parameter options over their versions and it\u2019s been decided to only support cellpose v2.0.5 and beyond. This simplifies the logic handling for us and allows us to use the Global normalization.</p>\n<p>We have not tested how it plays with Omnipose right now, as the omnipose flavor is currently out of sync with cellpose\u2026</p>\n<p>A great thank you to <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> and his team for the massive upgrade to QuPath and its extensions, which I very much look forward to testing and discovering in 2023.</p>\n<p>Of course, these breaking changes will be causing a few issues. Please use this post or the forum to get in touch and we\u2019ll try to address them.</p>\n<p>The ReadMe for the project is updated and you can find the latest release under this post</p>\n<p>All the best</p>\n<p>Oli</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/BIOP/qupath-extension-cellpose/releases/tag/v0.6.1\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/BIOP/qupath-extension-cellpose/releases/tag/v0.6.1\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317.png 2x\" data-dominant-color=\"F4F1ED\"></div>\n\n<h3><a href=\"https://github.com/BIOP/qupath-extension-cellpose/releases/tag/v0.6.1\" target=\"_blank\" rel=\"noopener\">Release Tile creation rollback \u00b7 BIOP/qupath-extension-cellpose</a></h3>\n\n  <p>This update simply adds a rollback on how tiles were computed for cellpose.\nWhen QuPath 0.4.1 came out, the logic of the tiles was rewritted based on the StarDist Extension, which would force tiles...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>EDIT: 2023 01 16: Updated link to latest release</p>", "<p>Hello again,</p>\n<p>I am using the cellpose script model to calculate muscle fiber number and other parameters.<br>\nMy question is why I can\u2019t access the measuremnt map option?<br>\nFor example to select objects based on size.<br>\nLet me show you:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47541fc141f8b71504fc0352a8b3dcd58fe11c24.jpeg\" data-download-href=\"/uploads/short-url/ab08odoNR48BOPLHkj3RIWd2kII.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47541fc141f8b71504fc0352a8b3dcd58fe11c24.jpeg\" alt=\"image\" data-base62-sha1=\"ab08odoNR48BOPLHkj3RIWd2kII\" width=\"690\" height=\"493\" data-dominant-color=\"B59BB5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">940\u00d7672 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks!<br>\nPawel</p>", "<p>Hi!<br>\nAre you sure you added the <code>measureShape()</code> and <code>measureIntensity()</code> options in the builder?</p>", "<p>HI!<br>\nPlease see my model :</p>\n<pre><code class=\"lang-auto\">import qupath.ext.biop.cellpose.Cellpose2D\n\n// Specify the model name (cyto, nuc, cyto2, omni_bact or a path to your custom model)\ndef pathModel = 'C:\\\\QuPath - training cellpose 2\\\\models\\\\cellpose_residual_on_style_on_concatenation_off_train_2022_10_27_22_17_06.207472'\n\ndef cellpose = Cellpose2D.builder( pathModel )\n        .pixelSize( 0.5 )              // Resolution for detection\n        .channels( 'Green' )            // Select detection channel(s)\n//        .preprocess( ImageOps.Filters.median(1) )                // List of preprocessing ImageOps to run on the images before exporting them\n//        .tileSize(2048)                // If your GPU can take it, make larger tiles to process fewer of them. Useful for Omnipose\n//        .cellposeChannels(1,2)         // Overwrites the logic of this plugin with these two values. These will be sent directly to --chan and --chan2\n//        .maskThreshold(-0.2)           // Threshold for the mask detection, defaults to 0.0\n//        .flowThreshold(0.5)            // Threshold for the flows, defaults to 0.4 \n       .diameter(85)                   // Median object diameter. Set to 0.0 for the `bact_omni` model or for automatic computation\n//        .setOverlap(60)                // Overlap between tiles (in pixels) that the QuPath Cellpose Extension will extract. Defaults to 2x the diameter or 60 px if the diameter is set to 0 \n//        .invert()                      // Have cellpose invert the image\n//        .useOmnipose()                 // Add the --omni flag to use the omnipose segmentation model\n//        .excludeEdges()                // Clears objects toutching the edge of the image (Not of the QuPath ROI)\n//        .clusterDBSCAN()               // Use DBSCAN clustering to avoir over-segmenting long object\n//        .cellExpansion(5.0)            // Approximate cells based upon nucleus expansion\n//        .cellConstrainScale(1.5)       // Constrain cell expansion using nucleus size\n//        .classify(\"My Detections\")     // PathClass to give newly created objects\n        .measureShape()                // Add shape measurements\n        .measureIntensity()            // Add cell measurements (in all compartments)  \n        .createAnnotations()           // Make annotations instead of detections. This ignores cellExpansion\n        .useGPU()                      // Optional: Use the GPU if configured, defaults to CPU only\n        .build()\n\n// Run detection for the selected objects\ndef imageData = getCurrentImageData()\ndef pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"Cellpose\", \"Please select a parent object!\")\n    return\n}\ncellpose.detectObjects(imageData, pathObjects)\nprintln 'Done!'\n</code></pre>", "<p>Yep so you have <code>createAnnotations()</code> on. The measurement map only displays measurements for detections. If you comment out that line, you should get what you want.</p>\n<p>Best</p>", "<p>A small update that hopefully makes the extension a bit faster. This is more of a rollback than a new feature.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/BIOP/qupath-extension-cellpose/releases/tag/v0.6.1\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/BIOP/qupath-extension-cellpose/releases/tag/v0.6.1\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/077fbfc2052c923643acbad6fdcf1fd5e4a61317.png 2x\" data-dominant-color=\"F4F1ED\"></div>\n\n<h3><a href=\"https://github.com/BIOP/qupath-extension-cellpose/releases/tag/v0.6.1\" target=\"_blank\" rel=\"noopener\">Release Tile creation rollback \u00b7 BIOP/qupath-extension-cellpose</a></h3>\n\n  <p>This update simply adds a rollback on how tiles were computed for cellpose.\nWhen QuPath 0.4.1 came out, the logic of the tiles was rewritted based on the StarDist Extension, which would force tiles...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>The upper link has also been updated</p>", "<p>Hi Olivier!</p>\n<p>Thanks for the update!<br>\nI\u2019m testing the new extension and may I ask is it possible to train cellpose model for whole project?<br>\nI have some ground truth in several slides so I hope to use all of them for training.</p>\n<p>Thanks!<br>\nYuan</p>", "<p>The training workflow will go through the whole project automatically and look for rectangles with Training or Validation Classes. It does not train on the currently open image, so it should do what you want right out of the box <img src=\"https://emoji.discourse-cdn.com/twitter/relaxed.png?v=12\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi Oliver, I am trying to run cellpose /Quapth on the HPC of our institute. I have no problems running Quapth 0.4. but when I tried to run cellpose from Qupath using the script (you recommend when posted  the new version), I got this error</p>\n<p>INFO: Neither diameter nor overlap provided. Overlap defaulting to 60 pixels. Use <code>.setOverlap( int )</code> to modify overlap<br>\nINFO: If tiling is necessary, 60 pixels overlap will be taken between tiles<br>\nINFO: Folder creation of /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_OMEs/cellpose-temp was interrupted. Either the folder exists or there was a problem.<br>\nINFO: Computed percentile normalization offsets=[959.7971436767579, 624.2795650024414], scales=[1.0707489112931013E-4, 7.37700724007785E-5]<br>\nINFO: Saving images for 1 tiles<br>\nINFO: Saving to /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_OMEs/cellpose-temp/Temp_6122_2971_z0_t0.tif<br>\nINFO: Executing command:<br>\nbash -c \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205 -W ignore -m cellpose --dir /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_OMEs/cellpose-temp --pretrained_model tissuenet --no_norm --save_tif --no_npy --use_gpu --verbose\u201d<br>\nINFO: This command should run directly if copy-pasted into your shell<br>\nERROR: Failed to Run Cellpose<br>\njava.io.IOException: Cannot run program \u201cbash\u201d: error=0, Failed to exec spawn helper: pid: 74973, exit value: 127<br>\nat java.base/java.lang.ProcessBuilder.start(Unknown Source)<br>\nat java.base/java.lang.ProcessBuilder.start(Unknown Source)<br>\nat qupath.ext.biop.cmd.VirtualEnvironmentRunner.runCommand(VirtualEnvironmentRunner.java:181)<br>\nat qupath.ext.biop.cellpose.Cellpose2D.runCellpose(Cellpose2D.java:879)<br>\nat qupath.ext.biop.cellpose.Cellpose2D.detectObjects(Cellpose2D.java:524)<br>\nat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nat Cellpose_new.run(Cellpose_new.groovy:34)<br>\nat org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\nat org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nat qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nat qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1115)<br>\nat qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1480)<br>\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\nat java.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\nat java.base/java.lang.Thread.run(Unknown Source)<br>\nCaused by error=0, Failed to exec spawn helper: pid: 74973, exit value: 127        at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)<br>\nat java.base/java.lang.ProcessImpl.(Unknown Source)<br>\nat java.base/java.lang.ProcessImpl.start(Unknown Source)<br>\nat java.base/java.lang.ProcessBuilder.start(Unknown Source)<br>\nat java.base/java.lang.ProcessBuilder.start(Unknown Source)<br>\nat qupath.ext.biop.cmd.VirtualEnvironmentRunner.runCommand(VirtualEnvironmentRunner.java:181)<br>\nat qupath.ext.biop.cellpose.Cellpose2D.runCellpose(Cellpose2D.java:879)<br>\nat qupath.ext.biop.cellpose.Cellpose2D.detectObjects(Cellpose2D.java:524)<br>\nat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nat Cellpose_new.run(Cellpose_new.groovy:34)<br>\nat org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\nat org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nat qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nat qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1115)<br>\nat qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1480)<br>\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\nat java.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\nat java.base/java.lang.Thread.run(Unknown Source)<br>\nINFO: Done!<br>\nDu you have any idea now hat is this?.</p>\n<p>PSD: in my previous intent, I had Quapth in a container base on ubuntu and got the error 13, so I tried this with different container based on centos.</p>\n<p>THanks for your help</p>", "<p>From a previous experience this was an issue related to Java not being allowed to execute processes. Last time, this was solved by building QuPath from source on the architecture that was causing the problem (Linux I believe). Perhaps you can try and build QuPath on your HPC and try again?</p>", "<p>Hi Oliver, I was just able to get tested again (it is very hard to build Quapth in the HPC). And the extension is finally working. I was able to run the detection without problems. Thanks for your help and this amazing tool.<br>\nNow I have another problem; I can get detection using tissuenet (your trick using the \u201cserver magic\u201d is great). However, I need to retrain to get better results. My training script is not working. Can you check what I am doing wrong?.<br>\nHere is the script:</p>\n<blockquote>\n<p>import qupath.ext.biop.cellpose.Cellpose2D<br>\nimport qupath.lib.images.ImageData<br>\nimport qupath.lib.images.servers.ConcatChannelsImageServer<br>\nimport qupath.lib.images.servers.TransformedServerBuilder</p>\n<p>//First set channel name DAPI<br>\nsetChannelNames(\u2018DAPI\u2019)</p>\n<p>// Some server magic. Extract channels of interest and project them.<br>\ndef avgServer = new TransformedServerBuilder( getCurrentServer() ).extractChannels(2,3,4,7,8,9,13,14,16,17,18).averageChannelProject().build()</p>\n<p>// Extract the one other channel we want<br>\ndef singleChannel = new TransformedServerBuilder( getCurrentServer() ).extractChannels(\u2018DAPI\u2019).build()</p>\n<p>// Make a combined server. Notice the order here is DAPI first, then the average<br>\ndef combined = new ConcatChannelsImageServer( getCurrentServer(), [singleChannel, avgServer] )</p>\n<p>// Need to create a new in-place ImageData for cellpose later<br>\ndef imageData = new ImageData(combined)</p>\n<p>// run Cellpose. Careful of the channels names<br>\ndef pathModel = \u2018trined_SG\u2019<br>\ndef cellpose = Cellpose2D.builder( pathModel )<br>\n.pixelSize( 0.5 )             // Resolution for detection in um<br>\n.channels( \u201cAverage channels\u201d, \u201cDAPI\u201d )\t      // Select detection channel(s)<br>\n.normalizePercentilesGlobal(0.1, 99.8, 10)<br>\n.tileSize(512)                  // If your GPU can take it, make larger tiles to process fewer of them.<br>\n.cellposeChannels(1,2)         // Need these, otherwise it just sends the one channel. These will be sent directly to --chan and --chan2<br>\n.diameter(0)                    // Median object diameter. Set to 0.0 for the <code>bact_omni</code> model or for automatic computation<br>\n//         .cellExpansion(5.0)              // Approximate cells based upon nucleus expansion<br>\n.measureShape()                // Add shape measurements<br>\n.measureIntensity()             // Add cell measurements (in all compartments)<br>\n.simplify(0)                   // Simplification 1.6 by default, set to 0 to get the cellpose masks as precisely as possible<br>\n.build()</p>\n<p>// Run detection for the selected objects<br>\n//def annotations = getSelectedObjects()</p>\n<p>// Once ready for training you can call the train() method<br>\n// train() will:<br>\n// 1. Go through the current project and save all \u201cTraining\u201d and \u201cValidation\u201d regions into a temp folder (inside the current project)<br>\n// 2. Run the cellpose training via command line<br>\n// 3. Recover the model file after training, and copy it to where you defined in the builder, returning the reference to it<br>\n// 4. If it detects the run-cellpose-qc.py file in your QuPath Extensions Folder, it will run validation for this model</p>\n<p>def resultModel = cellpose.train()</p>\n<p>// Pick up results to see how the training was performed<br>\nprintln \"Model Saved under \"<br>\nprintln resultModel.getAbsolutePath().toString()</p>\n<p>// You can get a ResultsTable of the training.<br>\ndef results = cellpose.getTrainingResults()<br>\nresults.show(\u201cTraining Results\u201d)</p>\n<p>// You can get a results table with the QC results to visualize<br>\ndef qcResults = cellpose.getQCResults()<br>\nqcResults.show(\u201cQC Results\u201d)</p>\n<p>// Finally you have access to a very simple graph<br>\ncellpose.showTrainingGraph()</p>\n</blockquote>\n<p>When I try to run this, I get this error :</p>\n<blockquote>\n<p>INFO: Tile overlap was not set and diameter was set to 0.0. Will default to 60 pixels overlap. Use <code>.setOverlap( int )</code> to modify overlap<br>\nINFO: Tile overlap was not set, but diameter exists. Using provided diameter 0.0 x 2: 0 pixels overlap<br>\nINFO: If tiling is necessary, 0 pixels overlap will be taken between tiles<br>\nINFO: Found 1 Training objects and 3 Validation objects in image 2022_10_24__6174-1.czi - Scene <span class=\"hashtag\">#5_Overlay</span><br>\nWARN: Unable to write image<br>\nERROR: Unable to write /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_Train_Cellpose_01/cellpose-training/train/2022_10_24__6174-1.czi - Scene <span class=\"hashtag\">#5_Overlay_region_0</span>.tif!  No compatible writer found.<br>\nWARN: Unable to write image<br>\nERROR: Unable to write /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_Train_Cellpose_01/cellpose-training/test/2022_10_24__6174-1.czi - Scene <span class=\"hashtag\">#5_Overlay_region_0</span>.tif!  No compatible writer found.<br>\nWARN: Unable to write image<br>\nERROR: Unable to write /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_Train_Cellpose_01/cellpose-training/test/2022_10_24__6174-1.czi - Scene <span class=\"hashtag\">#5_Overlay_region_1</span>.tif!  No compatible writer found.<br>\nWARN: Unable to write image<br>\nERROR: Unable to write /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_Train_Cellpose_01/cellpose-training/test/2022_10_24__6174-1.czi - Scene <span class=\"hashtag\">#5_Overlay_region_2</span>.tif!  No compatible writer found.<br>\nINFO: Executing command:<br>\nbash -c \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205//bin/python -W ignore -m cellpose --train --dir /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_Train_Cellpose_01/cellpose-training/train --test_dir /gpfs/gsfs11/users/perezriverosp/hiplex_oct_nov_2022/QUAPTH04_Train_Cellpose_01/cellpose-training/test --pretrained_model trined_SG --no_norm --chan 1 --chan2 2 --diameter 0.0 --use_gpu --verbose\u201d<br>\nINFO: This command should run directly if copy-pasted into your shell<br>\nINFO: Cellpose2D-train: 2023-01-30 12:02:47,277 [INFO] WRITING LOG OUTPUT TO /home/perezriverosp/.cellpose/run.log<br>\nINFO: Cellpose2D-train: 2023-01-30 12:02:47,449 [INFO] TORCH CUDA version not installed/working.<br>\nINFO: Cellpose2D-train: 2023-01-30 12:02:47,449 [INFO] &gt;&gt;&gt;&gt; using CPU<br>\nINFO: Cellpose2D-train: 2023-01-30 12:02:47,450 [WARNING] pretrained model has incorrect path<br>\nINFO: Cellpose2D-train: Traceback (most recent call last):<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/runpy.py\u201d, line 194, in _run_module_as_main<br>\nINFO: Cellpose2D-train:     return _run_code(code, main_globals, None,<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/runpy.py\u201d, line 87, in _run_code<br>\nINFO: Cellpose2D-train:     exec(code, run_globals)<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/site-packages/cellpose/<strong>main</strong>.py\u201d, line 348, in <br>\nINFO: Cellpose2D-train:     main()<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/site-packages/cellpose/<strong>main</strong>.py\u201d, line 265, in main<br>\nINFO: Cellpose2D-train:     output = io.load_train_test_data(args.dir, test_dir, imf, args.mask_filter, args.unet, args.look_one_level_down)<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/site-packages/cellpose/io.py\u201d, line 225, in load_train_test_data<br>\nINFO: Cellpose2D-train:     images, labels, image_names = load_images_labels(train_dir, mask_filter, image_filter, look_one_level_down, unet)<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/site-packages/cellpose/io.py\u201d, line 198, in load_images_labels<br>\nINFO: Cellpose2D-train:     image_names = get_image_files(tdir, mask_filter, image_filter, look_one_level_down)<br>\nINFO: Cellpose2D-train:   File \u201c/gpfs/gsfs11/users/perezriverosp/conda/envs/cellpose-205/lib/python3.8/site-packages/cellpose/io.py\u201d, line 153, in get_image_files<br>\nINFO: Cellpose2D-train:     raise ValueError(\u2018ERROR: no images in --dir folder\u2019)<br>\nINFO: Cellpose2D-train: ValueError: ERROR: no images in --dir folder<br>\nINFO: Virtual Environment Runner Finished<br>\nERROR: Runner \u2018Cellpose2D-train\u2019 exited with value 1. Please check output above for indications of the problem.<br>\nINFO: Cellpose command finished running<br>\nERROR: null in CellPose_trainAlone.groovy at line number 51</p>\n<p>ERROR: java.base/java.util.Objects.requireNonNull(Unknown Source)<br>\nqupath.ext.biop.cellpose.Cellpose2D.moveAndReturnModelFile(Cellpose2D.java:1285)<br>\nqupath.ext.biop.cellpose.Cellpose2D.train(Cellpose2D.java:903)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nCellPose_trainAlone.run(CellPose_trainAlone.groovy:51)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>ERROR:<br>\nFor help interpreting this error, please search the forum at <a href=\"https://forum.image.sc/tag/qupath\" class=\"inline-onebox\">Topics tagged qupath</a><br>\nYou can also start a new discussion there, including both your script &amp; the messages in this log.</p>\n</blockquote>", "<p>Hi,</p>\n<p>I have a quick question about cellpose. Can someone explain the difference between the lines :</p>\n<p>.channels(\u201cDAPI\u201d, \u201cCY3\u201d)<br>\n.cellposeChannels( channel1, channel2 )</p>\n<p>And why should I use one instead of the other ? Or both ?<br>\nI coulndt find the information on the forum, maybe I missed it, sorry!</p>\n<p>Thx <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nMalo</p>", "<p>So thing is that channels() is a QuPath thing. Allowing you to send the channels you want.<br>\ncellposeChannels() corresponds to the <code>--chan</code> and <code>--chan1</code> flags from the cellpose command line.</p>\n<p>I do not have a specific use case in mind, though for example you might send an RGB image to cellpose (by not setting anything on the channels() ) and then want cellpose to only use the red channel, so you\u2019d use <code>cellposeChannels(1,0)</code></p>\n<p>Hope this clears it up a little bit.</p>\n<p>Oli</p>"], "52583": ["<p>Hello,<br>\nI am trying to learn how to run CellProfiler on a computing cluster with batch processing, but I am running into a problem. After I run<br>\n\u201ccellprofiler -p batch file location/Batch_data.h5 -c -r -f 1 -l 4 -o Output Folder\u201d I get the error \u201cOSError: Test for access to directory failed. Directory: ///Y:/TestImages\u201d. I have the cluster storage mounted on my local computer as the Y drive, and CellProfiler is trying to access the images from there rather than the images\u2019 location in the cluster.<br>\nThe pipeline I\u2019m using (attached at end) has CreateBatchFiles at the end, and I added the appropriate local and cluster root paths for the image locations. I thought that my problem was similar to <a href=\"https://forum.image.sc/t/cellprofiler-v4-0-4-batch-processing/48370\" class=\"inline-onebox\">Cellprofiler_v4.0.4 batch processing</a> where CP is using the local root path instead of the cluster root path. However, when I changed both the local and cluster root paths to the cluster root path just to see what would happen, I get the same error. This leads me to believe that CP is having a problem with reading the filepaths from the image module (since I drag and drop images from the Y drive there for making the batch file).</p>\n<p>I\u2019m not sure what I\u2019m doing incorrectly. I\u2019m new to cluster computing and batch file processing, so any help would be appreciated!<br>\nThank You!<br>\n<a class=\"attachment\" href=\"/uploads/short-url/w6dzVn6o4qaF2koIRhGTUDdRsEO.cpproj\">PracticePipeline_05102021.cpproj</a> (105.9 KB)</p>", "<p>Hmmm, that\u2019s weird, it looks like the find-and-replace that CreateBatchFiles SHOULD be doing from your Y path is not being applied.  It looks like CellProfiler is internally storing that path as <code>///Y:/</code> ; can you try putting that (and/or just <code>Y:/</code> ) in the <code>Local root path</code> part of CreateBatchFiles to see if those batch files seem to be updating the paths correctly?</p>\n<p>On our hand, we\u2019ll work to figure out why it\u2019s storing the paths this way.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45.png\" data-download-href=\"/uploads/short-url/nXNTyls0ohSzblTPKBmhPy2VIOx.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_690x110.png\" alt=\"image\" data-base62-sha1=\"nXNTyls0ohSzblTPKBmhPy2VIOx\" width=\"690\" height=\"110\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_690x110.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_1035x165.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1074\u00d7172 62.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/3/33f34a460144c48ec762278720369438fee5f518.png\" alt=\"image\" data-base62-sha1=\"7pzB3xcwxmkLMkOWKGkwg6SSHz2\" width=\"556\" height=\"188\"></p>", "<p>Hi CP Team,<br>\nJust FYI - the solutions suggested to me in <a href=\"https://forum.image.sc/t/cellprofiler-v4-0-4-batch-processing/48370\">Cellprofiler_v4.0.4 batch processing</a> didn\u2019t fix the issue for me either. I ran out of time and ended up reverting to the version 3.1.3 we have installed on our cluster (which works fine!).<br>\nKarla</p>", "<p>Thank you! I made the local root path ///Y:/ and kept the Cluster root path the same as you suggested, and it worked! Cellprofiler was able to go through all the modules for each image.</p>", "<p>I had a similar issue in CellProfiler 4.2.1, when trying to prototype the pipeline on Windows, then moving to a Linux-based cluster.</p>\n<p>I found using that using this mapping in the \u2018CreateBatchFiles\u2019 module:</p>\n<p>Local root path: C:\\images\\project_folder<br>\nCluster root path: /cluster/user/project_folder</p>\n<p>didn\u2019t work, but this did:</p>\n<p>Local root path: ///C:/images/project_folder<br>\nCluster root path: /cluster/user/project_folder</p>\n<p>where I\u2019ve copied the project_folder containing the images onto the cluster server, rather than referencing the same location from both my local Windows machine and the cluster.</p>\n<p>(Diagnosed by looking at the standard error output from the cluster and seeing what CellProfiler thought the filepaths were)</p>", "<p>Hi,</p>\n<p>I am confused about the Local root path. I want to run parallel jobs on Linux cluster where my data is stored. Why it needs the local root path when it not linked to the local computer? Thanks</p>"], "54634": ["<p>Hi everyone<br>\nrecently i have to analyze slide scanner .czi images which are really large and cant be opened by Fiji<br>\nis there a way to handle (open) and process (on Fiji) these large images better without losing data?</p>", "<p>Hi <a class=\"mention\" href=\"/u/kavator\">@kavator</a></p>\n<p>can you specify what does not work when opening the CZI in Fiji? Any error messages.</p>\n<p>Per default CZI from the AxioScan are compressed using JpegXR compression, but BioFormats supports unpacking this (nice colab with our ZEISS team). But it might take quite long, but this.</p>\n<p>But if you try a really large CZI Fiji may just not have enough memory to open it completely. How big is the image (XY pixels)?</p>\n<p>Did you try to open it in QuPath?</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b12d2ff8048c1274d495300518317c542ff71c41.png\" data-download-href=\"/uploads/short-url/phnkciGf0JPvv9mMRsHRJeK0gJX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b12d2ff8048c1274d495300518317c542ff71c41_2_690x438.png\" alt=\"image\" data-base62-sha1=\"phnkciGf0JPvv9mMRsHRJeK0gJX\" width=\"690\" height=\"438\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b12d2ff8048c1274d495300518317c542ff71c41_2_690x438.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b12d2ff8048c1274d495300518317c542ff71c41_2_1035x657.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b12d2ff8048c1274d495300518317c542ff71c41_2_1380x876.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b12d2ff8048c1274d495300518317c542ff71c41_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71221 182 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\ni got this msg</p>", "<p>Hi <a class=\"mention\" href=\"/u/kavator\">@kavator</a></p>\n<p>well it might be really just the pure file size, but I am not sure. It looks that your RAM is only 6GB? (Our ZEN workstations we typically use to process such images have &gt;128GB of RAM)</p>\n<p>Maybe <a class=\"mention\" href=\"/u/dgault\">@dgault</a> has a better idea?</p>", "<p>The error is due to an intrinsic limitation of ImageJ1 which cannot process images which have more than 2Gpixels on a single plane. I double <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a>\u2019s proposition and strongly suggest you to work with <a href=\"https://qupath.github.io/\" rel=\"noopener nofollow ugc\">QuPath</a> with such images.</p>", "<p>Hi <a class=\"mention\" href=\"/u/kavator\">@kavator</a> ,</p>\n<p>and of course I would be happy to see you suing ZEN blue to analyze, segment, register and correlate those images, since ZEN obviously can handle such large images - since we never load everything into memory etc. <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>"], "75129": ["<p>I am relatively new to QuPath and have very basic coding knowledge. I am trying to analyze RNAscope fluorescent images by labeling whether a cell is positive or negative based on the number of \u201cdots\u201d (~1um^2 per dot) per cell.</p>\n<p>I know how to detect and threshold positive cells based on intensity, I was wondering if there was a way to tweak this to base it on fluorescent area instead.</p>\n<p>My images have two channels, blue (DAPI staining) and either green, red, or magenta (RNAscope puncta).</p>\n<p>Thanks for any help!</p>", "<p>Fluorescent area is roughly the estimated spot count as long as you set the expected spot size to 1um^2.<br>\nSome older discussion of that here <a href=\"https://forum.image.sc/t/qupath-accurate-cytoplasmic-stain-measurements/26245\" class=\"inline-onebox\">QuPath-Accurate cytoplasmic stain measurements</a></p>\n<p>These days you could also apply a thresholder or pixel classifier to each cell, though you\u2019d have to add those measurements manually via script.</p>", "<p>You can follow the SOP from ACDBio (developers of RNAscope) for how to analyze RNA FISH images: <a href=\"https://acdbio.com/system/files_force/MK-51-154-Using-QuPath-to-analyze-RNAscope-BaseScope-and-miRNAscope-images.pdf?download=1\">https://acdbio.com/system/files_force/MK-51-154-Using-QuPath-to-analyze-RNAscope-BaseScope-and-miRNAscope-images.pdf?download=1</a></p>\n<p>Pages 19 and onwards cover FISH.</p>", "<p>How can you filter spots (from subcellular analysis) in cytoplasmic compartments ?</p>", "<p>Have you tried the existing script? It might need updating for 0.4.x <a href=\"https://forum.image.sc/t/need-help-quantifying-nuclear-and-cytoplasmic-rnascope-foci/51077/2\" class=\"inline-onebox\">Need help quantifying nuclear and cytoplasmic RNAscope foci - #2 by Research_Associate</a></p>"], "54659": ["<p>Hi all, and perhaps especially <a class=\"mention\" href=\"/u/wayne\">@Wayne</a>,</p>\n<p>I wanted to bring up an issue/question regarding the behavior of the <em>Edit&gt;Invert</em> command. In scientific images, I <em>think</em> the expectation when inverting an image is that the image would be \u201cfully\u201d inverted. I could be wrong, but several others agreed that this is what they would expect.</p>\n<p>Ideally, 0 would become 255 in an 8bit image, and 0 would become 65535 in a 16bit image, and vice versa. Using the command, this behavior is not observed in some cases, as the <em>min and max values</em> in the image are used for the inversion.<br>\nThis behavior has been noticed before: <a href=\"https://forum.image.sc/t/edit-invert-does-not-invert-the-values/32798/3\" class=\"inline-onebox\">Edit&gt;invert does not invert the values - #3 by Belegon</a></p>\n<p>I brought up my experiences with the <em>Invert</em> in a <a href=\"https://quarep.org/\">QUAREP</a> meeting today, and a few people were a bit surprised. As an example of the issue, I have a link here to two 16bit images.<br>\n<a href=\"https://drive.google.com/drive/folders/1XFsntU3baJ69SX7_sNMOm8MbOU7g-RbJ?usp=sharing\" class=\"inline-onebox\">Other Sharing Content - Google Drive</a></p>\n<p>These are two out of a set of images, but I had expected that by inverting each image in the set, I would obtain equivalent images as a result. This is not the case, as the \u201cblack\u201d bars in the grid, with pixel values of around 500, become ~20,000 in one inverted image, while they become ~65,000 in the second inverted image.<br>\nI understand now that in some cases invert is using the min and max values of the current image, but that seems like it could be a major problem when inverting several images in batch. Or, really, any scientific use of an inversion - as the resulting images can have wildly different values for areas that were originally the same intensity.</p>\n<p>On the other hand, changing this function could have repercussions for people expecting the old behavior. Would it be possible to have a toggle or additional command such that images can be easily \u201ctruly\u201d inverted, across their entire pixel value range?</p>\n<p>Here is some additional information from <a class=\"mention\" href=\"/u/schmiedc\">@schmiedc</a> who looked into the issue after I brought it up.</p>\n<aside class=\"quote no-group\" data-username=\"schmiedc\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/schmiedc/40/14633_2.png\" class=\"avatar\"> schmiedc:</div>\n<blockquote>\n<p>In the ShortProcessor this is defined such:<br>\n<a href=\"https://github.com/imagej/imagej1/blob/a6562a681a8b7e65fc0470656afbddfec73fe17d/ij/process/ShortProcessor.java\" class=\"inline-onebox\">imagej1/ShortProcessor.java at a6562a681a8b7e65fc0470656afbddfec73fe17d \u00b7 imagej/imagej1 \u00b7 GitHub</a></p>\n<pre><code class=\"lang-auto\">case INVERT:\n    v2 = max2 - (v1 - min2);\n    //v2 = 65535 - (v1+offset);\n    break;\n</code></pre>\n<p>It will use the minimum and maximum gray values of the entire image to perform the inversion.</p>\n<p>In one image you have a very bright structure. Thus the max value of that image is very different from the other image.</p>\n<p>The inversion for 8-bit images is defined differently:<br>\n<a href=\"https://github.com/imagej/imagej1/blob/master/ij/process/ImageProcessor.java\">https://github.com/imagej/imagej1/blob/a6562a681a8b7e65fc0470656afbddfec73fe17d/ij/process/ImageProcessor.java</a></p>\n<pre><code class=\"lang-auto\">case INVERT:\n    v = 255 - i;\n    break;\n</code></pre>\n<p>Still not the behavior one would wish for. The out commented version hints that this might be even something that has changed\u2026</p>\n</blockquote>\n</aside>\n<p>I would also like to point out that I was a bit fooled by some of the descriptions of how Invert works:<br>\n<a href=\"https://imagejdocu.tudor.lu/gui/edit/invert\" class=\"inline-onebox\">Invert [ImageJ Documentation Wiki]</a> Indicates that the Brightness and Contrast settings are used, but this is not technically the case (I first tried adjusting the brightness and contrast before Inverting, and found no change).<br>\nAnd the official description is a bit vague: <a href=\"https://imagej.nih.gov/ij/docs/menus/edit.html#invert\" class=\"inline-onebox\">Edit Menu</a></p>\n<p>I appreciate that there may be other uses for this command where the current functionality is expected, and would welcome any feedback on why the current functionality is useful! However, I would also like the option for a command that works across the full bit depth of the image, so that a resulting set of <em>Invert</em>-ed images would be consistent.</p>\n<p><a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> <a class=\"mention\" href=\"/u/christian_tischer\">@Christian_Tischer</a> <a class=\"mention\" href=\"/u/schmiedc\">@schmiedc</a> <a class=\"mention\" href=\"/u/eliceiri\">@eliceiri</a></p>\n<p>Cheers,<br>\nMike</p>", "<p>Additional note, I ended up circumventing this by using:</p>\n<pre><code class=\"lang-auto\">run(\"32-bit\");\nrun(\"Multiply...\", \"value=-1\");\nrun(\"Add...\", \"value=65535\");\nrun(\"16-bit\");\n</code></pre>\n<p>Which I think ended up giving me the inverted result that I was expecting - so that <em>could</em> be an option for anyone looking for the behavior I was expecting. If your image is 16bit.</p>", "<p><a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> Definitely far too late to change the behavior in the original ImageJ. I\u2019m sure it would break tons of macros and scripts in the wild.</p>\n<p>I do want to give a heads up that we had similar discussions about how invert should work in ImageJ Ops; here are some that were written down:</p>\n<ul>\n<li><a href=\"https://github.com/imagej/imagej-ops/issues/507\" class=\"inline-onebox\">Improve InvertII \u00b7 Issue #507 \u00b7 imagej/imagej-ops \u00b7 GitHub</a></li>\n<li><a href=\"https://github.com/imagej/imagej-ops/issues/517\" class=\"inline-onebox\">Improve invert ii by gselzer \u00b7 Pull Request #517 \u00b7 imagej/imagej-ops \u00b7 GitHub</a></li>\n</ul>", "<aside class=\"quote no-group\" data-username=\"ctrueden\" data-post=\"3\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png\" class=\"avatar\"> ctrueden:</div>\n<blockquote>\n<p>I\u2019m sure it would break tons of macros and scripts in the wild.</p>\n</blockquote>\n</aside>\n<p>That is part of what I was worried about - there may be a lot of macros and scripts in the wild that are currently broken because users believe a certain thing is happening and it is not! Certainly everyone I have talked to so far (that did not already know the code) has been surprised by the current behavior.</p>\n<p>A disable-able pop-up, warning the user the first time they attempt to use the function, might even be a good option\u2026 especially if it pointed to a different version of Invert that has a uniform result <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<aside class=\"quote no-group\" data-username=\"Mike_Nelson\" data-post=\"4\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mike_nelson/40/26532_2.png\" class=\"avatar\"> Mike_Nelson:</div>\n<blockquote>\n<p>there may be a lot of macros and scripts in the wild that are currently broken because users believe a certain thing is happening and it is not!</p>\n</blockquote>\n</aside>\n<p>I agree with you here, the current behavior can be considered already \u201cbroken\u201d as it is not logical IMHO and it is difficult to understand how it works without examining the source code. I think the most logical way to do this would be to invert the full 16-bit range, and update the existing min/max values into the corresponding inverted min/max values of the inverted image. Then the visual result would be the same, and subsequent conversion to other bit-depths would give the same result. Ideally there could be an option in the settings somewhere (disabled by default) to activate the \u201cnew\u201d behavior, to avoid breaking existing macros in case they should rely on the \u201cold\u201d behavior.</p>", "<aside class=\"quote no-group\" data-username=\"Mike_Nelson\" data-post=\"4\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mike_nelson/40/26532_2.png\" class=\"avatar\"> Mike_Nelson:</div>\n<blockquote>\n<p>A disable-able pop-up, warning the user the first time they attempt to use the function, might even be a good option\u2026 especially if it pointed to a different version of Invert that has a uniform result</p>\n</blockquote>\n</aside>\n<p>Yeah, keeping \u201cbroken\u201d functionalities for backwards compatibility is an annoying issue <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=9\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\"></p>\n<p>I think above is a good suggestion. I thought something very similar (warning pop-up) was implemented in the <code>[ Apply ]</code> button in the Brightness &amp; Contrast adjustment (although I could not reproduce this right now in my Fiji).</p>", "<p>I would agree with that. It is unexpected behavior and inconsistent with the 8-bit inversion. Thus changing it would actually fix a bug and correct potential macros\u2026</p>\n<p>But maybe there is an important reasoning for having this that I have not in mind.</p>", "<p>Interesting, I never noticed this.<br>\nWhat would the right way to invert a 32bit image?  I suppose it is also using the min and max values.<br>\nI wonder if  the case for 16bits is such so it can invert 10, 12, etc bit images (which are all loaded in the 16 bit container) without generating out of range values?<br>\nI am sure <a class=\"mention\" href=\"/u/wayne\">@Wayne</a> should be able to clarify this.<br>\nMaybe it would be useful to have a invert16bit command that does the full range?</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"8\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p>10, 12, etc bit images</p>\n</blockquote>\n</aside>\n<p>This is certainly a valid concern. I don\u2019t see it too often, but certainly the default CZI files for the widefield system that I used were 14bit by default.</p>\n<p>Perhaps an <em>Invert</em> and an <em>Invert by bit depth</em>, where you can select the bit depth that the inversion will take place across- in case the bit depth has been lost during a previous step (metadata lost). I would not want to trust that the bit depth would be accessible in the metadata in all images.</p>", "<p>This could be maybe a reason. But for ImageJ the loaded 10, 12, 14 bit image would be just like any other 16-bit image. Thus the inverted image is a 16-bit. Even if you want to resave this then in the original bit-depth (which ImageJ by its default way could not do anyways as far as I know) then one would then need to go via rescaling\u2026 or are these somehow handled differently?</p>", "<p>The container is 16 bit, but the data is not.</p>", "<p>I looked it up the 32-bit images get processed the same way as the 16-bit images.</p>", "<p>Yes, but 16-bit is able to completely encompass the 12-bit range\u2026 It is not like it would rescale the data. It is just that the image in memory is larger than it actually needs to be. Values cannot overflow or would not be \u201cincorrect\u201d even after inverting (thus inverting the inversion would get the same result)\u2026Or is my logic entirely flawed\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/confused.png?v=9\" title=\":confused:\" class=\"emoji\" alt=\":confused:\"> I mean that is why one uses the 32-bit image for image calculations\u2026 to prevent overflowing\u2026</p>", "<p>After inverting a 10bit image, having a maximum value of 65535 does not make much sense in some circumstances. But I agree that using the min, when the data and the container start at 0 is not useful.</p>\n<p>See the menu that you get from the \u201cSet\u201d button in the B/C applet, where you can set the bit range for the display. I suppose that this could be used to set the range for the Invert command, but that does not seem to work even if you \u201cApply\u201d afterwards. Maybe that is what needs sorting out?</p>", "<p>Hm ok, true from a measurement perspective this could be confusing since such values could not occur in the original.</p>\n<p>I agree, this range setting in the \u201cSet\u201d sub menu would be the best choice for retrieving the bounding of the inversion rather than a more variable min and max gray values.</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"8\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p>What would the right way to invert a 32bit image?</p>\n</blockquote>\n</aside>\n<p>In my opinion that would be to simply multiply the values with -1.0, and update the new min/max range to match the corresponding old min/max range. The whole point with 32 bit data is that the values should represent a physical value without the need to rely on a density calibration, so an inversion should be the same as a negation. But I acknowledge that such a change might break existing workflows, so a new behaviour should be controllable by a setting.</p>\n<p>I also agree with you all that inversion should be based on the full data range (e.g. 0-65535 for 16 bit and 0-4095 for 12 bit; the \u201cunsigned 16-bit range\u201d setting should be the basis here. The min/max values is an important property and must be preserved, but these should be considered a display window only, and therefore be updated to correspond to the old range. The min/max values should not affect the data values except when they are used to set the range when converting to a different bit depth. I think the current behaviour when converting bit depth is the correct one: When reducing the bit depth, the new bit depth should range from the old min/max value range.</p>", "<p>Already back in 2012 (nine years ago!), Alfred Wagner <a href=\"http://imagej.1557.x6.nabble.com/8-16-32-bit-images-tp4902050.html\">questioned</a> the invert behavior of ImageJ on the mailing list:</p>\n<blockquote>\n<p>I\u2019m trying to understand the behavior of 8, 16, and 32 bit images.<br>\n[\u2026]<br>\nIs there a bug or am I missing something.</p>\n</blockquote>\n<p>\u2026 and got an <a href=\"http://imagej.1557.x6.nabble.com/8-16-32-bit-images-tp4902050p4902207.html\">informative reply</a> by <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> (highlighting and omissions by myself):</p>\n<aside class=\"quote no-group\" data-username=\"ctrueden\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png\" class=\"avatar\"> ctrueden:</div>\n<blockquote>\n<p>For 16-bit and 32-bit data, the Invert command will use the actual minimum<br>\nand maximum data values (rather than the full range of the pixel type) when<br>\ninverting.</p>\n<p>For 8-bit and RGB, invert always uses min=0 and max=255, regardless of the<br>\ndata values.</p>\n<p>There seems to be a special case for 32-bit only when min=max=0, where it<br>\nuses max=1.0 instead, presumably since normalized floating point data over<br>\n[0.0, 1.0] is so common.</p>\n<p>Relevant source code is at:</p>\n<p><a href=\"https://github.com/fiji/ImageJA/blob/20013d47/src/main/java/ij/process/FloatProcessor.java#L419\" class=\"inline-onebox\">ImageJA/FloatProcessor.java at 20013d472a0e4c54f30b8460d6b5483a71977b69 \u00b7 imagej/ImageJA \u00b7 GitHub</a><br>\n<a href=\"https://github.com/fiji/ImageJA/blob/20013d47/src/main/java/ij/process/ShortProcessor.java#L451\" class=\"inline-onebox\">ImageJA/ShortProcessor.java at 20013d472a0e4c54f30b8460d6b5483a71977b69 \u00b7 imagej/ImageJA \u00b7 GitHub</a><br>\n<a href=\"https://github.com/fiji/ImageJA/blob/20013d47/src/main/java/ij/process/ImageProcessor.java#L874\" class=\"inline-onebox\">ImageJA/ImageProcessor.java at 20013d472a0e4c54f30b8460d6b5483a71977b69 \u00b7 imagej/ImageJA \u00b7 GitHub</a></p>\n<p>[\u2026]</p>\n<p><strong>I am guessing all these behaviors are by design.</strong></p>\n</blockquote>\n</aside>\n<p>So I agree if it hasn\u2019t changed since then, there\u2019s little chance that the behavior can and will be \u201cfixed\u201d, since even if it seems unintuitive or wrong, some scripts in the wild might be <em>relying</em> on that behavior.</p>\n<hr>\n<p>As the GitHub discussions linked by Curtis above show, ImageJ2 makes an effort to be more in line with the behavior that can also be seen in other frameworks (see e.g. <a href=\"https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.invert\"><code>skimage.util.invert</code></a> in <a class=\"hashtag\" href=\"/tag/scikit-image\">#<span>scikit-image</span></a>).</p>\n<p>Here\u2019s a Groovy script using ImageJ Ops, that \u2013 if you save it e.g. in <code>Fiji.app/scripts/Edit/Invert_(ImageJ2).groovy</code> \u2013 will create a menu entry <em>Edit \u203a Invert (ImageJ2)</em> that shows the \u201cexpected\u201d behavior:</p>\n<pre><code class=\"lang-groovy\">#@ Dataset input\n#@ OpService ops\n\nresult = ops.create().img(input)\nops.image().invert(result, input)\n</code></pre>", "<p>Update to the ImageJ 1.53k16 daily build,  enable \u201cModern mode\u201d in the Edit&gt;Options&gt;Misc dialog and the Edit&gt;Invert command will work as expected. A future version will respect the \u201cUnsigned 16-bit range\u201d setting in the \u201cSet\u201d dialog of the Brightness/Contrast window.</p>", "<aside class=\"quote no-group\" data-username=\"Wayne\" data-post=\"18\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/w/0ea827/40.png\" class=\"avatar\"> Wayne:</div>\n<blockquote>\n<p>enable \u201cModern mode\u201d in the Edit&gt;Options&gt;Misc dialog and the Edit&gt;Invert command will work as expected</p>\n</blockquote>\n</aside>\n<p>Thanks, yes this works as expected. The new version makes a second inversion revert back to the original image values, which is very useful.</p>\n<p>In the future, I suggest you rename this option to something like \u201cuse 16-bit range for inversion\u201d and place this under the \u201cConversion Options\u201d dialog where I think it logically belongs.</p>", "<aside class=\"quote no-group\" data-username=\"steinr\" data-post=\"19\" data-topic=\"54659\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/aca169/40.png\" class=\"avatar\"> steinr:</div>\n<blockquote>\n<p>Thanks, yes this works as expected.</p>\n</blockquote>\n</aside>\n<p>Yes, thanks <a class=\"mention\" href=\"/u/wayne\">@wayne</a> for the quick fix!</p>\n<p>I suppose my main concern now is all of the users of the command who will not realize any changes have been made, or that the current command\u2019s behavior is not what they would expect. I do not have any great solutions for that, other than possibly updating the description here: <a href=\"https://imagej.nih.gov/ij/docs/menus/edit.html#invert\" class=\"inline-onebox\">Edit Menu</a></p>\n<p>Updating the documentation would not prevent new users from using <em>Invert</em> under the assumption that it works the way it does in <em>Modern mode</em>, however.</p>"], "71054": ["<p>Hi all, I have an image that I\u2019d like to analyze according to this paper (linked below). I took some pictures of cells with fluorescent mitochondria and I\u2019d like to classify mitochondria and quantify them. This paper has a macro listed that\u2019s said to aid in morphometry, but when it pastes into the new macro window, it pastes as one line and it\u2019s difficult to distinguish what\u2019s a comment and what\u2019s code, and I am completely new to making macros and programming in general. I\u2019d like to know how to use this macro since the paper did not provide that much instruction on how to implement it. I\u2019d also like to know if there are better macros/plugins for ImageJ/Fiji for mitochondrial quantification and morphological classification.<br>\n(<a href=\"https://link.springer.com/protocol/10.1007/978-1-4939-6890-9_2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Measuring Mitochondrial Shape with ImageJ | SpringerLink</a>)</p>\n<p>Thank you for any responses <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi Afrina,</p>\n<p>Yikes! I looked at the code from the link and it is a righful mess in terms of format.</p>\n<p>While I cannot assert as to wether it works or not, I have at least managed to format it correctly (I hope) so that you can try to run it at least\u2026 Hope this helps</p>\n<pre><code class=\"lang-auto\">var ch = 0; \n// channel to be analyzed for RGB images\n/** Measure mitochondrial morphology in the current selection\n * Ctrl+Shift+O closes current and opens next image*/\nmacro \"Morphometry [F7]\" {\n\ttitle = getTitle();\n\tmorphometry(title, false); \n\t// not batch mode\n}\n\n/** Batch-apply a set of \"named\" ROIs to analyze images with that file name*/\nmacro \"Batch Morphometry [F8]\" {\n\tdir = getDirectory(\"Select an image directory\");\n\twhile (roiManager(\"Count\") == 0) waitForUser(\"Please open named ROIs into ROI manager\");\n\tprevName = imgName = \"\";\n\tn = roiManager(\"Count\");\n\tfor (i = 0; i &lt; n; ++i) {\n\t\t// loop through the ROI Manager table\n\t\tprevName = imgName;\n\t\timgName = call(\"ij.plugin.frame.RoiManager.getName\", i);\n\t\tif (isOpen(imgName)) {\n\t\t\t// named image is open\n\t\t\tselectWindow(imgName);\n\t\t} else { \n\t\t\t// done with current image, close and open next\n\t\t\tif (isOpen(prevName)) {\n\t\t\t\tselectWindow(prevName);\n\t\t\t\tclose();\n\t\t\t}\n\t\t\topen(dir + imgName);\n\t\t}\n\t\troiManager(\"Select\", i);\n\t\tmorphometry(imgName, true); \n\t\t// batch mode\n\t}\n}\n\nfunction morphometry(title, batchMode) {\n\twhile (ch &lt; 1 || ch &gt; 3) {\n\t\t/* RGB channel not yet selected, initialize; reinstall macro to change channel */\n\t\tch = getNumber(\"Analyze RGB channel(1-3):\", 1);\n\t}\n\trun(\"Set Measurements...\", \"decimal=5 area perimeter fit\");\n\tprint(\"image\\t n\\t area2\\t area-weighted ff\\t form factor\\t aspect ratio\\t length\"); \n\t/* header for results table */\n\t\n\tif (bitDepth == 24) // RGB image\n\t\trun(\"Make Composite\");\n\tif (isOpen(\"Binary\")) {\n\t\tselectWindow(\"Binary\");close();\n\t} \n\t// close previous working image\n\tif (isOpen(\"Skeleton\")) {\n\t\tselectWindow(\"Skeleton\");close();\n\t} // close previous working image\n\tselectWindow(title);\n\tif (selectionType() == -1) // no selection\n\trun(\"Select All\");\n\tif (!batchMode) {\n\t\troiManager(\"Add\"); // save selection to ROI Manager for batch processing\n\t\tlast = roiManager(\"Count\") - 1;\n\t\troiManager(\"Select\", last);\n\t\troiManager(\"Rename\", title);\n\t\t/* roiManager(\"Save\", File.directory + \"named_ROIs.zip\"); */ \n\t\t/* un-comment to save ROIs automatically */\n\t}\n\t// copy selection to new window and clear outside\n\tsetSlice(ch); \n\t// ignored if grayscale\n\trun(\"Duplicate...\", \"title=Binary\");\n\trun(\"Make Inverse\");\n\tif (selectionType != -1) { \n\t\t// outside of ROI is selected\n\t\trun(\"Duplicate...\", \" \"); \n\t\t// make a mask of the background\n\t\trun(\"Convert to Mask\");\n\t\trun(\"Create Selection\");\n\t\trun(\"Make Inverse\");\n\t\troiManager(\"Add\");\n\t\tclose();\n\t\tn = roiManager(\"Count\");\n\t\troiManager(\"Select\", n - 1);\n\t\tgetRawStatistics(_area, backG); \n\t\t// mean is background\n\t\tsetColor(backG);\n\t\trun(\"Restore Selection\"); \n\t\t// fill outside of selection with background\n\t\tfill();\n\t\trun(\"Gaussian Blur...\", \"radius=64\"); \n\t\t// smooth abrupt background transition\n\t\troiManager(\"Delete\"); \n\t\t/* delete masking selection (ROI manager has cell selections) */\n\t}\n\trun(\"Select None\");\n\t// subtract background and threshold\n\trun(\"Subtract Background...\", \"rolling=50\"); \n\t/* non-destructive filter even if already applied */\n\trun(\"Make Binary\");\n\t// also try other threshold methods included with Fiji, e.g.: run(\"Auto Threshold\", \"method=Li white\");\n\t// create Results table of metrics, one line/particle\n\trun(\"Analyze Particles...\", \"size=9-Infinity circularity=0.00-1.00 show=Masks pixel clear\");\n\tawff = ff = ar = sum_a = a2 = len = 0;\n\tfor (i = 0; i &lt; nResults; i++) { \n\t\t// for every particle in table\n\t\ta = getResult(\"Area\", i);\n\t\tp = getResult(\"Perim.\", i);\n\t\tar += getResult(\"Major\", i) / getResult(\"Minor\", i);\n\t\t/* aspect ratio = length / width */\n\t\tsum_a += a;\n\t\ta2 += a * a;\n\t\t// area2 = a2 / (sum_a * sum_a)\n\t\tawff += b = (p * p) / (4 * 3.14159265358979); \n\t\t// awff = ff * (a / sum_area)\n\t\tff += b / a; \n\t\t// ff = p^2 / (4 * pi * a)}\n\t\tnParticles = nResults;\n\t\t// skeletonize to get length\n\t\tselectWindow(\"Mask of Binary\"); \n\t\t/* created by Analyze Particles .., excludes noise (&lt; 9 pixels) */\n\t\trename(\"Skeleton\");\n\t\trun(\"Skeletonize\");\n\t\trun(\"Analyze Particles...\", \"size=0-Infinity show=Nothing pixel clear\");\n\t\tfor (i = 0; i &lt; nResults; i++) len += getResult(\"Area\", i);\n\t\t// average and output\n\t\ta2 /= sum_a * sum_a;\n\t\tawff /= sum_a;\n\t\tff /= nParticles;\n\t\tar /= nParticles;\n\t\tlen /= nResults;\n\t\tprint(title + \"\\t \" + nParticles + \"\\t \" + a2 + \"\\t \" + awff + \"\\t \" + ff + \"\\t \" + ar + \"\\t \" + len);\n\t\tselectWindow(title);\n\t}\n}\n</code></pre>", "<p>Hi Oliver,</p>\n<p>It works, but it unfortunately doesn\u2019t give me the results that the paper advertised <img src=\"https://emoji.discourse-cdn.com/twitter/smiling_face_with_tear.png?v=12\" title=\":smiling_face_with_tear:\" class=\"emoji\" alt=\":smiling_face_with_tear:\" loading=\"lazy\" width=\"20\" height=\"20\">. I don\u2019t think this is an error on your part, I think that the paper has a lot of issues because I\u2019ve had to change the other macro listed in the paper as it provided bad results.</p>\n<p>Thank you so much anyway, you saved a lot of my time!</p>", "<p>Glad I could help, even if it was just to realize that the macro was of no use to you <img src=\"https://emoji.discourse-cdn.com/twitter/smiling_face_with_tear.png?v=12\" title=\":smiling_face_with_tear:\" class=\"emoji\" alt=\":smiling_face_with_tear:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>I have had success using MitoSegNet with their pretrained model, in case you want to downlaod and try it. All you need is 8 bit images of your mitochondria, to download a standalone program and the pretrained model file.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/MitoSegNet/MitoS-segmentation-tool\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/MitoSegNet/MitoS-segmentation-tool\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/565daf6527fcc5ac164b804e391d16b06f0bbf0d_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/565daf6527fcc5ac164b804e391d16b06f0bbf0d_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/565daf6527fcc5ac164b804e391d16b06f0bbf0d_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/565daf6527fcc5ac164b804e391d16b06f0bbf0d.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/6/565daf6527fcc5ac164b804e391d16b06f0bbf0d_2_10x10.png\"></div>\n\n<h3><a href=\"https://github.com/MitoSegNet/MitoS-segmentation-tool\" target=\"_blank\" rel=\"noopener\">GitHub - MitoSegNet/MitoS-segmentation-tool</a></h3>\n\n  <p>Contribute to MitoSegNet/MitoS-segmentation-tool development by creating an account on GitHub.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Maybe this helps</p>\n<p>Oli</p>", "<p>Hi Oliver,</p>\n<p>Now I am trying to use MitoSegNet with their pretrained model with 8 bit images of my mitochondria, and found your message about success with MitoSegNet.</p>\n<p>I run prediction in the Basic mode with Post-segmentation filtering, but after prediction had been done, two windows opened - one with original raw image and another one - blank white image with no prediction. Have you ever had this issue and what can cause it?</p>\n<p>I would be very grateful for any help, there are few publicly available examples of this software usage.</p>"], "77206": ["<p>Hello,</p>\n<p>I\u2019m currently trying to use ABBA to do registration with macaque slices and I would need to import my own atlas into ABBA from ImageJ. However I cannot find any way of doing so. Do you know how to do it or even if it is an available feature or not ?</p>\n<p>Thank you very much,</p>", "<p>Hi <a class=\"mention\" href=\"/u/ambistic\">@Ambistic</a> ,</p>\n<p>It\u2019s possible, but it\u2019s involved\u2026 In which form is your atlas currently ? Tiff file ?  Do you have an ontology ? In which form ? Json ? Is it an atlas you can/want to share or not ?</p>\n<p>If you want to stick to ABBA, I can try to help make it work in Java / Fiji (technically speaking, you need to implement the <a href=\"https://github.com/BIOP/ijp-atlas/blob/8b5a470b359d79a514fc148d8966187f5d287c8c/src/main/java/ch/epfl/biop/atlas/struct/Atlas.java\">ch.epfl.biop.atlas.struct.Atlas</a> interface.).</p>\n<p>An alternative, if the atlas can be shared, is to put the Atlas in Brainglobe (<a href=\"https://docs.brainglobe.info/bg-atlasapi/adding-a-new-atlas\" class=\"inline-onebox\">Adding a new atlas - brainglobe</a>) and then use ABBA-Python to work with it (<a href=\"https://github.com/NicoKiaru/ABBA-Python\" class=\"inline-onebox\">GitHub - NicoKiaru/ABBA-Python: How to use Aligning Big Brains and Atlases with Python</a>).</p>\n<p>Cheers,</p>\n<p>Nicolas</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>,</p>\n<p>My atlas is actually just a 3D MRI file, which is a DICOM but can be converted to any 3D format. I don\u2019t have any annotation in the atlas because I just need the transform functions.</p>\n<p>I don\u2019t think the Brainglobe option is the best for us because we need to build an atlas for each individual we have with its corresponding MRI due to the macaque biological variability.</p>\n<p>So what is the most straightforward way to create an atlas using the ch.epfl.biop.atlas.struct.Atlas interface you were mentioning ?</p>\n<p>Thank you,</p>", "<p>Hi <a class=\"mention\" href=\"/u/ambistic\">@Ambistic</a>,</p>\n<p>You can try to run this groovy script:</p>\n<aside class=\"onebox githubgist\" data-onebox-src=\"https://gist.github.com/NicoKiaru/6ceede088044b3878dae3f1fcd879d23\">\n  <header class=\"source\">\n\n      <a href=\"https://gist.github.com/NicoKiaru/6ceede088044b3878dae3f1fcd879d23\" target=\"_blank\" rel=\"noopener\">gist.github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://gist.github.com/NicoKiaru/6ceede088044b3878dae3f1fcd879d23\" target=\"_blank\" rel=\"noopener\">https://gist.github.com/NicoKiaru/6ceede088044b3878dae3f1fcd879d23</a></h4>\n\n  <h5>ImagePlusToAtlas.groovy</h5>\n  <pre><code class=\"Groovy\">#@ImagePlus structural_images\n\n#@ImagePlus label_image\n\n#@Double atlas_precision_mm\n\n#@ObjectService os\n\n\n</code></pre>\n    This file has been truncated. <a href=\"https://gist.github.com/NicoKiaru/6ceede088044b3878dae3f1fcd879d23\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n<p>\n</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>For this to work:</p>\n<ul>\n<li>open your image in ImageJ</li>\n<li>set the proper calibration, and sets the units to millimeter (Image&gt;Properties\u2026)</li>\n<li>duplicate your image and multiply it by zero (Process&gt;Math&gt;Multiply 0) (this will be the fake label image)</li>\n<li>run the groovy script, and set your original image to the structural image and the black image to label image. Specify the pixel size in millimeter.</li>\n</ul>\n<p>Then run ABBA, the created atlas should show up in the start  up menu.</p>\n<p>Here\u2019s a quick demo:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/88e46a528a1823b414058ab95dbd5b729e9f5578.gif\" alt=\"imageatlas\" data-base62-sha1=\"jx0jri4mtHXGxs0HKQne2jIc0Ry\" width=\"586\" height=\"499\" class=\"animated\"></p>\n<p>There are rough edges, but let me know how this goes.</p>\n<p>Cheers,</p>\n<p>Nicolas</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>,</p>\n<p>Thank you very much for your answer, the creation of the atlas is working fine !</p>\n<p>Just a precision for others :</p>\n<ul>\n<li>the file encoding needs to be 16-bits unsigned int (I had trouble with 32-bits float)</li>\n<li>the value range should not be too high (I had trouble because my MRI values were up to 40000, dividing by 100 solve the issue)</li>\n<li>also I had to save my processed file to tif on the disk (I didn\u2019t work if it was not saved on the disked but directly used)</li>\n<li>and finally I had to swap the axes of the MRI using Image / Stack / Reslice</li>\n</ul>\n<p>Have a nice day,</p>", "<p>Hi I had a somewhat similar question, but instead of a different atlas I just want to update the annotations. Specifically I have edited the labels of the ccf2017 atlas to divide a region into subregions, giving them unique id values. In order to use these labels in abba what alterations need to be made?</p>\n<p>I assume I need to update the ontology to include my new regions as children of the parent region. When setting the id value (and other parameters) is enough that it is unique or do they need to be inserted in order such that all subsequent ids are incremented? It would be easier if I can just make the ids (40,000, 40,001 \u2026) something far outside the range.<br>\ne.g. in the file \u201c1.json\u201d in ./abba_atlases</p>\n<pre><code class=\"lang-auto\">{\n                     \"id\": 1085,\n                     \"atlas_id\": 1125,\n                     \"ontology_id\": 1,\n                     \"acronym\": \"MOs6b\",\n                     \"name\": \"Secondary motor area, layer 6b\",\n                     \"color_hex_triplet\": \"1F9D5A\",\n                     \"graph_order\": 29,\n                     \"st_level\": 11,\n                     \"hemisphere_id\": 3,\n                     \"parent_structure_id\": 993,\n                     \"children\": [\n                        # proposed subdivisions\n                        {\"id\": 1086, \"atlas_id\":1126, ...}, \n                        {\"id\":1087, \"atlas_id\":1127, ...}, \n]\n                    }\n</code></pre>\n<p>As for the atlas images can I just \u201cswap out\u201d the existing labels for my own, update the region outlines (maybe unnecessary if only utilized for visualization), and recompile the images into a new .h5 with multiple resolutions? Then replace the atlas and ontologies located in the abba_atlases folder?</p>\n<p>Or is there a simpler way such as just exporting the transformations applied by abba and applying to my label image? If I can get this transformed image I can apply it to my section images to extract the regions in python.</p>", "<p><a class=\"mention\" href=\"/u/paschamber\">@paschamber</a> sorry I missed your post. Here\u2019s the other thread: <a href=\"https://forum.image.sc/t/customizing-atlas-labels-of-ccf2017-for-use-in-abba/78523\" class=\"inline-onebox\">Customizing atlas labels of ccf2017 for use in abba</a></p>"], "56794": ["<p>Dear All,</p>\n<p>Is anybody working on bringing TF 2.0 into Fiji\u2019s TensorFlow versions manager?<br>\nOr something very close to it?</p>\n<p>Bottom line: During the <a href=\"https://forum.image.sc/t/fiji-hackathon-2021-online/53391/16\">Fiji hackathon 2021</a> we came across the topic of using modern (that is, based on recent TensorFlow lib) DL algorithms \u201creachable\u201d from Fiji. Fiji has some support for it in its TensorFlow update site but it is only up to TF 1.15 (and corresponding old versions of Cuda), see Fiji \u2192 Edit \u2192 Options \u2192 TensorFlow\u2026 So, \u201cwe\u201d thought of updating it potentially. However, there is likely another \u201cwe\u201d who is working on it, or? I\u2019d be curious to know and would like to offer help (that is, manpower \u2192 developer).</p>", "<p>Hi Vladimir,</p>\n<p>we talked about this in the Jug Group before, but so far, nobody has really started working on it. I\u2019ll bring in up in next mondays group meeting and will come back to you with more information if we have any plans to actively work on that in the near future.</p>\n<p>What I can tell you is that we decided against an approach that only supports TF2.0. Instead we looked into a Fiji interface to <a href=\"https://djl.ai/\" rel=\"noopener nofollow ugc\">Deep Java Library (by Amazon)</a> or <a href=\"https://onnx.ai/\" rel=\"noopener nofollow ugc\">ONNX (by Microsoft)</a> to also support PyTorch, ONNX and other frameworks, DJL being the most promising at this point in time.</p>", "<p><a class=\"mention\" href=\"/u/eliceiri\">@eliceiri</a> Is assisting with a framework-agnostic machine learning layer for ImageJ something that might be in scope for <a class=\"mention\" href=\"/u/elevans\">@elevans</a>\u2019s project? It would be substantial effort, but could help unify these efforts, at least on the ImageJ side.</p>\n<p><a class=\"mention\" href=\"/u/tomburke-rse\">@tomburke-rse</a> Is there a similar idea for Python, and/or for other languages? Or does everyone in Python just use their favorite framework and not worry about portability there?</p>", "<p>Not that I know of, no. ONNX is at least an framework agnostic format that also works in python.</p>\n<p>We never talked about any solutions for that in our group though, so there might be something out there.</p>", "<p>I spoke with <a class=\"mention\" href=\"/u/eliceiri\">@eliceiri</a> offline, and we are planning to discuss further with <a class=\"mention\" href=\"/u/elevans\">@elevans</a> and <a class=\"mention\" href=\"/u/hinerm\">@hinerm</a> about this topic. We would love to help achieve TF2 and/or ML-framework-agnostic support in ImageJ/Fiji, but we are not sure yet what our group\u2019s role might be here. As always, I can offer advice and support as part of my ImageJ/Fiji maintenance role, but we may have too much else on our plate to take the lead on this.</p>\n<p><a class=\"mention\" href=\"/u/ulman\">@ulman</a> Were you suggesting that you or a colleague might be able to make an initial attempt to add TF2 support to the ImageJ-TensorFlow project?</p>", "<p>If I ignore the \u2018versions manager\u2019 bit of the question, some of what we are doing in QuPath might be relevant.</p>\n<p>A while back I extracted our TensorFlow code to its own repository and updated to use TensorFlow 2. Our extension is <a href=\"https://github.com/qupath/qupath-extension-tensorflow\">here</a>.</p>\n<p>However, our intention is that TensorFlow should be one option among several; our main commitment is to <a href=\"https://github.com/bytedeco/javacpp\">JavaCPP</a>. For anyone unfamiliar with JavaCPP, I like the creator\u2019s phrase <a href=\"https://github.com/bytedeco/javacpp/issues/468#issuecomment-834904701\">\u201cthe anaconda of Java\u201d</a> to explain it.</p>\n<p>JavaCPP is already used by the (somewhat) \u2018official\u2019 <a href=\"https://github.com/tensorflow/java\">TensorFlow Java</a>, therefore you\u2019ll almost certainly have to use it anyway of adding TensorFlow 2 support in Java.  However, JavaCPP Presets provides <a href=\"https://github.com/bytedeco/javacpp-presets\"><em>lot</em> more</a> (including OpenCV, ONNX, PyTorch, mxnet\u2026 and CUDA-redist). Deeplearning4J is also <a href=\"https://deeplearning4j.konduit.ai/multi-project/how-to-guides/developer-docs/javacpp\">heavily dependent on JavaCPP</a>.</p>\n<p>Because QuPath already uses OpenCV via JavaCPP, the first practical implication for us is that our next release will support both TensorFlow saved models as well as anything that <a href=\"https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\">OpenCV\u2019s DNN module</a> can support when using StarDist, as described <a href=\"https://github.com/qupath/qupath-extension-stardist\">here</a>. This includes at least partial support for ONNX and a few other formats. I find OpenCV\u2019s DNN performance to be OK; not as fast as TensorFlow on the CPU, but it supports different backends (including CUDA) and ease of distribution is a big plus.</p>\n<p>Although StarDist is the only \u2018supported\u2019 use of deep learning in QuPath, the underlying design is much more general. The two main longer-term goals are to:</p>\n<ol>\n<li>Make any deep learning models interchangeable with QuPath\u2019s conventional machine learning pixel and object classifiers, so that everything else downstream (creating objects, making measurements) is immediately available \u2018for free\u2019</li>\n<li>Make it \u2018easy\u2019 to add support for any other deep learning framework via extensions/plugins, which should involve writing minimal glue code (our current minimal interface is <a href=\"https://github.com/qupath/qupath/blob/4eb74946602d8df605402909c753fca027fa3c26/qupath-core-processing/src/main/java/qupath/opencv/dnn/DnnModel.java\">here</a>).</li>\n</ol>\n<p>Both of these things will already kind of work in QuPath v0.3.0 (coming very soon!), but probably remain too hack-y for wider use until we have a better API. Addressing that will be the main focus of QuPath v0.4.</p>\n<p>The <strong>relevance to Fiji</strong> is:</p>\n<ol>\n<li>I think JavaCPP is worth a look, because it\n<ul>\n<li>Gives a standard way to access many interesting libraries</li>\n<li>Includes dependencies for all platforms (except Apple Silicon for now), both with/without GPU support</li>\n<li>Has <a href=\"http://bytedeco.org/javacpp/apidocs/org/bytedeco/javacpp/PointerScope.html\">PointerScope</a>, which can greatly help with managing pointers using try-with-resources blocks.</li>\n</ul>\n</li>\n<li>I\u2019d ultimately like to extract all QuPath\u2019s classification stuff into a separate library with minimal dependencies that can be used from both QuPath and Fiji. \u2018Minimal dependencies\u2019 might be JavaCPP, ImgLib2 and (if we can\u2019t figure out how to avoid it) OpenCV.</li>\n</ol>\n<p>I\u2019d be very happy to chat about working on this with others.</p>\n<p>Finally, a couple of other notes:</p>\n<ol>\n<li>I frequently encounter problems when converting models between formats, such as layers not being supported. This means that, while ONNX sounds great (and probably is great for many uses), relying on it exclusively could end up frustrating people who use more framework-specific functionality. So I think the <em>\u2018ML-framework-agnostic support\u2019</em> idea would be best.</li>\n<li>Much as I\u2019ve come to like JavaCPP, <em>sometimes</em> the API of a JavaCPP preset is a lot more complicated than the corresponding \u2018official\u2019 Java API. For example, I can\u2019t make much sense of JavaCPP\u2019s <a href=\"https://github.com/bytedeco/javacpp-presets/tree/master/pytorch\">PyTorch</a>; the <a href=\"https://pytorch.org/javadoc/1.9.0/\">the \u2018official\u2019 PyTorch javadocs</a> are a great deal simpler. But it looks to me like the official PyTorch would bring with it many more installation and path headaches.</li>\n</ol>", "<p>I can really recommend ONNX for the inference part. Easy, fast and you can control memory usage much better than with TF.<br>\nThis is why we decided to use it as our standard (for inference) for our <a href=\"https://pypi.org/project/czmodel/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">czmodel \u00b7 PyPI</a> package and in ZEN as well.</p>", "<p>Guys, thank you so much for your replies so far.</p>\n<p>To explain my standpoint. We (my group at IT4I in CZE) aim for bringing distributed (and ideally also parallel) computing into Fiji: You start with own code for your task, a normal (thus serial) IJM (or anything better); you enable some update site(s); a new wizard dialog helps you modify (sort of \u201cannotate parallelize-able blocks\u201d) your code that you then give to a \u201cHPC workflow manager\u201d (might change its name later) that takes care of the execution. We want further focus on tasks definitions and image data cherry-picking (the DataStore) etc. \u2013 that\u2019s our core business. However, when testing it with DeepImageJ (that we commanded from IJM), we bumped into available vs. required libs issue\u2026 so we have to deal with it, either a hacky (and short-lived) way, or with \u201csomething better\u201d. Since the \u201csomething better\u201d is minor topic to us and we\u2019re anyway no DL experts, I basically scan the community with this thread and offer some manpower to help the DL-experienced others to accelerate their progress so everybody+we could use it soon (totally naive, I know, still\u2026).</p>\n<hr>\n<p>To summarize what I see for the moment (not closing the discussion!). We want (obviously\u2026) to support more than TF alone. There are frameworks that seems to be advanced in doing that. I think I see two strong candidates here: ONNX and JavaCPP.</p>\n<p>(only skimmed fast over it)</p>\n<p>ONNX: Supported by a big company (\u2026could live for a while), C++ under the hood, with (main?) API in Python. But, isn\u2019t it mostly a \u201ccontainer\u201d to store models and architectures that then can be converted to be executed using some particular backend (e.g. TF)? Is there a chance that the current (and wanted) networks (e.g. StarDist, CellPose, some U-nets) will be translated into this?</p>\n<p>JavaCPP: Sort of a small-community project (me a bit worried about its future), primarily any-C+\u00b1to-Java \u201cbinder/converter\u201d, but it now pulls in many packages (incl. ONNX) in its \u201cpresets\u201d package. But, isn\u2019t it really \u201conly\u201d exposing foreign-language API into Java?</p>\n<p>An outsider\u2019s observation: The home of deep learning development is clearly centered around Python, I see no good reason to move it into or mirror/duplicate it into some other language (e.g. Java/Fiji). In my eyes, we seek only \u201cFiji connection-bridge to outsource the jobs (inference predominantly, but also training, both as a black box) onto some decent framework\u201d.</p>\n<p>Aren\u2019t we actually in a need of defining a simple, fixed, agreed interface in Python that network developers\u2019 solutions would be instantiating, and that Fiji/anyone would be therefore able to operate on? Net-developers could then use anything they wish underneath, could package and ship their Python envs, etc. The interface could be like: here I provide source image, this executes the network on it, here I read back the resulting image, here I read back the resulting 1D vector; and this collection of images (or collection of image references\u2026 file names?) shall be used for training\u2026; no network-specific details here, these be \u201chardcoded\u201d downstream (why not have multiple instances).</p>\n<p>(I probably have a naive Friday, right?)</p>\n<hr>\n<blockquote>\n<p>I\u2019d be very happy to chat about working on this with others.</p>\n</blockquote>\n<p>Maybe we should have a mini Zoom conference about all this\u2026 share experience, bad/good experience with this/that, warn about caveats, strong opinions\u2026 you know, a few hours max.</p>", "<aside class=\"quote no-group\" data-username=\"ctrueden\" data-post=\"5\" data-topic=\"56794\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/ctrueden/40/13505_2.png\" class=\"avatar\"> ctrueden:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/ulman\">@ulman</a> Were you suggesting that you or a colleague might be able to make an initial attempt to add TF2 support to the ImageJ-TensorFlow project?</p>\n</blockquote>\n</aside>\n<p>Actually, yes. I see realistic opportunity to have a developer working on this for a few months (ATM, not beyond end of 06/2022).</p>", "<p>Hi, thanks for brining up the topic. We discussed this briefly during the <a href=\"https://bioimage.io\">BioImage Model Zoo</a> weekly meeting today. I will write down some notes plus my own opinion here.</p>\n<ul>\n<li>\n<p>We investigated the ONNX solution during a hackathon a few month ago, what we concluded is the current ONNX is not ready yet. Mostly due to it has already many so-called opset versions that are not backward compatible, and many critical ops are missing even with the latest opset version. Perhaps it make sense if you have very fixed network architecture that you know the conversion will work, it is too easy to hit conversion errors for most of the models.</p>\n</li>\n<li>\n<p>For Fiji/ImageJ, we think the most pragmatic and sustainable solution is to take advantage of the bridge between python and imagej (i.e. pyimagej).</p>\n<ul>\n<li>As <a class=\"mention\" href=\"/u/ulman\">@ulman</a> also mentioned, it is a much cheaper solution and will likely sustain in long-term. Considering that you can already implement plugins and commands etc. in Python then register to Fiji/ImageJ (see an <a href=\"https://forum.image.sc/t/defining-a-scijava-command-in-python-with-pyimagej/56515\">example</a> by <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a> ) and the imagej data types can be easily converted to numpy and tensors, we don\u2019t really need much work to support the vast majority deep learning frameworks in Python.</li>\n<li>Plus, the CPython executable itself is only around 30MB, we can basically make a pyimagej based Fiji launcher and ship CPython. (As I understand, <a class=\"mention\" href=\"/u/eliceiri\">@eliceiri</a> 's team is working along this direction already)</li>\n</ul>\n</li>\n<li>\n<p>Regarding the JavaCPP solution <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>, I think it is really great that JavaCPP can support lots of libraries. It totally make sense for QuPath to use that since the use case is more concrete. Compared to distributing Python+dependencies, it is also much easier to distribute statically compiled libraries with JavaCPP. However, that means we have to write all the \u201cglue\u201d code in Java with JavaCPP. With the Python bridge, we can reuse all the glue code which are already written in Python.</p>\n</li>\n</ul>\n<p>Overall, I think the PyImageJ is more suitable for porting cutting edge stuff to work with ImageJ (e.g. with Jupyter/Colab notebooks etc.), but we will need to solve the dependency management with PyPI/Conda for distribution. If we don\u2019t mind the package size, we can also package conda virtual environment (it seems ilastik is doing that) or go for Docker/BinderHub(espeicially for cluster environments). In fact, within the <a href=\"https://imjoy.io\">ImJoy</a> project, we have been working on this direction and potentially we can support the distribution of PyImageJ based deep learning tools. If you want something more portable, then JavaCPP seems to be better, but it will require more work on porting glue code to Java.</p>\n<p>One question though, <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> since you mentioned JavaCPP is the \u201cthe anaconda of java\u201d, is there a chance to solve the issue of version management? E.g. switch between tensorflow version, or we might have plugin A requires tensorflow 1.15 and plugin B requires tensorflow 2.3.</p>", "<aside class=\"quote no-group\" data-username=\"oeway\" data-post=\"10\" data-topic=\"56794\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/oeway/40/5860_2.png\" class=\"avatar\"> oeway:</div>\n<blockquote>\n<p>One question though, <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> since you mentioned JavaCPP is the \u201cthe anaconda of java\u201d, is there a chance to solve the issue of version management? E.g. switch between tensorflow version, or we might have plugin A requires tensorflow 1.15 and plugin B requires tensorflow 2.3.</p>\n</blockquote>\n</aside>\n<p>As far as I understand it, if you decide on a <a href=\"https://github.com/bytedeco/javacpp\">JavaCPP</a> version (e.g. currently v1.5.6) then there are compatible versions of <a href=\"https://github.com/bytedeco/javacpp-presets\">all the other presets</a> associated with that. You can identify them on Maven central because they have the JavaCPP version appended at the end, e.g. <a href=\"https://search.maven.org/artifact/org.bytedeco/opencv/4.5.3-1.5.6/jar\">opencv-4.5.3-1.5.6</a> (TensorFlow Java is a bit different because it\u2019s developed separately on a different repository, but still with a specific JavaCPP version).</p>\n<p>But I don\u2019t think you can (reliably) mix and match between different versions for individual presets. This would mean the version of OpenCV, TensorFlow, PyTorch, ONNX etc. would be determined by the single currently-supported JavaCPP version.</p>\n<p>I haven\u2019t personally encountered problems running any TensorFlow 1.x stuff in TensorFlow 2.x (although I haven\u2019t tried a lot); the <a href=\"https://www.tensorflow.org/guide/versions#compatibility_of_graphs_and_checkpoints\">TensorFlow version compatibility documentation is good</a>.</p>\n<aside class=\"quote no-group\" data-username=\"oeway\" data-post=\"10\" data-topic=\"56794\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/oeway/40/5860_2.png\" class=\"avatar\"> oeway:</div>\n<blockquote>\n<p>Plus, the CPython executable itself is only around 30MB</p>\n</blockquote>\n</aside>\n<p>Yes, JavaCPP <a href=\"https://github.com/bytedeco/javacpp-presets/tree/master/cpython\">has that too</a> <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> Thanks for the pointers! I think this a really interesting approach indeed. And it is very much like <a href=\"https://github.com/pyodide/pyodide\">Pyodide</a> which brings python scientific libraries to run in the browser via Webassembly. In Pyodide, it\u2019s basically the same, you need to compile C/C++ based python modules, but you can directly install all the pure python packages via pip. I did a quick search and with JavaCPP, you can actually call <code>pip install</code> (see <a href=\"https://github.com/bytedeco/javacpp-presets/issues/1015#issuecomment-792129891\">here</a>).</p>\n<p>If this works, it will be a very neat way of integrating CPython(+many crucial scientific libraries) to ImageJ. It will be more portable than shipping the executable of CPython and the exchange between CPython and Java will be more efficient. The only thing missing I think is the interoperability layer of data types between CPython/Numpy and ImageJ (e.g. numpy array &lt;==&gt; ImagePlus/ImageJ2 Dataset etc.). Ideally we should be able to install <code>pyimagej</code> but I doubt it can work because it depends on <code>jpype1</code> which is not available in JavaCPP.</p>\n<p><a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> Any thoughts on this? Since now it\u2019s all in java, it should be much easier to share memory between numpy and imagej2 Dataset, right?</p>", "<p>We also evaluated ONNX and despite being not perfect yet out approach for production code (Zen and APEER) is:</p>\n<ul>\n<li>train with whatever you like in Python</li>\n<li>export to ONNX, which is improving very fast and use it NET-runtime or Python</li>\n<li>extend the <a href=\"https://pypi.org/project/czmodel/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">czmodel \u00b7 PyPI</a> package and specificatuon to also support exporting PyTorch \u2192 ONNX</li>\n</ul>\n<p>But we are happy to adapt and modify our approach.</p>\n<p>It would be interesting to learn what specific disadvantages ONNX still has from our perspective.</p>\n<p>Cheers, <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a></p>", "<p>Any progress on this? It\u2019s becoming increasingly annoying to keep supporting TensorFlow 1 in Python.</p>", "<p>Is anyone still working on this? <a class=\"mention\" href=\"/u/ulman\">@ulman</a>, <a class=\"mention\" href=\"/u/tomburke-rse\">@tomburke-rse</a>, <a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a><br>\nEven a quick \u201cyes\u201d or \u201cno\u201d would be appreciated.</p>\n<p>Thanks,<br>\nUwe</p>", "<p>Yes. And also all the other nice frameworks too. Still needs more time though. <a class=\"mention\" href=\"/u/carlosuc3m\">@carlosuc3m</a> is working on something pretty amazing and I plan to bring that into Fiji.</p>", "<aside class=\"quote no-group\" data-username=\"tomburke-rse\" data-post=\"16\" data-topic=\"56794\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tomburke-rse/40/38478_2.png\" class=\"avatar\"> Tom Burke:</div>\n<blockquote>\n<p>And also all the other nice frameworks too. Still needs more time though. <a class=\"mention\" href=\"/u/carlosuc3m\">@carlosuc3m</a> is working on something pretty amazing and I plan to bring that into Fiji.</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/tomburke-rse\">@tomburke-rse</a> What is the plan for <a href=\"https://github.com/imagej/imagej-tensorflow/\">ImageJ-TensorFlow</a> and CSBDeep with respect to that? Are you and <a class=\"mention\" href=\"/u/carlosuc3m\">@carlosuc3m</a> going to update it? Or will it stay stuck at TF1.x? I could potentially allocate some programmer resources at LOCI to help, but more details here would be nice.</p>", "<p><a class=\"mention\" href=\"/u/ctrueden\">@ctrueden</a> my plan is to update ImageJ-TensorFlow, ImageJ-Modelzoo and everything building on top of that is in the juggroup scope whereever necessary. I\u2019m not sure yet if ImageJ-TensorFlow is even needed anymore after that (currently I think not).<br>\nHelp would be appreciated, but more details have to wait until <a class=\"mention\" href=\"/u/carlosuc3m\">@carlosuc3m</a> work is made public.</p>", "<p><a class=\"mention\" href=\"/u/tomburke-rse\">@tomburke-rse</a> Sounds good, keep us posted and good luck!</p>", "<p>Thanks for letting us know, <a class=\"mention\" href=\"/u/tomburke-rse\">@tomburke-rse</a>! Is there a rough timeline for that? E.g. planned to make an initial release this year, or next year?</p>"], "77285": ["<p>We now have the preprint of QUAREP \u201cCommunity-developed checklists for publishing images and image analysis\u201d online on Feb 14th, 2023.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://arxiv.org/abs/2302.07005\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/f/9faebd14d324873b179990cefa192fb2dd187f75.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://arxiv.org/abs/2302.07005\" target=\"_blank\" rel=\"noopener\">arXiv.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://arxiv.org/abs/2302.07005\" target=\"_blank\" rel=\"noopener\">Community-developed checklists for publishing images and image analysis</a></h3>\n\n  <p>Images document scientific discoveries and are prevalent in modern biomedical\nresearch. Microscopy imaging in particular is currently undergoing rapid\ntechnological advancements. However for scientists wishing to publish the\nobtained images and image...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I started this thread for feedback on this preprint. This has been a huge community effort led by <a class=\"mention\" href=\"/u/helena_klara_jambor\">@Helena_Klara_Jambor</a> &amp; <a class=\"mention\" href=\"/u/schmiedc\">@schmiedc</a></p>\n<aside class=\"onebox twitterstatus\" data-onebox-src=\"https://twitter.com/helenajambor/status/1625756848212135937\">\n  <header class=\"source\">\n\n      <a href=\"https://twitter.com/helenajambor/status/1625756848212135937\" target=\"_blank\" rel=\"noopener\">twitter.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/92496d8ee03607ca7f741e307ee1315ab0cb8786.jpeg\" class=\"thumbnail onebox-avatar\" width=\"400\" height=\"400\">\n<h4><a href=\"https://twitter.com/helenajambor/status/1625756848212135937\" target=\"_blank\" rel=\"noopener\">HelenaKJambor</a></h4>\n<div class=\"twitter-screen-name\"><a href=\"https://twitter.com/helenajambor/status/1625756848212135937\" target=\"_blank\" rel=\"noopener\">@helenajambor</a></div>\n\n<div class=\"tweet\">\n  <span class=\"tweet-description\">Roses are red,\nDAPI is blue,\nScale bars are sweet,\nHappy Imaging for you!\n\nWe \u2764\ufe0fmicroscopy! Use our checklists to publish understandable and accessible images and image analyses, out Feb 14: <a target=\"_blank\" href=\"http://arxiv.org/abs/2302.07005\" rel=\"noopener\">arxiv.org/abs/2302.07005</a>\n<a href=\"https://twitter.com/search?q=%23Bioimaging\" target=\"_blank\" rel=\"noopener\">#Bioimaging</a> <a href=\"https://twitter.com/search?q=%23Images\" target=\"_blank\" rel=\"noopener\">#Images</a> <a href=\"https://twitter.com/search?q=%23ScienceCommunication\" target=\"_blank\" rel=\"noopener\">#ScienceCommunication</a> <a href=\"https://twitter.com/search?q=%23phdlife\" target=\"_blank\" rel=\"noopener\">#phdlife</a> <a href=\"https://t.co/SswDK83ZPD\" target=\"_blank\" rel=\"noopener\">https://t.co/SswDK83ZPD</a><div class=\"aspect-image-full-size\" style=\"--aspect-ratio:690/402;\"><img class=\"tweet-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/e/6efdb477b48ad3a955f38f3fad0091802933ea28_2_690x402.jpeg\" width=\"690\" height=\"402\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/e/6efdb477b48ad3a955f38f3fad0091802933ea28_2_690x402.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/e/6efdb477b48ad3a955f38f3fad0091802933ea28_2_1035x603.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/e/6efdb477b48ad3a955f38f3fad0091802933ea28.jpeg 2x\" data-dominant-color=\"EAEAEA\"></div></span>\n</div>\n\n<div class=\"date\">\n  <a href=\"https://twitter.com/helenajambor/status/1625756848212135937\" class=\"timestamp\" target=\"_blank\" rel=\"noopener\">7:20 AM - 15 Feb 2023</a>\n\n    <span class=\"like\">\n      <svg viewbox=\"0 0 512 512\" width=\"14px\" height=\"16px\" aria-hidden=\"true\">\n        <path d=\"M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z\"></path>\n      </svg>\n      35\n    </span>\n\n    <span class=\"retweet\">\n      <svg viewbox=\"0 0 640 512\" width=\"14px\" height=\"16px\" aria-hidden=\"true\">\n        <path d=\"M629.657 343.598L528.971 444.284c-9.373 9.372-24.568 9.372-33.941 0L394.343 343.598c-9.373-9.373-9.373-24.569 0-33.941l10.823-10.823c9.562-9.562 25.133-9.34 34.419.492L480 342.118V160H292.451a24.005 24.005 0 0 1-16.971-7.029l-16-16C244.361 121.851 255.069 96 276.451 96H520c13.255 0 24 10.745 24 24v222.118l40.416-42.792c9.285-9.831 24.856-10.054 34.419-.492l10.823 10.823c9.372 9.372 9.372 24.569-.001 33.941zm-265.138 15.431A23.999 23.999 0 0 0 347.548 352H160V169.881l40.416 42.792c9.286 9.831 24.856 10.054 34.419.491l10.822-10.822c9.373-9.373 9.373-24.569 0-33.941L144.971 67.716c-9.373-9.373-24.569-9.373-33.941 0L10.343 168.402c-9.373 9.373-9.373 24.569 0 33.941l10.822 10.822c9.562 9.562 25.133 9.34 34.419-.491L96 169.881V392c0 13.255 10.745 24 24 24h243.549c21.382 0 32.09-25.851 16.971-40.971l-16.001-16z\"></path>\n      </svg>\n      18\n    </span>\n</div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Note: the checklist is the \u201cminimal\u201d of what can be included as we agreed to draw a bottom line after two years of discussion. We can of course think about more details in various imaging modalities and analysis methods, but that would become a huge volume like an encyclopedia and not appropriate as a quick and practical checklist for publication.</p>", "<p>Hi <a class=\"mention\" href=\"/u/kota\">@Kota</a></p>\n<p>I think you should add a short section on deconvolution and point spread functions.  This could be integrated into the workflow section.   Of course you probably want to get feedback from <a class=\"mention\" href=\"/u/svi_huygens\">@SVI_Huygens</a> on this (I saw they are part of your group and have much more experience then me on general recommendations for Deconvolution, my experience in the last 10 years is on small niche applications).</p>\n<p>Nevertheless my recommendations would be:</p>\n<p>Any workflow utilizing deconvolution should at a minimum provide the Point Spread Function used as an image file (if measured or \u2018adaptive\u2019).  Or a description of the algorithm used to generate the point spread function if theoretical.    This is more important than any of the exact settings (such as iterations, or noise regularization), but of course these settings would be nice to have to.  The PSF though is by far the the most important part of deconvolution, but most people don\u2019t think to provide it.</p>\n<p>The recommended would be to output the point spread function as a file even if theoretical.  I came across a case last year, in this <a href=\"https://sites.google.com/view/lmrg-image-analysis-study\">ABRF study</a> where the point spread function used to generate the simulation was wrong, and it took months to figure out what happened, this error would have been trivial to spot if the PSF had been provided.</p>\n<p>Ideal would be to provide the bead image used for calibrating the PSF.</p>\n<p>In general people should not use \u2018blind\u2019 deconvolution for publication as it lacks scientific validation and convergence stability (ie essentially a different PSF is used through the process, and PSF and image lack co-convergence).  However other adaptive techniques (such as spherical aberration detection)  are OK.  As long as the PSF generated from the technique is output.</p>\n<p>Brian</p>", "<p>Hi <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a> ,</p>\n<p>Thanks a lot for providing your expertise in this!</p>\n<p>We found out that for every image processing and analysis technique, there are certain requirements for properly writing down the methods. Besides deconvolution, what about colocalization analysis? What about FRAP?   SMLM analysis? \u2026 the list will be huge, even if we limit our scope to well-known image analysis techniques. We can probably compile a thick guideline book by the united effort of all experts to cover them all - but now, if that happens, would every author, editor, and reviewer open that book for each paper submission to check those requirements? We doubted. We indeed better have such detailed guidelines, but not as a checklist in the back of journals.</p>\n<p>For this reason, and to be more practical, we decided to go on a different way: make a precise recommendation on how to write the method in a way that other people can examine the workflow in detail - Detailed enough so that other people can be convinced or test by themselves, that the method is OK.</p>\n<p>This of course does not exclude further effort by the community of each expert domain to make a checklist, for example with deconvolution. But as I explained, domain expertise knowledge will not be added to this paper, in order to avoid becoming too long, which might erode the usability of this generic checklist.</p>", "<p>Hi <a class=\"mention\" href=\"/u/kota\">@Kota</a></p>\n<p>Sorry for jumping in without thinking through everything you guys must have already talked about.  I can see that every algorithm will have different requirements.  So  there is no need to address specific deconvolution parameters (like iterations, noise parameters etc.) or address niche deconvolution requirements, which yes, I am sure everyone has their own list of what is important to them, and including it all would be counter productive.</p>\n<p>However methods like colocalization and SMLM analysis also often use the PSF of the system.</p>\n<p>Should a paper that uses a PSF based algorithm be encouraged to include the PSF, just as a machine learning algorithm should include the training set and model?    I would be curious to hear what SMLM and colocalization users think.  If they have to replicate a workflow do they need the PSF?</p>\n<p>What about other algorithms that use bead or calibration datasets, for example to detect chromatic shift?   Should it be recommended that such calibration datasets are included?  I would say yes.</p>\n<p>Brian</p>", "<aside class=\"quote no-group\" data-username=\"bnorthan\" data-post=\"4\" data-topic=\"77285\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bnorthan/40/71_2.png\" class=\"avatar\"> Brian Northan:</div>\n<blockquote>\n<p>Should a paper that uses a PSF based algorithm be encouraged to include the PSF, just as a machine learning algorithm should include the training set and model?</p>\n</blockquote>\n</aside>\n<p>I would say absolutely. It also would not be within the scope of the paper. We wanted a list that could \u201cget the ball rolling\u201d rather than something comprehensive that would scare most users off. We are also building this list along with other more comprehensive lists from other working groups that will be making an appearance in the near future - they are more \u201cmicroscopy methods\u201d oriented, rather than \u201cimage figure\u201d oriented. You may be interested in <a href=\"https://quarep.org/working-groups/wg-11-microscopy-publication-standards/\">checking out WG11, for instance, as that group is dedicated to building a better microscopy methods section,</a> with the end goal of reproducibility. Though, the first foray will definitely not truly attempt to achieve this, for some of the same reasons listed above by <a class=\"mention\" href=\"/u/kota\">@Kota</a>.</p>\n<p>Most of these initial papers are also focused on the rather more standard methods of widefield and confocal microscopy, with the hopes that added papers may be published with specific details for modalities like SMLM.</p>\n<p>If the lists are too long or too complicated, even if it would be ideal for science, they will not be used by the biologists. Once more people are including the basics, and more companies are making it easier to report further details of the instruments, we can push for more.</p>", "<p>Hi <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a></p>\n<aside class=\"quote no-group\" data-username=\"bnorthan\" data-post=\"4\" data-topic=\"77285\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bnorthan/40/71_2.png\" class=\"avatar\"> Brian Northan:</div>\n<blockquote>\n<p>Should a paper that uses a PSF based algorithm be encouraged to include the PSF, just as a machine learning algorithm should include the training set and model? I would be curious to hear what SMLM and colocalization users think. If they have to replicate a workflow do they need the PSF?</p>\n</blockquote>\n</aside>\n<p>I agree with this and would say, in my personal opinion, that the \"Machin Learning Workflow \"checklist can be dropped from this paper.  As long as ** the reproducibility of the method** (not the reproducibility of results) is secured, it\u2019s flexible enough to cover all including machine learning workflow, but without specifics to each modality, to allow fair examination of methods e.g. reviewing. The training set and the model can become a part of \u201ccomponents and platform\u201d or \u201cexample data and code\u201d. It\u2019s just that it is quite a new way to use machine learning models in biological research, so a special section was proposed in the paper.</p>\n<p>As <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> mentioned, PSF is an element important in microscopy itself and I also suggest you communicate with WG11 to know how PSF is included in their recommendation.</p>", "<p>Be careful. The danger here is that people only see within the blinkers of their own super-specialist experience and make requirements too narrow. Once a specific counter example is provided you can always say \u201cOh, yes of course, and that too.\u201d - but if these guidelines become enacted at some point such super-flexibility will fizzle out (else what is the point of guidelines if they can be changed on a whim?).<br>\nFor example my deconvolution research has nothing to do with beads - I do brightfield decon. Also \u2018blind\u2019 decon is a vast field which I also do and should not be treated as an homogenous whole \u2018lacking scientific validation\u2019 - also, if you say this, how can such methods ever achieve scientific validation if no one can publish the scientific validation studies because blind decon is excluded from publication as being unscientific? I disagree with this. Also the peripheral settings are of paramount importance in blind/myopic and also some instances of non-blind decon and spatially variant PSF decon - so I disagree with that too.  I don\u2019t do fluorescence work per se. Perhaps in specific fields there are some specific arguments - but those should be discussed on a paper-by-paper basis by peer reviewers and not enshrined in generic guidelines. Be careful.</p>", "<aside class=\"quote no-group\" data-username=\"P_Tadrous\" data-post=\"7\" data-topic=\"77285\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/p_tadrous/40/49994_2.png\" class=\"avatar\"> P Tadrous:</div>\n<blockquote>\n<p>Be careful. The danger here is that people only see within the blinkers of their own super-specialist experience and make requirements too narrow. Once a specific counter example is provided you can always say \u201cOh, yes of course, and that too.\u201d - but if these guidelines become enacted at some point such super-flexibility will fizzle out (else what is the point of guidelines if they can be changed on a whim?).<br>\nFor example my deconvolution research has nothing to do with beads - I do brightfield decon. Also \u2018blind\u2019 decon is a vast field which I also do and should not be treated as an homogenous whole \u2018lacking scientific validation\u2019 - also, if you say this, how can such methods ever achieve scientific validation if no one can publish the scientific validation studies because blind decon is excluded from publication as being unscientific?</p>\n</blockquote>\n</aside>\n<p>Hi <a class=\"mention\" href=\"/u/p_tadrous\">@P_Tadrous</a></p>\n<p>You are totally correct in that I am being biased by specific issues in my own super-niche experience.  It\u2019s hard not to because reproducibility issues I\u2019ve seen are at the top of my mind and I immediately try and see how these guidelines could address those.</p>\n<p>Common issue in all the papers I am concerned about is the deconvolution or convolution model was not available for inspection.    A Deep Neural Network is a series of convolutions (and other operations) as well.  So it seems inconsistent to have to provide the convolution kernels in one case, but not the other.  Of course that leads us into a niche conversation about how convolution kernels are used in both DL and Deconvolution.</p>\n<p><a class=\"mention\" href=\"/u/kota\">@Kota</a> I did think the Machine Learning section was good.  My bias when I read it though was 'if they have to output their learned model for one case why not for other cases like a learned decon model?   I did a search for the word \u2018model\u2019 in the paper and I believe in every case it was tied to machine learning.  It would be nice if it was made clear that many other techniques use models and the models should be provided just as \u2018machine learning\u2019 ones are.</p>\n<p>Brian</p>", "<p>Your comment got me thinking.</p>\n<p>The problem in a general sense is the use of non-converging optimizers applied to ill-posed inverse problems.  Very common case is the first \u2018research\u2019 paper on an optimization or machine learning technique has flaws but they are undetected.   Non-converging means by definition the technique will not be reproducible by others, because slight differences in initial conditions or inputs, result in different (unstable) answers.</p>\n<p>Then downstream biologists start using the technique in their papers.  However the biologist may not know how the technique works, or what would need to be output to replicate the technique.   Some people may have questions, but there is no framework to even discuss the issue, as \u2018non-converging\u2019 is a vague term.  Each case has different model files and/or parameters that would need to output for reproducibility.   Question is how to express this in a general, accessible and concise way to get biologists and reviewers thinking about what would need to be output to replicate a model?</p>\n<p>Brian</p>", "<aside class=\"quote no-group\" data-username=\"bnorthan\" data-post=\"8\" data-topic=\"77285\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bnorthan/40/71_2.png\" class=\"avatar\"> Brian Northan:</div>\n<blockquote>\n<p>I did a search for the word \u2018model\u2019 in the paper and I believe in every case it was tied to machine learning. It would be nice if it was made clear that many other techniques use models and the models should be provided just as \u2018machine learning\u2019 ones are.</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a><br>\nI see. Can the solution be \u201cComponents and Platform\u201d added with a note somewhere in the text that components include models?</p>\n<p>Kota</p>", "<p>Hi <a class=\"mention\" href=\"/u/kota\">@Kota</a></p>\n<p>So if I understand components are just additional files needed to run the workflow?  So for example if the workflow included machine learning it would be the file with the ML model, if deconvolution the file with PSF, if an alignment the file with the registration model.</p>\n<p>You are much more of an expert than I am on what wording would properly convey the concept to your target audience, so I am not sure I have strong feelings on the exact terminology.</p>\n<p>Brian</p>", "<p>Thanks <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a></p>\n<p>Let\u2019s ask the leads Helena &amp; Chris about our wish to add some notes to add example details of components (or it could be associated with the workflow as \u201cdata\u201d) for workflows using machine learning and deconvolution.</p>\n<p><a class=\"mention\" href=\"/u/helena_klara_jambor\">@Helena_Klara_Jambor</a> <a class=\"mention\" href=\"/u/schmiedc\">@schmiedc</a> ?</p>", "<p><a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a> <a class=\"mention\" href=\"/u/kota\">@Kota</a></p>\n<p>Thank you very much for the feedback and the very interesting discussion.</p>\n<p>The key challenge that this paper tries to address is that there are very little concrete standards for publishing images and image analysis overall. Also authors that want to publish results of image analysis methods are no experts and have received only minimal formal training in them. Thus, the primary goal is to establish a first set of very <strong>generic</strong>, <strong>easy to understand</strong> and <strong>easy to action on</strong> guidelines. For a (hopefully) very broad general scientific audience.</p>\n<p>We would like to ask to keep this primary objective of this particular article in mind.</p>\n<p>Adding a section about a very specific method would not serve this objective very well.</p>\n<p>Going forward what we can easily envision is that specific details and examples can be better addressed in additional accompanying materials. For example via a companion website or training material.</p>\n<p>Happy to discuss further.</p>", "<p><a class=\"mention\" href=\"/u/kota\">@Kota</a> <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a></p>\n<p>\u201cSo if I understand components are just additional files needed to run the workflow? So for example if the workflow included machine learning it would be the file with the ML model, if deconvolution the file with PSF, if an alignment the file with the registration model.\u201d</p>\n<p>As for this more specific point I would see this easily under the umbrella of \u201cKey settings\u201d within the workflow checklists. The component aspect is just the specific algorithm or rather implementation of an algorithm that has been used in a workflow step.</p>\n<p>One could imagine giving some more brief examples within the article that would make this more helpful for authors. <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a>  would this go in a useful direction for you?</p>", "<p>Hi <a class=\"mention\" href=\"/u/schmiedc\">@schmiedc</a></p>\n<p>That sounds like a good plan</p>\n<p>My interpretation of a \u201cmodel\u201d is something that is too complicated to store just as \u2018settings\u2019.   A good rule of thumb is anything (PSFs, machine learning models, files generated by registration, etc.) that is output to a file, then can be reloaded to apply to other images.</p>\n<p>Brian</p>", "<p>Dear <a class=\"mention\" href=\"/u/kota\">@Kota</a> and @<a href=\"https://forum.image.sc/u/schmiedc\">schmiedc</a>,</p>\n<p>Great to see this Quarep activity being materialized in a paper.</p>\n<p>Of course we would love to see a section on deconvolution included, as Brian <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a> suggested, Mentioning this is of course self-promotion, yet we\u2019re a strong supporter of publicizing the use of any type of (pre-)processing because of its significant influence on image segmentation and analysis, and therefore reproducibility.</p>\n<p>Also because nowadays many (confocal and widefield) microscopes on the market are already partially processing the raw data (on the fly), obscuring the imaging process and challenging the definition of \u2018raw data\u2019. It is very likely that many users are not even aware of the application of this pre-preprocessing, let alone the details of the steps that are involved.</p>\n<p>Furthermore, the presence of imaging problems like drift or crosstalk, issues which should be dealt with before any image deconvolution is applied, is already disturbed and can thus no longer be identified and corrected. Thus, as is suggested, it is key to have the raw data and details on the subsequent processing.</p>\n<p>As far as our Huygens Software is concerned, we give some suggestions online for our users on how to cite and reproduce the results: <a href=\"https://svi.nl/HowtoCiteHuygens\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">HowtoCiteHuygens | Scientific Volume Imaging</a>. Maybe other companies could also have their specific recommendations? Information about the PSF is important, yet there are more parameters that influence the results, important ones being background subtraction and the SNR value.</p>\n<p>We have noticed that different software produce PSF in different formats that are not easily transferable for use in other software. So some standardization in this would be best. The suggestion of a bead image would have the additional benefit that more aberrations are included, such as chromatic aberration.</p>\n<p>Revisiting \u201cself-promotion\u201d: may we suggest to complete the list of commercial vendor with other players as well, such as for example us (Huygens). See top of page 2: \"such as ZEN Blue, Amira, Imaris, Arivis, and Python software libraries (Perkel, 2021), <a href=\"https://scikit-image.org/\" rel=\"noopener nofollow ugc\">https://scikit-image.org/</a>; see also Eliceiri 2012).</p>\n<p>Best,</p>\n<p>Vincent</p>", "<p>Sure I agree with you that a registration model or a PSF is a bit more than just a setting. However, for us the perspective of the user/author is key. From this viewpoint I would see this fitting very well in the Key settings category. Currently the checklists include the following descriptions under this category:</p>\n<p>ESTABLISHED WORKFLOWS - Key settings: \u201cKey processing parameters must be reported.\u201d</p>\n<p>NOVEL WORKFLOWS - Key settings: \u201cKey settings (e.g., settings that deviate from default settings) must be documented in the methods section.\u201d</p>\n<p>My suggestion would be to extend these sentences a bit including such important parameter files (e.g. registration model, PSF for deconvolution). As this would be something that authors also could easily identify in the components they employ. For deeper explanation and examples I would suggest delivery via a different format (website, teaching material).</p>", "<aside class=\"quote no-group\" data-username=\"schmiedc\" data-post=\"17\" data-topic=\"77285\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/schmiedc/40/14633_2.png\" class=\"avatar\"> Christopher Schmied:</div>\n<blockquote>\n<p>settings that deviate from default settings</p>\n</blockquote>\n</aside>\n<p>It is important to appreciate that \u2018default\u2019 settings may change from version to version of any software so you should not make \u2018if deviates from default\u2019 an exception. Just specifiying the software version (as anyone should) is not good enough because it puts the onus on the reader (who may be reading 20 years later) to go back and identify what the defaults of version 1.32.4 were compared to the current version of 7.32.5 - and that may or may not be possible or practicable.</p>\n<p>Also, please, everyone, if you are making these standards for fluorescence microscopy then say so explicitly. All this PSF business you are talking about is so out of whack with other forms of microscopy deconvolution.</p>\n<p>It is important to appreciate that a measured PSF will vary from one acquisition to another, even with the same microscope on the same day using the same software and settings. A PSF belongs to an experiment - that is its scope of existence. What is important is how the PSF is generated and whether that method gives reproducible or comparable endpoint results by others in other labs using other equipment and other software (yes, other software) - and reproducible is not necessarily the same as \u2018plug in the same settings to the same software for the same microscope\u2019 [I am speaking of measured PSF\u2019s here, not synthetic calculated ones]. Other than for the most simple of situations, that is not a sufficient approach to reproduciblity in deconvolution experiments (and probably also other similarly under-determined inverse problem methods).</p>\n<p>So, yes, record all settings, software versions, equipment etc. but anyone trained in the scientific method will do that already, won\u2019t they? So what new things are you suggesting here? I am getting confused.</p>\n<p>P</p>", "<p>thank you very much for your comment. We can of course discuss if specific examples work well in the context of the work. I agree the cited example here could be misunderstood.</p>\n<p>But I would like to ask you to understand the article and the discussion in its intention. We are currently creating consise checklists that allow a wide audience of biologists to easily improve the quality of publications of image figures and image analysis. We are NOT defining anything about microscopy methods. This is outside the scope of this article.</p>\n<p>Of course every scientist should record all settings. Of course scientists should report everything that is needed to reproduce their work. However, we do see in the existing literature today a lack of reporting of critical information for image figures and image analysis. With image analysis in particular we often see a complete absence of reporting - The infamous \u201cImage analysis was performed in ImageJ\u201d sentence. This unfortunate reality we would like to improve with our checklists.</p>\n<p>Please also keep in mind the checklist article cannot be an extensive textbook. It should bring forward the key concept so we can also start the necessary discussion of improving reporting standards in this area.</p>", "<aside class=\"quote no-group\" data-username=\"P_Tadrous\" data-post=\"18\" data-topic=\"77285\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/p_tadrous/40/49994_2.png\" class=\"avatar\"> P Tadrous:</div>\n<blockquote>\n<p>So, yes, record all settings, software versions, equipment etc. but anyone trained in the scientific method will do that already, won\u2019t they?</p>\n</blockquote>\n</aside>\n<p>It would be nice if they would, but they clearly do not.<br>\n<a href=\"https://www.nature.com/collections/djiciihhjh\">Entire focus issues are devoted to the topic</a>.<br>\nMethods sections <a href=\"https://elifesciences.org/articles/55133\">are woefully incomplete</a>.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3de369d4211690f71a9b30ba5a63acd8bce3f02b.png\" data-download-href=\"/uploads/short-url/8Pum6WufkNAWtc2mPr0zQQdC9ib.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3de369d4211690f71a9b30ba5a63acd8bce3f02b_2_517x365.png\" alt=\"image\" data-base62-sha1=\"8Pum6WufkNAWtc2mPr0zQQdC9ib\" width=\"517\" height=\"365\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3de369d4211690f71a9b30ba5a63acd8bce3f02b_2_517x365.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/d/3de369d4211690f71a9b30ba5a63acd8bce3f02b_2_775x547.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/d/3de369d4211690f71a9b30ba5a63acd8bce3f02b.png 2x\" data-dominant-color=\"F5F2F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1014\u00d7717 169 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>There are systematic issues with relying on scientists to do exactly those standard things, and others you might consider default behaviors <a href=\"https://www.sciencedirect.com/science/article/pii/S089543562200141X?via%3Dihub\">like providing data upon request</a>.<br>\nA particularly fun example.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/eee04abb41f1f2b00ff3b116b7cbd1d0a3255c0e.png\" data-download-href=\"/uploads/short-url/y5cbeunq5WikkZIX2roKiwK8r26.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee04abb41f1f2b00ff3b116b7cbd1d0a3255c0e_2_517x170.png\" alt=\"image\" data-base62-sha1=\"y5cbeunq5WikkZIX2roKiwK8r26\" width=\"517\" height=\"170\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee04abb41f1f2b00ff3b116b7cbd1d0a3255c0e_2_517x170.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee04abb41f1f2b00ff3b116b7cbd1d0a3255c0e_2_775x255.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/eee04abb41f1f2b00ff3b116b7cbd1d0a3255c0e.png 2x\" data-dominant-color=\"EFEFEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">981\u00d7323 40.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>A set of guidelines that <em>could</em> be enforced by journals might change this behavior to more like what it seems you expect. Comprehensive dictionaries of guidelines would of course be ignored, but it would be great to get to the point where even minimal standards can be enforced - or better yet, do not need to be.</p>\n<p>As <a class=\"mention\" href=\"/u/schmiedc\">@schmiedc</a> says, this is not specifically about microscopy methods, but those articles will be coming <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "77286": ["<p>We are happy to announce the launch of the <strong>First AI4Life Open Call</strong>, an opportunity designed to provide <strong>support with Deep Learning</strong> to life scientists with unmet image analysis needs.</p>\n<p>With this Open Call, we want to reach out to life scientists that need help to improve their image analysis workflows, data, training data creation or simply need some consultancy on the tools to use. A team of experts from AI4Life will collaborate with the successful applicants for 6 months.</p>\n<p>To learn more about the AI4Life Open Call and submit your application, please visit our website at <a href=\"https://ai4life.eurobioimaging.eu/first-open-call/\" rel=\"noopener nofollow ugc\">https://ai4life.eurobioimaging.eu/first-open-call/</a></p>\n<p>The application deadline is <strong>March 31, 2023</strong>. While this sounds like a lot, don\u2019t wait too long to submit your application! It will only take some minutes and once done, it\u2019s done!</p>\n<p>We would like to encourage you to also share this opportunity widely with your colleagues who might also benefit from this opportunity. Thank you!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/3/f35b0e459db3ee2069b15fcc8e771c386e1cd6f5.png\" data-download-href=\"/uploads/short-url/yIP6U4YJs25YfiXwwX3HGtJ1xu5.png?dl=1\" title=\"OC2023_slide\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f35b0e459db3ee2069b15fcc8e771c386e1cd6f5_2_690x388.png\" alt=\"OC2023_slide\" data-base62-sha1=\"yIP6U4YJs25YfiXwwX3HGtJ1xu5\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f35b0e459db3ee2069b15fcc8e771c386e1cd6f5_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f35b0e459db3ee2069b15fcc8e771c386e1cd6f5_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f35b0e459db3ee2069b15fcc8e771c386e1cd6f5_2_1380x776.png 2x\" data-dominant-color=\"D6DEDE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">OC2023_slide</span><span class=\"informations\">1600\u00d7900 185 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>If you might apply with some topic, consider clicking the <img src=\"https://emoji.discourse-cdn.com/twitter/heart.png?v=12\" title=\":heart:\" class=\"emoji\" alt=\":heart:\" loading=\"lazy\" width=\"20\" height=\"20\"> icon above\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nI\u2019m excited like a child on their birthday to see who will apply and with what exciting topics to look at!!!</p>", "<p>Join our upcoming Q&amp;A session dedicated to the First AI4Life Open Call happening on March 20, 2023, at 4 pm CET. This one-hour session is designed to help you with any queries or concerns you might have about the Open Call and provide support throughout the application process.</p>\n<p>To register for the Q&amp;A session, visit <a href=\"https://bit.ly/oc-ask-us-anything\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Meeting Registration - Zoom</a>.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c.png\" data-download-href=\"/uploads/short-url/x8WuPN4WclfLCFXBTzWvZbdEKvi.png?dl=1\" title=\"Ask Us Anything!\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_690x388.png\" alt=\"Ask Us Anything!\" data-base62-sha1=\"x8WuPN4WclfLCFXBTzWvZbdEKvi\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e84a8cfc92abb1822b20f7cf0ec584d40aea3c3c_2_1380x776.png 2x\" data-dominant-color=\"DCE4E2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Ask Us Anything!</span><span class=\"informations\">1600\u00d7900 168 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If you can\u2019t make it to the live Q&amp;A, check out our video where Florian Jug addresses some of the frequently asked questions about the Open Call:</p>\n<div class=\"onebox lazyYT lazyYT-container\" data-youtube-id=\"ZwqL3AmU3og\" data-youtube-title=\"FAQs about the AI4Life Open Calls\" data-parameters=\"feature=oembed&amp;wmode=opaque\">\n  <a href=\"https://www.youtube.com/watch?v=ZwqL3AmU3og\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img class=\"ytp-thumbnail-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f97705ea26ec32c8e86a367f5db0de62b7584042.jpeg\" title=\"FAQs about the AI4Life Open Calls\" width=\"480\" height=\"360\">\n  </a>\n</div>\n\n<p>Don\u2019t miss this opportunity to learn more about the AI4Life Open Call and get the support you need. We look forward to seeing you there!</p>"], "77294": ["<p>ping <a class=\"mention\" href=\"/u/hoerldavid\">@hoerldavid</a> <a class=\"mention\" href=\"/u/stephanpreibisch\">@StephanPreibisch</a></p>\n<p>I am using the advanced grouping options of BigStitcher, specifically I would like to group tiles but compare timepoints.<br>\nThe reason is that the tiles have already been. registered in a previous step. This used to work but seems to fail since recent updates.</p>\n<p>From what I can understand, this narrows down to a problem in<br>\n<code>net.preibisch.mvrecon.process.downsampling.DownsampleTools.openAndDownsample</code>,<br>\nPlease see details below.</p>\n<ul>\n<li>minimal dataset to download (h5/xml, 2 tiles, 2 timepoints):<br>\n<a href=\"https://drive.switch.ch/index.php/s/DyeTp0P1jrS9LIb\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">SWITCHdrive</a>\n</li>\n</ul>\n<p>IJ macro code to reproduce the issue:</p>\n<pre><code class=\"lang-auto\">run(\"Calculate pairwise shifts ...\", \"select=U:/VAMP/schlda00/ines-seqfish/20230201_185157_675/minimal/dev_h5/dev.xml process_angle=[All angles] process_channel=[All channels] process_illumination=[All illuminations] process_tile=[All tiles] process_timepoint=[All Timepoints] method=[Phase Correlation] tiles=[Average Tiles] channels=[Average Channels] show_expert_grouping_options how_to_treat_timepoints=compare how_to_treat_channels=group how_to_treat_illuminations=group how_to_treat_angles=[treat individually] how_to_treat_tiles=group downsample_in_x=2 downsample_in_y=2 downsample_in_z=2\");\n</code></pre>\n<ul>\n<li>tested WORKING using e.g. Big_Stitcher-0.8.3.jar + multiview_reconstruction-0.11.2.jar</li>\n<li>This code is NOT working in the latest version, e.g. Big_Stitcher-1.1.2.jar + multiview_reconstruction-1.2.5.jar</li>\n</ul>\n<p>See console output and Exception window below:</p>\n<pre><code class=\"lang-auto\">non translations equal\njava.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: net.preibisch.mvrecon.process.downsampling.DownsampleTools.openAndDownsample(Lmpicbg/spim/data/generic/sequence/BasicImgLoader;Lmpicbg/spim/data/sequence/ViewId;Lnet/imglib2/realtransform/AffineTransform3D;[JZZ)Lnet/imglib2/RandomAccessibleInterval;\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat net.preibisch.stitcher.algorithm.globalopt.TransformationTools.computePairs(TransformationTools.java:639)\n\tat net.preibisch.stitcher.plugin.Calculate_Pairwise_Shifts.processPhaseCorrelation(Calculate_Pairwise_Shifts.java:179)\n\tat net.preibisch.stitcher.plugin.Calculate_Pairwise_Shifts.run(Calculate_Pairwise_Shifts.java:149)\n\tat ij.IJ.runUserPlugIn(IJ.java:237)\n\tat ij.IJ.runPlugIn(IJ.java:203)\n\tat ij.Executer.runCommand(Executer.java:152)\n\tat ij.Executer.run(Executer.java:70)\n\tat ij.IJ.run(IJ.java:319)\n\tat ij.IJ.run(IJ.java:330)\n\tat ij.macro.Functions.doRun(Functions.java:703)\n\tat ij.macro.Functions.doFunction(Functions.java:99)\n\tat ij.macro.Interpreter.doStatement(Interpreter.java:281)\n\tat ij.macro.Interpreter.doStatements(Interpreter.java:267)\n\tat ij.macro.Interpreter.run(Interpreter.java:163)\n\tat ij.macro.Interpreter.run(Interpreter.java:93)\n\tat ij.macro.Interpreter.run(Interpreter.java:107)\n\tat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)\n\tat ij.IJ.runMacro(IJ.java:158)\n\tat ij.IJ.runMacro(IJ.java:147)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)\n\tat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)\n\tat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)\n\tat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:164)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NoSuchMethodError: net.preibisch.mvrecon.process.downsampling.DownsampleTools.openAndDownsample(Lmpicbg/spim/data/generic/sequence/BasicImgLoader;Lmpicbg/spim/data/sequence/ViewId;Lnet/imglib2/realtransform/AffineTransform3D;[JZZ)Lnet/imglib2/RandomAccessibleInterval;\n\tat net.preibisch.stitcher.algorithm.GroupedViewAggregator.aggregate(GroupedViewAggregator.java:381)\n\tat net.preibisch.stitcher.algorithm.globalopt.TransformationTools.computeStitching(TransformationTools.java:271)\n\tat net.preibisch.stitcher.algorithm.globalopt.TransformationTools$2.call(TransformationTools.java:586)\n\tat net.preibisch.stitcher.algorithm.globalopt.TransformationTools$2.call(TransformationTools.java:568)\n\t... 4 more\n\n</code></pre>\n<p>Exception</p>\n<pre><code class=\"lang-auto\">(Fiji Is Just) ImageJ 2.9.0/1.53t; Java 1.8.0_322 [64-bit]; Windows 10 10.0; 398MB of 196238MB (&lt;1%)\n \njava.lang.NullPointerException\n\tat net.preibisch.stitcher.plugin.Calculate_Pairwise_Shifts.processPhaseCorrelation(Calculate_Pairwise_Shifts.java:201)\n\tat net.preibisch.stitcher.plugin.Calculate_Pairwise_Shifts.run(Calculate_Pairwise_Shifts.java:149)\n\tat ij.IJ.runUserPlugIn(IJ.java:237)\n\tat ij.IJ.runPlugIn(IJ.java:203)\n\tat ij.Executer.runCommand(Executer.java:152)\n\tat ij.Executer.run(Executer.java:70)\n\tat ij.IJ.run(IJ.java:319)\n\tat ij.IJ.run(IJ.java:330)\n\tat ij.macro.Functions.doRun(Functions.java:703)\n\tat ij.macro.Functions.doFunction(Functions.java:99)\n\tat ij.macro.Interpreter.doStatement(Interpreter.java:281)\n\tat ij.macro.Interpreter.doStatements(Interpreter.java:267)\n\tat ij.macro.Interpreter.run(Interpreter.java:163)\n\tat ij.macro.Interpreter.run(Interpreter.java:93)\n\tat ij.macro.Interpreter.run(Interpreter.java:107)\n\tat ij.plugin.Macro_Runner.runMacro(Macro_Runner.java:162)\n\tat ij.IJ.runMacro(IJ.java:158)\n\tat ij.IJ.runMacro(IJ.java:147)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1174)\n\tat net.imagej.legacy.IJ1Helper$3.call(IJ1Helper.java:1170)\n\tat net.imagej.legacy.IJ1Helper.runMacroFriendly(IJ1Helper.java:1121)\n\tat net.imagej.legacy.IJ1Helper.runMacro(IJ1Helper.java:1170)\n\tat net.imagej.legacy.plugin.IJ1MacroEngine.eval(IJ1MacroEngine.java:145)\n\tat org.scijava.script.ScriptModule.run(ScriptModule.java:164)\n\tat org.scijava.module.ModuleRunner.run(ModuleRunner.java:163)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:124)\n\tat org.scijava.module.ModuleRunner.call(ModuleRunner.java:63)\n\tat org.scijava.thread.DefaultThreadService.lambda$wrap$2(DefaultThreadService.java:225)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n</code></pre>", "<p>ping <a class=\"mention\" href=\"/u/stephanpreibisch\">@StephanPreibisch</a> and <a class=\"mention\" href=\"/u/hoerldavid\">@hoerldavid</a></p>", "<aside class=\"quote no-group\" data-username=\"CellKai\" data-post=\"1\" data-topic=\"77294\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cellkai/40/14527_2.png\" class=\"avatar\"> CellKai:</div>\n<blockquote>\n<p>This code is NOT working in the latest version, e.g. Big_Stitcher-1.1.2.jar + multiview_reconstruction-1.2.5.jar</p>\n</blockquote>\n</aside>\n<p>Just to report that this is still happening with MVR 1.2.8\u2026</p>", "<p>just as an update, the pairwise registration is now working again with Big_Stitcher-1.1.3 / multiview-reconstruction version: 1.2.8 <img src=\"https://emoji.discourse-cdn.com/twitter/+1.png?v=12\" title=\":+1:\" class=\"emoji\" alt=\":+1:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/smile.png?v=12\" title=\":smile:\" class=\"emoji\" alt=\":smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><a class=\"mention\" href=\"/u/oburri\">@oburri</a></p>\n<p>I noticed though an odd behavior. When using<br>\n<code> Plugins \u203a BigStitcher \u203a Batch Processing \u203a Calculate pairwise shifts ...</code></p>\n<p>A second instance of Fiji is started, and the previous instance crashes.</p>\n<p>The pairwise shift calculation finishes successfully, but afterwards its necessary to close/kill both instances and then restart Fiji</p>"], "77324": ["<p>Hi all</p>\n<p>I have annotated several ROIs and have masked cells within ROIs and calculated cellular intensity features for a couple of proteins and it gives the values as \u201cROI: 1um per pixel: TH mean: 188.1497\u201d. I was wondering what is this value? Is it the mean cellular intensity of that protein for the ROI area or is it the mean intensity of a pixel in that ROI?</p>\n<p>Cheers<br>\nGiselle</p>", "<p>Hi Giselle,<br>\nThis is the average intensity of that channel (I assume this is a fluorescence image) of the <em>pixels</em> in that ROI. Before calculating the average, the image was downsampled to 1 um / pixel.* For a large ROI, downsampling rarely effects the final average.</p>\n<p>*Note: I think it actually uses the nearest pyramid level to 1 um/pixel, not exactly 1um.</p>", "<p>Ok thank you for clearing that up Sarah!</p>", "<aside class=\"quote no-group\" data-username=\"Giselle\" data-post=\"1\" data-topic=\"77324\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/e495f1/40.png\" class=\"avatar\"> Giselle Sagredo:</div>\n<blockquote>\n<p>annotated several ROIs and have masked cells within ROIs</p>\n</blockquote>\n</aside>\n<p>Just in case, wanted to emphasize that the annotation measurements have nothing to do with the cell measurements inside of them. The ROI just means the object itself.</p>", "<p>Ok thank you. Just to confirm, the mean value is the mean cellular intensity of that annotation object? So there is no need to multiply by the total area of the object as it is the average of the pixels for that object?</p>", "<p>Those two statements do not seem to be related.</p>\n<p>The ROI mean intensity has nothing to do with cells or cellular intensity, it\u2019s just the mean fluorescence in a given area, in a given channel.</p>\n<p>If you multiply the mean by the area you get a total \u201camount of fluorescence\u201d, which doesn\u2019t mean anything in and of itself. You could do the multiplication if you want a value that is affected by area.</p>", "<p>I have created a thresholder to mask my positive cells and to determine the intensity of other channels within these cells. Once I created this mask object, I added intensity features and selected 1um as my preferred pixel size. This gives me \u201cROI: 1.00 \u00b5m per pixel: channel: Mean 168.9679\u201d. Is this value the intensity of the given channel for the masked area or do I need to multiply it by the area to get the intensity of that channel for that area?<br>\nBecause if it is 1um per pixel I believe the calculation would be Mean intensity value * total area/1um2.</p>", "<aside class=\"quote no-group\" data-username=\"Giselle\" data-post=\"7\" data-topic=\"77324\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/e495f1/40.png\" class=\"avatar\"> Giselle Sagredo:</div>\n<blockquote>\n<p>\u201cROI: 1.00 \u00b5m per pixel: channel: <strong>Mean</strong> 168.9679\u201d</p>\n</blockquote>\n</aside>\n<p>It\u2019s the <strong>mean</strong> intensity of the pixels in the ROI. If you multiply it by the area you instead get the intensity <strong>sum</strong>. There is no \u201cintensity of the channel for that area\u201d, only <em>types</em> of measurements, and which one is most useful depends on the experiment and the biology.</p>\n<p>If you created a single object, the ROI is the entire object and all of its pixels. If you chose to split the object, the ROI measurement is only relevant to each individual area within the ROI. As <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> said, the 1um relates the the amount of smoothing, and for highest accuracy should equal the pixel size. 1um is fine for most low res microscopy images (about 20x or less), but might need to be smaller if you are studying cells using a 63x oil.</p>", "<p>Ok thank you very much for your response:)</p>", "<p>Hello everyone,</p>\n<p>I also have questions regarding this topic and did not want to create another thread so soon after the creation of this one; I am looking at data for a 4x and 10x scan of the same wells and have used the default 2um mean intensity measurements for both. However, both the 4x and 10x have different pixel sizes (1.6252um by 1.6252um for 4x and .65um by .65um for 10 x) and I am wondering if I should go ahead and use 1.6252 and .65 for the 4x and 10x pixel sizes respectively to accurately compare their data (I am also unsure whether 1.6252 and .65 are the pixel size or whether their square is the pixel size).</p>\n<p>Thank you</p>", "<p>It\u2019s the size of one side of the pixel, and I believe that it snaps to even pixel amounts, regardless of what you put. So you are probably at either the exact pixel resolution or double the pixel resolution, or something like that. Depends on whether your pixel size metadata for the image is accurate though.</p>"], "42532": ["<p>Hello <a class=\"mention\" href=\"/u/OMEAnnouncements\">@OMEAnnouncements</a>,</p>\n<p>I have a nd file (generated by metamorph) which is defining a dataset of:</p>\n<ul>\n<li>3 channels</li>\n<li>33 timepoints</li>\n<li>~ 100 positions</li>\n</ul>\n<p>Leading to around 10 000 individual tiff files.</p>\n<p>Reading/opening these stacks (time series)  with bioformats is quite slow, because ( I guess ) a TIFFReader is generated for each individual tiff file - re-reading over and over again the same metadata.</p>\n<p>Is there a way to make to process faster by avoiding the creation of a TIFFReader for each tiff ?</p>\n<p>I tried to \u2018memoize the nd file\u2019 but there was no significant speed increase. Also - the goal here is to read tiff stacks in order to resave them to another file format - so only one read is performed for each tiff. (and the resaving should be fast as well)</p>\n<p>Happy to get any hints on how to tackle this issue!</p>", "<p>Hi <a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a></p>\n<p>I was also worried about the slowness of reading the oib file (the content is tiff) for each time series created by Olympus\u2019s multi-area time-lapse imaging.<br>\nSo I created a plugin that simply merges open files one by one in parallel processing.<br>\nThe load time seems to have improved significantly.<br>\nThis makes it possible to read files that are divided into time series by oif.<br>\nIn addition, because bio-formats makes a mistake in the order of numbers (specifically, a sort mistake that occurs when numbers such as 01, 02\u202610,11\u2026), I also add an operation to absorb it.<br>\nI\u2019m not sure if you can do that with a metamorph file, but in my case this has resolved my complaints.<br>\nIs this information helpful?</p>\n<p>hwada</p>", "<p>This seems to be a known issue.<br>\nThere are several posts in the forum mentioning slow TIFF performance.</p>\n<p>In <a href=\"https://imagej.net/TIFF\">https://imagej.net/TIFF</a> you can find the hint:</p>\n<p>The <a href=\"https://imagej.net/Bio-Formats\">Bio-Formats</a> plugins offer a more complete TIFF importer, accessible via the <em>File  \u203a Import  \u203a Bio-Formats</em> command.</p>\n<ul>\n<li>\n<strong>Pro:</strong> The Bio-Formats TIFF reader can handle many more varieties of TIFF.</li>\n<li>\n<strong>Con:</strong> The Bio-Formats TIFF support is not as speedy as ImageJ1\u2019s TIFF reader.</li>\n</ul>\n<p>and</p>\n<p>The <a href=\"https://imagej.net/SCIFIO\">SCIFIO</a> library \u2026 adapted from the <a href=\"https://imagej.net/Bio-Formats\">Bio-Formats</a>  \u2026 supports a wider variety of TIFFs, but is less performant than the ImageJ 1.x reader.</p>", "<aside class=\"quote no-group\" data-username=\"NicoKiaru\" data-post=\"1\" data-topic=\"42532\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/nicokiaru/40/3076_2.png\" class=\"avatar\"> NicoKiaru:</div>\n<blockquote>\n<p>I have a nd file (generated by metamorph) which is defining a dataset of:</p>\n<ul>\n<li>3 channels</li>\n<li>33 timepoints</li>\n<li>~ 100 positions</li>\n</ul>\n</blockquote>\n</aside>\n<p>I think we are facing a similar problem here trying to import a similar experiment (with a .nd file) to Omero.<br>\naround 9 GB and 18.000 individual files.<br>\nNormal time transfer server to server: around 10 minutes in my institute, importing in Omero I gave up after 45 minutes.</p>", "<p>thanks <a class=\"mention\" href=\"/u/hwada\">@hwada</a> and <a class=\"mention\" href=\"/u/phaub\">@phaub</a>, this is some helpful information!</p>\n<p>I think I\u2019ll try a \u2018brute\u2019 approach by opening/concatenating these as raw data (these are uncompressed tiff) in order to see how much speed can be gained  - and depending on the outcome I\u2019ll either 1 -  give up if it\u2019s not worth 2 - or make my own plugin or 3 - try to create another optimized <code>MetamorphReader</code>dedicated to this special case</p>", "<p><a class=\"mention\" href=\"/u/emartini\">@emartini</a> Interesting to know! It\u2019s probably a different issue because then it involves network transfer</p>", "<p>I am really not sure that is a network transfer problem (maybe related, but not only) since we imported quite fast in Omero much larger experiment (by now up to 100 GB) with more or less the same time we were taking to transfer server to server.</p>\n<p>It really seems to me that Omero is trying to do something with every single tif with bio-formats.<br>\nAnyway I will open a new thread for this issue, but I really think it\u2019s related <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"></p>", "<aside class=\"quote no-group\" data-username=\"emartini\" data-post=\"7\" data-topic=\"42532\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/emartini/40/292_2.png\" class=\"avatar\"> emartini:</div>\n<blockquote>\n<p>I am really not sure that is a network transfer problem (maybe related, but not only)</p>\n</blockquote>\n</aside>\n<p>No you\u2019re right, but because the network is involved, there\u2019s probably an extra layer of complexity</p>\n<aside class=\"quote no-group\" data-username=\"emartini\" data-post=\"7\" data-topic=\"42532\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/emartini/40/292_2.png\" class=\"avatar\"> emartini:</div>\n<blockquote>\n<p>Anyway I will open a new thread for this issue, but I really think it\u2019s related</p>\n</blockquote>\n</aside>\n<p>I\u2019ll keep an eye on it as well!</p>", "<p><a class=\"mention\" href=\"/u/nicokiaru\">@NicoKiaru</a>, what sort of read time are you roughly seeing?</p>\n<p>If you are reading the nd file then there will indeed be a separate reader for each of the associated files and is indeed likely slowing the process. I quickly profiled a slightly smaller dataset (3000 tiff files) and the breakdown was roughly 50% of the time for the initial parsing and initialisation, 25% of the time on setting id for each of the individual readers and 25% on actually reading the pixel data.</p>", "<p>Hi <a class=\"mention\" href=\"/u/dgault\">@dgault</a>,</p>\n<p>Thanks for the benchmarking.</p>\n<p>The data I have is a <a href=\"https://www.dropbox.com/s/fh692pw3er6opv9/code.rar?dl=0\" rel=\"nofollow noopener\">metamorph file</a> (from <a href=\"https://twitter.com/DeriveryLab/status/1302882184001458178\" rel=\"nofollow noopener\">this paper</a> of the Derivery lab), downsized to 10 positions.</p>\n<p>Reading and resaving tiff stacks (one stack per position - around 270 Mb per stack - 2.7 Gb total) with bioformats with this macro takes 4.8 s per image:</p>\n<pre><code class=\"lang-auto\">nPositions=10; //number of positions (I get it elsewhere)\n\nfilePath = \"C:\\\\Users\\\\...\";\nname=\"NotchLanding_all36_control36.nd\";\n\ntimeStart = getTime();\nfor (iPosition=1;iPosition&lt;nPositions+1; iPosition++){\n    //load the images corresponding to position iPosition\n    run(\"Bio-Formats Importer\", \"open=\"+filePath+name+\" color_mode=Default rois_import=[ROI manager] view=Hyperstack stack_order=XYCZT series_\"+iPosition);\n    //save\n    saveAs(\"tiff\", filePath+\"position_\"+iPosition);\n    close();\n}\ntimeEnd = getTime();\n\ntotalTime = ((timeEnd-timeStart)/1000)\n\nprint(\"Export took \"+totalTime+\" seconds for \"+nPositions+\" images\");\nprint(\"Export took \"+(totalTime/nPositions)+\" seconds per image\");\n\n//Export took 47.676 seconds for 10 images\n//Export took 4.7676 seconds per image\n</code></pre>\n<p>Using a direct Image Sequence (giving up metadata) read and save takes around 1.5 s per image:</p>\n<pre><code class=\"lang-auto\">\nsetBatchMode(true);\nfilePath = \"C:\\\\Users\\\\...\";\nfilePrefix = \"NotchLanding_all36_control36_\";\n\nnPositions=10; //number of positions (I get it elsewhere)\ntimeStart = getTime();\nfor (iPosition=1;iPosition&lt;nPositions+1; iPosition++){\n\trun(\"Close All\");\n\trun(\"Image Sequence...\", \"open=[\"+filePath+\"NotchLanding_all36_control36_w1TIRF 637 single LP_s1_t1.TIF] file=[\"+filePrefix+\"w1TIRF 637 single LP_s\"+iPosition+\"_t] number=33 sort\");\n\trename(\"637\");\n\trun(\"Image Sequence...\", \"open=[\"+filePath+\"NotchLanding_all36_control36_w1TIRF 637 single LP_s1_t1.TIF] file=[\"+filePrefix+\"w3TIRF 488 single_s\"+iPosition+\"_t] number=33 sort\");\n\trename(\"488\");\n\trun(\"Image Sequence...\", \"open=[\"+filePath+\"NotchLanding_all36_control36_w1TIRF 637 single LP_s1_t1.TIF] file=[\"+filePrefix+\"w2TIRF 561 single_s\"+iPosition+\"_t] number=33 sort\");\n\trename(\"561\");\n\trun(\"Merge Channels...\", \"c1=561 c2=488 c3=637 create\");\n\tsaveAs(\"Tiff\", filePath+filePrefix+iPosition+\".TIF\");\n}\n\ntimeEnd = getTime();\n\ntotalTime = ((timeEnd-timeStart)/1000)\n\nprint(\"Export took \"+totalTime+\" seconds for \"+nPositions+\" images\");\nprint(\"Export took \"+(totalTime/nPositions)+\" seconds per image\");\n\n// use = virtual\n// Export took 15.93 seconds for 10 images\n// Export took 1.593 seconds per image\n\n// not virtual\n// Export took 15.127 seconds for 10 images\n// Export took 1.5127 seconds per image\n\n</code></pre>\n<p>On my machine there\u2019s a factor 3.2 between bioformats and a \u2018direct read\u2019. Maybe some of this (bioformats and \u2018image sequence\u2019) can be even faster with parallelization but I\u2019m not sure and I don\u2019t really know in which way to go to do that.</p>", "<p>Running the importer multiple times will add a significant overhead as the metadata parsing and initialisation is taking place for each position. I can put together a jython script that would eliminate that duplication and provide some improvement.</p>\n<p>If the goal is to convert each position to its own tiff then the <code>bftools</code> (<a href=\"https://www.openmicroscopy.org/bio-formats/downloads/\">https://www.openmicroscopy.org/bio-formats/downloads/</a>) can also perform that conversion if thats an option (<code>./bfconvert path/to/myFile.nd path/to/position_%s.tiff</code>).</p>", "<p>I need to try! However it\u2019s not working on the linked data currently because of this bug : <a href=\"https://forum.image.sc/t/error-during-opening-an-nd-file/7131/9\" class=\"inline-onebox\">Error during opening an .nd file</a>. For the tests in this thread I compiled the gpl readers library with your fix <a class=\"mention\" href=\"/u/dgault\">@dgault</a> , but I think I\u2019ll wait until the fix makes its way to bftools to try it</p>", "<p>Hello <a class=\"mention\" href=\"/u/dgault\">@dgault</a>,</p>\n<p>facing the same issue here. I also see the extremely low reading with files coming from Abberior (obj files). The uploaded file is 1.6 MB but requires a lot of time (several seconds) before showing the series option<br>\n<a class=\"attachment\" href=\"/uploads/short-url/a7SSLUSjxk4STgWRJt6Aypex9pW.zip\">IMG0010_SNAPTMR_50nM_610CP_50nM_10uMVera (2).zip</a> (1.6 MB)</p>\n<p>For metamorph file I have a time lapse of 179 time points and 128 positions, each time point is a separate stk file with 3 Z-slices a 28 MB. All in all over 22000 files.<br>\nJust the opening of the series choice menu takes between 1 and 2 hour.<br>\nCurrently I am reverting to importing each position as image sequence with IJ1.<br>\nI was wondering what Bioformat is doing on the back? Loading and computing the thumbnails? reading the metadata of each single file from the 22000 files?<br>\ncould one not disable some of the features ?</p>\n<p>I tried to use the reader directly. However, this does not help much as even in this case bioformats seems to reprocess part of the metadata.</p>\n<p>I guess bfconvert will also do the first reading of the file.</p>\n<p>Thanks</p>\n<p>Antonio</p>", "<p>Hi <a class=\"mention\" href=\"/u/apoliti\">@apoliti</a>, the performance will vary depending on the particular format and numerous other factors. For the uploaded file, the majority of the time is spent retrieving the schema and validating the XML contained within. If you are using the reader directly for files such as this one then using the Memoizer reader wrapper to cache the reader will see an improvement in reducing the initialisation step.</p>\n<p>For the Metamorph files it will be quite different. I don\u2019t have a MetaMorph dataset as large as that and none that take anywhere near that length of time to load. I suspect in that scenario it may be the thumbnails that are taking most of the time. The metadata from each individual file should only be accessed when the pixel data for that particular file is loaded. To check if it is thumbnail related, if you use the command line tools from <a href=\"https://www.openmicroscopy.org/bio-formats/downloads/\">https://www.openmicroscopy.org/bio-formats/downloads/</a>, and run <code>showinf -nopix path/to/myFile.nd</code>, does that take significantly less time?</p>\n<p>There are some options we could add to the Bio-Formats plugin to try and improve things, such as disabling thumbnails and adding the Memoizer caching.</p>", "<p>Hello <a class=\"mention\" href=\"/u/dgault\">@dgault</a>,<br>\nI will try, but the  download is currently not working. Somehow it gets a time-out after a while. Not sure if it is from my location or the bioformat website has an issue.</p>\n<p>Antonio</p>", "<p>Yeah, unfortunately the website was down(see <a href=\"https://forum.image.sc/t/ome-resources-down-due-to-uod-outage/43957\" class=\"inline-onebox\">OME Resources Down due to UoD outage</a> for full details), the download should now be back working at <a href=\"https://downloads.openmicroscopy.org/bio-formats/6.5.1/artifacts/bftools.zip\">https://downloads.openmicroscopy.org/bio-formats/6.5.1/artifacts/bftools.zip</a>.</p>", "<p>Hello <a class=\"mention\" href=\"/u/dgault\">@dgault</a>,</p>\n<p>I finally had to time to test it.  The command <code>./showinf -nopix /path/to/myFile.nd.</code> gave an error</p>\n<blockquote>\n<p>Initializing reader<br>\nMetamorphReader initializing D:/TMP/LI03/LI031.nd<br>\nInitializing D:/TMP/LI03/LI031.nd<br>\nLooking for STK file in D:\\TMP\\LI03<br>\nFailure during the reader initialization</p>\n</blockquote>\n<p>I was able to load the accompanying stk file and yes the usage of -nopix makes it faster.</p>\n<p>I tested with other files (nd files from Nikon system). Here again the -nopix option makes it faster.<br>\nI will let you know the difference.</p>", "<p>Performing the command on a 60 GB file with 1134 planes in total (multi-position, CZT):</p>\n<ol>\n<li>\n<code>swhoinf -nopix</code> 2.5 sec</li>\n<li>\n<code>showinf</code> 1.5 min. In imageJ the reading is faster</li>\n</ol>\n<p>Every time it reads the whole metadata and creates the thumbnails we observe a considerably longer loading time.</p>\n<p>The Memoizer I was not able to use in the right way. To open a series as image plus I was using BF.openImagePlus. The reader can help in reading the metadata but not sure how to use in order to open a complete series with all channels and Z-stacks.</p>\n<p>Also note that i had to increase the heapsize up to 5 GB when thumbnails are created.<br>\nOverall I think that a loading option without thumbnails would help to speed up the reading.</p>\n<p>Thanks</p>\n<p>antonio</p>", "<p>Thanks for testing and providing the feedback. It does look like having the option to disable thumbnail generation would be required here. We have an existing GitHub Issue with this feature request which I have updated to link to this thread: <a href=\"https://github.com/ome/bioformats/issues/3574\">https://github.com/ome/bioformats/issues/3574</a></p>", "<p>Hello <a class=\"mention\" href=\"/u/dgault\">@dgault</a>,<br>\nthanks.<br>\nI would like to add that this problem is not limited to metamorph file. We do observe also with large czi files (light-sheet data 80 GB) a 2-3 minutes initial loading time.</p>\n<p>When the data is present in the virtual stack the data can be browsed at a reasonable speed.</p>"], "50734": ["<p>Dear community,<br>\nThese issues exist when I updated imageJ to the latest version.<br>\nWhen I open an nd2 images with bio-formats, different from the situations in the older version, I have to change the colour mode from default to composed/colourized, or I will get a totally grey stack.<br>\nHowever, even when I choose the composed/colorized color mode, the channel colour seems quite different from what I have set when photographing. For example, the nd2 image consists of 4 channels: 1. FITC, 2. TRITC, 3. DAPI, 4. Cy5.  But the colours displayed in ImageJ always becomes 1.green, 2. red, 3. blue, 4. RED as followed(which is supposed to be MAGENTA and works well in the older version). The same in all images no matter what channels are used and what the channel order is.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4a59e6858af73c886b49aa81a8b39d042a39677.png\" data-download-href=\"/uploads/short-url/pM4MreRxUr5QMgozZVJKSMI7G3d.png?dl=1\" title=\"Snipaste_2021-03-27_23-12-42\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4a59e6858af73c886b49aa81a8b39d042a39677_2_690x383.png\" alt=\"Snipaste_2021-03-27_23-12-42\" data-base62-sha1=\"pM4MreRxUr5QMgozZVJKSMI7G3d\" width=\"690\" height=\"383\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4a59e6858af73c886b49aa81a8b39d042a39677_2_690x383.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4a59e6858af73c886b49aa81a8b39d042a39677.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4a59e6858af73c886b49aa81a8b39d042a39677.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4a59e6858af73c886b49aa81a8b39d042a39677_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Snipaste_2021-03-27_23-12-42</span><span class=\"informations\">809\u00d7450 168 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/092bc33166d52576316ba1a0ac8993e8bb15b36c.png\" data-download-href=\"/uploads/short-url/1j83AyYhE2dKH3jULXYL3dOW5L6.png?dl=1\" title=\"Snipaste_2021-03-27_23-13-20\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/092bc33166d52576316ba1a0ac8993e8bb15b36c_2_690x392.png\" alt=\"Snipaste_2021-03-27_23-13-20\" data-base62-sha1=\"1j83AyYhE2dKH3jULXYL3dOW5L6\" width=\"690\" height=\"392\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/092bc33166d52576316ba1a0ac8993e8bb15b36c_2_690x392.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/092bc33166d52576316ba1a0ac8993e8bb15b36c.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/092bc33166d52576316ba1a0ac8993e8bb15b36c.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/092bc33166d52576316ba1a0ac8993e8bb15b36c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Snipaste_2021-03-27_23-13-20</span><span class=\"informations\">822\u00d7468 202 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<ol>\n<li>I have checked the image information, the channel is recognized and the channel order is correct.</li>\n<li>I have also updated bio-formats to the latest version.<br>\nThough I can change the colour of each channel with channel tools, it is still big trouble because I have to change the color manually based on the information.<br>\nCan anyone help me please? Thanks.</li>\n</ol>", "<p>Do you see the same problem if you change the setting for using the Native ND2 library? You can change the setting in <code>Plugins &gt; Bio-Formats &gt; Bio-Formats Plugins Configuration &gt; Formats &gt; Nikon ND2 &gt; Use Nikons ND2 library</code></p>\n<p>If that makes no difference, do you know what version of Bio-Formats was being used in the older version? And if possible would yo be able to share a sample image showing the problem?</p>", "<p>Hi <a class=\"mention\" href=\"/u/dgault\">@dgault</a> !<br>\nThanks for your help. It doesn\u2019t work.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4a251f8b033ba523f4a2fe1f4e71e055923a8dd.png\" data-download-href=\"/uploads/short-url/yU8gSoJBkli1KfgJT6JjqCGvJ5b.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4a251f8b033ba523f4a2fe1f4e71e055923a8dd_2_610x500.png\" alt=\"image\" data-base62-sha1=\"yU8gSoJBkli1KfgJT6JjqCGvJ5b\" width=\"610\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4a251f8b033ba523f4a2fe1f4e71e055923a8dd_2_610x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4a251f8b033ba523f4a2fe1f4e71e055923a8dd.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/4/f4a251f8b033ba523f4a2fe1f4e71e055923a8dd.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/4/f4a251f8b033ba523f4a2fe1f4e71e055923a8dd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">881\u00d7721 102 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nActually, I updated imageJ (or FIJI) without updating Bio-Formats. And the images are displayed in the same way as mentioned, grey in all channels in default colour mode, or incorrect colour in some channels in composite/coloured mode.<br>\nI have also tried to return to the older version of imageJ via Help-&gt;Update ImageJ and choose v1.51 without changing the updated version of Bio-Formats. The bug remains.<br>\nBut unfortunately, I forget the version of Bio-Formats in the older version which can recognize the channel correctly.<br>\nThis is how the images are displayed in composite/coloured mode.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b34723f246bcca13228a044fc51f48bf931c3a5.jpeg\" data-download-href=\"/uploads/short-url/jRsJvzD59ZtKNFuu2lf1AWzCsVD.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b34723f246bcca13228a044fc51f48bf931c3a5_2_564x499.jpeg\" alt=\"image\" data-base62-sha1=\"jRsJvzD59ZtKNFuu2lf1AWzCsVD\" width=\"564\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b34723f246bcca13228a044fc51f48bf931c3a5_2_564x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b34723f246bcca13228a044fc51f48bf931c3a5.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b34723f246bcca13228a044fc51f48bf931c3a5.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b34723f246bcca13228a044fc51f48bf931c3a5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">774\u00d7686 57.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThis is in default mode.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b66c68633ed46f80d7b8b50e1e19621a913b883.jpeg\" data-download-href=\"/uploads/short-url/aL20m5h9yqvRK4mkokm6Ph63ytd.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b66c68633ed46f80d7b8b50e1e19621a913b883.jpeg\" alt=\"image\" data-base62-sha1=\"aL20m5h9yqvRK4mkokm6Ph63ytd\" width=\"657\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/b/4b66c68633ed46f80d7b8b50e1e19621a913b883_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">667\u00d7507 54.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I fail to upload the original nd2 file. But the same problem also occurs to my colleagues when opening any nd2 images after updating ImageJ. As I have noticed, the built-in sample tiff files or VSI files are opened properly with or without Bio-Formats.</p>", "<p>If you view the OME-XML, would you be able to post the Channel elements? It should look something similar to below:</p>\n<pre><code class=\"lang-auto\">&lt;Channel Color=\"-1\" EmissionWavelength=\"685.0\" EmissionWavelengthUnit=\"nm\" ID=\"Channel:0:0\" Name=\"CSU far red RNA\" SamplesPerPixel=\"1\"&gt;\n&lt;Channel Color=\"-16776961\" EmissionWavelength=\"607.0\" EmissionWavelengthUnit=\"nm\" ID=\"Channel:0:1\" Name=\"CSU red RNA\" SamplesPerPixel=\"1\"&gt;\n&lt;Channel Color=\"16711935\" EmissionWavelength=\"525.0\" EmissionWavelengthUnit=\"nm\" ID=\"Channel:0:2\" Name=\"CSU green RNA\" SamplesPerPixel=\"1\"&gt;\n&lt;Channel Color=\"65535\" EmissionWavelength=\"405.0\" EmissionWavelengthUnit=\"nm\" ID=\"Channel:0:3\" Name=\"CSU blue RNA\" SamplesPerPixel=\"1\"&gt;\n&lt;Channel Color=\"-1\" ID=\"Channel:0:4\" Name=\"CSU BF\" SamplesPerPixel=\"1\"&gt;\n</code></pre>", "<p>Thanks for replying.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c30c28bc4567c91e865672784fe9b83d5886ab22.png\" data-download-href=\"/uploads/short-url/rPt9LzImDbjc22R7nvo4vUcF5Au.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30c28bc4567c91e865672784fe9b83d5886ab22_2_690x343.png\" alt=\"image\" data-base62-sha1=\"rPt9LzImDbjc22R7nvo4vUcF5Au\" width=\"690\" height=\"343\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30c28bc4567c91e865672784fe9b83d5886ab22_2_690x343.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30c28bc4567c91e865672784fe9b83d5886ab22_2_1035x514.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30c28bc4567c91e865672784fe9b83d5886ab22_2_1380x686.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c30c28bc4567c91e865672784fe9b83d5886ab22_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1422\u00d7707 241 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIt is like this.</p>", "<p>Hi, <a class=\"mention\" href=\"/u/dgault\">@dgault</a> !<br>\nWhat\u2019s wrong with these elements? It seems that my channel elements lack \u201cname=*\u201d compared to your example.</p>", "<p>Yeah, it does seem like some of the channel metadata has not been populated correctly. I would really need to see a sample file to understand exactly why this is happening. Would you be able to try uploading a sample again? You could try <a href=\"https://zenodo.org/\">https://zenodo.org/</a> as a public upload location if you do not have access to one.</p>", "<p>I uploaded a sample file as followed. Thanks again</p><aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      \n      <a href=\"https://zenodo.org/record/4662744\" target=\"_blank\" rel=\"noopener nofollow ugc\">Zenodo</a>\n  </header>\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://zenodo.org/record/4662744\" target=\"_blank\" rel=\"noopener nofollow ugc\">sample image</a></h3>\n\n<p>The nd2 image is opened in an improper way. It lacks information on channel colours.</p>\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Thanks Zoey, I was able to debug the file and believe I have found the source of the problem. I have opened a Bio-Formats PR which will hopefully fix the issue: <a href=\"https://github.com/ome/bioformats/pull/3679\" class=\"inline-onebox\">ND2: Always fall back to check channel names by dgault \u00b7 Pull Request #3679 \u00b7 ome/bioformats \u00b7 GitHub</a></p>", "<p>Thanks a lot, <a class=\"mention\" href=\"/u/dgault\">@dgault</a> !<br>\nI hope that the update would be available as fast as possible. <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"></p>", "<p>I\u2019m having the same problem, but in the metadata I can see the appropriate wavelengths\u2026<br>\nI\u2019m wondering if it\u2019s possible to retrieve wavelengths info from metadata with a macro to then merge the mono channels based on their original wavelengths</p>\n<p>Thanks</p>", "<p>Hi <a class=\"mention\" href=\"/u/icala\">@ICala</a>, can you confirm which version of Bio-Formats you are using? The original issue in this thread should have been fixed in any version over 6.7.0.</p>", "<p>Dear David thanks for your reply and apologize me for the delay.<br>\nMy Bio-format version is the 6.10.1-20220731.012559</p>", "<p>Thanks <a class=\"mention\" href=\"/u/icala\">@ICala</a>, if you update to the latest Bio-Formats 6.12.0 do you still see the issue?</p>\n<p>When updating if you select manage update sites and de-select the Bio-Formats site and instead have the Java-8 site selected. It is a little confusing but the Java-8 has the official release versions while the Bio-Formats site has regular updates of pre-release development versions for testing.</p>"], "77380": ["<p>Hi all,</p>\n<p>I\u2019m trying to segment 3D cells in image stacks with YFP+ fluorescent cells and YFP- cells. I am utilizing CLIJ2 to develop a pipeline for cell segmentation, but I\u2019m having trouble identifying YFP- cells (the dark holes in the C2 channel, labeled by DAPI in the C3 channel). I tried 3 different approaches, but ImageJ insists on threshold the YFP+ cells despite my settings.</p>\n<p>You can access my full pipeline here:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/dTaxYxkLQyWhlN5lZdsyJfXCFWB.ijm\">CLIJ_Pipeline_3D_YFP.ijm</a> (4.2 KB)</p>\n<p>These are the original images that have been pre-processed with deconvolution and a median filter:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/wXfM3BmtLj713qY41vouBLPmWtJ.tif\">C1-YB536_f690_S1_MD.tif</a> (6.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/rCgTkQxsQIjHShRPscNQE1LhK2Y.tif\">C2-YB536_f690_S1_MD.tif</a> (6.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/y2vsgDvbSbC3ZFTQ3ne0FUF50YW.tif\">C3-YB536_f690_S1_MD.tif</a> (6.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/jjmZmjzmUPBpkuOvAxrBCHAJvuW.tif\">C5-YB536_f690_S1_MD.tif</a> (6.0 MB)</p>\n<p>C2 is the original image of the YFP cells<br>\nC5 is the inverted image of C2<br>\nC3 is the DAPI channel<br>\nC1 can be ignored</p>\n<p><strong>Method 1: I used Image \u2192 Adjust \u2192 Threshold to select cells below a specific intensity cut-off, and I set the background as bright</strong></p>\n<pre><code class=\"lang-auto\">run(\"Threshold...\");\nsetThreshold(0, 11000, \"raw\");\nrun(\"Convert to Mask\", \"method=Otsu background=Light calculate create\");\n</code></pre>\n<p>However, even though the preview perfectly marked out the dark YFP- cells (red), the image on the left was the product when I applied the settings. As you can see, the actual cells that were thresholded correspond to the bright YFP+ cells.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/74245c33e6be07493713ba1976bb938545523b4a.jpeg\" data-download-href=\"/uploads/short-url/gzrhQzBkJVfxR8O6Yh3r7En65js.jpeg?dl=1\" title=\"Screenshot 2023-02-15 at 6.02.19 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/74245c33e6be07493713ba1976bb938545523b4a_2_690x365.jpeg\" alt=\"Screenshot 2023-02-15 at 6.02.19 PM\" data-base62-sha1=\"gzrhQzBkJVfxR8O6Yh3r7En65js\" width=\"690\" height=\"365\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/74245c33e6be07493713ba1976bb938545523b4a_2_690x365.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/74245c33e6be07493713ba1976bb938545523b4a_2_1035x547.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/4/74245c33e6be07493713ba1976bb938545523b4a_2_1380x730.jpeg 2x\" data-dominant-color=\"453F3F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-15 at 6.02.19 PM</span><span class=\"informations\">1920\u00d71017 80.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>Method 2: I used a CLIJ2 pipeline to threshold for bright cells in an inverted image</strong></p>\n<pre><code class=\"lang-auto\">// invert YFP\nExt.CLIJ2_subtractImageFromScalar(rawNEG, invtNEG, 255);\nExt.CLIJ2_pull(invtNEG);\n\n// threshold otsu\nExt.CLIJ2_automaticThreshold(rawYFP, thYFP, \"Otsu\");\nExt.CLIJ2_automaticThreshold(invtNEG, thNEG, \"Otsu\");\nExt.CLIJ2_pull(thYFP);\nExt.CLIJ2_pull(thNEG);\n</code></pre>\n<p>However, ImageJ seems to have processed the image as dark cells with a bright background. The top 2 images below are the thresholding results for the original C2 YFP image, and the bottom 2 images are that of the inverted image:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/4/843ce736c89e9919081be113e0107ce41731535f.jpeg\" data-download-href=\"/uploads/short-url/iRPvEAJt75o1XpNd9h6Zupsqsph.jpeg?dl=1\" title=\"download\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/843ce736c89e9919081be113e0107ce41731535f_2_447x500.jpeg\" alt=\"download\" data-base62-sha1=\"iRPvEAJt75o1XpNd9h6Zupsqsph\" width=\"447\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/843ce736c89e9919081be113e0107ce41731535f_2_447x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/843ce736c89e9919081be113e0107ce41731535f_2_670x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/4/843ce736c89e9919081be113e0107ce41731535f_2_894x1000.jpeg 2x\" data-dominant-color=\"767676\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">download</span><span class=\"informations\">1920\u00d72145 155 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>Method 3: This is the approach used in the pipeline above. I masked the inverted image with an inverted binary image from YFP cell thresholding, and then I applied a threshold</strong></p>\n<pre><code class=\"lang-auto\">// threshold otsu\nExt.CLIJ2_automaticThreshold(rawYFP, thYFP, \"Otsu\");\nExt.CLIJ2_pull(thYFP);\n\n// invert YFP\nExt.CLIJ2_subtractImageFromScalar(rawNEG, invtNEG, 255);\nExt.CLIJ2_invert(thYFP, invtYFP);\nExt.CLIJ2_pull(invtNEG);\nExt.CLIJ2_pull(invtYFP);\n\n// mask YFP\nExt.CLIJ2_mask(invtNEG, invtYFP, maskNEG);\nExt.CLIJ2_automaticThreshold(maskNEG, thNEG, \"Otsu\");\nExt.CLIJ2_pull(thNEG);\n</code></pre>\n<p>However, ImageJ seemed to have ignored my mask and thresholded the YFP cells anyways. The 1st image is the original YFP thresholding image, 2nd image is the inverted binary image, 3rd image is the inverted original image, and 4th image is the result of the masked thresholding command:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c758feb232f83db6efbe8c3b4ca63755d3741390.jpeg\" data-download-href=\"/uploads/short-url/srvGC5mvuwUy0KEBCnzsGXj9Ik0.jpeg?dl=1\" title=\"Screenshot 2023-02-16 at 4.29.30 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c758feb232f83db6efbe8c3b4ca63755d3741390_2_690x193.jpeg\" alt=\"Screenshot 2023-02-16 at 4.29.30 PM\" data-base62-sha1=\"srvGC5mvuwUy0KEBCnzsGXj9Ik0\" width=\"690\" height=\"193\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c758feb232f83db6efbe8c3b4ca63755d3741390_2_690x193.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c758feb232f83db6efbe8c3b4ca63755d3741390_2_1035x289.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c758feb232f83db6efbe8c3b4ca63755d3741390_2_1380x386.jpeg 2x\" data-dominant-color=\"AEAEAD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-16 at 4.29.30 PM</span><span class=\"informations\">1920\u00d7538 66.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Overall, I\u2019m confused why I can\u2019t complete such a simple goal, and I would really appreciate any support. I don\u2019t understand why even manual thresholding is not working for me.</p>\n<p>Also, Is there a way that I can mask the DAPI channel with the YFP threshold and then use the remaining DAPI identifiers as a seed for labeling YFP- (dark) cells? Thank you!</p>", "<pre><code class=\"lang-auto\">run(\"Threshold...\");\nsetThreshold(0, 11000, \"raw\");\nrun(\"Convert to Mask\", \"method=Otsu background=Light calculate create\");\n</code></pre>\n<p>The first two lines above do not do anything towards the result.<br>\nIt is only the third line that matters and I do not think there is a bug, but the Otsu algorithm which you chose to create the binary mask does not detect the dark blobs as an object. It detects the bright blobs versus the rest of the image.<br>\nThe preview (line 2) is just a manual selection (that gets replaced by the following line). There is no guarantee that any automated threshold will return that result set manually.</p>", "<p>Hi <a class=\"mention\" href=\"/u/gabriel\">@gabriel</a></p>\n<p>Thank you for explaining this! Do you know if there are any methods to set a manual threshold to select dark holes corresponding to the tail of a continuous intensity histogram?</p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a></p>\n<p>Would it not be more accurate to count the number of nuclei in the DAPI channel and the number of YFP+ cells from the YFP channel and calculate the difference to generate the number of YFP- cells?<br>\nHow do you know a \u201cblack hole\u201d corresponds to a cell and not an actual hole without considering the presence of DAPI?</p>\n<p>Below is an image where I have segmented the YFP+ cell channel (63 objects) and DAPI channel (93 objects) from your C2and C3 images provided above. I have processed the Maximum intensity projection to be quicker but this can in principle be done in 3D. I used the Mean thresholding algorithm and the watershed operation to split touching objects:<br>\nBelow an overlay of the YFP+ cells (magenta + magenta outlines) and the DAPI signal (green + green outlines):<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a475d21e43a83c88e45ca4cc89fa3cbf7463dad7.jpeg\" data-download-href=\"/uploads/short-url/nsSJzkH1Za3gnQlLnpp7mQEthPx.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a475d21e43a83c88e45ca4cc89fa3cbf7463dad7_2_500x500.jpeg\" alt=\"image\" data-base62-sha1=\"nsSJzkH1Za3gnQlLnpp7mQEthPx\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a475d21e43a83c88e45ca4cc89fa3cbf7463dad7_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a475d21e43a83c88e45ca4cc89fa3cbf7463dad7_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/4/a475d21e43a83c88e45ca4cc89fa3cbf7463dad7_2_1000x1000.jpeg 2x\" data-dominant-color=\"120D12\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1024\u00d71024 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Does this help?</p>", "<aside class=\"quote no-group\" data-username=\"Slaine_Troyard\" data-post=\"3\" data-topic=\"77380\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/slaine_troyard/40/66032_2.png\" class=\"avatar\"> Slaine:</div>\n<blockquote>\n<p>Thank you for explaining this! Do you know if there are any methods to set a manual threshold to select dark holes corresponding to the tail of a continuous intensity histogram?</p>\n</blockquote>\n</aside>\n<p>You could try removing the white blobs from the image using greyscale reconstruction and re-threshold with some method to see if now the black blobs become detectable.</p>\n<ol>\n<li>Convert 16bit to 8bit</li>\n<li>Invert the image (now the bright blobs become dark)</li>\n<li>run the Fill_Greyscale_Holes macro (this is in the Morphology collection, link below)</li>\n<li>Invert the image again, (so the blobs you are interested in are again dark)</li>\n<li>Apply the threshold (in my test with your image, the MinError method returns the result below):</li>\n</ol>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/6/56a16b6a55baa410d0c0c73df8d7d558aa8519f0.png\" alt=\"slice:30-1\" data-base62-sha1=\"cmmTWAoWwDyyIFRiIfOUFjhMkG4\" width=\"341\" height=\"341\"></p>\n<p>The Morphology collection for ImageJ is here:<br>\n<a href=\"https://blog.bham.ac.uk/intellimic/g-landini-software/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://blog.bham.ac.uk/intellimic/g-landini-software/</a></p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/landinig/IJ-Morphology/blob/953503d11701d126b9113d51394dcfcfcddbb7fd/morphology.zip\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/landinig/IJ-Morphology/blob/953503d11701d126b9113d51394dcfcfcddbb7fd/morphology.zip\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/landinig/IJ-Morphology/blob/953503d11701d126b9113d51394dcfcfcddbb7fd/morphology.zip\" target=\"_blank\" rel=\"noopener nofollow ugc\">landinig/IJ-Morphology/blob/953503d11701d126b9113d51394dcfcfcddbb7fd/morphology.zip</a></h4>\n\n\n  This file is binary. <a href=\"https://github.com/landinig/IJ-Morphology/blob/953503d11701d126b9113d51394dcfcfcddbb7fd/morphology.zip\" target=\"_blank\" rel=\"noopener nofollow ugc\">show original</a>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nIn Fiji you can get it via the Morphology update site.</p>", "<p>Hi <a class=\"mention\" href=\"/u/marie-nkaefer\">@Marie-nkaefer</a></p>\n<p>Thank you for offering an alternative solution, but I do need to an accurate approximation of the size of the yfp- cells for downstream colocalization analysis with the C1 channel. I plan to verify the dark holes based on the presence of DAPI nuclei.</p>\n<p>I\u2019m relatively new to colocalization analysis, would you mind sharing what types of image math operations or plugins you usually use for 3D cell segmentation? I need to quantify the volume and average intensity of each segmented object. I also want to quantify the number of spots from C1 colocalized with each segmented cell.</p>\n<p>Thank you!</p>", "<p>Hi <a class=\"mention\" href=\"/u/gabriel\">@gabriel</a></p>\n<p>Thank you so much for the recommendation! I will certainly test out this method!</p>", "<p>For 3D segmentation you could try the 3D object counter. <a href=\"https://imagej.net/plugins/3d-objects-counter\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">3D Objects Counter</a><br>\nor BoneJ, or the 3D ImageJ Suite. There are a multitude of options.</p>\n<p>I would recommend to work out the workflow in 2D first and then scale up to 3D. That way you will get to your end goal quicker.</p>\n<p>For co-occurence of DAPI with YFP- you could</p>\n<ul>\n<li>measure amount of DAPI in object and determine a threshold where you would consider an object to contain a nucleus or not</li>\n<li>look at the ROI logical operators in the ROI manager (<a href=\"https://imagej.nih.gov/ij/docs/guide/146-30.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ImageJ User Guide - IJ 1.46r | Analyze Menu</a>). These logical operators only work on 2D images but you could iterate slice-wise over the whole stack.</li>\n<li>calculate Jaccard index of objects via MorpholibJ or CLIJ<br>\n-\u2026</li>\n</ul>", "<p>Hi <a class=\"mention\" href=\"/u/marie-nkaefer\">@Marie-nkaefer</a></p>\n<p>Sorry I didn\u2019t see this. Thank you so much for the great suggestions!</p>"], "77390": ["<p>Hi,</p>\n<p>On my windows, cellpose was detecting GPU. However, recently I reinstalled it on the same system, and it\u2019s not detecting the GPU. It gives me an error, \u201cTORCH CUDA version not installed/working\u201d. Does anyone have a solution to this problem?</p>\n<p>Best wishes,<br>\nAftab</p>", "<p>Hi <a class=\"mention\" href=\"/u/toxinbiologist\">@ToxinBiologist</a><br>\nMaybe you can try installing torch and cuda as explained <a href=\"https://pytorch.org/get-started/locally/\" rel=\"noopener nofollow ugc\">here</a>. If you are using Anaconda you can try running the command below<br>\n<code>conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia</code><br>\nHope it helps, best wishes</p>", "<p>Hi Pau,</p>\n<p>Thanks, I already tried this command. Unfortunately, it\u2019s still not working on Anaconda.</p>\n<p>-Aftab</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/8071a458352f030427424276551b043733d83fbf.gif\" data-download-href=\"/uploads/short-url/ikgALyDcJrtP6nBI9gnenNSdMTl.gif?dl=1\" title=\"Error message\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/8071a458352f030427424276551b043733d83fbf.gif\" alt=\"Error message\" data-base62-sha1=\"ikgALyDcJrtP6nBI9gnenNSdMTl\" width=\"690\" height=\"366\" data-dominant-color=\"232424\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Error message</span><span class=\"informations\">982\u00d7522 28.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThe system get stuck with solving environment:</p>", "<p>I finally made it work with the following command. \u201cpip3 install torch torchvision torchaudio --extra-index-url <a href=\"https://download.pytorch.org/whl/cu117\" rel=\"noopener nofollow ugc\">https://download.pytorch.org/whl/cu117</a>\u201d</p>", "<p>For anyone else with this problem, I would like to add that it may seem like CUDA installation is stuck but it just happens to take forever (~40 min sometimes).</p>\n<p>You can really see if conda is doing something by typing -vv (verbose output) at the end of the command. Lots of debugging info will appear but it will give you a sense of progression.</p>"], "11890": ["<p>Dear all,</p>\n<p>Earlier today everything was fine and I was working in Fiji on .lif files. Unfortunately there seems to have been a temporary problem with the WiFi while Fiji was downloading an update. I noticed when trying to cut a part of an image out, because the newly generated image was completely black. I quit Fiji and tried to reopen it, but it no longer opened. Concluding that the aborted update must have corrupted the program, I deleted the version of Fiji I had and downloaded another one. This new version (ImageJ 1.52d, Java 1.8.0_172 [64-bit]) does manage to open the program, but fails to open my image files. Whichever way I try, whether via drag&amp;drop, File&gt;Open or File&gt;Import&gt;Bio-Formats, I always get error messages. Using the latter, the error message is:</p>\n<pre><code class=\"lang-nohighlight\">(Fiji Is Just) ImageJ 2.0.0-rc-68/1.52d; Java 1.8.0_172 [64-bit]; Mac OS X 10.13.5; 139MB of 2464MB (5%)\n \njava.lang.NullPointerException\n\tat org.scijava.nativelib.NativeLibraryUtil.getPlatformLibraryPath(NativeLibraryUtil.java:189)\n\tat org.scijava.nativelib.NativeLibraryUtil.loadNativeLibrary(NativeLibraryUtil.java:308)\n\tat loci.formats.services.JPEGTurboServiceImpl.&lt;init&gt;(JPEGTurboServiceImpl.java:107)\n\tat loci.formats.in.NDPIReader.&lt;init&gt;(NDPIReader.java:69)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.lang.Class.newInstance(Class.java:442)\n\tat loci.formats.ImageReader.&lt;init&gt;(ImageReader.java:129)\n\tat loci.formats.in.FilePatternReader.&lt;init&gt;(FilePatternReader.java:77)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.lang.Class.newInstance(Class.java:442)\n\tat loci.formats.ImageReader.&lt;init&gt;(ImageReader.java:129)\n\tat loci.plugins.util.LociPrefs.makeImageReader(LociPrefs.java:100)\n\tat loci.plugins.in.ImportProcess.createBaseReader(ImportProcess.java:619)\n\tat loci.plugins.in.ImportProcess.initializeReader(ImportProcess.java:485)\n\tat loci.plugins.in.ImportProcess.execute(ImportProcess.java:138)\n\tat loci.plugins.in.Importer.showDialogs(Importer.java:140)\n\tat loci.plugins.in.Importer.run(Importer.java:76)\n\tat loci.plugins.LociImporter.run(LociImporter.java:78)\n\tat ij.IJ.runUserPlugIn(IJ.java:228)\n\tat ij.IJ.runPlugIn(IJ.java:192)\n\tat ij.Executer.runCommand(Executer.java:137)\n\tat ij.Executer.run(Executer.java:66)\n\tat java.lang.Thread.run(Thread.java:748)\n</code></pre>\n<p>I would be very grateful for any tips on how to resolve this issue.<br>\nIn case this matters, I am using a MacBook Air (13-inch, Mid 2012) running High Sierra (10.13.5).<br>\nThank you!</p>", "<p>I could reproduce this problem opening .lsm; .lif or .oir when I used Bio-Formats Importer.</p>\n<p>(Fiji Is Just) ImageJ 2.0.0-rc-68/1.52d; Java 1.8.0_66 [64-bit]; Windows 10 10.0; 70MB of 24448MB (&lt;1%)</p>\n<p>java.lang.NullPointerException<br>\n\u2026</p>", "<p>Please see my answer on this related forum topic:</p>\n<aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"11892\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/imagejan/40/14151_2.png\" class=\"avatar\">\n    <a href=\"http://forum.image.sc/t/fiji-does-not-open-deltavision-dv-files-with-bioformat-importer-window-anymore/11892/2\">Fiji does not open DeltaVision .dv files with Bioformat Importer window anymore</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    I can reproduce this problem with an up-to-date Fiji installation. The issue is caused by the library native-lib-loader-2.3.0.jar in the ./Fiji.app/jars folder. \nAs a workaround, you can downgrade to version 2.2.0 of this file by deleting it and replacing it with native-lib-loader-2.2.0.jar downloaded from here: \n<a href=\"http://maven.imagej.net/service/local/repositories/central/content/org/scijava/native-lib-loader/2.2.0/native-lib-loader-2.2.0.jar\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">http://maven.imagej.net/service/local/repositories/central/content/org/scijava/native-lib-loader/2.2.0/native-lib-loader-2.2.0.jar</a> \nThis worked for me for opening czi files which are al\u2026\n  </blockquote>\n</aside>\n", "<p>I do have the exact same problem of opening .lif and .czi files. I tried the solution you provide, but I get another message:<br>\nThe Java JAR file \u201cnative-lib-loader-2.2.0.jar\u201d could not be launched. Check the Console for possible error message.</p>\n<p>I use a Macbook Pro with High Sierra</p>\n<p>Please help!</p>", "<p>This solution has worked for me, thank you very much!</p>", "<p>The easiest way to solve this issue: <a href=\"https://mac.eltima.com/how-to-open-jar-file-mac.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How to Open a JAR File on Mac Computer | Commander One</a></p>"], "77460": ["<p>I have a large set of images with many rectangular annotations. However, the rectangles are a slightly different size and I want to standardize this to be 361um x 270um. I know you can create annotations of a specific size through the GUI but how would I go about editing the size of existing annotations in batch to be this particular size?</p>\n<p>Thanks for your help!</p>", "<p>How would you want to do this? Adjust the length and width to be a match thus keeping the upper left?<br>\nGet the centroid and create a new object on the centroid and delete the old one?<br>\nSomething else?<br>\nI suspect it will be easiest to delete the annotations and replace them, but not sure about all the new functions.</p>\n<p>Are all of the annotations rectangles or only a subset?</p>", "<p>I don\u2019t have a preference. I\u2019m open to either of those options as they should do what I\u2019d be looking for and I can always do some manual adjustment of the locations afterwards. Therefore, whatever is more straightforward to implement would seem like the best way forward.</p>\n<p>I do have other annotations besides rectangles on the images but only want to modify the rectangular ones.</p>", "<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/ROIs.html#createRectangleROI(double,double,double,double,qupath.lib.regions.ImagePlane)\">\n  <header class=\"source\">\n\n      <a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/ROIs.html#createRectangleROI(double,double,double,double,qupath.lib.regions.ImagePlane)\" target=\"_blank\" rel=\"noopener\">qupath.github.io</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://qupath.github.io/javadoc/docs/qupath/lib/roi/ROIs.html#createRectangleROI(double,double,double,double,qupath.lib.regions.ImagePlane)\" target=\"_blank\" rel=\"noopener\">ROIs (QuPath 0.4.0)</a></h3>\n\n  <p>declaration: package: qupath.lib.roi, class: ROIs</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<pre><code class=\"lang-auto\">rectangles = getAnnotationObjects().findAll{it.getName().contains(\"Rectangle\")}\nsizeX = 200 //pixels\nsizeY = 250 //pixels\nnewObjects = []\nfor (r in rectangles){\nROI = r.getROI()\n//upper left\ncx = ROI.getCentroidX() - sizeX/2\ncy =ROI.getCentroidY() - sizeY/2\nroi = ROIs.createRectangleROI(cx, cy, sizeX, sizeY)\nnewObjects &lt;&lt; PathObject.createAnnotation(roi)\n}\nremoveObjects(rectangles, true)\naddObjects(newObjects)\n\nfireHierarchyUpdate()\n</code></pre>\n<p>Not 100% sure that will work since writing it on the fly but give it a shot. If you want to use microns, you can find other examples of converting pixel values to microns on the forum and integrate that at the top of the script.<br>\nThat\u2019s the basic idea though.<br>\nAnother example of creating rectangles you can find on the forum <a href=\"https://forum.image.sc/t/digital-chalkley-point-graticule-overlay/51211/25\" class=\"inline-onebox\">Digital Chalkley point graticule overlay - #25 by Research_Associate</a><br>\nOr look for posts involving \u201ccreateRectangleROI\u201d</p>", "<p>Thanks! So I looked and things may be a bit more complicated. The reason why I need to change size is because the annotations have been warped and transferred and the resulting geojson files have been brought back into QuPath. Therefore, it looks like they are now of the polygon ROI rather than rectangle (image attached) so the script fails at line 1.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/a/4a5237c88068e0d9eae925a11008298319322b88.png\" data-download-href=\"/uploads/short-url/aBttZWZBWL9y1d1ovBJygw3Ct4s.png?dl=1\" title=\"Screen Shot 2023-02-19 at 12.19.38 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/a/4a5237c88068e0d9eae925a11008298319322b88_2_690x415.png\" alt=\"Screen Shot 2023-02-19 at 12.19.38 PM\" data-base62-sha1=\"aBttZWZBWL9y1d1ovBJygw3Ct4s\" width=\"690\" height=\"415\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/a/4a5237c88068e0d9eae925a11008298319322b88_2_690x415.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/a/4a5237c88068e0d9eae925a11008298319322b88.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/a/4a5237c88068e0d9eae925a11008298319322b88.png 2x\" data-dominant-color=\"F2F2F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-19 at 12.19.38 PM</span><span class=\"informations\">788\u00d7474 39.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Can the annotations be retrieved by a list of names rather than get rectangles?</p>\n<p>I did try adding in a rectangle in to an image to check the rest of the script and found the same error:</p>\n<p>ERROR: Cannot invoke method contains() on null object in QuPathScript at line number 1</p>\n<p>ERROR: org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:95)<br>\norg.codehaus.groovy.vmplugin.v8.IndyGuardsFiltersAndSignatures.invokeGroovyObjectInvoker(IndyGuardsFiltersAndSignatures.java:149)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nQuPathScript$_run_closure1.doCall(QuPathScript:1)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)<br>\njava.base/java.lang.reflect.Method.invoke(Unknown Source)<br>\norg.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:343)<br>\ngroovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:328)<br>\norg.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:279)<br>\ngroovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1009)<br>\norg.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:39)<br>\norg.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)<br>\norg.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)<br>\norg.codehaus.groovy.runtime.callsite.BooleanReturningMethodInvoker.invoke(BooleanReturningMethodInvoker.java:49)<br>\norg.codehaus.groovy.runtime.callsite.BooleanClosureWrapper.call(BooleanClosureWrapper.java:52)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.findMany(DefaultGroovyMethods.java:4834)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.findAll(DefaultGroovyMethods.java:4688)<br>\norg.codehaus.groovy.runtime.DefaultGroovyMethods.findAll(DefaultGroovyMethods.java:4675)<br>\norg.codehaus.groovy.runtime.dgm$247.doMethodInvoke(Unknown Source)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nQuPathScript.run(QuPathScript:2)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>", "<p>The findall is standard groovy to figure out which objects you want. If your objects have the class ACST as shown you could</p>\n<pre><code class=\"lang-auto\">rectangles = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"ACST\")}\n</code></pre>\n<p>Or whatever gets the correct subset of objects.</p>\n<p>You can find other examples by searching: <a href=\"https://forum.image.sc/t/how-to-rename-the-tiles-in-qupath/74166/14\" class=\"inline-onebox\">How to rename the tiles in qupath? - #14 by ym.lim</a></p>", "<p>Thanks! That helped. I\u2019ve gotten past that bit and then hit the following error:</p>\n<p>ERROR: It looks like you\u2019ve tried to access a property \u2018main\u2019 that doesn\u2019t exist</p>\n<p>ERROR: No such property: main for class: QuPathScript in QuPathScript at line number 6</p>\n<p>ERROR: org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:67)<br>\norg.codehaus.groovy.vmplugin.v8.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:161)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nQuPathScript.run(QuPathScript:6)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>Could this be because the rectangles are found within other annotations (image attached):</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/444546d432b49bf2e79a20a5303904f07d02fc78.jpeg\" data-download-href=\"/uploads/short-url/9JWTl5wE0vzUIHaYkqhotZ6PvYc.jpeg?dl=1\" title=\"Screen Shot 2023-02-19 at 3.26.57 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/444546d432b49bf2e79a20a5303904f07d02fc78_2_369x500.jpeg\" alt=\"Screen Shot 2023-02-19 at 3.26.57 PM\" data-base62-sha1=\"9JWTl5wE0vzUIHaYkqhotZ6PvYc\" width=\"369\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/444546d432b49bf2e79a20a5303904f07d02fc78_2_369x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/444546d432b49bf2e79a20a5303904f07d02fc78_2_553x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/444546d432b49bf2e79a20a5303904f07d02fc78_2_738x1000.jpeg 2x\" data-dominant-color=\"E1DADF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-19 at 3.26.57 PM</span><span class=\"informations\">788\u00d71066 67.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>right, that was my mistake/miscopy. The original script looked for a main object. That should be <code>r</code> instead since it is cycling through all <code>r</code>s.<br>\nI think the code above has been adjusted to remove the mainROIs</p><aside class=\"quote quote-modified\" data-post=\"4\" data-topic=\"58870\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/melvingelbard/40/26268_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/how-to-split-wand-annotation-into-quadrant-tiles/58870/4\">How to split wand annotation into quadrant tiles?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    It\u2019s not the most trivial operation to be honest, and there are many ways to do this. \nFor instance, if you have something like this: \n <a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/599aa75ebedf72e68433ff06c6c5a6fc168814cb.jpeg\" data-download-href=\"/uploads/short-url/cMFQ1rCCgUAeLM1I9OoxTGvYSoj.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\">[image]</a> \nYou could write a script that create rectangle ROIs from the boundaries of the main annotation, then calculate the intersections with the main annotation\u2019s ROI. After that you can add them to the hierarchy. E.g.: \nimport qupath.lib.roi.GeometryTools\n\ndef main = getSelectedObject()\ndef mainROI = main.getROI()\n\ndef x = mainROI.getBoundsX()\ndef y = mainROI\u2026\n  </blockquote>\n</aside>\n", "<p>That helped! Getting closer it seems. Just hit the following:</p>\n<p>ERROR: It looks like you\u2019ve tried to access a method that doesn\u2019t exist.</p>\n<p>ERROR: No signature of method: qupath.lib.objects.PathAnnotationObject.getCentroidX() is applicable for argument types: () values: <span class=\"chcklst-box fa fa-square-o fa-fw\"></span> in QuPathScript at line number 8</p>\n<p>ERROR: org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:72)<br>\norg.codehaus.groovy.vmplugin.v8.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:161)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nQuPathScript.run(QuPathScript:8)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>", "<p>Yep, changed too much, the other mainROIs were fine. Still, adjusted the variables to be a little bit more accurate. r is an annotation, ROI is an ROI.</p>", "<p>Thanks! Line 10 seems to then throw an error:</p>\n<p>ERROR: It looks like you\u2019ve tried to access a method that doesn\u2019t exist.</p>\n<p>ERROR: No signature of method: static qupath.lib.roi.ROIs.createRectangleROI() is applicable for argument types: (Double, Double, BigDecimal, BigDecimal) values: [22365.183285, 14432.7710665625, 1432.53968, 1071.42857]<br>\nPossible solutions: createRectangleROI(double, double, double, double, qupath.lib.regions.ImagePlane), createRectangleROI(qupath.lib.regions.ImageRegion) in QuPathScript at line number 10</p>\n<p>ERROR: groovy.lang.MetaClassImpl.invokeStaticMissingMethod(MetaClassImpl.java:1656)<br>\ngroovy.lang.MetaClassImpl.invokeStaticMethod(MetaClassImpl.java:1642)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nQuPathScript.run(QuPathScript:10)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>", "<p>Not sure, try putting a double in front of sizeX and sizeY</p><aside class=\"quote quote-modified\" data-post=\"3\" data-topic=\"76889\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/y/f17d59/40.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/qupath-errors-when-creating-annotations-and-second-errors-merge-objects/76889/3\">Qupath errors when creating annotations and second errors merge objects</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    Just to add that if \norg.locationtech.jts.geom.GeometryOverlay.isOverlayNG = true\n\ndoes not work (most of the time it resolves the issue, but there are occasional times it does not for me), there are several other options to try: \n\n\nGeometry Precision Reducer (<a href=\"https://forum.image.sc/t/topologyexception-found-non-noded-intersection-between-linestring/38549/10\" class=\"inline-onebox\">TopologyException: found non-noded intersection between LINESTRING - #10 by smcardle</a>) \n\n\nSimplifying the shape to decrease polygon vertices and hence probability for non-noded intersections. \n\n\n// Simplify annotations\nimport qupath.lib.roi\u2026\n  </blockquote>\n</aside>\n", "<p>Just tried and that seems to throw a similar error:</p>\n<p>ERROR: It looks like you\u2019ve tried to access a method that doesn\u2019t exist.</p>\n<p>ERROR: No signature of method: static qupath.lib.roi.ROIs.createRectangleROI() is applicable for argument types: (Double, Double, Double, Double) values: [22365.183285, 14432.7710665625, 1432.53968, 1071.42857]<br>\nPossible solutions: createRectangleROI(double, double, double, double, qupath.lib.regions.ImagePlane), createRectangleROI(qupath.lib.regions.ImageRegion) in QuPathScript at line number 10</p>\n<p>ERROR: groovy.lang.MetaClassImpl.invokeStaticMissingMethod(MetaClassImpl.java:1656)<br>\ngroovy.lang.MetaClassImpl.invokeStaticMethod(MetaClassImpl.java:1642)<br>\norg.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:318)<br>\nQuPathScript.run(QuPathScript:10)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:331)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:161)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>Not sure what qupath.lib.regions.ImagePlane refers to as part of the possible solution or createRectangleROI(qupath.lib.regions.ImageRegion)?</p>", "<p>Ah, yeah, just include the imageplane. More recent code examples have that included. <a href=\"https://forum.image.sc/t/view-miss-classified-patches-on-original-svs-file/50769/6\" class=\"inline-onebox\">View miss classified patches on original .svs file - #6 by petebankhead</a></p>", "<p>So I ran the following code in the end:</p>\n<pre><code class=\"lang-auto\">rectangles = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"ACST\")}\ndouble sizeX = 1432.53968 //pixels\ndouble sizeY = 1071.42857 //pixels\nnewObjects = []\nfor (r in rectangles){\nROI = r.getROI()\n//upper left\ncx = ROI.getCentroidX() - sizeX/2\ncy =ROI.getCentroidY() - sizeY/2\nroi = ROIs.createRectangleROI(cx, cy, sizeX, sizeY,ImagePlane.getDefaultPlane())\nnewObjects &lt;&lt; PathObjects.createAnnotationObject(roi)\n}\nremoveObjects(rectangles, true)\naddObjects(newObjects)\n\nfireHierarchyUpdate()\n</code></pre>\n<p>However, I went from the following picture:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/f/ff6c4ae3367e25574107fc4c2aee4497e68deebf.jpeg\" data-download-href=\"/uploads/short-url/ArzLMkPmoNUgh7w5lI7AREACtsP.jpeg?dl=1\" title=\"Screen Shot 2023-02-19 at 4.19.18 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff6c4ae3367e25574107fc4c2aee4497e68deebf_2_376x500.jpeg\" alt=\"Screen Shot 2023-02-19 at 4.19.18 PM\" data-base62-sha1=\"ArzLMkPmoNUgh7w5lI7AREACtsP\" width=\"376\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff6c4ae3367e25574107fc4c2aee4497e68deebf_2_376x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff6c4ae3367e25574107fc4c2aee4497e68deebf_2_564x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/f/ff6c4ae3367e25574107fc4c2aee4497e68deebf_2_752x1000.jpeg 2x\" data-dominant-color=\"E1D9DE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-19 at 4.19.18 PM</span><span class=\"informations\">826\u00d71096 73.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>To the following picture:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/10ebae7ec297dc33574146ce0a292b08cb5de627.jpeg\" data-download-href=\"/uploads/short-url/2pGAh1fQxjaHLRc1g9QXjzXJKCj.jpeg?dl=1\" title=\"Screen Shot 2023-02-19 at 4.20.02 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ebae7ec297dc33574146ce0a292b08cb5de627_2_345x500.jpeg\" alt=\"Screen Shot 2023-02-19 at 4.20.02 PM\" data-base62-sha1=\"2pGAh1fQxjaHLRc1g9QXjzXJKCj\" width=\"345\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ebae7ec297dc33574146ce0a292b08cb5de627_2_345x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ebae7ec297dc33574146ce0a292b08cb5de627_2_517x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ebae7ec297dc33574146ce0a292b08cb5de627_2_690x1000.jpeg 2x\" data-dominant-color=\"E3D6D7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-19 at 4.20.02 PM</span><span class=\"informations\">762\u00d71102 70.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I didn\u2019t realize the orientation wouldn\u2019t be preserved. Is there a way to potentially preserve that?</p>", "<p>Ah, that\u2019s probably why they aren\u2019t rectangles, but polygons. Also a different problem if you want orientation. If they were actual rectangles, you might be able to adjust the lengths, but you can\u2019t do the same with a polygon.</p>\n<p>Not really sure what the best option is here, but it would probably involve a bit of coding math since the objects aren\u2019t even all tilted the same way within a group. You\u2019ll need to do something like get the points of the polygon, hopefully only 4 of them, and calculate what the new points should be based on the old points.<br>\ngetAllPoints <a href=\"https://forum.image.sc/t/how-to-split-wand-annotation-into-quadrant-tiles/58870/11\" class=\"inline-onebox\">How to split wand annotation into quadrant tiles? - #11 by Mike_Nelson</a><br>\nBut you\u2019ll need to calculate the angles/slopes of the various lines and then choose the new coordinates based on the combination of old coordinates, slopes, and desired line lengths. You\u2019ll also need to keep track of which lines are long or short, since the orientation won\u2019t always be the same (can\u2019t just assume x1 y1 is the top corner).</p>\n<p>That is a whole lot trickier than simply resizing standard rectangles, but it should be possible with enough effort.</p>\n<p>Unless there\u2019s some new trick <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> has up his sleeve to make this easy. Not familiar with all the new 0.4.x stuff yet.</p>\n<p>Summary for Pete: Resize polygon annotation objects that are roughly rectangular but might not be perfect rectangles into rectangles with a given height/width.</p>", "<p>Oh so just to confirm: If I use specify annotation and insert a rectangle of a specific size, if I then rotate it the rectangle becomes a polygon and the area isn\u2019t preserved?</p>\n<p>For example (in the following picture), I inserted 2 rectangles of the same size (97470um2). One of them I left the same (colored golden). I then took one and rotated it (colored red). The red rectangle became a polygon ROI with a slightly different area (97470.1326um2). This may be naive but why is that?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/59e27f6df870fb9d30744ecfac738482d0264b4a.jpeg\" data-download-href=\"/uploads/short-url/cP9LnejCtLwtgHOuc0ee4y8SZIu.jpeg?dl=1\" title=\"Screen Shot 2023-02-19 at 5.11.23 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/59e27f6df870fb9d30744ecfac738482d0264b4a.jpeg\" alt=\"Screen Shot 2023-02-19 at 5.11.23 PM\" data-base62-sha1=\"cP9LnejCtLwtgHOuc0ee4y8SZIu\" width=\"690\" height=\"426\" data-dominant-color=\"CFBFBD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-19 at 5.11.23 PM</span><span class=\"informations\">892\u00d7552 52.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Pixels are essentially squares, so you are on a square coordinate grid. You aren\u2019t going to rotate something at an angle and be able to pin it to 100% accuracy at the same coordinates as when it started (even doubles end up getting <strong>rounded/truncated</strong> at some point). It shouldn\u2019t be toooo far off, but it won\u2019t be perfect.</p>\n<p>What happens when you rotate images is far worse, though, which is why it\u2019s generally better to transform the objects and accept some accuracy loss then rotate the image and damage it.</p>\n<p>If you specify a bunch of annotations then rotate them, they shouldn\u2019t be too far off. And is probably about the closest you will get to replicating the area.</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"17\" data-topic=\"77460\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>Unless there\u2019s some new trick <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> has up his sleeve to make this easy. Not familiar with all the new 0.4.x stuff yet.</p>\n<p>Summary for Pete: Resize polygon annotation objects that are roughly rectangular but might not be perfect rectangles into rectangles with a given height/width.</p>\n</blockquote>\n</aside>\n<p>If you can figure out the scale factor, then this could work:</p>\n<pre><code class=\"lang-groovy\">double scale = 0.9\ndef annotation = getSelectedObject()\n\ndef roi = annotation.getROI()\ndef roiScaled = roi.scale(scale, scale, roi.getCentroidX(), roi.getCentroidY())\nannotation.setROI(roiScaled)\n</code></pre>\n<aside class=\"quote no-group\" data-username=\"ADW123\" data-post=\"18\" data-topic=\"77460\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/adw123/40/52558_2.png\" class=\"avatar\"> ADW123:</div>\n<blockquote>\n<p>Oh so just to confirm: If I use specify annotation and insert a rectangle of a specific size, if I then rotate it the rectangle becomes a polygon and the area isn\u2019t preserved?</p>\n</blockquote>\n</aside>\n<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"19\" data-topic=\"77460\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>Pixels are essentially squares, so you are on a square coordinate grid. You aren\u2019t going to rotate something at an angle and be able to pin it to 100% accuracy at the same coordinates as when it started (even doubles end up getting <strong>rounded/truncated</strong> at some point). It shouldn\u2019t be toooo far off, but it won\u2019t be perfect.</p>\n<p>What happens when you rotate images is far worse, though, which is why it\u2019s generally better to transform the objects and accept some accuracy loss then rotate the image and damage it.</p>\n<p>If you specify a bunch of annotations then rotate them, they shouldn\u2019t be too far off. And is probably about the closest you will get to replicating the area.</p>\n</blockquote>\n</aside>\n<p>I\u2019d just add that the area is calculated using the pixel size information. Some scanners seem to store pixels that <em>aren\u2019t</em> exactly square (i.e. the pixel width and height are slightly different), which compounds any rounding issue.</p>", "<p>Thanks <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> and <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>!</p>\n<p>Since the area is not the same for each polygon, one scale factor wouldn\u2019t ultimately get the target area for each tile. Therefore, can the scale factor be systematically calculated using the current area of each polygon and the goal area (ie 97,470um2)? Given we know that the 0.9 scale factor seemed to provide a ratio of post-area to pre-area of 0.81 (makes sense given the double call to scale I imagine?) can the scale factor call be an equation that equals the target area/current area in the script and then cycle through each polygon?</p>"], "67256": ["<p>I have a RAMPS 1.4 connected to Arduino to try and control an X stage.<br>\nI have uploaded Marlin 1.1.9.1 and configured it to control the stage. I can connect to the Arduino through a G-Code sender called Pronterface and control the stage with Gcode commands. Homing.etc all seems to work as expected through pronterface.</p>\n<p>When I connect this to micromanager I cannot get the stage to move at all.<br>\nAll I am able to do is to set the origin through the stage position page. It then moves the axis to its home and says \u201creturn to original position\u201d at which point it does nothing.</p>\n<p>Any help would be most appreciated.</p>\n<p>Many Thanks<br>\nDan</p>", "<p>Welcome to this forum <a class=\"mention\" href=\"/u/hollandsd\">@hollandsd</a>!</p>\n<p>I have no experience with the RAMPS or its device adapter, but wonder how you configured things in Micro-Manager and how you tried to move the stage through the Micro-Manager interface.  Did you find the STage Control window?  The Stage Position List (which has the \u201cSet Origin\u201d button) records positions, but does not let you move the stage directly.</p>", "<p>Hi Nico</p>\n<p>I configured the stage through the hardware config page and then used the stage control window to try and move the stage.<br>\nI have had a little more of a play and it looks like the control software on the arduino behind the RAMPS board was not compatible with the RAMPS device adaptor. I was using Marlin (I tried 3 different versions) as the arduino software which is used in many 3d printers and I was expecting to be suitable. I had to revert to a very old bit of software called Sprinter to be able to get it to work. I suspect there was something in the communications protocol which caused it to not work.<br>\nI have now managed to get the stage to move correctly using the Sprinter software.</p>\n<p>Dan</p>", "<p>Hi,</p>\n<p>I found this Daniel\u2019s post and debugged a bit differences between marlin and sprinter\u2026 (just basic linux commands used\u2026 nothing fancy). I used an old arduino uno board and \u201d&gt;\u201d sprinter and marlin responses to a file, just to check is there something different with the format. And the visible/printed format was quite the same, but when cat -e theese files it started to come clear why marlin does not work with this \u201dplug-in\u201d</p>\n<p>The main reason why (any)marlin version does not work is, because line ending on sprinter is in \u201dwindows\u201d format and on marlin it is in unix format. Windows format is lfcr (line feed carriage return) and in unix it is just lf (line feed)</p>\n<p>that is why ramps \u201dplug-in\u201d does not recognize \u201dok\u201d lines correctly. It try to search \u201dok\u201d with <em>lfcr</em> or such and fails with \u201dok\u201d and <em>lf</em></p>\n<p>I found this out while playing with 3d printer with marlin firmware just trying to use it like xyz stage\u2026 reason just for fun and idea was to check out bugs and plants (leafs and flowers mostly) with my kids. I have cheap usb microscope unused. Bit overkill but\u2026</p>\n<p>I am not good at coding/writing programs\u2026 hopefully somebody could make new \u201dmarlin compatible\u201d version of this \u201dplug-in\u201d with this information\u2026 but it is maybe so that nobody wants to play with this kind of a \u201dtoys\u201d\u2026</p>\n<p>Big thanks to op for doing great job trying out different versions (I tried just different marlin versions never tought about sprinter)</p>"], "77554": ["<p>I am working on an 8 marker + DAPI fluorescence image on QuPath 0.4.2. I have created a classifier for each individual channel. Due to the 3D nature of cells and the way QuPath defines a cell (also some artifacts), the composite classifier creates subcategories that are not biologically possible or relevant. Is there a way to eliminate these subcategories and merge them into others? Is there a way to create a hierarchy for the individual classifiers so that when they are combined a cell can only be classified as CK+, CD68+ or CD3+, for example, but any of these could also be PD-L1+? Is there any other ways to do this efficiently in QuPath?<br>\nThanks,<br>\nPaula</p>", "<p>Hi <a class=\"mention\" href=\"/u/paulage\">@paulage</a>!</p>\n<p>There are quite a few options for handling classes once you get into scripting - you could easily check for certain class combinations and convert them to other classes. Scripts like that are somewhat common on the forum. On the other hard, that requires figuring out every possible combination of class you do not want, or some logic for how to bin them into the appropriate class.</p>\n<p>One paper that handles classes in what sounds like the way you want can be found here: <a href=\"https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1878-0261.12764\">https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1878-0261.12764</a><br>\nI also refer to it on my section on classifiers, along with a few comments about what can go wrong if you want to create your own decision tree classifier.<br>\n<a href=\"https://www.imagescientist.com/creating-a-classifier\" class=\"inline-onebox\">Script the creation of a Classifier in QuPath \u2014 Image Scientist</a><br>\nThe latter part of the page details some of the complications with creating your own classifier.</p>\n<p>The simplest method, of course, is if you can create an <em>accurate</em> machine learning classifier for each cell type. That forces only certain possible outcome classes. That does mean that <em>every</em> cell will end up being one of those classes, even if it should not be. <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6855269/#:~:text=Macrophages%20have%20high%20plasticity%2C%20and,infectious%20and%20non%2Dinfectious%20scenarios.\">If you do not have a CD3+ macrophage  class, but run into a CD3+ macrophage</a>, it will still end up in one of the classes you choose.</p>\n<p>All of these options can be combined together in a variety of ways - say a machine learning classifier for 3 base classes of cell, which is then added to a composite classifier to sub-classify each of the main three types. Then some class pruning at the end to bin together certain sets of classes via scripting.</p>\n<p>Hope this helps,<br>\nMike</p>", "<p>Thanks <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a>!!</p>\n<p>I created a tumor, t-cell, macrophage and null classifier, so that all cells are classified as one of these cell types. Then made individual classifiers (some using single measurement classifier and some training an object classifier) for the other markers that can be positive/negative in these cell classes.</p>\n<p>I would like only apply some of the individual classifiers to each of the particular primary 4 cell types from the first classifier to avoid creating non-desired subclasses. For example CD8 only on T cells, CD11b on T cells, macrophages or null, PDL1 on any cell type. I was trying to modify the \u201capply to unclassified cells only\u201d script at the end of your \u201ccreation of a classifier\u201d section, but don\u2019t know how to do it correctly. Could you help me out?</p>\n<p>Thanks,</p>\n<p>Paula</p>", "<p>Quick script</p>\n<pre><code class=\"lang-auto\">oldClassName = \"CD8:PDL1\"\nnewClassName = \"CD8\"\ncellsToSwap = getCellObjects().findAll{it.getPathClass() == getPathClass(oldClassName)}\nfor (cell in cellsToSwap){\n\nit.setPathClass(newClassName)\n}\n</code></pre>\n<p>Quite a few iterations of that, per class that you would like to remove or rename. Far better to be efficient about the design of the classifier, but it is there as a stopgap measure.</p>", "<p>Thanks! Cleaning up unwanted classes can be complicated. This is why I wanted to just apply certain classifiers to certain cell types to avoid subclasses that I don\u2019t want all together. I tried modifying the \u201conly apply to unclassified cells\u201d script but get an error.</p>\n<pre><code class=\"lang-auto\">// Include your classifier names in order\ndef classifiers = [\n    \"tumor_tcell_macro_null\",  \n]\n\n// Reset existing classifications\nresetDetectionClassifications()\n\n// Loop through all the classifiers in order\ndef imageData = getCurrentImageData()\ndef cells = getCellObjects()\nfor (name in classifiers) {\n    // Just keep the macrophage cells \n    cells = cells.findAll {it.getPathClass() == macrophage}\n    // Apply the next classifier\n    def classifier = loadObjectClassifier(\"CD68CD163CD11bPDL1PD1\")\n    classifier.classifyObjects(imageData, cells, false)\n    \n    // Just keep the t cells \n    cells = cells.findAll {it.getPathClass() == CD3}\n    // Apply the next classifier\n    def classifier = loadObjectClassifier(\"CD8CD11bPDL1PD1\")\n    classifier.classifyObjects(imageData, cells, false)\n    \n    // Just keep the tumor cells \n    cells = cells.findAll {it.getPathClass() == CK}\n    // Apply the next classifier\n    def classifier = loadObjectClassifier(\"PDL1PD1\")\n    classifier.classifyObjects(imageData, cells, false\n    \n    // Just keep the cells that haven't been classified\n    cells = cells.findAll {it.getPathClass() == null}\n    // Apply the next classifier\n    def classifier = loadObjectClassifier(\"CD11bPDL1PD1\")\n    classifier.classifyObjects(imageData, cells, false)\n}\nfireHierarchyUpdate()\n</code></pre>\n<p>ERROR: startup failed:<br>\nHierarchialSubclassifiers.groovy: 33: Unexpected input: \u2018{\\n    // Just keep the macrophage cells \\n    cells = cells.findAll {it.getPathClass() == macrophage}\\n    // Apply the next classifier\\n    def classifier = loadObjectClassifier(\u201cCD68CD163CD11bPDL1PD1\u201d)\\n    classifier.classifyObjects(imageData, cells, false)\\n    \\n    // Just keep the t cells \\n    cells = cells.findAll {it.getPathClass() == CD3}\\n    // Apply the next classifier\\n    def classifier = loadObjectClassifier(\u201cCD8CD11bPDL1PD1\u201d)\\n    classifier.classifyObjects(imageData, cells, false)\\n    \\n    // Just keep the tumor cells \\n    cells = cells.findAll {it.getPathClass() == CK}\\n    // Apply the next classifier\\n    def classifier = loadObjectClassifier(\u201cPDL1PD1\u201d)\\n    classifier.classifyObjects(imageData, cells, false\\n    \\n    // Just keep the cells that haven\u2019t been classified\\n    cells\u2019 @ line 32, column 5.<br>\ncells = cells.findAll {it.getPathClass() == null}<br>\n^</p>\n<p>1 error<br>\nin HierarchialSubclassifiers.groovy at line number 32</p>\n<p>ERROR: org.codehaus.groovy.control.ErrorCollector.failIfErrors(ErrorCollector.java:292)<br>\norg.codehaus.groovy.control.ErrorCollector.addFatalError(ErrorCollector.java:148)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.collectSyntaxError(AstBuilder.java:4792)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.access$100(AstBuilder.java:169)<br>\norg.apache.groovy.parser.antlr4.AstBuilder$3.syntaxError(AstBuilder.java:4803)<br>\ngroovyjarjarantlr4.v4.runtime.ProxyErrorListener.syntaxError(ProxyErrorListener.java:44)<br>\ngroovyjarjarantlr4.v4.runtime.Parser.notifyErrorListeners(Parser.java:543)<br>\ngroovyjarjarantlr4.v4.runtime.DefaultErrorStrategy.notifyErrorListeners(DefaultErrorStrategy.java:154)<br>\norg.apache.groovy.parser.antlr4.internal.DescriptiveErrorStrategy.reportNoViableAlternative(DescriptiveErrorStrategy.java:92)<br>\ngroovyjarjarantlr4.v4.runtime.DefaultErrorStrategy.reportError(DefaultErrorStrategy.java:139)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.statement(GroovyParser.java:7140)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.loopStatement(GroovyParser.java:6410)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.statement(GroovyParser.java:6999)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.scriptStatement(GroovyParser.java:520)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.scriptStatements(GroovyParser.java:427)<br>\norg.apache.groovy.parser.antlr4.GroovyParser.compilationUnit(GroovyParser.java:363)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:243)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:221)<br>\norg.apache.groovy.parser.antlr4.AstBuilder.buildAST(AstBuilder.java:262)<br>\norg.apache.groovy.parser.antlr4.Antlr4ParserPlugin.buildAST(Antlr4ParserPlugin.java:58)<br>\norg.codehaus.groovy.control.SourceUnit.buildAST(SourceUnit.java:255)<br>\njava.base/java.util.Iterator.forEachRemaining(Unknown Source)<br>\njava.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Unknown Source)<br>\njava.base/java.util.stream.ReferencePipeline$Head.forEach(Unknown Source)<br>\norg.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:663)<br>\ngroovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:373)<br>\ngroovy.lang.GroovyClassLoader.lambda$parseClass$2(GroovyClassLoader.java:316)<br>\norg.codehaus.groovy.runtime.memoize.StampedCommonCache.compute(StampedCommonCache.java:163)<br>\norg.codehaus.groovy.runtime.memoize.StampedCommonCache.getAndPut(StampedCommonCache.java:154)<br>\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:314)<br>\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:298)<br>\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:258)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getScriptClass(GroovyScriptEngineImpl.java:350)<br>\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:159)<br>\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)<br>\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)<br>\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)<br>\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)<br>\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)<br>\njava.base/java.lang.Thread.run(Unknown Source)</p>\n<p>ERROR:<br>\nFor help interpreting this error, please search the forum at <a href=\"https://forum.image.sc/tag/qupath\" class=\"inline-onebox\">Topics tagged qupath</a><br>\nYou can also start a new discussion there, including both your script &amp; the messages in this log.</p>", "<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"5\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p>Unexpected input: \u2018{\\n</p>\n</blockquote>\n</aside>\n<p>It looks like the script was copied incorrectly. \\n is standard for return or nextline, which will not be included as part of the script if copied as code. It is one of the reasons that it is recommended that all code posted on the forum is formatted as code, using the Preformatted text option, &lt;/&gt;</p>\n<p>Cheers,<br>\nMike</p>", "<p><a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> thanks for the prompt response. I am sorry I am not completely following you here. I try my best with scripting but my background is in pathology.</p>\n<p>The script has an actual return, no /n.</p>\n<p>It says the error is in line 32: cells = cells.findAll {it.getPathClass() == null}</p>\n<p>Also not sure if the \u201cnull\u201d (as well as the other classifications) should be between \u201c\u201d</p>", "<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"5\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">   def classifier = loadObjectClassifier(\"PDL1PD1\")\n    classifier.classifyObjects(imageData, cells, false\n</code></pre>\n</blockquote>\n</aside>\n<p>After formatting your code, a few things jumped out at me. The first is that the above line is missing a closing parenthesis.</p>\n<p>The second is that after the first part of the loop, you will have 0 cells in your \u201ccells\u201d variable as you keep overwriting the cells variable.</p>\n<p>Maybe three things, I suspect continuously using <code>def</code> may throw an error at some point. You should only use <code>def</code> with a variable once. If it does work, that might be something that has changed with a newer version of Groovy, but there are other forum posts that refer to similar issues in QuPath macros.</p>\n<p>Is this an incomplete version of the script? It does not appear that the variables <code>macrophage</code>, <code>CD3</code> and so on are defined anywhere. If you included the whole script, then those will throw an error at some point. The correct formatting was used in the script I posted above.</p>\n<p>Cheers,<br>\nMike</p>", "<p>Al variables were created using \u201cobject classification\u201d on GUI. I didn\u2019t use a script to define any of the classifiers. \u201cMacrophage\u201d, \u201cCD3\u201d and \u201cCK\u201d are defined on a classifier trained using \u201ctrain object classifier\u201d that I named \u201ctumor_tcell_macro_null\u201d. The other variables (CD68, CD163, CD8, CD11b, PDL1 and PD1) were created with \u201ccreate a  single measurement classifier\u201d or \u201ctrain object classifier\u201d for each marker separatly.</p>\n<p>I don\u2019t know if I am explaining it correctly. I want to apply \u201cCD68, CD163, CD11b, PDL1, and  PD1\u201d on cells classified as \u201cmacrophages\u201d by the first classifier; \u201cCD8, CD11b, PDL1 and PD1\u201d  on cells classified as \u201cCD3\u201d by the first classifier; \u201cPDL1 and PD1\u201d  on cells classified as \u201cCK\u201d by the first classifier; and \u201cCD11b, PDL1, and PD1\u201d on cells classified as \u201cnull\u201d by the first classifier.</p>\n<p>The <strong>Specific forum topics</strong> from your link had a script I thought could be used for this. After correcting the missing parenthesis I do get an error due to multiple uses of \u201cdef classifier\u201d as you mentioned. Is there another way? I tried selecting the classified object and running the second classifier with runObjectClassifier() but the just replaces the first classifier with the second.</p>\n<p>Thanks,</p>\n<p>Paula</p>", "<p>You have not created macrophages as a variable, it is an empty bit of text that the script will expect to be a variable. The previous script shows how to choose cells that belong to a particular class.</p>\n<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"9\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p>I do get an error due to multiple uses of \u201cdef classifier\u201d as you mentioned. Is there another way?</p>\n</blockquote>\n</aside>\n<p>Do not use def. That is the only change needed there. If it is easier for you, remove every def in the entire script. It will not cause any problems for a simple script - the error mostly warns you and prevents you from overwriting something that already exists.<br>\nAlternatively, name them <code>classifier1</code>, <code>classifier2</code>, etc.</p>\n<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"9\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p>I want to apply \u201cCD68, CD163, CD11b, PDL1, and PD1\u201d on cells classified as \u201cmacrophages\u201d by the first classifier; \u201cCD8, CD11b, PDL1 and PD1\u201d on cells classified as \u201cCD3\u201d by the first classifier; \u201cPDL1 and PD1\u201d on cells classified as \u201cCK\u201d by the first classifier; and \u201cCD11b, PDL1, and PD1\u201d on cells classified as \u201cnull\u201d by the first classifier.</p>\n</blockquote>\n</aside>\n<aside class=\"quote no-group\" data-username=\"Mike_Nelson\" data-post=\"8\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mike_nelson/40/26532_2.png\" class=\"avatar\"> Mike Nelson:</div>\n<blockquote>\n<p>The second is that after the first part of the loop, you will have 0 cells in your \u201ccells\u201d variable as you keep overwriting the cells variable.</p>\n</blockquote>\n</aside>\n<p><code>cells</code> will have no cells in it, you are overwriting that list.<br>\nAs an example:<br>\n<code>def cells = getCellObjects()</code><br>\n<code>cells</code> has all of the cell objects in the image at this point.<br>\n<code>cells = cells.findAll {it.getPathClass() == getPathClass(\"macropahge\")}</code><br>\n<code>cells</code> now contains <em><strong>only</strong></em> the cells classified as macrophage at this point.<br>\n<code>cells = cells.findAll {it.getPathClass() == getPathClass(\"CD3\"}</code><br>\n<code>cells</code> is now empty. There are no cells that are <code>both</code> class macrophage and class CD3.<br>\nBy continuously overwriting the cells list, you are eliminating all of the cells. You should store the whole list of cells as a separate variable that you do not overwrite if you want to keep taking subsets from the total list.</p>", "<p>I see the problems with that script.</p>\n<ol>\n<li>Will running my phenotype classifier not define cells as \u201cmacrophage\u201d, \u201ctumor\u201d, \u201ctcell\u201d and \u201cnull\u201d? The specific cell type is selected with:</li>\n</ol>\n<p><code>selectObjectsByClassification(\"macrophage\")</code></p>\n<p>Do I still have to add a line defining each category?</p>\n<ol start=\"2\">\n<li>\n<p>I tried using the above prompt instead to call the cell phenotypes so as not to have the problem you explained above where \u201ccell\u201d is empty on the second loop.</p>\n</li>\n<li>\n<p>runObjectClassifier() will run the classifier on the whole image, not only on the selected cells. So I was hoping to use:</p>\n</li>\n</ol>\n<pre><code class=\"lang-auto\">classifier = loadObjectClassifier(\"CD68CD163CD11bPDL1PD1\")\nclassifiers.classifyObjects(imageData, cells, false)\n</code></pre>\n<p>To just apply this classifier on \u201cmacrophages\u201d</p>\n<p>However I get a \u201cmethod does not exist error\u201d that I think is linked to</p>\n<p><code>classifiers.classifyObjects(imageData, cells, false)</code></p>\n<p>This is the whole script I tried:</p>\n<pre><code class=\"lang-auto\">runObjectClassifier(\"tumor_tcell_macro_null\")\ndef imageData = getCurrentImageData()\ndef cells = getCellObjects()\n{\nselectObjectsByClassification(\"macrophage\")\nclassifier = loadObjectClassifier(\"CD68CD163CD11bPDL1PD1\")\nclassifiers.classifyObjects(imageData, cells, false)\n}\nfireHierarchyUpdate()\n</code></pre>", "<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"11\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p>Will running my phenotype classifier not define cells as \u201cmacrophage\u201d, \u201ctumor\u201d, \u201ctcell\u201d and \u201cnull\u201d?</p>\n</blockquote>\n</aside>\n<p>That depends on how you set it up, you certainly could set it up that way if those were the names you selected for your classes.</p>\n<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"11\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p><code>selectObjectsByClassification(\"macrophage\")</code></p>\n<p>Do I still have to add a line defining each category?</p>\n</blockquote>\n</aside>\n<p>That line selects cells that are classified a certain way. If you want to select other cells, you need to use a different string. That function uses the string to create the class without you needing to create a class. Other functions require a class instead of a string of characters.</p>\n<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"11\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p>I tried using the above prompt instead to call the cell phenotypes so as not to have the problem you explained above where \u201ccell\u201d is empty on the second loop.</p>\n</blockquote>\n</aside>\n<p>Based on the script below, that line does nothing. You select the cells but do not use the selected cells for anything. You then pass all of the cells to the <code>classifyObjects</code> function.</p>\n<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"11\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<p>However I get a \u201cmethod does not exist error\u201d that I think is linked to</p>\n<p><code>classifiers.classifyObjects(imageData, cells, false)</code></p>\n</blockquote>\n</aside>\n<p>Correct, <code>classifiers</code> does not exist. You loaded the object classifier into a variable named <code>classifier</code>, not a variable named <code>classifiers</code>.</p>", "<aside class=\"quote no-group\" data-username=\"paulage\" data-post=\"11\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/p/b5a626/40.png\" class=\"avatar\"> Paula Gonzalez Ericsson:</div>\n<blockquote>\n<pre><code class=\"lang-auto\">runObjectClassifier(\"tumor_tcell_macro_null\")\ndef imageData = getCurrentImageData()\ndef cells = getCellObjects()\n{\nselectObjectsByClassification(\"macrophage\")\nclassifier = loadObjectClassifier(\"CD68CD163CD11bPDL1PD1\")\nclassifiers.classifyObjects(imageData, cells, false)\n}\nfireHierarchyUpdate()\n</code></pre>\n</blockquote>\n</aside>\n<p>Modifying this miniscript as mentioned above, and <a href=\"https://forum.image.sc/t/hierarchical-phenotyping-with-trained-classifiers/78298/4\">borrowing formatting from Pete\u2019s update here</a>, yeilds:</p>\n<pre><code class=\"lang-auto\">runObjectClassifier(\"tumor_tcell_macro_null\")\ndef imageData = getCurrentImageData()\ndef cells = getCellObjects()\n//the brackets do nothing and would cause an error here I think.\n//{\nsubsetOfCells= cells.findAll(cell-&gt; 'macrophages' in cell.classifications)\nclassifier = loadObjectClassifier(\"CD68CD163CD11bPDL1PD1\")\nclassifiers.classifyObjects(imageData, subsetOfCells, false)\n//}\nfireHierarchyUpdate()\n</code></pre>\n<p>I cannot say whether that is exactly correct since I do not know what the output of the classifier is, or what exactly the names of the classes that currently exist are.</p>", "<p><a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a>, thank you so much for going through each step!</p>\n<p>That worked perfectly! Thank you so much!</p>\n<p>I added the other phenotype loops (see bellow), and it all worked well except for the \u2018null\u2019. This is the correct term, right? Would I need an \u201celse\u201d term in between? or is it beacuse \u201cnull\u201d is not in cell.classifications?</p>\n<pre><code class=\"lang-auto\">runObjectClassifier(\"tumor_tcell_macro_null\")\ndef imageData = getCurrentImageData()\ndef cells = getCellObjects()\nsubsetOfCells= cells.findAll(cell-&gt; 'macrophage' in cell.classifications)\nclassifier = loadObjectClassifier(\"CD68CD163CD11bPDL1PD1\")\nclassifier.classifyObjects(imageData, subsetOfCells, false)\nsubsetOfCells= cells.findAll(cell-&gt; 'CD3' in cell.classifications)\nclassifier = loadObjectClassifier(\"CD8CD11bPDL1PD1\")\nclassifier.classifyObjects(imageData, subsetOfCells, false)\nsubsetOfCells= cells.findAll(cell-&gt; 'CK' in cell.classifications)\nclassifier = loadObjectClassifier(\"PDL1PD1\")\nclassifier.classifyObjects(imageData, subsetOfCells, false)\nsubsetOfCells= cells.findAll(cell-&gt; 'null' in cell.classifications)\nclassifier = loadObjectClassifier(\"CD11bPDL1PD1\")\nclassifier.classifyObjects(imageData, subsetOfCells, false)\nfireHierarchyUpdate()\n</code></pre>", "<p>Ah, null is a little bit weird since it is a value, not a class. You might think of it as being the value 0, rather than typing out \u2018zero\u2019.<br>\nSo <code>null</code> does not include any quotation marks as it is not a string or a name. It is just\u2026 <code>null</code>.</p>\n<p>That also means you cannot use a variable called null since the programming language needs it.</p>", "<p>I see. I tried without quotation marks but it still doesn\u2019t work. I also tried using the old <code>getPathClass()</code> command but get a \u201cmethod does not exist error\u201d.</p>", "<p>All of those findAlls should use curly braces {} , not parentheses ()</p>\n<p>EDIT: I\u2019m wrong about this apparently?</p>", "<p>I am not sure about the new scripting methods, so going back to the old ones you would use:</p>\n<pre><code class=\"lang-auto\">subsetOfCells= cells.findAll{it.getPathClass() == null}\n</code></pre>\n<p>if <code>null in cell.classifications</code> does not work, maybe <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> will have an idea for how to frame that.  Or it might be the same, where <code>cell -&gt; cell.classifications == null</code></p>\n<p>In retrospect, there would not be a <code>null</code> \u201cwithin\u201d the classifications. <code>null</code> indicates that there is no classification, at all.</p>", "<aside class=\"quote no-group\" data-username=\"Mike_Nelson\" data-post=\"18\" data-topic=\"77554\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mike_nelson/40/26532_2.png\" class=\"avatar\"> Mike Nelson:</div>\n<blockquote>\n<p>if <code>null in cell.classifications</code> does not work, maybe <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> will have an idea for how to frame that. Or it might be the same, where <code>cell -&gt; cell.classifications == null</code></p>\n</blockquote>\n</aside>\n<p>I haven\u2019t followed all this, but all of these should work to find cells without classifications:</p>\n<pre><code class=\"lang-groovy\">def unclassifiedCells = cells.findAll {!it.classifications}\ndef unclassifiedCells2 = cells.findAll {it.classifications.isEmpty()}\ndef unclassifiedCells3 = cells.findAll {!it.pathClass}\ndef unclassifiedCells4 = cells.findAll {it.pathClass == null}\n</code></pre>\n<p>Although I prefer to write it like this (which works too\u2026)</p>\n<pre><code class=\"lang-groovy\">def unclassifiedCells = cells.findAll(c -&gt; c.classifications)\ndef unclassifiedCells2 = cells.findAll(c -&gt; c.classifications.isEmpty())\ndef unclassifiedCells = cells.findAll(c -&gt; !c.pathClass)\ndef unclassifiedCells2 = cells.findAll(c -&gt; c.pathClass == null)\n</code></pre>\n<p>since I think it looks a (little) bit less confusing.</p>", "<p>Thank you all</p>"], "77570": ["<p>Hello everyone!</p>\n<p><a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a> and I discussed having a community call regarding metadata in OME-NGFF. Several metadata guidelines exist that cover different fields. While parts of these metadata guidelines overlap, there are parts that are substantially different. Furthermore, individual users might not be willing to fully comply with a given metadata standard, either by leaving out fields or having fields not covered by a metadata schema.</p>\n<p>It is an open question where this metadata should live within the OME-NGFF spec and how it should be built up in order to facilitate metadata storage needs in a flexible manner while making validation of metadata compliance practical.</p>\n<p>At this community call, we would like to hear about the metadata needs that people have and then discuss to what extent and how we could cover these. I will briefly give a presentation on some of the metadata standards / guidelines in particular fields after which the floor is open for discussion. I hope to see many of you there!</p>\n<p>In order to cover timezones around the globe we will have 2 virtual sessions:</p>\n<ul>\n<li>\n<strong>Length of sessions</strong>: ~2 hours</li>\n<li>\n<strong>Times and zoom links</strong>\n<ul>\n<li>\n<span data-date=\"2023-03-15\" data-time=\"11:00:00\" class=\"discourse-local-date\" data-timezone=\"Europe/Berlin\" data-email-preview=\"2023-03-15T10:00:00Z UTC\">2023-03-15T10:00:00Z</span>: <a href=\"https://embl-org.zoom.us/j/96563142871?pwd=a252UitreWJZSFZjdHF6UEFCY0RFdz09\" rel=\"noopener nofollow ugc\">zoom link</a>\n</li>\n<li>\n<span data-date=\"2023-03-15\" data-time=\"18:00:00\" class=\"discourse-local-date\" data-timezone=\"Europe/Berlin\" data-email-preview=\"2023-03-15T17:00:00Z UTC\">2023-03-15T17:00:00Z</span>: <a href=\"https://embl-org.zoom.us/j/96639737153?pwd=NGNHMUgwTWgzOEo3VlloRWJWRm5Cdz09\" rel=\"noopener nofollow ugc\">zoom link</a>\n</li>\n</ul>\n</li>\n</ul>\n<p>A link for notes will soon be made available here. I hope to see many of you there and am looking forward to the discussions!</p>", "<p>This is great. <a class=\"hashtag\" href=\"/tag/quarep\">#<span>quarep</span></a> is looking forward to participating in this important call.</p>", "<p>I assume that the times are Central European Time?</p>", "<p>Not entirely certain whether that is the case on this forum. In the zulip chat if you give a time it automatically adjusts to the timezone of the person who looks at the post. I believe here you can click on the times and then you see for which timezones it is. For example I can see for the Amsterdam and Los Angeles time zone.</p>", "<p>Some CC\u2019s that I\u2019ve promised: <a class=\"mention\" href=\"/u/andy-sweet\">@andy-sweet</a> <a class=\"mention\" href=\"/u/nclack\">@nclack</a> and of course <a class=\"mention-group notify\" href=\"/groups/ngff\">@ngff</a></p>", "<p>Hello everyone,</p>\n<p>Hereby an additional draft agenda for the calls next week:</p>\n<ol>\n<li>Recap\n<ul>\n<li>Paper and NGFF status (Josh 10m)</li>\n<li>Update on tables and transforms specs (Josh / Kevin 10m)</li>\n</ul>\n</li>\n<li>Brief introduction to metadata (Wouter-Michiel ~ 10m)\n<ul>\n<li>Phases of metadata generation</li>\n<li>Examples of current metadata standards</li>\n</ul>\n</li>\n<li>Metadata in ome-ngff  (Wouter-Michiel ~ 10m)\n<ul>\n<li>Choices regarding extent of standardization</li>\n<li>Possible design choices</li>\n<li>Tooling</li>\n</ul>\n</li>\n<li>Pitch metadata needs (~ 20m)</li>\n<li>Open discussion (~ 50m)\n<ul>\n<li>Opinions on standardization</li>\n<li>Opinions on design choices</li>\n<li>Implementation in OME-NGFF</li>\n</ul>\n</li>\n<li>Summary and providing outlook (~ 10m)</li>\n</ol>\n<p>This agenda is still subject to change. For point 4 on the agenda, I would like to ask people willing to provide a brief overview of their metadata requirements to reply to this post and also state your field (Thanks in advance!). Each person will get about 2-3 minutes.</p>\n<p>Looking forward to seeing many of you next week!</p>", "<p>Regarding requirements, we need metadata to give enough context to enable re-use to address biological questions. From my experience trying to re-use public image data in cell biology, we need at least:</p>\n<ul>\n<li>identification of samples and treatments associated with each image\n<ul>\n<li>including a standardized way of distinguishing control vs treatment when relevant</li>\n</ul>\n</li>\n<li>identification of entities visible in the images\n<ul>\n<li>this goes beyond just the dye/fluorophore associated with each channel but something like which protein is targeted with which antibody</li>\n</ul>\n</li>\n</ul>\n<p>In all cases, database identifiers and ontologies should be used where possible to avoid ambiguities.</p>", "<p>Definitely agreed, <a class=\"mention\" href=\"/u/jkh1\">@jkh1</a></p>\n<p><strong><em>However</em></strong>, I\u2019ll say here (and re-iterate at the start of the meetings) I think we should be careful to limit ourselves to very short explanations (or pitches) of <em>what metadata</em> and <em>why</em> we need it at this stage. Not because it\u2019s not critical, valuable, and interesting, but just because there are some preliminaries we need to work out first.</p>\n<p>In fact, I\u2019d already consider it a success if we just have a list of the metadata models<code>[1]</code>, perhaps what relationship they have to one another, e.g. whether one extends another, and how many attendees are interested in each. We can then use that to schedule further discussions.</p>\n<p>Instead, I\u2019d suggest focusing more on <em>how</em> we are going to share the metadata. What are the <strong>requirements</strong> for each of the different metadata types that everyone has pitched?<code>[2]</code>  And very importantly, who is interested in actually specifying how this will work? And by when? <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>In terms of the mechanics of storage, <a class=\"mention\" href=\"/u/jkh1\">@jkh1</a>\u2019s points already ticked a few of my boxes (identifiers and ontologies <img src=\"https://emoji.discourse-cdn.com/twitter/+1.png?v=12\" title=\":+1:\" class=\"emoji\" alt=\":+1:\" loading=\"lazy\" width=\"20\" height=\"20\">) but there are probably various ways to do even just that. Other questions that come to mind are:</p>\n<ul>\n<li>What existing formats are already in use that people MUST see integrated? (e.g. XML)?</li>\n<li>What metadata is so latency-intolerant that it MUST be in the Zarr JSON attribute so that it\u2019s loaded in the first GET?</li>\n<li>and what metadata can we store separately and just \u201cregister\u201d in the model?</li>\n<li>How do we link between concepts in the various models?</li>\n<li>Are there related, less metadata-py things that need similar consideration (points, geojson, etc.)?</li>\n<li>And then the ever fun topics of versioning, upgrades, plugins (i.e. dynamically finding the necessary code for metadata), etc.?</li>\n</ul>\n<p>Thoughts / additions?</p>\n<p>Looking forward to it.<br>\n~Josh</p>\n<hr>\n<sup>\n<code>[1]</code> I specifically mention models here rather than ontologies since NGFF is more the former. Whatever we put in place should have a way to make use of ontologies, but I think discussing which ontologies to use is even <i>less</i> what we want to do next week.\n</sup>\n<br><br>\n<sup>\n<code>[2]</code> If others would like to add their pitches above, as @jkh1 did, that might help.\n</sup>", "<p>I\u2019m almost certain my needs are a subset of the union of all others, but I would call out things like:</p>\n<ul>\n<li>In my day to day work I build visualization tools.  Channel names are a minimal piece of semantically important data that are not found in the core spec right now.  (granted it was covered in the <code>omero</code> metadata, which is considered \u201ctransitional\u201d).  Channel names are something that is relatively important to surface early - for example, letting users select channels to load based on reading their names.</li>\n<li>We also have invested in work to convert from Zeiss CZI metadata format to OME-XML (<a href=\"https://github.com/AllenCellModeling/czi-to-ome-xslt\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - AllenCellModeling/czi-to-ome-xslt: A repository of XSL transform sheets to map CZI metadata to OME.</a>).  Getting possibly arbitrarily detailed microscopy settings does seem to imply the need for a catchall thing like the StructuredAnnotation concept.  I think the basic ome-xml model is nice, where you try to have a reasonable coverage of consensus important stuff, and then allow extensibility.</li>\n<li>Furthermore on the above point, we will still be in a world where we want to convert from proprietary file format to open standard file format, while maintaining metadata as losslessly as possible.</li>\n<li>Notwithstanding the XSLT library I linked above, (and the excellent ome-types library) we aren\u2019t especially tied to continuing to use XML.</li>\n<li>It would seem reasonable if .zattrs just held some other url link to the \u201cextended\u201d metadata.</li>\n</ul>", "<p><a class=\"mention\" href=\"/u/wmv1992\">@wmv1992</a> - happy to pitch from the <a href=\"http://www.ebi.ac.uk/bioimage-archive\" rel=\"noopener nofollow ugc\">BioImage Archive</a>/<a href=\"https://www.nature.com/articles/s41592-021-01166-8\" rel=\"noopener nofollow ugc\">REMBI</a> perspective!</p>\n<p>We\u2019re interested in OME-NGFF metadata from a couple of slightly different directions:</p>\n<ol>\n<li>To be able to use metadata from within OME-NGFF images submitted to us, so that submitters don\u2019t have to supply this information manually. For this we need to be able to verify that it meets some set of minimal requirements and extract to enable indexing and search.</li>\n<li>To be able to package metadata we have into OME-NGFF as a standardised distribution format.</li>\n</ol>\n<p>Flexibility/extensibility is important in both cases since we\u2019ll need to cover varying community requirements - +1 for <a class=\"mention\" href=\"/u/dmt\">@dmt</a> \u2019s point above about coverage of consensus metadata &amp; extensibility. <a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a>\u2019s point about being able to store some metadata separately and \u201cregister\u201d in the model is likely to be pretty critical to avoid duplication.</p>", "<p><a class=\"mention\" href=\"/u/matthew_hartley\">@Matthew_Hartley</a> Thanks Matthew! Some of the points we discussed last week would tackle the flexibility/extensibility I think. I will also mention this during the meeting to see what people in the community think about it.</p>", "<p>Dear <a class=\"mention-group notify\" href=\"/groups/ngff\">@ngff</a> group,</p>\n<p>One day reminder for this community call! <img src=\"https://emoji.discourse-cdn.com/twitter/smile.png?v=12\" title=\":smile:\" class=\"emoji\" alt=\":smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>~J</p>\n<hr>\n<p><em>p.s. I just noticed that use of the <code>@ngff</code> handle had stopped contacting the group because of the size it reached. (I guess that counts as a success!) I\u2019ve temporarily changed the setting in order to notify everyone, but we may need to set up a different form of communication soon.</em></p>", "<p>Only 2 more hours to go for the first community call! For both sessions, this is the hackmd: <a href=\"https://hackmd.io/BqnK9Wm4QpGYAhYOoaFBQQ\" rel=\"noopener nofollow ugc\">OME-NGFF community call: 2023-03-15 - HackMD</a></p>\n<p>Hope to see many of you there!<br>\nWouter-Michiel</p>", "<p>Hi <a class=\"mention\" href=\"/u/wmv1992\">@wmv1992</a> ,<br>\nThanks much for organizing these sessions last week. That was extremely useful. Besides the HackMD record, would you be able to share your presentation? And was either (or both) of the sessions recorded?<br>\nThanks,<br>\nDamir</p>", "<p>Hi <a class=\"mention\" href=\"/u/dsudar\">@dsudar</a>,</p>\n<p>Yes, the recordings will be made available very soon. <a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a> will put them here:<br>\n<a href=\"https://downloads.openmicroscopy.org/presentations/2023/NGFF-community-call-2023-03-15/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Index of /presentations/2023/NGFF-community-call-2023-03-15</a>.</p>\n<p>Here is the presentation for now already: <a href=\"https://docs.google.com/presentation/d/1QOW8Py_BCpQiysE0Kmyvdui3ghkjB6rD/edit?usp=sharing&amp;ouid=103031079254476210617&amp;rtpof=true&amp;sd=true\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Loading Google Slides</a></p>\n<p>A summary of the meeting will soon be made available as well.</p>", "<p>Hello all,</p>\n<p>Please find the summary of the meetings below. In case anything was missed or unclear, please let me know!</p>\n<h1>\n<a name=\"summary-1\" class=\"anchor\" href=\"#summary-1\"></a>Summary</h1>\n<h3>\n<a name=\"brief-ome-ngff-status-2\" class=\"anchor\" href=\"#brief-ome-ngff-status-2\"></a>Brief OME-NGFF status</h3>\n<p>The specifications scheduled for v0.5 are near final call for comments and are already used by several communities. Outstanding comments on the PRs exist<br>\nand should be revisited after testing in the wild and after initial merge.</p>\n<p>The timeline for conversion to Zarr v3 is unclear but would be accelerated by help from the community. Norman R agreed to help with conversion. Likely there are no significant changes with respect to metadata in NGFF.</p>\n<h3>\n<a name=\"metadata-in-ngff-3\" class=\"anchor\" href=\"#metadata-in-ngff-3\"></a>Metadata in NGFF</h3>\n<p>The primary purpose of the meetings was to discuss how NGFF might deal with metadata.</p>\n<p>General<br>\nThere are different ideas of what NGFF should / could do with metadata in general. In short these range from having full freedom regarding metadata to having more constraints:</p>\n<ol>\n<li>NGFF should only define where and how metadata is stored (e.g., \u201cload file A/B/C which is in JSON\u201d.) For the rest, users should have full freedom.</li>\n<li>NGFF comes up with a minimal model of metadata, however, what is minimal to one is not minimal to someone else. Getting consensus on this could prove challenging.</li>\n<li>NGFF supports a single, unified metadata framework like <a href=\"https://linkml.io\" rel=\"noopener nofollow ugc\">LinkML</a>, but there was some discussion that this could be a parallel but related effort.\n<ul>\n<li><em><strong>Potential Details</strong>: Standards that are maintained by communities can choose from fields present in these models. If a field is not present this could be added by opening a PR. Fields have a IRI (international resource identifier) as key. Valid values are determined by a given metadata standard itself. Metatadata standards have a name and can be registered in a registry such as linkml registry. Parent, child system of metadata models was discussed in case people partially would use a metadata specification.</em></li>\n</ul>\n</li>\n</ol>\n<p>Some of these options could live next to one another, too. First priority for NGFF would be to have format and location. Point 3 could be interoperable with that and could possibly encourage consensus building.</p>\n<p>Format<br>\nIn general the community is in favor of moving away from xml to another format. There were no objections to linkml. People have volunteered for experimenting with the usage of linkml. Linkml could be used for schema authoring, provides loaders and dumpers and validation in several RDF related formats and formats often used by biologists (csv). Several people will be experimenting with this in <a href=\"https://github.com/ome/linkml-sandbox\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - ome/linkml-sandbox: Experimental repository for exploring a common framework for bioimaging metadata models</a></p>\n<p>Location<br>\nOpinions differ as to what the storage location of metadata should be. Multiple options have been discussed which are not neccesarily mutually exclusive:</p>\n<ol>\n<li>Metadata in one file stored at a given location within Zarr.</li>\n<li>Metadata stored at multiple locations within Zarr. Location depends on what kind of metadata. A possible issue was raised regarding the time it would take to parse metadata from all locations to find what you want. Concept of chunked metadata raised by Josh Moore in response.</li>\n<li>Links to external metadata. An example here would be donor metadata in case imaging and sequencing was performed. Without links this would lead to metadata duplication.</li>\n</ol>\n<p>A need for the possibility of consolidation of metadata at the top level of the Zarr store was raised. At the same time the metadata specification should allow for subsetting data without metadata loss.</p>\n<p>Viewing configs<br>\nOmero metadata is transitional, but only <a href=\"https://github.com/ome/ngff/issues/78\" rel=\"noopener nofollow ugc\">issue 78</a> was closest to where someone has gotten for a specification. For location it was mentioned that it should not be near image data as this would be more difficult when having multiple volumes with different rendering settings. An alternative would be storing with image data, but with allowing multiple viewconfigs.</p>\n<h3>\n<a name=\"other-matters-of-business-4\" class=\"anchor\" href=\"#other-matters-of-business-4\"></a>Other matters of business</h3>\n<p>Future meetings<br>\nA need for more frequent meetings was raised to discuss current ongoing activities. Possibilities include more community meetings, spontaneous meeting in case of hot topic on github / image sc, or hybrid form of these.</p>", "<p>Dear all,</p>\n<p>Could you tell me please if migration scripts from v0.4 or earlier versions will be provided?</p>\n<p>Thank you,<br>\nAliaksei</p>", "<aside class=\"quote no-group\" data-username=\"wmv1992\" data-post=\"15\" data-topic=\"77570\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/wmv1992/40/55089_2.png\" class=\"avatar\"> Wouter-Michiel Vierdag:</div>\n<blockquote>\n<p>Yes, the recordings will be made available very soon. <a class=\"mention\" href=\"/u/joshmoore\">@joshmoore</a> will put them here:</p>\n</blockquote>\n</aside>\n<p>Files are now uploaded to <a href=\"https://downloads.openmicroscopy.org/presentations/2023/NGFF-community-call-2023-03-15/\" class=\"inline-onebox\">Index of /presentations/2023/NGFF-community-call-2023-03-15</a></p>\n<aside class=\"quote no-group\" data-username=\"aaxx\" data-post=\"17\" data-topic=\"77570\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/aaxx/40/59505_2.png\" class=\"avatar\"> Aliaksei Chareshneu:</div>\n<blockquote>\n<p>Could you tell me please if migration scripts from v0.4 or earlier versions will be provided?</p>\n</blockquote>\n</aside>\n<p>The intention, Aliaksei, is very much that migration scripts will always be provided between released versions.</p>\n<p>~J.</p>"], "73479": ["<p>Hi everybody,<br>\nis there any possibility to embedded own tab in Qupath like on the picture?<br>\nThanks in advance.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9ed25d602bb0e7e8bb6ece848c1e3cb1a09d3260.jpeg\" data-download-href=\"/uploads/short-url/mF08Ykm3eG5FqdVA6uCZuNqKaBO.jpeg?dl=1\" title=\"embed the tree view\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9ed25d602bb0e7e8bb6ece848c1e3cb1a09d3260_2_690x385.jpeg\" alt=\"embed the tree view\" data-base62-sha1=\"mF08Ykm3eG5FqdVA6uCZuNqKaBO\" width=\"690\" height=\"385\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9ed25d602bb0e7e8bb6ece848c1e3cb1a09d3260_2_690x385.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9ed25d602bb0e7e8bb6ece848c1e3cb1a09d3260_2_1035x577.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9ed25d602bb0e7e8bb6ece848c1e3cb1a09d3260_2_1380x770.jpeg 2x\" data-dominant-color=\"998C9A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">embed the tree view</span><span class=\"informations\">1538\u00d7859 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>It should certainly be possible, but there\u2019s no public API for doing it.</p>", "<p>Hello,</p>\n<p>this is something I played with a while back: <a href=\"https://forum.image.sc/t/adding-new-tabs-to-the-qupath-analysis-panel-from-groovy/49771\" class=\"inline-onebox\">Adding new tabs to the QuPath analysis panel from Groovy</a></p>\n<p>Let me know if the code still works or needs updating since I last looked at it.</p>\n<p>Cheers,<br>\nEgor</p>", "<p>Hi Egor,<br>\nI can\u2019t find any Java analog with your Groovy code.<br>\nCan you help me please?</p>\n<p>Kind regards.</p>", "<p>Hey <a class=\"mention\" href=\"/u/vitalijvictor\">@Vitalijvictor</a></p>\n<p>Really sorry I missed your post here. I personally only do these kind of experiments in Groovy. However, if you check the Github page for creating extensions: <a href=\"https://github.com/qupath/qupath/wiki/Creating-extensions\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Creating extensions \u00b7 qupath/qupath Wiki \u00b7 GitHub</a> \u2013 You will see:</p>\n<pre><code class=\"lang-auto\">\tpublic void installExtension(QuPathGUI qupath) {\n\t}\n\n</code></pre>\n<p>which I suspect is where you would add your tab initialisation. And since my Groovy code is modelled after things happening in the QuPath GUI JAVA code, I\u2019m pretty sure it\u2019s more or less directly translatable back to JAVA.</p>\n<p>That would actually be a fun experiment to try. If you get there first, would you mind posting a minimal extension code? Otherwise I\u2019ll try to get to it when I can, but I can\u2019t make promises right now, sorry.</p>\n<p>Kind regards,<br>\nEgor</p>"], "73482": ["<p>I trained a StarDist network and I would like to continue training with new dataset, but I don\u2019t now how to do it.</p>", "<p>Hi <a class=\"mention\" href=\"/u/diego_staub_felipe\">@Diego_Staub_Felipe</a></p>\n<p>Good question.   Passing in \u2018None\u2019 for the model configuration loads in a previously trained model, and I believe you can then train it further (with either new data, or more iterations on the previous training data).</p>\n<p>Though I will emphasize that I am not an expert on stardist.  Hopefully someone else can confirm.  I hope I am not missing another interface that should be used.</p>\n<p>That being said what I\u2019ve done when initializing a model is to create a configuration then pass it to a the StarDist2D constructor.</p>\n<pre><code class=\"lang-auto\">    conf=get_stardist_configuration()\n    model = StarDist2D(conf, name=modelname, basedir=modeldir)\n</code></pre>\n<p>What I\u2019ve done when loading a pre-trained model is pass \u2018None\u2019 for the configuration.  I believe if you pass a configuration when loading a pre-trained model, it will re-intitialize it (though I\u2019m not 100% sure).</p>\n<pre><code class=\"lang-auto\"># pass None for configuration to load previously trained model\nmodel = StarDist2D(None, name=modelname, basedir=modeldir)\n# now you can use the model for prediction or train it further...\n</code></pre>\n<pre><code class=\"lang-auto\"></code></pre>", "<p>Hi <span class=\"mention\">@Brain_Northan</span> thanks for helping me</p>\n<p>So I did it. But I need to reconfigure the model, passing it the new batch_size and new epochs for example. If I load the model with no configuration how can I change it? Ensuring that it continues training with the new data, not starting from scratch.</p>\n<p>In other models built from keras we have:<br>\nhistory = model.fit(train_dataloader,<br>\ninitial_epoch = curret_epochs,<br>\nepochs = curret_epochs + epochs,<br>\nsteps_per_epoch=len(train_dataloader),<br>\nvalidation_data=valid_dataloader,<br>\nvalidation_steps=len(valid_dataloader),<br>\nverbose=1,<br>\ncallbacks=callbacks</p>\n<p>But I can\u2019t find the option (initial_epoch) here on StarDist, or how change the new amount of epochs</p>\n<p>Att Diego</p>", "<p>Good question.  I\u2019d be interested to hear from the stardist experts.  It\u2019s not clear to me either how to reload an existing model, but change training parameters.</p>", "<p>Hi,</p>\n<p>You should be able to change the training parameters in <code>model.config</code> directly before running the training on a new dataset.</p>\n<p>Here is an example that finetunes a copy of a pretrained fluo model with new training data while changing the number of epochs, learning rate, etc:</p>\n<pre><code class=\"lang-python\">\nimport numpy as np\nfrom stardist.models import StarDist2D\nimport shutil \n\n# make a copy of a pretrained model into folder 'mymodel'\nmodel_pretrained = StarDist2D.from_pretrained('2D_versatile_fluo')\nshutil.copytree(model_pretrained.logdir, 'mymodel', dirs_exist_ok=True)\n\n\n# create new model from folder (loading the  pretrained weights)\nmodel = StarDist2D(None, 'mymodel')\n\n# create new training data \nX = np.zeros((16,128,128,1))\nX[:, 10:20,10:20] = 1.1\nY = np.zeros((16,128,128), np.uint16)\nY[:, 10:20,10:20] = 1\n\n# change some training params \nmodel.config.train_patch_size = (128,128)\nmodel.config.train_batch_size = 16 \nmodel.config.train_learning_rate = 1e-5\nmodel.config.train_epochs = 10\n\n# finetune on new data\nmodel.train(X,Y, validation_data=(X,Y))\n</code></pre>\n<p>Hope that helps!</p>", "<p>Thanks for describing your approach, <a class=\"mention\" href=\"/u/mweigert\">@mweigert</a> .</p>\n<p>I\u2019m trying to finetune the <code>2D_versatile_he</code> model using your approach, but i\u2019m getting the below error.</p>\n<pre><code class=\"lang-auto\">  File \"/exports/path-airmec-bosse/jur/stardist/.conda/lib/python3.8/site-packages/stardist/models/model2d.py\", line 448, in train\n    self.callbacks.append(CARETensorBoardImage(model=self.keras_model, data=data_val, log_dir=str(self.logdir/'logs'/'images'),\n  File \"/exports/path-airmec-bosse/jur/stardist/.conda/lib/python3.8/site-packages/csbdeep/utils/tf.py\", line 464, in __init__\n    (self.n_inputs == len(X) and self.n_outputs == len(Y)) or _raise(ValueError())\n  File \"/exports/path-airmec-bosse/jur/stardist/.conda/lib/python3.8/site-packages/csbdeep/utils/utils.py\", line 91, in _raise\n    raise e\nValueError\n</code></pre>\n<p>I think this has something to do with the fact that my task is a multi-class classification problem with <code>model.config.n_classes = 3</code>, which is different from <code>2D_versatile_he</code>. Does anyone here have an idea on how to solve this?</p>", "<aside class=\"quote no-group\" data-username=\"jjhbw\" data-post=\"6\" data-topic=\"73482\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jjhbw/40/60236_2.png\" class=\"avatar\"> Jurriaan:</div>\n<blockquote>\n<p>I think this has something to do with the fact that my task is a multi-class classification problem with <code>model.config.n_classes = 3</code>, which is different from <code>2D_versatile_he</code>.</p>\n</blockquote>\n</aside>\n<p>Yes, that\u2019s the problem. The neural network structure is actually different for the newer multi-class models, hence one can\u2019t directly use the weights from a pre-trained model that didn\u2019t perform any classification.</p>\n<aside class=\"quote no-group\" data-username=\"jjhbw\" data-post=\"6\" data-topic=\"73482\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jjhbw/40/60236_2.png\" class=\"avatar\"> Jurriaan:</div>\n<blockquote>\n<p>Does anyone here have an idea on how to solve this?</p>\n</blockquote>\n</aside>\n<p>One idea is to copy the weights of the <code>2D_versatile_he</code> model for the structure that is the same in the multi-class model, freeze those weights, and first train the remaining weights. Then unfreeze all weights and do another round of training. Implementing this does require some deeper knowledge of Keras though, which is used under the hood in StarDist. (Cf. this Keras guide on <a href=\"https://keras.io/guides/transfer_learning/\">Transfer learning &amp; fine-tuning</a>.)</p>", "<p>Hi,</p>\n<p>I think I have a related issue, that\u2019s the reason I am posting in this closed issue. Please, if I should start a new topic, let me know and I apologize for that.</p>\n<p>I am using config.json, weights_best.h5, and threshold.json from the pre-trained \u20182D_versatile_he\u2019. In my image, I have roundish cells and some elongated cells. When I segment the latter, the cells are split into (2 or more) chaining and overlapping cells.</p>\n<p>I want to play with different n_rays values as a potential way to improve the segmentation for the elongated cells. Here is what I am trying:</p>\n<p>In the the .py code, I re-assign the n_rays parameter (from 32 to 64).</p>\n<pre><code class=\"lang-auto\">model = StarDist2D(None, name=model_folder, basedir=base_dir)\nmodel.config.n_rays = 64\n</code></pre>\n<p>Right after re-assigning, I have the following model.config:</p>\n<pre><code class=\"lang-auto\">Config2D(axes='YXC', backbone='unet', grid=(2, 2), n_channel_in=3, n_channel_out=33, n_classes=None, n_dim=2, **n_rays=64**, net_conv_after_unet=128, net_input_shape=[None, None, 3], net_mask_shape=[None, None, 1], train_background_reg=0.0001, train_batch_size=8, train_checkpoint='weights_best.h5', train_checkpoint_epoch='weights_now.h5', train_checkpoint_last='weights_last.h5', train_class_weights=(1, 1), train_completion_crop=32, train_dist_loss='mae', train_epochs=200, train_foreground_only=0.9, train_learning_rate=0.0003, train_loss_weights=[1, 0.1], train_n_val_patches=3, train_patch_size=[512, 512], train_reduce_lr={'factor': 0.5, 'patience': 50, 'min_delta': 0}, train_sample_cache=True, train_shape_completion=False, train_steps_per_epoch=200, train_tensorboard=True, unet_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_kernel_size=[3, 3], unet_last_activation='relu', unet_n_conv_per_depth=2, unet_n_depth=3, unet_n_filter_base=32, unet_pool=[2, 2], unet_prefix='', use_gpu=False)\n</code></pre>\n<p>As soon as I call model.predict_instances(), I have the following error:</p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"/opt/venv/lib/python3.8/site-packages/stardist/models/base.py\", line 775, in predict_instances\n    for r in self._predict_instances_generator(*args, **kwargs):\n  File \"/opt/venv/lib/python3.8/site-packages/stardist/models/base.py\", line 727, in _predict_instances_generator\n    for res in self._predict_sparse_generator(img, axes=axes, normalizer=normalizer, n_tiles=n_tiles,\n  File \"/opt/venv/lib/python3.8/site-packages/stardist/models/base.py\", line 605, in _predict_sparse_generator\n    dista = np.asarray(dista).reshape((-1,self.config.n_rays))\nValueError: cannot reshape array of size 6085856 into shape (64)\n</code></pre>\n<p>When I use the default value n_rays=32, everything works as expected.</p>\n<p>My question is: is it possible to change the n_rays value for a pre-trained model? If not, how should I proceed in order to test different n_rays values?</p>\n<p>Thank you!</p>", "<aside class=\"quote no-group\" data-username=\"igorafsouza\" data-post=\"8\" data-topic=\"73482\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/igorafsouza/40/68590_2.png\" class=\"avatar\"> Igor:</div>\n<blockquote>\n<p>My question is: is it possible to change the n_rays value for a pre-trained model?</p>\n</blockquote>\n</aside>\n<p>No, sorry.</p>\n<aside class=\"quote no-group\" data-username=\"igorafsouza\" data-post=\"8\" data-topic=\"73482\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/igorafsouza/40/68590_2.png\" class=\"avatar\"> Igor:</div>\n<blockquote>\n<p>If not, how should I proceed in order to test different n_rays values?</p>\n</blockquote>\n</aside>\n<p>How do you mean?</p>\n<p>One thing you can always do is to use the pre-trained model for prediction on the images that you care about. After fixing potential prediction mistakes, use these results to train a new model with different parameters (e.g. with more rays).</p>"], "69396": ["<p>Hey everyone!<br>\nI\u2019m trying to analyze some astrocytes with sholl analysis tool and I have this problem:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03a3cbff7b584a461071cea41af7331eb66154eb.png\" data-download-href=\"/uploads/short-url/wcmFi5Ty3jAjJEivW1f3Onpk3V.png?dl=1\" title=\"astrocyte\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03a3cbff7b584a461071cea41af7331eb66154eb_2_541x500.png\" alt=\"astrocyte\" data-base62-sha1=\"wcmFi5Ty3jAjJEivW1f3Onpk3V\" width=\"541\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03a3cbff7b584a461071cea41af7331eb66154eb_2_541x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03a3cbff7b584a461071cea41af7331eb66154eb.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/3/03a3cbff7b584a461071cea41af7331eb66154eb.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/3/03a3cbff7b584a461071cea41af7331eb66154eb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">astrocyte</span><span class=\"informations\">667\u00d7616 66.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/e/febff235fd8371963b31612b012ec5a4a7df25e4.png\" data-download-href=\"/uploads/short-url/AlCwfCUPA7aPqm3IYA93vwWttmA.png?dl=1\" title=\"Capture1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/e/febff235fd8371963b31612b012ec5a4a7df25e4.png\" alt=\"Capture1\" data-base62-sha1=\"AlCwfCUPA7aPqm3IYA93vwWttmA\" width=\"232\" height=\"499\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/e/febff235fd8371963b31612b012ec5a4a7df25e4_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture1</span><span class=\"informations\">262\u00d7564 3.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/a/aa7e260ab87a0120abff27a87be00b0e4d0cf296.png\" data-download-href=\"/uploads/short-url/okfrpsDBPuDNFzAoL827F1kdls2.png?dl=1\" title=\"Capture2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/a/aa7e260ab87a0120abff27a87be00b0e4d0cf296.png\" alt=\"Capture2\" data-base62-sha1=\"okfrpsDBPuDNFzAoL827F1kdls2\" width=\"280\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/a/aa7e260ab87a0120abff27a87be00b0e4d0cf296_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture2</span><span class=\"informations\">328\u00d7584 15.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>As you can see, sometimes the plugin counts the intersection correctly (just the one point per branch), but sometimes it duplicates it, counting twice per branch, despite its black all along.<br>\nDoes anyone know why could it be and how can I solve it?</p>\n<p>Thanks in advance!</p>", "<p>Welcome to the forum <a class=\"mention\" href=\"/u/zurdotti\">@zurdotti</a>.</p>\n<p>Cannot reproduce this on the ddaC demo dataset. Can you share the original binarized image, center of analysis and analysis parameters (radius step size, samples per radius, etc.)? (Overlay and active ROI will be saved if the image header, if you save the image as TIFF).</p>", "<p>Hello <a class=\"mention\" href=\"/u/tferr\">@tferr</a> !<br>\nThank you for your quick response and predisposition.</p>\n<p>Here is the TIFF. I understand FiJi has saved the center of analysis in that file (let me know if not).</p>\n<p>The parameters are: start radius and radius step size both of 5 (end radius of 300) and just 1 sample per radius, considering a set scale where 5 pixel equals 1 um. Everything else is left to default.</p>\n<p>In the meantime, I found and tried the \u201cSkeletonize\u201d tool on \u201cprocess\u2026 binary\u2026\u201d menu and I\u2019m considering switching to that analysis given that I don\u2019t have this problem in skeleton format.<br>\nNever the less, it would be very useful to know and understand what\u2019s happening with the actual file and how is the plugin working. So any contribution will be really appreciated eitherway.</p>\n<p>Thank you very much once again.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/iOz73HHIOux2ANXnZ4wmNzWCqZU.tif\">MAX_C2-LR220527-001s.tif 8bTH0018-1302-1665.tif</a> (4.0 MB)</p>", "<p>Update: Actually, switching to skeletonize leeds to another problem.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/1/01c1318c2ba3321ddacf2c894953b3bd01566c4e.png\" data-download-href=\"/uploads/short-url/fwolEZt4Tha14eMlzCD2M4Dmc6.png?dl=1\" title=\"Skeletonize\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01c1318c2ba3321ddacf2c894953b3bd01566c4e_2_690x357.png\" alt=\"Skeletonize\" data-base62-sha1=\"fwolEZt4Tha14eMlzCD2M4Dmc6\" width=\"690\" height=\"357\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01c1318c2ba3321ddacf2c894953b3bd01566c4e_2_690x357.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01c1318c2ba3321ddacf2c894953b3bd01566c4e_2_1035x535.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01c1318c2ba3321ddacf2c894953b3bd01566c4e_2_1380x714.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01c1318c2ba3321ddacf2c894953b3bd01566c4e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Skeletonize</span><span class=\"informations\">1513\u00d7785 195 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>As shown, there is some artifact in the reconstruction. I think it\u2019s the thick branches that FiJi can\u2019t draw  (skeletonize) correctly. The sholl analysis from that result wouldn\u2019t be really representative of the original astrocyte.<br>\nI really don\u2019t know what to do.</p>\n<p>Thanks again.</p>", "<p>Sorry I\u2019m writing again so soon, but I think I found a key detail.<br>\nI\u2019ve noticed that all the extra points are located on what could be described as an imaginary vertical axis that passes through the center point. This is particularly visible in the very first image I left.<br>\nI\u2019ve already seen a previous post where something similar happened (<a href=\"https://forum.image.sc/t/solved-sholl-analysis-error-false-intersections/24024\" class=\"inline-onebox\">Solved: Sholl Analysis Error: False Intersections</a>). I actually tried modifying the settings of the background/foreground and inverting the image but nothing seems to help at all, I\u2019m still at the same place.<br>\nHopefully I gave a hint of what\u2019s going on.<br>\n\u00bfAny ideas?</p>\n<p>I think this should be a sort of setting that one could disable somehow.</p>\n<p>Thanks once again!</p>", "<p>This issue is different, but your intuition is right: The spurious intersections occur every time there is a connected component at 90 degrees from the center (i.e., if you draw a vertical line going through the center, it will include all the extra counts). I am shocked that nobody ever noticed this before, in all these years. Probably, the reason is that your processes are very wide. This would be hard to reproduce with thinner processes/neurites. Also, it seems the issue would disappear if your image had been rotated by 20 degrees or so.</p>\n<p>Unfortunately, I don\u2019t have time right now to work on this, but I can kludge an extra validator that hopefully fixes the issue. But I\u2019d need you (and others) to test it. Could you please:</p>\n<ol>\n<li>Close Fiji. Make a copy of your <code>Fiji.app</code> folder, and rename it to, e.g., <code>FijiShollDebug.app</code>\n</li>\n<li>Locate <code>FijiShollDebug.app/jars/SNT-4.1.2.jar</code> and delete it</li>\n<li>Download this <s><a class=\"attachment\" href=\"/uploads/short-url/hj8SY3QWsR49yciqEk12ViZJO9a.jar\">SNT-4.1.3-SNAPSHOT-botched.jar</a> (10.2 MB)</s>   <a class=\"attachment\" href=\"/uploads/short-url/pYBWYCz2oLizL4P3cwj7r3OWIMl.jar\">SNT-4.1.3-SNAPSHOT.jar</a> (10.2 MB) <strong>Updated file</strong> into <code>FijiShollDebug.app/jars/</code>\n</li>\n<li>Start the new <code>FijiShollDebug</code> Fiji instance, and run the analysis using this patch</li>\n</ol>\n<p>It would be important to test several images. Also, I\u2019ve added a preference to the <em>Options</em> prompt to set the size of the highlighted intersection points, which hopefully will make them more visible. Do  let us know how it goes.</p>", "<p>Thank you so much again for your answer and help.<br>\nI did as told, but it won\u2019t work, as i\u2019m getting the following exception:</p>\n<pre><code class=\"lang-auto\">Exception in thread \"SciJava-5fb97279-Thread-18\" java.lang.ArrayIndexOutOfBoundsException: 0\n\tat sc.fiji.snt.analysis.sholl.parsers.ImageParser2D.groupPositions(ImageParser2D.java:309)\n\tat sc.fiji.snt.analysis.sholl.parsers.ImageParser2D.targetGroupsPositions(ImageParser2D.java:197)\n\tat sc.fiji.snt.analysis.sholl.parsers.ImageParser2D.parse(ImageParser2D.java:147)\n\tat sc.fiji.snt.plugin.ShollAnalysisImgCmd$AnalysisRunner.run(ShollAnalysisImgCmd.java:1122)\n\tat java.lang.Thread.run(Thread.java:750)\n\n</code></pre>\n<p>We remain in touch</p>", "<p>Bummer. Please re-try with <a href=\"https://forum.image.sc/t/sholl-analysis-wrong-intersection-count/69396/6\">updated file</a>.</p>", "<p>Eureka!<br>\nAlthough the console claims the exception below multiple times, the analysis can be completed and without that tedious extra counts.<br>\nI used the TIFF I\u2019ve uploaded previously to test it. I\u2019ll be analyzing lots of similar images these days with this new plugin and of course I\u2019ll let you know how it goes.</p>\n<pre><code class=\"lang-auto\">java.lang.ArrayIndexOutOfBoundsException: 0\n\tat sc.fiji.snt.analysis.sholl.parsers.ImageParser2D.groupPositions(ImageParser2D.java:310)\n\tat sc.fiji.snt.analysis.sholl.parsers.ImageParser2D.targetGroupsPositions(ImageParser2D.java:197)\n\tat sc.fiji.snt.analysis.sholl.parsers.ImageParser2D.parse(ImageParser2D.java:147)\n\tat sc.fiji.snt.plugin.ShollAnalysisImgCmd$AnalysisRunner.run(ShollAnalysisImgCmd.java:1122)\n\tat java.lang.Thread.run(Thread.java:750)\n</code></pre>\n<p>I\u2019m really thankful for all your help.<br>\nBest regards</p>", "<p>Great! That exception seems to be triggered for every zero-count circle. It is innocuous. We\u2019ll suppress it once a proper release is made.</p>", "<p>\u00a1Hello again!</p>\n<p>After several weeks of working on sholl analysis, I collected a pack of images to show you how I did with this new plug-in. It turns out that it does fix the problem I initially brought here, but it doesn\u2019t work perfectly either. In some images (not all of them) it should count some intersections, but fails to do so. This often occurs when the branch in question is quite thin, although I\u2019ve encountered it on thicker braches as well.</p>\n<p>Below I put some examples, but I\u2019m also attaching a power point file (inside a .rar) where you can see many cases I found. In this file you will see the intersection table and the plot built from it. The missing countings are fixed and highlighted (keep in mind that, logically the \u201cFit\u201d value from the table won\u2019t be exact to that correction, and neither will be the plot). I have also left snapshots of the sholl analysis for every cell, similar to those I uploaded before, to compare. I think they\u2019ll do (you may just zoom it), but if you have any trouble seeing some of them (as they are png snapshots and not the original tiff files per s\u00e9) let me know.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/2/e27fefff1b8cf31ae8e298611cbb09e09b0c2307.png\" alt=\"example_1\" data-base62-sha1=\"wjI0OrjFxpLHg6FZpTWSCeoRTU3\" width=\"276\" height=\"266\"> <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/155767348449afb1c8abae86497057bc43e6f033.png\" alt=\"example_2\" data-base62-sha1=\"32NhOcHTJJVZSW6ESj1tDMdWGOL\" width=\"236\" height=\"256\"> <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/2431762ae330ab50c9baf6e93665efea75e48564.png\" alt=\"example_3\" data-base62-sha1=\"5ab9UGzQ7BlP3BgcItEcHwyjbsE\" width=\"252\" height=\"244\"></p>\n<p>I should clarify that I did use the original SNT Sholl plugin to compare both analysis and in deed the original plugin did found the missing points most of time (although it keeps duplicating as explained before). However, it was quite surprising seeing that there were some exceptions where neither found some intersections! This can be seen, for example, in this case:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/02ad6e614a54f07f62e1710d0b42d82eacd8e4bd.png\" data-download-href=\"/uploads/short-url/nGwPrQmTtD8Uv1pID5nLnCkcH3.png?dl=1\" title=\"Both\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/02ad6e614a54f07f62e1710d0b42d82eacd8e4bd_2_351x500.png\" alt=\"Both\" data-base62-sha1=\"nGwPrQmTtD8Uv1pID5nLnCkcH3\" width=\"351\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/02ad6e614a54f07f62e1710d0b42d82eacd8e4bd_2_351x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/02ad6e614a54f07f62e1710d0b42d82eacd8e4bd.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/02ad6e614a54f07f62e1710d0b42d82eacd8e4bd.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/02ad6e614a54f07f62e1710d0b42d82eacd8e4bd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Both</span><span class=\"informations\">380\u00d7541 19.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I hope all this really helps.<br>\nCheers.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/h8gviEhtpR23flVgiGeagFPvlne.rar\">To ImageJ Forum.rar</a> (3.1 MB)</p>", "<p>Thanks <a class=\"mention\" href=\"/u/zurdotti\">@zurdotti</a>  for looking into this. The illustrations are useful but we cannot really act on them. The most useful for us would be: 1) binary image, 2) the center of analysis and 2) analysis parameters (starting radius, radius step size, etc., [e.g., as recorded by the Macro Recorder]. That way we can study/debug the code while the analysis runs programmatically. Could you please provide such info for at least one of the images?</p>\n<p>Also note that with 3D images the program tries to mitigate segmentation (thresholding) artifacts by ignoring isolated foreground pixels (i.e., those not connected to any other foreground pixels). There is an option to toggle this in the Options prompt (If I recall correctly this used to apply only to 3D images, but maybe it remains active in 2D!?). This does not explain all discrepancies, but it does seem to apply to at least some of your examples (e.g. slide 2)</p>", "<p>Hi again <a class=\"mention\" href=\"/u/tferr\">@tferr</a>!<br>\nThanks for your answer.<br>\nI\u2019m uploading the (binarized) TIFFs of the first 10 images I left on the powerpoint file. I understand they already have set the center of analysis. Both \u2018starting radius\u2019 and \u2018radius step size\u2019 are set to 5 in a scale that equals 1 pixel to .16 \u03bcm (known distance). I think it should do but let me know in any case.</p>\n<p>Best<br>\n<a class=\"attachment\" href=\"/uploads/short-url/4HIi4b24lhk8zAOEbCk4uHlRKCl.rar\">Sholl\u2019s TIFFs.rar</a> (45.4 KB)</p>", "<p>Hello again!<br>\nIm very sorry to revive the thread, but I was wondering if there was any update about all this.<br>\nThank you so much in advance.</p>\n<p>Best regards</p>"], "75542": ["<p>Hey Everyone,<br>\nTogether with <a class=\"mention\" href=\"/u/fabio_tavora\">@Fabio_Tavora</a> we though it might be useful for all of you working on tissue image analysis project to be able to ask pathology questions to pathologists who are members of the forum.<br>\nFabio and myself are pathologists (Fabio - MD pathologist, myself DVM pathologist), and would be happy to discuss pathology questions you might have when working on your projects.</p>\n<p>The tag we will be using is \u201cpathology\u201d so if you have any pathology questions, feel free to use the tag.</p>\n<p>And if you are a pathologist, who wants to help with \u201cAsk the pathologist\u201d initiative, feel free to contribute as well, let\u2019s see where it takes us.</p>\n<p>Happy 2023!</p>", "<p>Thats great <a class=\"mention\" href=\"/u/aleks_dpp\">@aleks_dpp</a></p>\n<p>I can also engage the public on Twitter that is very active. There we use the <span class=\"hashtag\">#pathtwitter</span> to communicate, discuss cases and news on the field.</p>\n<p>FT</p>", "<p>Definitely! I will do that too and on LinkedIn <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Thanks Fabio!</p>", "<p>Thank you both for getting this started!</p>\n<p>*I am definitely not a pathologist, right <a class=\"mention\" href=\"/u/colt_egelston\">@Colt_Egelston</a>?</p>", "<p>Thank you.<br>\nI have a question please, I need to assess the cytoplasmic intensity of set of cases to test increased eosinophilic (eosin) stain versus control cases. However, the stain intensity could differ from a slide to another due to different concentration/ brightness/scanning. How can I test this hypothesis skipping differences not truely related to the research question?<br>\nThanks</p>", "<p>Sharouk,a slide is a self referencing entity. What tissue are you looking at? Do you have a non affected part of the tissue  that could be your  color reference (like an internal negative control) that you  could  compare the other elements to? Feel  free to ask clarifying questions if this answer is not specific  enough.</p>", "<p>Thanks for your reply. I am using breast tissue. You mean I can compare the tumour cells versus the normal breast ducts in every slide?<br>\nWhat about trying colour normalisation (I see a way using MATLAB), does that make sense?</p>", "<p>I\u2019m also a veterinary pathologist and I\u2019ve been using Qupath since 2018 and been on this forum and its previous iterations, getting very valuable help and occasionally helping others with pathology related questions when I can. This is a great initiative, I know there are few other pathologists here and I hope they notice it and can contribute too.</p>\n<p>I use a few different image analysis / digital pathology software for my work, others being commercial proprietary software, but Qupath is my favorite for most projects and one of the main reasons is the great community of users on this forum! Nothing like this exists for the other software I use.</p>", "<p>If color normalization is done systematically by a certain value across the image, than yes - as long as the color relationships between different elements in the slide remain the same you can quantify the differences. And yes, your \u201cinternal control\u201d could be the normal ducts. Is that feasible with MATLAB?</p>", "<p>Great to meet you here! And let\u2019s spread the word among other pathologists to join the thread <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Crosslinking here in case anyone has any thoughts on the PAS staining shown. <a href=\"https://forum.image.sc/t/aid-in-analysis-of-pas-stained-slides-using-qupath/78764\" class=\"inline-onebox\">Aid in analysis of PAS-stained slides using Qupath</a></p>"], "73507": ["<p>Hello,</p>\n<p>I am trying to run NN classification (Local) in ilastik as described <a href=\"https://www.ilastik.org/documentation/nn/nn.html\" rel=\"noopener nofollow ugc\">here</a>. I did not install TikTorch since I am running it on my local computer. After dragging an image to ilastik, when I load the .zip model, which I downloaded from the Bioimage Model Zoo, I get the following error:</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/3/f3f96a34f18dca46ffa06e64e9aaca1421469388.jpeg\" alt=\"Screenshot_error\" data-base62-sha1=\"yOiormk13SMwTungEre8xCVOqUg\" width=\"500\" height=\"246\"></p>\n<p>Any ideas on what this means and how to get it working?</p>\n<p>Pinging <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a></p>\n<p>Thanks,<br>\nVed</p>", "<p>Heya <a class=\"mention\" href=\"/u/vedsharma\">@vedsharma</a>,</p>\n<p>bummer that you run into problems running one of the models. Could you please add a little more detail - like which ilastik version are you using? And which network are you trying to access?</p>\n<p>In the latest version it\u2019s even easier to load models. You can copy paste their <code>nickname</code> or <code>doi</code> from the model card\u2026</p>\n<p>Hope we can resolve this quickly!!</p>\n<p>Cheers<br>\nDominik</p>", "<p>Hi <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a> - thanks for your reply!</p>\n<p>I checked the version, it\u2019s 1.4.0, which I downloaded, I think, a few weeks back. Should I try the latest release candidate? I don\u2019t see an option to load model by name or doi. My ilastik interface looks like this:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/9/29a5116cafcf57f08da9c8ea7ad6defa297b9d52.jpeg\" data-download-href=\"/uploads/short-url/5WperB4uvUicRlw2ahbrkdHQokG.jpeg?dl=1\" title=\"Screenshot2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/9/29a5116cafcf57f08da9c8ea7ad6defa297b9d52_2_576x500.jpeg\" alt=\"Screenshot2\" data-base62-sha1=\"5WperB4uvUicRlw2ahbrkdHQokG\" width=\"576\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/9/29a5116cafcf57f08da9c8ea7ad6defa297b9d52_2_576x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/9/29a5116cafcf57f08da9c8ea7ad6defa297b9d52_2_864x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/9/29a5116cafcf57f08da9c8ea7ad6defa297b9d52.jpeg 2x\" data-dominant-color=\"B3B2B3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot2</span><span class=\"informations\">918\u00d7796 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>yes, please try the latest release candidate!</p>", "<p>I downloaded and installed the latest version, 1.4.0rc6, and the previous error is gone, BUT I am now getting a new error:</p>\n<blockquote>\n<p>ERROR 2022-11-04 13:33:11,684 excepthooks 17864 26340 Unhandled exception in thread: \u2018MainThread\u2019<br>\nERROR 2022-11-04 13:33:11,685 excepthooks 17864 26340 Traceback (most recent call last):<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6\\lib\\site-packages\\ilastik\\applets\\neuralNetwork\\modelStateControl.py\u201d, line 282, in onModelInfoRequested<br>\nmodel_info = load_raw_resource_description(model_uri)<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6\\lib\\site-packages\\bioimageio\\spec\\io_.py\u201d, line 177, in load_raw_resource_description<br>\nraw_rd = schema.load(data)<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6\\lib\\site-packages\\marshmallow\\schema.py\u201d, line 723, in load<br>\ndata, many=many, partial=partial, unknown=unknown, postprocess=True<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6\\lib\\site-packages\\marshmallow\\schema.py\u201d, line 909, in _do_load<br>\nraise exc<br>\nmarshmallow.exceptions.ValidationError: {\u2018format_version\u2019: [\u2018Must be one of: 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7.\u2019]}</p>\n</blockquote>", "<p>Hi <a class=\"mention\" href=\"/u/vedsharma\">@vedsharma</a>,</p>\n<p>sorry you still have problems, but thank you for trying the latest version. In principle all networks should work there - could you maybe share a link to the network you downloaded? Maybe it needs some update\u2026</p>\n<p>Cheers<br>\nD.</p>", "<p>Hi <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>,</p>\n<p>I tried the following model: <a href=\"https://bioimage.io/#/?tags=electron-microscopy&amp;id=10.5281%2Fzenodo.7274275\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">BioImage.IO</a></p>\n<p>10.5281/zenodo.7274275</p>\n<p>Thanks,<br>\nVed</p>", "<p>Hello <a class=\"mention\" href=\"/u/vedsharma\">@vedsharma</a>,</p>\n<p>thanks for the link - I could reproduce the problem. I patched the network and made it available here: <a href=\"https://oc.embl.de/index.php/s/fRR27ASrbfWnO0H\" class=\"inline-onebox\">ownCloud</a></p>\n<p>Background: ilastik relies on the <a href=\"https://github.com/bioimage-io/core-bioimage-io-python\"><code>bioimageio.core</code></a> library to read networks from the model zoo. This library is in active development and the current version in ilastik (<code>0.4.7</code> is older than the one the network was packaged with <code>0.4.8</code>). So the older version will not open a network of a newer version. We will try to avoid this in the future!</p>\n<p>Cheers<br>\nDominik</p>", "<p>Thank you <a class=\"mention\" href=\"/u/k-dominik\">@k-dominik</a>! It works now.</p>", "<p>Dear,</p>\n<p>I have exactly the same problem, with the same model (joyful-deer). I am using the latest version of ilastik (1.4, 20 February 2023) with exactly the same error (see below). iLastik has version 0.5.5, which is higher than the highest required version (0.4.7).</p>\n<p>Small add-on: k-dominik mentioned a possible bioimageio.core version issue (I see mentions of \u2018marshmallow\u2019 in the log). I tried/hoped to circumvent the network io error (network as in \u2018the internet\u2019?) by downloading the zip file and drag and drop it directly in the box in ilastik. It also failed with the same error. So I guess \u2018network\u2019 was intended as in \u2018a neural network\u2019.</p>\n<p>Thank you!</p>\n<p>ERROR 2023-03-22 13:17:50,533 excepthooks 1192 15436 Unhandled exception in thread: \u2018MainThread\u2019<br>\nERROR 2023-03-22 13:17:50,533 excepthooks 1192 15436 Traceback (most recent call last):<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6-gpu\\lib\\site-packages\\ilastik\\applets\\neuralNetwork\\modelStateControl.py\u201d, line 282, in onModelInfoRequested<br>\nmodel_info = load_raw_resource_description(model_uri)<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6-gpu\\lib\\site-packages\\bioimageio\\spec\\io_.py\u201d, line 177, in load_raw_resource_description<br>\nraw_rd = schema.load(data)<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6-gpu\\lib\\site-packages\\marshmallow\\schema.py\u201d, line 723, in load<br>\ndata, many=many, partial=partial, unknown=unknown, postprocess=True<br>\nFile \u201cC:\\Program Files\\ilastik-1.4.0rc6-gpu\\lib\\site-packages\\marshmallow\\schema.py\u201d, line 909, in _do_load<br>\nraise exc<br>\nmarshmallow.exceptions.ValidationError: {\u2018format_version\u2019: [\u2018Must be one of: 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7.\u2019]}</p>", "<p>Hello <a class=\"mention\" href=\"/u/dimivanhecke\">@DimiVanhecke</a>,</p>\n<p>I\u2019m really sorry you run into this problem. It\u2019s really a bit of an annoying one, but mainly related to development being very fast in one of the libraries we use (<code>bioimageio...</code>). The preferred solution here (as in sustainable) would be to download the latest stable release <a href=\"https://www.ilastik.org/download.html\">from our website</a> (1.4.0 without the rc, you\u2019re running 1.4.0rc6). I verified this pretrained model works there. Of course I could modify and upload it for you, but this would only solve the problem for this one network (<a href=\"https://oc.embl.de/index.php/s/9uSKUDzVDXKkPKZ\">here it is in any case</a>).</p>\n<p>Cheers, and sorry again for running into this.</p>\n<p>Dominik</p>", "<p>Hi,</p>\n<p>No need to apologize, I am super-happy for this fast answer. As I understand it, the main issue is with the older version of bioimageio.core that the model is requesting, not an ilastik issue.</p>\n<p>For completeness: I \u2018messed\u2019 around by copying older bioimageio.core versions (acquired through pip install bioimageio.core version==0.4.7) and copying them into the bioimageio folder of ilastik, trying to fool ilastik in having an older version of the package. I did not succeed.</p>"], "67365": ["<p>Here I will give updates about new versions of <a href=\"https://www.jipipe.org/\">JIPipe</a> [pronounced as \u201c<a href=\"http://ipa-reader.xyz/?text=t%CA%83a%C9%AApa%C9%AAp\">t\u0283a\u026apa\u026ap </a>\u201d].</p>\n<h2>\n<a name=\"installation-1\" class=\"anchor\" href=\"#installation-1\"></a>Installation</h2>\n<p>You can always get the newest version via the <code>JIPipe</code> ImageJ update site. Alternatively, we provide the plugin JARs on <a href=\"https://github.com/applied-systems-biology/jipipe/releases\">GitHub</a>.</p>\n<h1>\n<a name=\"version-1720-2\" class=\"anchor\" href=\"#version-1720-2\"></a>Version 1.72.0</h1>\n<ul>\n<li>This is the first public release of JIPipe</li>\n</ul>\n<h1>\n<a name=\"versions-172x-3\" class=\"anchor\" href=\"#versions-172x-3\"></a>Versions 1.72.x</h1>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/arrow_right.png?v=12\" title=\":arrow_right:\" class=\"emoji\" alt=\":arrow_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> Mostly changes to make JIPipe compatible to SciJava Maven and cleanup</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cReduce &amp; split hyperstack\u201d (allows the slicing of Hyperstacks by indices; a more simple alternative to \u201cReduce &amp; split hyperstack (Expression)\u201d)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDraw/modify ROIs\u201d (interactive editing of ROI; it works but is currently incomplete and subject to change)</li>\n</ul>\n<h1>\n<a name=\"version-1730-4\" class=\"anchor\" href=\"#version-1730-4\"></a>Version 1.73.0</h1>\n<h2>\n<a name=\"trainable-weka-segmentation-nodes-5\" class=\"anchor\" href=\"#trainable-weka-segmentation-nodes-5\"></a>Trainable Weka Segmentation nodes</h2>\n<p>JIPipe nodes and data types encapsulating the <a href=\"https://imagej.net/plugins/tws/\">Trainable Weka Segmentation</a> were added, making it possible to utilize these without the <a href=\"https://www.jipipe.org/documentation/standard-library/macro-node/\">Macro node</a>.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Data type \u201cWeka model\u201d (saves classifier, data, and metadata of the segmenter)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cImport Weka model\u201d (allows to import existing models)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTrain Weka model from ROI (2D)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTrain Weka model from ROI (3D)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTrain Weka model from mask (2D)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTrain Weka model from labels (2D)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cWeka classifier 2D\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cWeka classifier 3D\u201d</li>\n</ul>\n<p><em>I could not get the 3D training for masks/labels to run from the Java API; so I would be really happy if someone could help me with this</em></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/a/3a5c078bfe8f25bdf139cfb4d7b18fcf48acdd1b.png\" data-download-href=\"/uploads/short-url/8kgRF7N0uqSp7Dirb0q4nq9VOoX.png?dl=1\" title=\"Example pipeline\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a5c078bfe8f25bdf139cfb4d7b18fcf48acdd1b_2_690x375.png\" alt=\"Example pipeline\" data-base62-sha1=\"8kgRF7N0uqSp7Dirb0q4nq9VOoX\" width=\"690\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a5c078bfe8f25bdf139cfb4d7b18fcf48acdd1b_2_690x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a5c078bfe8f25bdf139cfb4d7b18fcf48acdd1b_2_1035x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a5c078bfe8f25bdf139cfb4d7b18fcf48acdd1b_2_1380x750.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a5c078bfe8f25bdf139cfb4d7b18fcf48acdd1b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Example pipeline</span><span class=\"informations\">1920\u00d71044 343 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The project: (it\u2019s not very nicely organized, but it works; the bird is from the <a href=\"https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/dataset/images/color/42049.html\">Berkeley Segmentation Dataset</a>)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/yN1uO9VhWLVOUsDuYMFMOW8aHbO.zip\">bird-weka-test.zip</a> (30.5 KB)</p>\n<h2>\n<a name=\"skeletons-6\" class=\"anchor\" href=\"#skeletons-6\"></a>Skeletons</h2>\n<p>I was requested to include \u201cAnalyze skeleton 2D/3D\u201d as node, so the user did not have to utilize the Macro node.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cAnalyze skeleton 2D/3D\u201d (tip: Use the \u201cOutputs\u201d parameter section to enable more outputs)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \" Morphological skeletonize 3D\"</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/e/8e8815c13c9eff96ce3685fe03b29ae0a74eceaf.png\" data-download-href=\"/uploads/short-url/kkTmpIkhyJ9GdlQVNac1MKvpxCf.png?dl=1\" title=\"Example pipeline\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e8815c13c9eff96ce3685fe03b29ae0a74eceaf_2_690x375.png\" alt=\"Example pipeline\" data-base62-sha1=\"kkTmpIkhyJ9GdlQVNac1MKvpxCf\" width=\"690\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e8815c13c9eff96ce3685fe03b29ae0a74eceaf_2_690x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e8815c13c9eff96ce3685fe03b29ae0a74eceaf_2_1035x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e8815c13c9eff96ce3685fe03b29ae0a74eceaf_2_1380x750.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/e/8e8815c13c9eff96ce3685fe03b29ae0a74eceaf_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Example pipeline</span><span class=\"informations\">1920\u00d71044 200 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The project:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/1kD6YNlETURtb8vyGGwfg2SGQ99.zip\">skeleton-example.zip</a> (18.0 KB)</p>\n<h2>\n<a name=\"rois-and-labels-7\" class=\"anchor\" href=\"#rois-and-labels-7\"></a>ROIs and labels</h2>\n<p>New nodes were added that simplify the processing of ROI lists and label images</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cROI to labels (Expression)\u201d (the label value is assigned via a custom expression)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cROI to labels (by name)\u201d (the label value is assigned by the name of the ROI)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cLabels to ROI\u201d (based on <a href=\"https://labelstorois.github.io\">https://labelstorois.github.io</a>)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cColor ROI by name\u201d (tip: if you want to color by statistics, there is \u201cColor ROI by statistics\u201d)</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/94186f4a4125c62e6d7bb588bef68402b9233847.png\" data-download-href=\"/uploads/short-url/l8712wQBxPHPiJpLJVsFEzSH5t5.png?dl=1\" title=\"Example pipeline illustration that I quickly put together\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94186f4a4125c62e6d7bb588bef68402b9233847_2_613x500.png\" alt=\"Example pipeline illustration that I quickly put together\" data-base62-sha1=\"l8712wQBxPHPiJpLJVsFEzSH5t5\" width=\"613\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94186f4a4125c62e6d7bb588bef68402b9233847_2_613x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94186f4a4125c62e6d7bb588bef68402b9233847_2_919x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94186f4a4125c62e6d7bb588bef68402b9233847_2_1226x1000.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/94186f4a4125c62e6d7bb588bef68402b9233847_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Example pipeline illustration that I quickly put together</span><span class=\"informations\">1249\u00d71018 87.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The project:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/5auSeiNcDQDRX0tOIqcP3sMRbZk.zip\">roi-labels-test.zip</a> (9.2 KB)</p>\n<h2>\n<a name=\"tiles-and-borders-8\" class=\"anchor\" href=\"#tiles-and-borders-8\"></a>Tiles and borders</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cAdd Border 2D\u201d (adds a constant/reflective/repeating/tiling border around the image; the margin is user-defined)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/arrow_up.png?v=12\" title=\":arrow_up:\" class=\"emoji\" alt=\":arrow_up:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTile image\u201d (2D tiling) was improved to allow the generation of overlapping tiles. Internally, \u201cAdd Border 2D\u201d functionality is utilized to generate the border (configurable)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cUn-tile image\u201d assembles multiple tiles back into one image. Can properly handle overlapping tiles.</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/865e5d403d38952d51acbd8985101a613cdf5147.png\" data-download-href=\"/uploads/short-url/jaG9DT2NvC9DywJVyWpYe26DU8f.png?dl=1\" title=\"Example illustration\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/865e5d403d38952d51acbd8985101a613cdf5147_2_690x413.png\" alt=\"Example illustration\" data-base62-sha1=\"jaG9DT2NvC9DywJVyWpYe26DU8f\" width=\"690\" height=\"413\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/865e5d403d38952d51acbd8985101a613cdf5147_2_690x413.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/865e5d403d38952d51acbd8985101a613cdf5147_2_1035x619.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/865e5d403d38952d51acbd8985101a613cdf5147.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/865e5d403d38952d51acbd8985101a613cdf5147_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Example illustration</span><span class=\"informations\">1134\u00d7680 260 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The project:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/vdyOGCYPMSm0uw5AZwyOyUQYEYG.zip\">tile-untile-example.zip</a> (20.5 KB)</p>\n<h2>\n<a name=\"improved-image-viewer-9\" class=\"anchor\" href=\"#improved-image-viewer-9\"></a>Improved image viewer</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/arrow_up.png?v=12\" title=\":arrow_up:\" class=\"emoji\" alt=\":arrow_up:\" loading=\"lazy\" width=\"20\" height=\"20\"> Improved slider for selecting slice/channel/frame</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/arrow_up.png?v=12\" title=\":arrow_up:\" class=\"emoji\" alt=\":arrow_up:\" loading=\"lazy\" width=\"20\" height=\"20\"> Redesigned ROI manager</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Slider: Click to jump directly to a position</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/arrow_up.png?v=12\" title=\":arrow_up:\" class=\"emoji\" alt=\":arrow_up:\" loading=\"lazy\" width=\"20\" height=\"20\"> Fixed UX glitch caused by the label resizing</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0df2cdc5288b65a3f48ac4a5657ae0b3430ff748.png\" data-download-href=\"/uploads/short-url/1Zopx637gwzbCi1vtvRPRuq0QVa.png?dl=1\" title=\"Illustration of the improvements\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0df2cdc5288b65a3f48ac4a5657ae0b3430ff748_2_690x415.png\" alt=\"Illustration of the improvements\" data-base62-sha1=\"1Zopx637gwzbCi1vtvRPRuq0QVa\" width=\"690\" height=\"415\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0df2cdc5288b65a3f48ac4a5657ae0b3430ff748_2_690x415.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0df2cdc5288b65a3f48ac4a5657ae0b3430ff748_2_1035x622.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0df2cdc5288b65a3f48ac4a5657ae0b3430ff748_2_1380x830.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0df2cdc5288b65a3f48ac4a5657ae0b3430ff748_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Illustration of the improvements</span><span class=\"informations\">1491\u00d7898 139 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"other-enhancements-10\" class=\"anchor\" href=\"#other-enhancements-10\"></a>Other enhancements</h2>\n<ul>\n<li>Renamed duplicate \u201cOverlay images\u201d nodes to \u201cInsert image (masked)\u201d</li>\n<li>Cited algorithms/libraries are now marked as \u201calso cite\u201d</li>\n<li>Macro node now adds a delay before and after running the script to resolve a mysterious absence of some table columns (Many thanks to the comments here: <a href=\"https://forum.image.sc/t/imagej-hiccups-random-false-errors/66970\" class=\"inline-onebox\">ImageJ \"Hiccups\" - Random False Errors</a>)</li>\n</ul>\n<h2>\n<a name=\"api-11\" class=\"anchor\" href=\"#api-11\"></a>API</h2>\n<ul>\n<li>JIPipeDataDisplayOperation now automatically available as JIPipeDataImportOperation</li>\n<li>JIPipeRunnable: Quickly hook into the events (started/finished/interrupted)</li>\n<li>JIPipeParameterAccess: Now requires getAnnotationsOfType()</li>\n<li>New API for annotating DefaultExpressionParameter with variables for the expression builder</li>\n</ul>\n<p><strong>Full Changelog</strong> : <a href=\"https://github.com/applied-systems-biology/jipipe/commits/pom-jipipe-1.73.0\" class=\"inline-onebox\">Commits \u00b7 applied-systems-biology/jipipe \u00b7 GitHub</a></p>", "<p>Continuing the discussion from <a href=\"https://forum.image.sc/t/jipipe-changelog/67365\">JIPipe changelog</a>:</p>\n<p>Great tool that has already saved me a lot of time: prevents writing more python code where you not too rarely have to check the image type between two image operators and adjust it if necessary.</p>\n<p>Especially the Analyze skeleton node has sped up and simplified my image analysis workflow.<br>\nKeep up the good work!</p>", "<h1>\n<a name=\"version-1740-1\" class=\"anchor\" href=\"#version-1740-1\"></a>Version 1.74.0</h1>\n<p>We recently released the newest version of JIPipe that comes with a lot of improvements and new features.</p>\n<p>This changelog is also available on <a href=\"https://www.jipipe.org/download/changelog/1.74.0/\">the JIPipe website</a>.</p>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> Installation guides can be found <a href=\"https://www.jipipe.org/installation/\">here</a><br>\n<img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> We published a new set of text <a href=\"https://www.jipipe.org/tutorials/\">tutorials</a></p>\n<p>If you have any issues or feature request, please do not hesitate to write a post or contact us.</p>\n<h2>\n<a name=\"plugin-manager-2\" class=\"anchor\" href=\"#plugin-manager-2\"></a>Plugin manager</h2>\n<p>In older JIPipe versions, all extensions are automatically loaded during the JIPipe startup. This was changed to reduce the number of dependencies by letting the users decide which extensions should be loaded (excluding core extensions that are mandatory).</p>\n<ul>\n<li>\n<strong>New installations will come with the following extensions enabled:</strong> Annotations, Filesystem, Forms, ImageJ algorithms, ImageJ integration, Plots, Python, R, Strings, Table operations, Tools, Utils, ImageJ2 integration, Multi-parameter algorithms</li>\n<li>\n<strong>Existing installations will enable the following extensions:</strong> Annotations, Filesystem, Forms, ImageJ algorithms, ImageJ integration, Plots, Python, R, Strings, Table operations, Tools, Utils, ImageJ2 integration, Multi-parameter algorithms, Cellpose, CLIJ2 integration, Multi-Template matching, Weka, OMERO</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/eee802ba9f64eb0a25efccc3dde12dd0a9a71115.jpeg\" data-download-href=\"/uploads/short-url/y5sIxXVUi3IP7w4x2GhbcT7RGkJ.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee802ba9f64eb0a25efccc3dde12dd0a9a71115_2_690x455.jpeg\" alt=\"image\" data-base62-sha1=\"y5sIxXVUi3IP7w4x2GhbcT7RGkJ\" width=\"690\" height=\"455\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee802ba9f64eb0a25efccc3dde12dd0a9a71115_2_690x455.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee802ba9f64eb0a25efccc3dde12dd0a9a71115_2_1035x682.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/e/eee802ba9f64eb0a25efccc3dde12dd0a9a71115_2_1380x910.jpeg 2x\" data-dominant-color=\"DFE1E4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1410\u00d7930 212 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"environment-easyinstaller-3\" class=\"anchor\" href=\"#environment-easyinstaller-3\"></a>Environment EasyInstaller</h2>\n<p>In older JIPipe versions, external environments (Python, Cellpose, R, \u2026) can be installed via prepackaged installation scripts that replicate the installation procedure that would be applied by a user. We were made aware of issues that are caused by updated to the software or differences in the system configuration. To simplify the installation of various environments, JIPipe now provides ready-to-used installation packages that can be simply downloaded and extracted via a user-friendly interface termed \u201cEasyInstaller\u201d.</p>\n<p>Following EasyInstaller packages are available:</p>\n<ul>\n<li>Cellpose 2.x (Windows, Linux, macOS) CPU/GPU</li>\n<li>Omnipose (Windows, Linux, macOS) CPU/GPU</li>\n<li>Python with skimage, pandas, \u2026 (Windows, Linux, macOS)</li>\n<li>R (Windows)</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/882affe199451103f3fc1483f25b68b82e26d4ba.png\" data-download-href=\"/uploads/short-url/jqB3QzbAf4yuVB6l8XgSQh6PI9Q.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/882affe199451103f3fc1483f25b68b82e26d4ba_2_642x500.png\" alt=\"image\" data-base62-sha1=\"jqB3QzbAf4yuVB6l8XgSQh6PI9Q\" width=\"642\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/882affe199451103f3fc1483f25b68b82e26d4ba_2_642x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/8/882affe199451103f3fc1483f25b68b82e26d4ba_2_963x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/8/882affe199451103f3fc1483f25b68b82e26d4ba.png 2x\" data-dominant-color=\"E5E5E5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1154\u00d7898 94.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"trackmate-integration-beta-4\" class=\"anchor\" href=\"#trackmate-integration-beta-4\"></a>TrackMate integration (Beta)</h2>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> The TrackMate integration needs to be activated via the extensions manager. Go to \u2018Plugins &gt; Manage JIPipe plugins\u2019 and activate the TrackMate extension.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The feature set of TrackMate was integrated into JIPipe</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\">  The functions are split across multiple nodes to ensure maximum flexibility</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\">  TrackMate plugins (e.g., Cellpose spot detector) are automatically incorporated</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0e3c1d32159b3dabd618d2ebf6e8d196c269b7ac.png\" data-download-href=\"/uploads/short-url/21VtEIyIavmKKq9rlZZdctehV2k.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e3c1d32159b3dabd618d2ebf6e8d196c269b7ac_2_690x401.png\" alt=\"image\" data-base62-sha1=\"21VtEIyIavmKKq9rlZZdctehV2k\" width=\"690\" height=\"401\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/e/0e3c1d32159b3dabd618d2ebf6e8d196c269b7ac_2_690x401.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0e3c1d32159b3dabd618d2ebf6e8d196c269b7ac.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/e/0e3c1d32159b3dabd618d2ebf6e8d196c269b7ac.png 2x\" data-dominant-color=\"EBF0EF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">727\u00d7423 25.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><a href=\"https://www.jipipe.org/changelog/1.74.0/trackmate-example.zip\">Example project</a> | <a href=\"https://imagej.net/plugins/trackmate/getting-started\">Dataset</a> | <a href=\"https://www.jipipe.org/examples/how-to-load-projects/\">How to load examples</a></p>\n<h2>\n<a name=\"cellpose-integration-5\" class=\"anchor\" href=\"#cellpose-integration-5\"></a>Cellpose integration</h2>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> The Cellpose integration needs to be activated via the extensions manager. Go to \u2018Plugins &gt; Manage JIPipe plugins\u2019 and activate the Cellpose extension.</p>\n<p>The Cellpose integration was rewritten to make use of the improved Cellpose CLI.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\">  New nodes designed for Cellpose 2.x</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\">  Old Cellpose nodes will still work but are deprecated</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\">  Cellpose EasyInstaller that provides prepacked versions of Cellpose</li>\n</ul>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/7/d72ca88077049730ad9fde6ebec67c735c343f0f.png\" alt=\"image\" data-base62-sha1=\"uHwkPhiGWW7WxSaCGEuW88zKtZl\" width=\"509\" height=\"279\"></p>\n<h2>\n<a name=\"omnipose-integration-beta-6\" class=\"anchor\" href=\"#omnipose-integration-beta-6\"></a>Omnipose integration (Beta)</h2>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> The Omnipose integration needs to be activated via the extensions manager. Go to \u2018Plugins &gt; Manage JIPipe plugins\u2019 and activate the Omnipose extension.</p>\n<p>Based on the improved Cellpose integration, Omnipose was integrated using dedicated nodes. Please note that Omnipose is based on Cellpose 1.x and requires a dedicated environment.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\">  Omnipose segmentation and training nodes</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Cellpose EasyInstaller that provides prepacked versions of Cellpose</li>\n</ul>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/3/d3cf9b36d37d231129c05734eb7a51e587c977c9.png\" alt=\"image\" data-base62-sha1=\"udLxwxk5nuBbZc2kUTgGshHYt6N\" width=\"597\" height=\"278\"></p>\n<p><a href=\"https://www.jipipe.org/changelog/1.74.0/omnipose-example.zip\">Example project</a> | <a href=\"http://www.cellpose.org/dataset_omnipose\">Dataset</a> | <a href=\"https://www.jipipe.org/examples/how-to-load-projects/\">How to load examples</a></p>\n<h2>\n<a name=\"image-processing-7\" class=\"anchor\" href=\"#image-processing-7\"></a>Image processing</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cSet physical dimensions\u201d/\u201cSet physical dimensions from expressions\u201d now can set the time and value units</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cSet physical dimensions from annotations\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cRender overlay\u201d (Renders the overlay ROI of an image)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cSlice ROI list\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cSort and extract ROI by statistics\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cSort ROI list (expression)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cObject-based iterative thresholding 2D\u201d (finds an optimal threshold based on the properties of the detected object; criteria are defined via expressions)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cFlood fill\u201d (flood fill starting at ROI locations)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cConvert to centroid\u201d (converts ROI to point ROI that contain its centroid)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDetect lines 2D (Hough)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> New node for splitting channels (\u201cSplit channels\u201d) that fully replicates the behavior of ImageJ\u2019s channel splitter</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\">  The old \u201cSplit channels\u201d node was deprecated</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cMorphological operation 2D\u201d: option to add border around the image before processing (border is then removed; to avoid artifacts)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> New node \u201cZ-Project\u201d that can properly handle 5D images and can also apply C-Project and T-Project</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\">  The old \u201cZ-Project\u201d node was deprecated</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cOutline ROI\u201d now can generate minimum bounding rectangle</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cKey/Value Histogram 5D\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cRound float image\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cVoronoi 2D\u201d can now binarize the output</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cImage calculator 2D\u201d is now properly creating Float32 output if requested</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cBlend images\u201d (Blends multiple images as in an image editor)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\">  The \u201cOverlay images\u201d node was deprecated (replaced by \u201cBlend images\u201d)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cThreshold/Value statistics 5D (fast, average)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cThreshold/Value statistics 5D (fast)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cThreshold/Value statistics 5D\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDraw scale bar\u201d</li>\n</ul>\n<h2>\n<a name=\"roi-processing-8\" class=\"anchor\" href=\"#roi-processing-8\"></a>ROI processing</h2>\n<p>There were various ROI-generating nodes with different feature sets. A new set of nodes was developed that make use of modern JIPipe API features. All nodes consume two optional inputs:</p>\n<ol>\n<li>\n<strong>ROI</strong>: Connect an existing ROI list to append to it. If left unconnected, a new ROI list is generated</li>\n<li>\n<strong>Reference</strong>: Connect an image to position ROI according to the image dimensions. If nothing is connected, the boundaries of the ROI provided by <strong>ROI</strong> is used, otherwise the width and height is assumed to be zero.</li>\n</ol>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDraw oval ROI\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDraw rectangular ROI\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDraw text ROI\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\"> Deprecated: Append rectangular ROI, Define rectangular ROI, Append rectangular ROI (referenced), Define rectangular ROI (referenced)</li>\n</ul>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/3/d3d4ac74444b0e8cbd74e0c46937d3c9cf021ec0.png\" alt=\"image\" data-base62-sha1=\"udWoDP0zcdUmI3kOTVGmH3OMSc0\" width=\"581\" height=\"334\"></p>\n<h2>\n<a name=\"coloc2-integration-9\" class=\"anchor\" href=\"#coloc2-integration-9\"></a>Coloc2 integration</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The functionality of <a href=\"https://imagej.net/plugins/coloc-2\">Coloc2</a> was integrated into a node \u201cColoc2\u201d.</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2bd3ad2f76699fb7bb617f84981f2af8334d5116.png\" data-download-href=\"/uploads/short-url/6fI2R1avPkPbjfM5wnEmMGnXaPY.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bd3ad2f76699fb7bb617f84981f2af8334d5116_2_690x455.png\" alt=\"image\" data-base62-sha1=\"6fI2R1avPkPbjfM5wnEmMGnXaPY\" width=\"690\" height=\"455\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bd3ad2f76699fb7bb617f84981f2af8334d5116_2_690x455.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bd3ad2f76699fb7bb617f84981f2af8334d5116_2_1035x682.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2bd3ad2f76699fb7bb617f84981f2af8334d5116_2_1380x910.png 2x\" data-dominant-color=\"EDEEEE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1410\u00d7930 105 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a href=\"https://www.jipipe.org/changelog/1.74.0/coloc2-example.zip\">Example project</a> | <a href=\"https://forum.image.sc/t/colocalization-analysis-using-coloc2-help/22742\">Dataset</a> | <a href=\"https://www.jipipe.org/examples/how-to-load-projects/\">How to load examples</a></p>\n<h2>\n<a name=\"table-processing-10\" class=\"anchor\" href=\"#table-processing-10\"></a>Table processing</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cAdd missing rows (series)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node for importing tables from XLSX</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Export tables now allows export to XLSX</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTable to histogram\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cApply expression per row\u201d now has access to other column values</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cApply expression to columns\u201d now has access to other column values</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cAnnotate data with table values\u201d</li>\n</ul>\n<h2>\n<a name=\"plotting-11\" class=\"anchor\" href=\"#plotting-11\"></a>Plotting</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> New plotting nodes that are generated for each plotting type</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\">  Node \u201cPlot tables\u201d was deprecated</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47d9f3512671e4a4bea551e404259cd58b1f162e.png\" data-download-href=\"/uploads/short-url/afCR8HvvL70Zkx9fC5xgO3QHPXg.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/7/47d9f3512671e4a4bea551e404259cd58b1f162e.png\" alt=\"image\" data-base62-sha1=\"afCR8HvvL70Zkx9fC5xgO3QHPXg\" width=\"690\" height=\"444\" data-dominant-color=\"EBECEE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">830\u00d7535 26.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"general-data-processing-12\" class=\"anchor\" href=\"#general-data-processing-12\"></a>General data processing</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cSort data rows (Expression)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cOverride annotations\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cLoop start\u201d: if set to \u201cPass through\u201d, the loop mode is set to \u201cPass through\u201d</li>\n</ul>\n<h2>\n<a name=\"parameters-13\" class=\"anchor\" href=\"#parameters-13\"></a>Parameters</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cGenerate parameters from expression\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The parameter table editor (\u201cDefine multiple parameters\u201d) was redesigned and simplified</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/2429542a87ac495efa5a07adc3154c98fd617c96.png\" data-download-href=\"/uploads/short-url/59TJAYBAVGbMuResBlTpIGA0Nym.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/2429542a87ac495efa5a07adc3154c98fd617c96_2_642x500.png\" alt=\"image\" data-base62-sha1=\"59TJAYBAVGbMuResBlTpIGA0Nym\" width=\"642\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/2429542a87ac495efa5a07adc3154c98fd617c96_2_642x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/2429542a87ac495efa5a07adc3154c98fd617c96_2_963x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/2429542a87ac495efa5a07adc3154c98fd617c96.png 2x\" data-dominant-color=\"EAEAEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1154\u00d7898 54.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Adaptive parameters and multi-parameter settings were moved into a dedicated panel \u201cAdvanced parameters\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The \u201cAdvanced parameters\u201d panel comes with documentation that explains how the features work</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/3/53cf8e1b12fa1b591591f7564bf67ac8e584a85c.png\" data-download-href=\"/uploads/short-url/bXqjw5wVdzSuWCa2UPUwPeGdFak.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/3/53cf8e1b12fa1b591591f7564bf67ac8e584a85c_2_335x499.png\" alt=\"image\" data-base62-sha1=\"bXqjw5wVdzSuWCa2UPUwPeGdFak\" width=\"335\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/3/53cf8e1b12fa1b591591f7564bf67ac8e584a85c_2_335x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/3/53cf8e1b12fa1b591591f7564bf67ac8e584a85c_2_502x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/3/53cf8e1b12fa1b591591f7564bf67ac8e584a85c.png 2x\" data-dominant-color=\"E8E8E9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">586\u00d7873 52.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"expressions-14\" class=\"anchor\" href=\"#expressions-14\"></a>Expressions</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Escape operator to simplify the creation of functions. Example <code>${x + \"y\"}</code> will evaluate to <code>\"x + \\\"y\\\"\"</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Pair operator <code>x: y</code> which evaluates to <code>PAIR(x, y)</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Custom expression variables: various nodes allow users to define custom variables based on annotations or other properties of the processed data</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>IF_ELSE_EXPR</code> (lazy <code>IF_ELSE</code>)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>FUNCTION</code> to define new user functions</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>RUN_FUNCTION</code> to run a user function</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Existing <code>RUN_FUNCTION</code> was renamed to <code>APPLY_FUNCTION_TO_ARRAY</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>SET_VARIABLES</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>STRING_TRUNCATE</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>SUMMARIZE_ANNOTATIONS_MAP</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>ROUNDD</code> (Round to specific number of decimals)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Quantity conversion functions support new units: inch, foot, yard, mile, Dalton, ounce, pound</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>SLICE</code> for slicing arrays/lists</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>MAKE_SEQUENCE_EXPR</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> <code>GET_ITEM</code> can handle negative indices (accessing the -nth last item)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>TRANSFORM_ARRAY_CUMULATIVE</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Function <code>TRANSFORM_ARRAY</code>\n</li>\n</ul>\n<h2>\n<a name=\"node-examples-15\" class=\"anchor\" href=\"#node-examples-15\"></a>Node examples</h2>\n<p>In previous JIPipe versions, various nodes included a \u201cLoad example\u201d button that allowed users to learn about how to utilize the node. An issue with this functionality is that it cannot be easily discovered by users, e.g., via the search box. Additionally, examples cannot be created and distributed by users due to the reliance on Java code. The new version of JIPipe introduces a standardized node example system that can dynamically load examples from files, JAR resources, and node templates.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node examples that can be accessed via the \u201cExamples\u201d tab on selecting a node</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Search box / available nodes were improved to include node examples</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Examples for various existing nodes</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Old examples (\u201cLoad example\u201d) were migrated to new example API</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/125064e4c48bc8ce25dd1ce69710e062496c729c.png\" data-download-href=\"/uploads/short-url/2C0PXwK8RK14uVhGjhNDQR2kb0g.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/125064e4c48bc8ce25dd1ce69710e062496c729c_2_690x498.png\" alt=\"image\" data-base62-sha1=\"2C0PXwK8RK14uVhGjhNDQR2kb0g\" width=\"690\" height=\"498\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/125064e4c48bc8ce25dd1ce69710e062496c729c_2_690x498.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/125064e4c48bc8ce25dd1ce69710e062496c729c_2_1035x747.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/2/125064e4c48bc8ce25dd1ce69710e062496c729c_2_1380x996.png 2x\" data-dominant-color=\"F5F4F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1837\u00d71327 206 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"user-interface-16\" class=\"anchor\" href=\"#user-interface-16\"></a>User interface</h2>\n<h3>\n<a name=\"project-management-17\" class=\"anchor\" href=\"#project-management-17\"></a>Project management</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> An online-repository system was implemented that allows to download template projects from the web (if you want to add your own project as downloadable example, file a pull request to <a href=\"https://github.com/applied-systems-biology/JIPipe-Repositories/blob/64abba677014fe326474668d1b8ad3cc9211d8cd/project-templates/project-templates.json\" class=\"inline-onebox\">JIPipe-Repositories/project-templates.json at 64abba677014fe326474668d1b8ad3cc9211d8cd \u00b7 applied-systems-biology/JIPipe-Repositories \u00b7 GitHub</a>)</li>\n</ul>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> By default, JIPipe uses the repository <a href=\"https://raw.githubusercontent.com/applied-systems-biology/JIPipe-Repositories/main/project-templates/project-templates.json\">https://raw.githubusercontent.com/applied-systems-biology/JIPipe-Repositories/main/project-templates/project-templates.json</a>. Feel free to configure the repository list within Project &gt; Application settings &gt; General &gt; Projects &gt; Template downloader repositories</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/1/210380bf81d421cbde7b4cc737776446675cff0c.png\" data-download-href=\"/uploads/short-url/4I3gdcZT8QuXFOhhjtuJFy5wSTi.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/210380bf81d421cbde7b4cc737776446675cff0c_2_690x476.png\" alt=\"image\" data-base62-sha1=\"4I3gdcZT8QuXFOhhjtuJFy5wSTi\" width=\"690\" height=\"476\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/210380bf81d421cbde7b4cc737776446675cff0c_2_690x476.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/210380bf81d421cbde7b4cc737776446675cff0c_2_1035x714.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/1/210380bf81d421cbde7b4cc737776446675cff0c_2_1380x952.png 2x\" data-dominant-color=\"EEEEEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1770\u00d71222 300 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"parameters-18\" class=\"anchor\" href=\"#parameters-18\"></a>Parameters</h3>\n<p>In older JIPipe versions, the help of a parameter was displayed upon hovering the item via the mouse. This lead to issues with usability, as well as a slowdown of the interface due to the involvement of performance-intensive Java functions. Since this version, the parameter documentation can be opened by clicking the <code>?</code> icon next to the entry.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Parameter help now is displayed by clicking the <code>?</code> button</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> General node parameters are now separated from node-specific settings</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> All list parameter editors were improved</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The \u201cAdd parameter\u201d dialog was redesigned</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Custom/dynamic parameters now are added via a dedicated parameter editor dialog</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The parameter reference editor was redesigned</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/9/b96b0febc92cd8fd15edb425ff559343a2a3c4f3.png\" data-download-href=\"/uploads/short-url/qshI7QWFLvJcpHyU57noFPHAdKH.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b96b0febc92cd8fd15edb425ff559343a2a3c4f3_2_690x498.png\" alt=\"image\" data-base62-sha1=\"qshI7QWFLvJcpHyU57noFPHAdKH\" width=\"690\" height=\"498\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b96b0febc92cd8fd15edb425ff559343a2a3c4f3_2_690x498.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b96b0febc92cd8fd15edb425ff559343a2a3c4f3_2_1035x747.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b96b0febc92cd8fd15edb425ff559343a2a3c4f3_2_1380x996.png 2x\" data-dominant-color=\"F2F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1837\u00d71327 214 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"graph-editor-19\" class=\"anchor\" href=\"#graph-editor-19\"></a>Graph editor</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Double-click compartment inputs to navigate to the associated compartment</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Overlapping nodes could not always be dragged</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creating edges is now smarter: if there is only one input/output, it is sufficient to drag a line to the node (before: needed to drag a line to the slot). Dragged edges now \u201csnap\u201d to the last output.</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Resolved slow redraw on Linux</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Slots can be right-clicked to open the slot menu</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Edges are now displayed with arrow heads</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cIsolate\u201d now has a hotkey</li>\n</ul>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/9/79ba5b05a36f3f4cbb9a8dc72f6a000077701c8f.gif\" alt=\"graph-editor-snapping\" data-base62-sha1=\"hmR2pncIBmOQnzxka9uZxb79HKT\" width=\"600\" height=\"458\" class=\"animated\"></p>\n<h3>\n<a name=\"compartment-editor-20\" class=\"anchor\" href=\"#compartment-editor-20\"></a>Compartment editor</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Compartments now can be \u201cexecuted\u201d by clicking the play button. This will update the cache / execute a quick-run for the compartment output</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Pasting compartments did not preserve the locations of the contained nodes</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Compartments now can reference parameters from their nodes</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1ba62c2446fee3d0e159a76cf2574e20b8213e0.png\" data-download-href=\"/uploads/short-url/pmfQctlnRmCF509b2laHrXcuxji.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1ba62c2446fee3d0e159a76cf2574e20b8213e0_2_690x491.png\" alt=\"image\" data-base62-sha1=\"pmfQctlnRmCF509b2laHrXcuxji\" width=\"690\" height=\"491\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1ba62c2446fee3d0e159a76cf2574e20b8213e0_2_690x491.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1ba62c2446fee3d0e159a76cf2574e20b8213e0_2_1035x736.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1ba62c2446fee3d0e159a76cf2574e20b8213e0_2_1380x982.png 2x\" data-dominant-color=\"F4F2F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1837\u00d71308 188 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"cache-browser-results-viewer-21\" class=\"anchor\" href=\"#cache-browser-results-viewer-21\"></a>Cache browser / Results viewer</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The UI was redesigned with a Ribbon and greatly simplified</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Annotations of selected data rows can be opened as table<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5d3d90c0041182d8e65a8223a189997d1d1b2ff4.png\" data-download-href=\"/uploads/short-url/diQj6fo5aqzZ4SC7vMbKQaV67UE.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5d3d90c0041182d8e65a8223a189997d1d1b2ff4_2_690x491.png\" alt=\"image\" data-base62-sha1=\"diQj6fo5aqzZ4SC7vMbKQaV67UE\" width=\"690\" height=\"491\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5d3d90c0041182d8e65a8223a189997d1d1b2ff4_2_690x491.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5d3d90c0041182d8e65a8223a189997d1d1b2ff4_2_1035x736.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/d/5d3d90c0041182d8e65a8223a189997d1d1b2ff4_2_1380x982.png 2x\" data-dominant-color=\"F2F1F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1837\u00d71308 200 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div>\n</li>\n</ul>\n<h3>\n<a name=\"imagej-alias-menu-22\" class=\"anchor\" href=\"#imagej-alias-menu-22\"></a>ImageJ alias menu</h3>\n<p>To facility the transition from ImageJ, a function was introduced to organize JIPipe functionality into different menu locations (alias). Various existing JIPipe nodes were organized into a menu \u201cIJ\u201d that follows the structure of the ImageJ menu. For example, <code>Images &gt; Threshold &gt; Auto Threshold 2D</code> is also organized into <code>IJ &gt; Image &gt; Adjust &gt; Auto threshold ...</code>.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node menu <code>IJ</code> that follows the structure of ImageJ</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\">  Aliases are displayed and searchable via the search box / available nodes search</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\">  Aliases are displayed in the algorithm compendium</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/5/3572619190a49d4bdd5d36b332b2fb360eb68b92.png\" data-download-href=\"/uploads/short-url/7COmIk010mqV2NHnDsmhHpfCsy6.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/5/3572619190a49d4bdd5d36b332b2fb360eb68b92.png\" alt=\"image\" data-base62-sha1=\"7COmIk010mqV2NHnDsmhHpfCsy6\" width=\"467\" height=\"500\" data-dominant-color=\"E8E9EA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">503\u00d7538 27.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"image-viewer-23\" class=\"anchor\" href=\"#image-viewer-23\"></a>Image viewer</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Increased animation speed for channel/frame/depth slider</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Enhanced ROI rendering via a standardized component based on the \u201cConvert ROI to RGB\u201d node., Users have full control over many aspects of ROI rendering.</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> ROI rendering as overlay (as done in ImageJ). This means that ROI renders will not pixelate on zooming into images</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Improved zooming behavior (new formula)</li>\n</ul>\n<h4>\n<a name=\"roi-manager-24\" class=\"anchor\" href=\"#roi-manager-24\"></a>ROI manager</h4>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The UI was redesigned by implementing a Ribbon interface</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The ROI manager now can measure the selected ROI</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> ROI can now be selected via a mouse selection tool (\u201cPick\u201d)</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/653c70a921284de9c52b759ef344d71487258bee.png\" data-download-href=\"/uploads/short-url/erzIrlnEjx21q9MrcsnMFGYy2C2.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653c70a921284de9c52b759ef344d71487258bee_2_642x500.png\" alt=\"image\" data-base62-sha1=\"erzIrlnEjx21q9MrcsnMFGYy2C2\" width=\"642\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653c70a921284de9c52b759ef344d71487258bee_2_642x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/653c70a921284de9c52b759ef344d71487258bee_2_963x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/653c70a921284de9c52b759ef344d71487258bee.png 2x\" data-dominant-color=\"9E9C90\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1154\u00d7898 165 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h4>\n<a name=\"measurement-tool-mask-drawer-roi-drawer-25\" class=\"anchor\" href=\"#measurement-tool-mask-drawer-roi-drawer-25\"></a>Measurement tool / mask drawer / ROI drawer</h4>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The UI was redesigned by implementing a Ribbon interface</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4990ba2f0a72038f1f03b4ac76bc839ede24406.png\" data-download-href=\"/uploads/short-url/pLDQgHQ2QB7Qvv2If3KPyYdvAto.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4990ba2f0a72038f1f03b4ac76bc839ede24406_2_690x427.png\" alt=\"image\" data-base62-sha1=\"pLDQgHQ2QB7Qvv2If3KPyYdvAto\" width=\"690\" height=\"427\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4990ba2f0a72038f1f03b4ac76bc839ede24406_2_690x427.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4990ba2f0a72038f1f03b4ac76bc839ede24406_2_1035x640.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4990ba2f0a72038f1f03b4ac76bc839ede24406_2_1380x854.png 2x\" data-dominant-color=\"ACABAB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1448\u00d7898 128 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"plot-editor-26\" class=\"anchor\" href=\"#plot-editor-26\"></a>Plot editor</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The plot editor now saves ZIP files instead of directories (easier to handle)</li>\n</ul>\n<h3>\n<a name=\"table-editor-27\" class=\"anchor\" href=\"#table-editor-27\"></a>Table editor</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> The table editor was redesigned with a Ribbon UI</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/6552957569ab2c013409e91a5686ed2632ff4b42.png\" data-download-href=\"/uploads/short-url/esl9TgMSiBml3PHDd9Fr1qsBAj0.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/6552957569ab2c013409e91a5686ed2632ff4b42_2_642x500.png\" alt=\"image\" data-base62-sha1=\"esl9TgMSiBml3PHDd9Fr1qsBAj0\" width=\"642\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/6552957569ab2c013409e91a5686ed2632ff4b42_2_642x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/6552957569ab2c013409e91a5686ed2632ff4b42_2_963x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/6552957569ab2c013409e91a5686ed2632ff4b42.png 2x\" data-dominant-color=\"E7E6E6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1154\u00d7898 115 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"dependency-management-28\" class=\"anchor\" href=\"#dependency-management-28\"></a>Dependency management</h3>\n<p>On loading pipelines with missing dependencies, users are prompted with dialogs to indicate that extensions are missing.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Redesigned \u201cMissing dependencies\u201d dialog that allows to enable the missing extensions directly within the dialog</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Users are now informed about missing environments (Cellpose, \u2026) on loading a project</li>\n</ul>\n<h3>\n<a name=\"node-templates-29\" class=\"anchor\" href=\"#node-templates-29\"></a>Node templates</h3>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> You can now right-click entries in the node template editor</li>\n</ul>\n<h2>\n<a name=\"data-management-30\" class=\"anchor\" href=\"#data-management-30\"></a>Data management</h2>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> Due to the improved data exporter interface, some parameters were deleted. Please evaluate the parameters of \u201cExport data\u201d/\u201cExport images\u201d/\u201cExport table\u201d</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Greatly simplified data exporter interface (for example used in \u201cExport data\u201d node): functions for automated name generation were removed and merged into expressions</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Exporting data tables to *.zip</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Importing data tables from *.zip</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Tables can now exported to XLSX (table editor/cache browser/\u2026)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> ImageJ images now save ROI overlays</li>\n</ul>\n<h2>\n<a name=\"data-api-31\" class=\"anchor\" href=\"#data-api-31\"></a>Data API</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\">  The standardized output format now saves thumbnails, thus avoiding the loading of data in the result preview window</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\">  New API to simplify the handling of streamed storage (ZIP etc.)</li>\n</ul>\n<h2>\n<a name=\"bugfixes-and-small-improvements-32\" class=\"anchor\" href=\"#bugfixes-and-small-improvements-32\"></a>Bugfixes and small improvements</h2>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> Please refer to the commits in the JIPipe GitHub repository to find a list of all bugfixes and improvements.</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\">  Default to 1 compartment projects (requested by users)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Measurements: default to generate all available measurements</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Better detection of R</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Added various color maps from ImageJ</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> The application settings UI was improved</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Various measurement nodes (\u201cExtract ROI statistics\u201d, \u201cFind particles 2D\u201d, \u201cExtract image statistics\u201d, \u2026) did not support the measurement with physical units. A toggle was added (defaults to enabled) that instructs ImageJ to measure with physical sizes.</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Local threshold were no applied to stacks</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cReorder dimensions\u201d and \u201cSet Hyperstack dimensions\u201d now behave as expected</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> OMERO data could not be imported due to missing default constructor</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\">  Cellpose Miniconda installer is now deprecated due to unintended side effects that are introduced by the Miniconda installer</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Backups are now sorted properly in the \u201cRestore backup\u201d dialog</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cIO interface\u201d: Pass through works as expected</li>\n</ul>\n<h2>\n<a name=\"known-issues-33\" class=\"anchor\" href=\"#known-issues-33\"></a>Known issues</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\">  Links within the HTML editor are currently not clickable</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\">  Omnipose: we were so far not able to train an Omnipose model (due to the lack of training data that works for us)</li>\n</ul>", "<p>Unfortunately I had no time to update this post with the newest changes. So here are all new releases, including the latest one that released today (12.01.2023).</p>\n<h1>\n<a name=\"version-175x-1\" class=\"anchor\" href=\"#version-175x-1\"></a>Version 1.75.x</h1>\n<h2>\n<a name=\"image-processing-2\" class=\"anchor\" href=\"#image-processing-2\"></a>Image processing</h2>\n<p>Various nodes that generate statistics on labels</p>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\">  Node \u201cMerge 2D slices into hyperstack\u201d (inverse operation of \u201cSplit into 2D slices\u201d). Intended for more advanced filtering of hyperstack slices.</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cMerge small labels\u201d (merges labels with a low number of pixels into a neighboring label index)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cKey/Value Histogram 5D\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cKey/Value threshold statistics 5D\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cKey/Value statistics 5D (fast averages)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cKey/Value threshold statistics 5D (fast averages)\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\">  Convolve operators now have normalization parameter</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Convolve operators now support RGB images (same behavior as ImageJ)</li>\n</ul>\n<h2>\n<a name=\"data-processing-3\" class=\"anchor\" href=\"#data-processing-3\"></a>Data processing</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cFilter by annotation\u201d (more compact version of \u201cSplit &amp; filter by annotation\u201d that applies the same operation)</li>\n</ul>\n<h2>\n<a name=\"expressions-4\" class=\"anchor\" href=\"#expressions-4\"></a>Expressions</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> New function <code>STRING_FORMAT</code> (C-style string formatting)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> New function <code>NUM</code> (Alternative to <code>TO_NUMBER</code>)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Expressions that return numbers: valid numeric strings are automatically converted to numbers (before there was an exception if not a <code>Number</code> type is returned)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Escaping of expressions did not work as expected (errors if sub-strings contained spaces)</li>\n</ul>\n<h2>\n<a name=\"user-interface-5\" class=\"anchor\" href=\"#user-interface-5\"></a>User interface</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> New \u201cMemory\u201d menu in the status bar that contains all memory-related operations (before was only a gear icon)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cReduce memory\u201d moved into the \u201cMemory\u201d menu</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Number editor behaved erratically in non-english locales (now uses the advanced number parser capable of parsing NA and infinity)</li>\n</ul>\n<h2>\n<a name=\"bugfixes-and-small-improvements-6\" class=\"anchor\" href=\"#bugfixes-and-small-improvements-6\"></a>Bugfixes and small improvements</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Various expression-based nodes now support custom expression variables: Filter labels by statistics, Filter ROI by statistics, Change ROI properties (Expression), Set ROI metadata by statistics (expression), Sort ROI list (expression)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Numeric function expression now appropriately sets the <code>default</code> variable</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Crop 2D node: expression variables are now all annotated</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cExamples\u201d panel now always shown</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Data processing: empty data batches are removed/ignored (only affects nodes with optional inputs)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Various exceptions caused by the \u201cData batches\u201d panel</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Image viewer: Movie/sequence export was not adhering to magnification level (cut-off images)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Users were able to enable ROI/Mask for \u201cBright spots segmentation 2D\u201d, \u201cHessian segmentation 2D\u201d and \u201cInternal gradient segmentation 2D\u201d. Users are now advised to build a custom set of nodes (documentation was added)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Convolution nodes now behave exactly as the convolution operations from ImageJ</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Expressions are now marked more clearly in the parameter UI</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical bug where enum parameters could not be initialized</li>\n</ul>\n<h2>\n<a name=\"version-1760-7\" class=\"anchor\" href=\"#version-1760-7\"></a>Version 1.76.0</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Layout manager: do not move nodes (by default)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cAdd table columns\u201d now supports annotations and custom variables</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cAdd table columns\u201d now allows to ensure a minimum number of rows</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cSplit ROI into individual ROI lists\u201d: support for custom annotation value</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> UX of the cache manager button in the menu bar</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cReduce &amp; split stacks (slice)\u201d now supports annotations</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> UX of the task/process manager button in the menu bar (now can cancel enqueued tasks)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Cache cleanup performance</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Image viewer: minima and maxima for calibration are now sourced from all slices</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Organization of annotation nodes (now in sub-menus that indicate the type of addressed data)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> UX of \u201cAdd slots\u201d/\u201cEdit slots\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Cache manager: saving/loading all cached data from directory/zip</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cDraw scale bar ROI\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cCombine ROI lists\u201d (like Merge ROI lists but with strict ordering)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cPull annotations from data tables\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cAnnotate with data table properties\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cPush annotations into data tables\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Draw ROI nodes not applying ROI properties</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Comment nodes: crash if icon is null</li>\n</ul>\n<h2>\n<a name=\"version-1770-8\" class=\"anchor\" href=\"#version-1770-8\"></a>Version 1.77.0</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Crash on starting JIPipe with a fresh Fiji installation</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Previews of RGB images were changing the image</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Freeze caused by updating certain parameters</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Archive functionality for projects (<code>Project &gt; Archive project ...</code>) that collects all files and generates a directory or ZIP containing the project and all its data</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cReplace label values by table\u201d (mapping labels)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cTable column to image\u201d (convert a table column into a 1px wide image with height = number of rows; allows to apply image operations on table columns)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node \u201cImage to table column\u201d (put image back into a table column)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Image viewer measurement tool now measures physical sizes</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Rename \u201cIntegrate table columns\u201d to \u201cSummarize table\u201d</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Performance of previews improved</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> Expression-based hyperstack slicer now has custom variables</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> \u201cSet annotations\u201d now has access to the number of rows in the data table</li>\n</ul>", "<h1>\n<a name=\"version-1780-1\" class=\"anchor\" href=\"#version-1780-1\"></a>Version 1.78.0</h1>\n<p>This version contains major reworks of some core features and the user interface, including \u2026</p>\n<ul>\n<li>a new more compact and faster rendering of nodes</li>\n<li>redesign of the cache to fix memory and UX issues</li>\n<li>new functions to simplify the management of larger pipelines</li>\n<li>fixing of memory leaks that fill up memory with unnecessary data</li>\n<li>fixing of bugs related to LUT and image contrast</li>\n</ul>\n<h2>\n<a name=\"redesigned-node-ui-2\" class=\"anchor\" href=\"#redesigned-node-ui-2\"></a>Redesigned node UI</h2>\n<p>In previous JIPipe versions, nodes could be displayed in three different modes (Vertical Compact [default], Vertical, and Horizontal). These were replaced by exactly one mode, which is a even more compact version of \u2018vertical compact\u2019.</p>\n<p>The new display is faster and reduces the clutter by re-arranging the UI elements. The zoom now works as expected as well.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/a/4ad4648e3900fc7dcc58d1c0a2951a8d1132781b.png\" alt=\"jipipe-1.78-new-nodes\" data-base62-sha1=\"aFYnHtAB2NrTNaF31UdC89dG4Mb\" width=\"386\" height=\"294\"></p>\n<p>The new node design features a more clear indicator on its status:</p>\n<ul>\n<li>Red line on an input: the node is missing an input</li>\n<li>Green line on an output: the output is cached (an icon is also present)</li>\n</ul>\n<p>The status is also explained on clicking the slot\u2019s gear icon.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/4/746b2a72425940f80408a317a10bcca37af66ccd.png\" alt=\"jipipe-1.78-new-nodes-indicators\" data-base62-sha1=\"gBSZgQe7bj6Qj614QJXzg3BqjAF\" width=\"458\" height=\"165\"></p>\n<h2>\n<a name=\"auto-hiding-edges-3\" class=\"anchor\" href=\"#auto-hiding-edges-3\"></a>Auto-hiding edges</h2>\n<p>Large pipelines with long connections can be be very busy and hard to read. By default JIPipe now automatically hides long connections that may contribute towards clutter by displaying them as dashed line only. The inputs are then automatically labelled with the slot and node where the data was sourced from.</p>\n<p>You can control this feature by \u2026</p>\n<ul>\n<li>enabling/disabling this display option on a per-node/per-connection level (slot settings &gt; Customize edges or Manage existing connections)</li>\n<li>Temporarily suppressing the feature via the <strong>Graph</strong> menu</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/0944e72cbb94d61c736e0c0af20561468bf53f15.png\" data-download-href=\"/uploads/short-url/1jZV7qvCsRqkOG2TXCjSI0PFi3H.png?dl=1\" title=\"jipipe-1.78-hidden-edges\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/0944e72cbb94d61c736e0c0af20561468bf53f15_2_434x375.png\" alt=\"jipipe-1.78-hidden-edges\" data-base62-sha1=\"1jZV7qvCsRqkOG2TXCjSI0PFi3H\" width=\"434\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/0944e72cbb94d61c736e0c0af20561468bf53f15_2_434x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/9/0944e72cbb94d61c736e0c0af20561468bf53f15_2_651x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/9/0944e72cbb94d61c736e0c0af20561468bf53f15.png 2x\" data-dominant-color=\"F4F2F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">jipipe-1.78-hidden-edges</span><span class=\"informations\">688\u00d7594 40.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"graph-toolbar-4\" class=\"anchor\" href=\"#graph-toolbar-4\"></a>Graph toolbar</h2>\n<p>A toolbar was added to the left side of the graph editor. In its current form it contains the following tools:</p>\n<ul>\n<li>No tool (default)</li>\n<li>Node movement tool (for arranging nodes without accidentally making connections)</li>\n<li>Connection tool (make connections without accidentally rearranging nodes)</li>\n<li>Center to view (Crops empty areas around the graph)</li>\n</ul>\n<p></p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/137940c495beb9a6bb9a57ed760d1f823ba256fc.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/137940c495beb9a6bb9a57ed760d1f823ba256fc.mp4\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/137940c495beb9a6bb9a57ed760d1f823ba256fc.mp4</a>\n    </source></video>\n  </div><p></p>\n<h2>\n<a name=\"duplicate-with-inputs-5\" class=\"anchor\" href=\"#duplicate-with-inputs-5\"></a>Duplicate with inputs</h2>\n<p>A variant of the <code>Duplicate</code> operation was introduced that preserves the input connections. Similar to the standard <code>Duplicate</code> tool, you can select the node(s) of interest, point your mouse to the target location and press Ctrl+Shift+D</p>\n<p></p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/c/1c8d97e7062bcd67e5ebeac08106ed646ba369c9.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/c/1c8d97e7062bcd67e5ebeac08106ed646ba369c9.mp4\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/c/1c8d97e7062bcd67e5ebeac08106ed646ba369c9.mp4</a>\n    </source></video>\n  </div><p></p>\n<h2>\n<a name=\"rewire-tool-6\" class=\"anchor\" href=\"#rewire-tool-6\"></a>Rewire tool</h2>\n<p>The rewire tool allows to move multiple connections to another input or output. This is for example useful if you want to introduce additional steps to the processing of an image that is utilized as input of many nodes.</p>\n<p></p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/9/19d9333430742b8647169901d8f318aa35ee1052.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/9/19d9333430742b8647169901d8f318aa35ee1052.mp4\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/9/19d9333430742b8647169901d8f318aa35ee1052.mp4</a>\n    </source></video>\n  </div><p></p>\n<h2>\n<a name=\"additional-improvements-7\" class=\"anchor\" href=\"#additional-improvements-7\"></a>Additional improvements</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> New bookmark system for the file chooser (you can now name bookmarks; old bookmarks will be unfortunately lost)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> <code>Update predecessor caches</code> function that allows to cache only the direct predecessors of a node. This can be useful if you want to experiment with the parameters of a node.</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node <code>Rotate ROI</code>\n</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Data batches tab: the items now have a documentation/help button</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Image viewer: use the display contrast as set in the image</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Display contrast and LUT should now be properly preserved and rendered</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node <code>Apply displayed contrast</code> (Equivalent of B&amp;C apply)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Removed the old virtual data system (Memory menu/Reduced memory mode) that caused many issues for many people that accidentally activated it. Alternative solutions will be added in a future update.</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> The algorithm finder UX was improved</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Various memory leaks were detected and resolved</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> CLIJ2 Pull Image node: Option to  deallocate the GPU data after the conversion.</li>\n</ul>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/point_right.png?v=12\" title=\":point_right:\" class=\"emoji\" alt=\":point_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> <strong>Various older nodes were modernized to use a newer API. Please inform us if you get a message about a missing connection on loading your project</strong></p>", "<h1>\n<a name=\"version-1790-1\" class=\"anchor\" href=\"#version-1790-1\"></a>Version 1.79.0</h1>\n<p>This version focuses on improving the 3D capabilities of JIPipe by including the following features:</p>\n<ul>\n<li>a 3D viewer for images</li>\n<li>integration of the 3D ROI data type from the 3D ImageJ Suite</li>\n<li>porting existing JIPipe ROI processing functionalities to 3D ROI</li>\n<li>introducing a new filaments processing feature</li>\n</ul>\n<h2>\n<a name=\"h-3d-viewer-and-processing-2\" class=\"anchor\" href=\"#h-3d-viewer-and-processing-2\"></a>3D viewer and processing</h2>\n<p>The JIPipe image viewer received major internal improvements that allowed us to include the basic functions of the <a href=\"https://imagej.net/plugins/3d-viewer/\">ImageJ 3D Viewer</a> into the interface. Please note that the JIPipe 3D viewer is only designed for viewing 3D images and 3D ROI and does not have the full feature set of the ImageJ plugin.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d.png\" data-download-href=\"/uploads/short-url/xWjyjVXe2gMk2HrWIuOTOeDGyT3.png?dl=1\" title=\"version-1.79.0-3d-viewer\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_481x375.png\" alt=\"version-1.79.0-3d-viewer\" data-base62-sha1=\"xWjyjVXe2gMk2HrWIuOTOeDGyT3\" width=\"481\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_481x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_721x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/eddf49d117eb1370d6f652edb87d2f9789f6f03d_2_962x750.png 2x\" data-dominant-color=\"716582\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">version-1.79.0-3d-viewer</span><span class=\"informations\">1154\u00d7898 536 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>To allow the processing of 3D data, the functions of the <a href=\"https://imagej.net/plugins/3d-imagej-suite/\">3D ImageJ Suite</a> were included into JIPipe via a dedicated extension (Tools &gt; Manage JIPipe plugins).</p>\n<p>Similar to how ImageJ ROI are handled, JIPipe introduces a \u201c3D ROI List\u201d data type that allows to process 3D ROI directly via dedicated nodes. Additionally, the new plugin wraps existing 3D ImageJ Suite functions as nodes.</p>\n<p>* Please note that some 3D ImageJ Suite functions were split or slightly renamed to improve the user experience</p>\n<h2>\n<a name=\"filaments-3\" class=\"anchor\" href=\"#filaments-3\"></a>Filaments</h2>\n<p>The new JIPipe version comes with the first iteration of the \u201cFilaments\u201d plugin that focuses around the processing and measuring of filamentous structures represented by a graph.</p>\n<p>Filaments are stored in a dedicated data type and are capable of representing 3D structures (X, Y, Z). Additionally, filament vertices can also store the channel and frame, metadata, radius, and value.</p>\n<p>The generation of filament data begins with a binary skeleton (\u201cMorphological skeletonize 2D\u201d) that is converted into a filament via \u201cBinary skeleton to 2D filaments\u201d. Small loops and noise are removed via \u201cSmooth filaments\u201d.</p>\n<p>Please note that the filaments library is still unfinished and will be expanded with additional functionality in the future.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d.png\" data-download-href=\"/uploads/short-url/3TN3nbpB2OiqPvNfdrGpqdrnRtH.png?dl=1\" title=\"version-1.79.0-filaments\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_481x375.png\" alt=\"version-1.79.0-filaments\" data-base62-sha1=\"3TN3nbpB2OiqPvNfdrGpqdrnRtH\" width=\"481\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_481x375.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_721x562.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b550619bd7299db7c40857b8f9d11ea4ad7dd5d_2_962x750.png 2x\" data-dominant-color=\"5B5B5A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">version-1.79.0-filaments</span><span class=\"informations\">1154\u00d7898 66.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"clij2-improvements-4\" class=\"anchor\" href=\"#clij2-improvements-4\"></a>CLIJ2 improvements</h2>\n<p>All CLIJ2 nodes now have an option \u201cAvoid allocating GPU memory\u201d that will only allocate GPU memory during the processing. Afterwards, the image data is automatically de-allocated.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168.png\" data-download-href=\"/uploads/short-url/bAV0VJjprDxypsmzf3ZSrlYKpn2.png?dl=1\" title=\"1.79.0-clij-nodes\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168_2_517x324.png\" alt=\"1.79.0-clij-nodes\" data-base62-sha1=\"bAV0VJjprDxypsmzf3ZSrlYKpn2\" width=\"517\" height=\"324\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168_2_517x324.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168_2_775x486.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/51444dbc858fdd14125187ba840732396c6e4168.png 2x\" data-dominant-color=\"F0F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">1.79.0-clij-nodes</span><span class=\"informations\">784\u00d7491 39.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h2>\n<a name=\"additional-improvements-5\" class=\"anchor\" href=\"#additional-improvements-5\"></a>Additional improvements</h2>\n<ul>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Rewire graph tool (toolbar on the left side of the graph) to quickly rewire multiple connections from one input/output to another input/output</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Copy-paste &amp; deletion of compartments will automatically generate convenient IO interface nodes that preserve the connections</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Histogram plot: set bin axis limits</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/new.png?v=12\" title=\":new:\" class=\"emoji\" alt=\":new:\" loading=\"lazy\" width=\"20\" height=\"20\"> Node templates/Bookmarks are now shown even when selecting a node (in parameters panel)</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Fixed bug that prevented JIPipe from being usable on X remote connections</li>\n<li>\n<img src=\"https://emoji.discourse-cdn.com/twitter/white_check_mark.png?v=12\" title=\":white_check_mark:\" class=\"emoji\" alt=\":white_check_mark:\" loading=\"lazy\" width=\"20\" height=\"20\"> Fixed various memory leaks</li>\n</ul>"], "48948": ["<p>Wondering if anyone have a ImageJ plugin, macro, or Python script which allows converting between ImageJ ROI file and <a href=\"https://geojson.org/\">GeoJSON format</a> (which is used by, for example, QuPath, and we use it in <a href=\"https://kaibu.org\">Kaibu</a>).</p>\n<p>I would like to use it in <a href=\"https://ij.imjoy.io\">ImageJ.JS</a> so we can import GeoJSON annotation files.</p>", "<p>I\u2019m not aware of anything \u2013 QuPath should contain the necessary logic, but it moves through its own intermediate representation so I suspect you\u2019d want to skip that for something more direct.</p>\n<p>I expect conversion <em>to</em> an ImageJ ROI should be <em>relatively</em> ok, since the GeoJSON specification is nicely-written.</p>\n<p>The relevant bits in the QuPath code:</p>\n<ul>\n<li><a href=\"https://github.com/qupath/qupath/blob/43aad4ecda893a7eb03c30774e64da5b9547bc86/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java#L1019\">Geometry to QuPath ROI (mostly similar to ImageJ)</a></li>\n<li><a href=\"https://github.com/qupath/qupath/blob/43aad4ecda893a7eb03c30774e64da5b9547bc86/qupath-core-processing/src/main/java/qupath/imagej/tools/IJTools.java#L860\">QuPath ROI to ImageJ</a></li>\n</ul>\n<p>Note that, at the end, for complicated stuff it creates a <code>java.awt.Shape</code> and generates a <code>ShapeRoi</code> from this\u2026 leaving a lot of the most difficult logic up to Java AWT. To help, I used the <a href=\"https://github.com/qupath/qupath/blob/43aad4ecda893a7eb03c30774e64da5b9547bc86/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java#L1011\"><code>ShapeWriter</code> from JTS</a>: (source <a href=\"https://github.com/locationtech/jts/blob/3e2634b42cec7b250b150eaac5c1e93e12091fc4/modules/core/src/main/java/org/locationtech/jts/awt/ShapeWriter.java\">here</a>).</p>\n<hr>\n<p>Conversion from ImageJ to GeoJSON is likely <em>much</em> more awkward.</p>\n<p>The relevant bits of the QuPath code:</p>\n<ul>\n<li><a href=\"https://github.com/qupath/qupath/blob/43aad4ecda893a7eb03c30774e64da5b9547bc86/qupath-core-processing/src/main/java/qupath/imagej/tools/IJTools.java#L894\">Convert from ImageJ ROI to QuPath ROI</a></li>\n<li><a href=\"https://github.com/qupath/qupath/blob/43aad4ecda893a7eb03c30774e64da5b9547bc86/qupath-core/src/main/java/qupath/lib/roi/GeometryTools.java#L602\">Convert from QuPath ROI to JTS Geometry</a></li>\n<li><a href=\"https://github.com/qupath/qupath/blob/43aad4ecda893a7eb03c30774e64da5b9547bc86/qupath-core/src/main/java/qupath/lib/io/ROITypeAdapters.java#L61\">Create the GeoJSON</a></li>\n</ul>\n<p>By far the most awkward / error-prone part is handling multipolygons \u2013 particularly as they can contain holes, and (most evil of all) positive regions nested within holes. Earlier QuPath pre-release versions continually failed with some obscure shapes, but I <em>think</em> the logic is pretty robust now.</p>\n<p>The most general way I can think of achieving a conversion is:</p>\n<ul>\n<li>Handle the \u2018easy\u2019 cases (lines, points, rectangles\u2026 ellipses aren\u2019t really supported)</li>\n<li>Convert any complicated areas (including ImageJ polygons, since they may have self-intersections \u2013 forbidden in geojson) into a <code>ShapeRoi</code> and then:\n<ul>\n<li>Get a <code>java.awt.Shape</code> from the <code>ShapeRoi</code>\n</li>\n<li>Generate an <code>java.awt.geom.Area</code> object (since this has <a href=\"https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/java/awt/geom/Area.html\">some guarantees</a>)</li>\n<li>Use the <code>PathIterator</code> from the <code>Area</code> to generate polygons\u2026 see <a href=\"https://github.com/locationtech/jts/issues/408#issuecomment-671501891\">here</a> for info on how I did it in the end</li>\n</ul>\n</li>\n</ul>\n<p>I fould JTS useful/indispensable in this, but you should avoid its unreliable <code>ShapeReader</code> \u2013 which only works in the easy cases.</p>", "<p>Hi <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> Thanks a lot for these pointers! this is super useful, I will try to see if we can make a ImageJ plugin for importing and exporting GeoJSON files.</p>", "<p>7 posts were split to a new topic: <a href=\"/t/qupath-and-visium-json/59908\">QuPath and Visium JSON</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/oeway\">@oeway</a>  <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>,</p>\n<p>I wonder if there is any progress with ImageJ plugin for importing and exporting GeoJSON files.</p>\n<p>Thanks<br>\nOfra</p>", "<p>I wrote this:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/petebankhead/imagejts-tools\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/petebankhead/imagejts-tools\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/e/be1e7fa275deb4467851eaef3568c9ab5cd47796.png 2x\" data-dominant-color=\"F4F2F0\"></div>\n\n<h3><a href=\"https://github.com/petebankhead/imagejts-tools\" target=\"_blank\" rel=\"noopener\">GitHub - petebankhead/imagejts-tools: Example ImageJ plugins to convert ROIs...</a></h3>\n\n  <p>Example ImageJ plugins to convert ROIs to and from GeoJSON - GitHub - petebankhead/imagejts-tools: Example ImageJ plugins to convert ROIs to and from GeoJSON</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>It kinda works, but there is some awkward stuff around where in the GeoJSON z-stack/timepoint/channel info is stored. If you need these features, exchange with QuPath might not work properly. For 2D it might be ok.</p>\n<p>I\u2019ve temporarily abandoned it due to a lack of time, but hope to return one day.</p>"], "38725": ["<p>The posts about colour deconvolution today have reminded me of a question I had some time ago\u2026</p>\n<p>Color deconvolution, as described by Ruifrok and Johnston, involves generating a 3x3 stain matrix using three stain vectors.</p>\n<p>I understand that if two stains are available, then the remaining elements can be created by generating a third (pseudo)stain that is orthogonal to the first two.</p>\n<p>As far as I can tell, this third stain is generated using the cross product in several places:</p>\n<ul>\n<li>QuPath (<a href=\"https://github.com/qupath/qupath/blob/a03756328188999c0b7f12c290cda0589c50bd4b/qupath-core/src/main/java/qupath/lib/color/StainVector.java#L316\">code</a>)</li>\n<li>scikit-image (<a href=\"https://github.com/scikit-image/scikit-image/blob/23829977f23c957dbff24757069dbdc4d929248f/skimage/color/colorconv.py#L487\">code</a>)</li>\n<li>HistomicsTK (<a href=\"https://github.com/DigitalSlideArchive/HistomicsTK/blob/8e79d265c95468b8ca3f6057c06ce1402bbabaae/histomicstk/preprocessing/color_deconvolution/complement_stain_matrix.py\">code</a>)</li>\n</ul>\n<p>I wasn\u2019t able to tell what approach CellProfiler takes (I figure <a href=\"https://github.com/CellProfiler/CellProfiler/blob/39b812636f97708dffd5eb525cefc2d9f6e4b038/cellprofiler/modules/unmixcolors.py\">this code</a> is relevant, but I got lost <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> ).</p>\n<p>However, I understand that this is not exactly how it is implemented in the <a class=\"mention\" href=\"/u/gabriel\">@gabriel</a>\u2019s ImageJ/Fiji plugin. From a quick look at the code, this may be because of negative values being clipped to 0:<br>\n</p><aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https://github.com/fiji/Colour_Deconvolution/blob/dca9a107f1d90b2f15dd7fac4e737534984e64f1/src/main/java/sc/fiji/colourDeconvolution/StainMatrix.java#L193\" target=\"_blank\">github.com</a>\n  </header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/fiji/Colour_Deconvolution/blob/dca9a107f1d90b2f15dd7fac4e737534984e64f1/src/main/java/sc/fiji/colourDeconvolution/StainMatrix.java#L193\" target=\"_blank\">fiji/Colour_Deconvolution/blob/dca9a107f1d90b2f15dd7fac4e737534984e64f1/src/main/java/sc/fiji/colourDeconvolution/StainMatrix.java#L193</a></h4>\n<pre class=\"onebox\"><code class=\"lang-java\"><ol class=\"start lines\" start=\"183\" style=\"counter-reset: li-counter 182 ;\">\n<li>if (cosx[1]==0.0){ //2nd colour is unspecified</li>\n<li>  if (cosy[1]==0.0){</li>\n<li>    if (cosz[1]==0.0){</li>\n<li>      cosx[1]=cosz[0];</li>\n<li>      cosy[1]=cosx[0];</li>\n<li>      cosz[1]=cosy[0];</li>\n<li>    }</li>\n<li>  }</li>\n<li>}</li>\n<li>\n</li><li class=\"selected\">if (cosx[2]==0.0){ // 3rd colour is unspecified</li>\n<li>  if (cosy[2]==0.0){</li>\n<li>    if (cosz[2]==0.0){</li>\n<li>      if ((cosx[0]*cosx[0] + cosx[1]*cosx[1])&gt; 1){</li>\n<li>        if (doIshow)</li>\n<li>          IJ.log(\"Colour_3 has a negative R component.\");</li>\n<li>        cosx[2]=0.0;</li>\n<li>      }</li>\n<li>      else</li>\n<li>        cosx[2]=Math.sqrt(1.0-(cosx[0]*cosx[0])-(cosx[1]*cosx[1]));</li>\n<li>\n</li></ol></code></pre>\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>In any case, the following Groovy script written for Fiji shows that the third stain vector is <em>not</em> orthogonal using the Fiji plugin, i.e. the dot product is not zero:</p>\n<pre><code class=\"lang-groovy\">import sc.fiji.colourDeconvolution.*\ndef mat = new StainMatrix()\nmat.init(\"H&amp;E\", 0.644211000,0.716556000,0.266844000,0.09278900,0.95411100,0.28311100,0.00000000,0.00000000,0.0000000)\nmat.compute(true, false, new ij.ImagePlus(\"Anything\", new ij.process.ColorProcessor(10, 10)))\n\ndouble[] stain1 = [mat.cosx[0], mat.cosy[0], mat.cosz[0]]\ndouble[] stain2 = [mat.cosx[1], mat.cosy[1], mat.cosz[1]]\ndouble[] stain3 = [mat.cosx[2], mat.cosy[2], mat.cosz[2]]\nprintln 'Stain 1: ' + stain1\nprintln 'Stain 2: ' + stain2\nprintln 'Stain 3: ' + stain3\nprintln 'Dot product stain 1 x stain 2: ' + dot(stain1, stain2)\nprintln 'Dot product stain 1 x stain 3: ' + dot(stain1, stain3)\nprintln 'Dot product stain 2 x stain 3: ' + dot(stain2, stain3)\n\n\ndouble dot(double[] v1, double[] v2) {\n\tdouble s = 0\n\tfor (int i = 0; i &lt; v1.length; i++)\n\t\ts += v1[i] * v2[i]\n\treturn s\n}\n</code></pre>\n<p>It\u2019s not completely clear to me that negative values must be avoided in the stain matrix, and that this is more important than orthogonality.</p>\n<p>It\u2019s also not clear to me if/how much this matters.</p>\n<p>I think probably all of us benefited from <a class=\"mention\" href=\"/u/gabriel\">@gabriel</a>\u2019s implementation \u2013 I know I did, and I\u2019ve seen it referred to a lot of time in other people\u2019s code. But even though it seems to be pretty much the standard reference for many (in the absence of the original macro), I\u2019m not sure it\u2019s widely recognized that other implementations seem to have deviated a bit in this detail.</p>\n<p>In any case, I\u2019d be really interested to understand if there is a \u2018right\u2019 way to do it.</p>\n<p>I\u2019m also very interested in whether <a class=\"mention\" href=\"/u/phaub\">@phaub</a> might have any more best practice suggestions from <a href=\"https://www.nature.com/articles/srep12096\">this</a> <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Yes, I am aware of that implementation detail in Ruifrok\u2019s code about the 3rd residual colour. A couple of people have made that observation in the past. I will have a look at the other implementations.</p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> The Histomics TK link does not work.</p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a><br>\nHave you seen the supplement to this paper, <a href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep12096/MediaObjects/41598_2015_BFsrep12096_MOESM1_ESM.pdf\">here</a>.<br>\nWe discussed this issue in S1.5_A.</p>\n<p>In case of two stains and using the 3x3 matrix approach to solve the over determined system a third \u2018pseudo vector\u2019 should be perpendicular to the first two. This can lead to negative values in the pseudo vector. The resulting 3rd cannel value is a quasi measure of the deconvolution error.<br>\nThe negative vector components have a meaning and should not be corrected.</p>\n<p>The right way to do it \u2026?<br>\nSince there are a lot of disturbances (homogeneity and stability of the illumination, background and gamma correction, quality, stability and reliability of the staining, the non-linarity of IHC preparation, ect ect ect. and last be not least the invalid assumption of a linear realionship between the polychromatic measured absorbance and the stain concentration) the color deconvolution is questionable in any case.<br>\nSolve the system of equations will not dramatically change this.</p>\n<p>So, just do it in one of the correct ways and avoid a wrong one.</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"3\" data-topic=\"38725\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> The Histomics TK link does not work.</p>\n</blockquote>\n</aside>\n<p>Thanks, fixed now (hopefully).</p>\n<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"4\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<p>We discussed this issue in S1.5_A.</p>\n</blockquote>\n</aside>\n<p>I hadn\u2019t seen that - thanks!</p>\n<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"4\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<p>So, just do it in one of the correct ways and avoid a wrong one.</p>\n</blockquote>\n</aside>\n<p>Do I understand correctly that using the cross product would count as one of the correct/least wrong ways\u2026? With all the caveats regarding the use of color deconvolution generally, of course.</p>\n<p>I ask because as QuPath becomes more widely used (approaching 100k downloads now\u2026) I\u2019m keen that it should promote good practice \u2013 or at least not inadvertently makes bad practice widespread. I think this is partly a matter of implementation and partly of documentation.</p>\n<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a> and <a class=\"mention\" href=\"/u/phaub\">@phaub</a> since you are the people I have encountered who I think know most about this topic, I particularly value your suggestions <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> I already link people to your writings, but wondering if there is more could be done \u2013 perhaps my using the UI to offer guidance that helps encourage better analysis and appropriate interpretation.</p>", "<p>Some quick tests to compute the 3rd vector via the cross product show that you one cannot create nice looking LUTs: the negative components represent impossible colours. I assume that the original implementation (which generated OD images with LUTs to represent the dyes) avoided this by generating an alternative LUT.<br>\nPerhaps the plugin should not attempt to generate LUTs in those situations (more of a cosmetic issue) and generate also 32bit images instead.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"5\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>Do I understand correctly that using the cross product would count as one of the correct/least wrong ways\u2026?</p>\n</blockquote>\n</aside>\n<p>Yes.</p>\n<p>To show the connection between a third orthogonal pseudo vector and the GaussTransformation see the following information</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca8068aed6f27bd1c7aba5069bcd34c122aabf4b.gif\" alt=\"GaussTransformation\" data-base62-sha1=\"sTpyXvCselUpUzOnYhrY39MEuPp\" width=\"628\" height=\"426\"></p>\n<p>So you can either solve the over determined systems by</p>\n<ul>\n<li>on of the projections into 2D (RG, RB, GB)</li>\n<li>usage of a third orthogonal residual vector</li>\n<li>GaussTransformation / Moore-Penrose-Inverse (<a href=\"https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse\">MPI</a>)</li>\n<li>SVD</li>\n<li>QR decomposition</li>\n</ul>\n<p>On no account you will get a error free result because of the incorrect assumption underlaying the approach. Even under theoretical perfect conditions (stoichiometric and linear staining, perfect imaging \u2026) the linear approach can not model the nonlinear signal formed by a spectral integration without errors.</p>\n<p>The extent of this error can not be estimated easily. The type of and the combination of staining strongly influences this error.</p>\n<p>Its like modelling a circle by its tangent and asking for the error of that approximation.<br>\nThe answer is: It depends.<br>\nSimplification only works under particular conditions.</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"6\" data-topic=\"38725\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p>Some quick tests to compute the 3rd vector via the cross product show that you one cannot create nice looking LUTs: the negative components represent impossible colours. I assume that the original implementation (which generated OD images with LUTs to represent the dyes) avoided this by generating an alternative LUT.<br>\nPerhaps the plugin should not attempt to generate LUTs in those situations (more of a cosmetic issue) and generate also 32bit images instead.</p>\n</blockquote>\n</aside>\n<p>QuPath generates a LUT color as described <a href=\"https://github.com/qupath/qupath/blob/85700bbb026e1e72425e266ee3105b2f39c4f4fd/qupath-core/src/main/java/qupath/lib/color/StainVector.java#L220\">here</a> (basically clipping values that end up out of range). I\u2019m not sure that\u2019s the best approach, but as you say it\u2019s really cosmetic.</p>\n<p>I do however like the option of 32-bit output, since this preserves negative values \u2013 which can help as a sanity-check / preserve useful information. One can always clip it later.</p>\n<p>However, I understand better now this is starting to move from the original NIH Macro, and a faithful interpretation of the macro is also desirable.</p>\n<p><a class=\"mention\" href=\"/u/phaub\">@phaub</a> thank you for the explanation!</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"6\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p>cannot create nice looking LUTs: the negative components represent impossible colours</p>\n</blockquote>\n</aside>\n<p>For this 3rd residual vector there is no \u2018true\u2019 color. The information of this 3rd channel is the amount of error. You can simply assign a FALSE color.</p>", "<p>Thanks Peter. Did you mean \u201cnot necessarily\u201d a true colour (e.g. when there is a negative component)? If there isn\u2019t a negative value, one can generate such 3rd colour as well.</p>", "<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a><br>\nA 3rd residual vector has in no case a color!<br>\nIts direction is only defined in a mathematical sense.<br>\nIn case all components are possitive a derived color will lead to the believe that this color has a meaning. But it doesn\u2019t have.<br>\nI personally would assign a unique color to this residual channel to indicate that this channel contains a kind of quality measure (e.g. LUT Phase or Fire).</p>", "<p>Hi all,</p>\n<p>Forgive my maybe naive question, but in case there is more than 3 colors on the image ; would it be possible to make a color deconvolution with 4, 5 or 6 components (wich is done in some commercial products like Akoya\u2019s Inform). Maybe it is written in Ruifrok and Johnston 's paper, but I fear my mathematical level is not good enough to understand it.</p>\n<p>Nico</p>", "<p>The linear approach discussed here is based on solving a system of linear equations.<br>\n</p><aside class=\"onebox wikipedia\">\n  <header class=\"source\">\n      <a href=\"https://en.wikipedia.org/wiki/System_of_linear_equations\" target=\"_blank\">en.wikipedia.org</a>\n  </header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:220/220;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/5/85630a2ac62768fa3a88cf2c35b8045fa043d5e8.png\" class=\"thumbnail\" width=\"220\" height=\"220\"></div>\n\n<h3><a href=\"https://en.wikipedia.org/wiki/System_of_linear_equations\" target=\"_blank\">System of linear equations</a></h3>\n\n<p>In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same set of variables. For example,\n is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by\n since it makes all three equations valid. The word \"system\" indicates that the equations are to be ...</p>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>To solve a SLE it needs at least as much or more equations than unknowns.</p>\n<p>Each unknown is a stain.<br>\nEach equation comes from one color channel of your image.</p>\n<p>With a color image - typically 3-channels RGB - you can \u2018separate\u2019 up to 3 stains.<br>\nIf you want to apply the linear approach to more than 3 stains than you need more color channels. The images have to be captured as multi-channel images with more then 3 channels.</p>\n<p>If you find a suitable camera it would be nice to post it here <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"></p>", "<p>I have heard that the camera on the Vectra systems could be used to generate a multichannel brightfield image (7 channels?), in theory. Though I have never tested it.</p>", "<p><a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, <a class=\"mention\" href=\"/u/phaub\">@phaub</a>, That\u2019s the camera I use. And it does a multi-chanel image (the whole spectra is split in 35 images). But as I mainly use it for fluorescence, I had never imagined that the brightfield acquisition was the same.<br>\nBut the color deconvolution implemented works very well (if you have the right mono-staining sample in your library <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> ).</p>\n<p>Nico</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"14\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> Research_Associate:</div>\n<blockquote>\n<p>the Vectra systems</p>\n</blockquote>\n</aside>\n<p>is based on a liquid crystal tunable filter (LCTF).</p>", "<aside class=\"quote no-group\" data-username=\"VirtualSlide\" data-post=\"15\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/v/b5a626/40.png\" class=\"avatar\"> VirtualSlide:</div>\n<blockquote>\n<p>But as I mainly use it for fluorescence, I had never imagined that the brightfield acquisition was the same</p>\n</blockquote>\n</aside>\n<p>I would guess that it merges all of those channels into 3, as RGB is used for display on the screen. I\u2019m not sure how you would really look at a 35 channel brightfield image, but it might allow the deconvolution of more channels from the data side?</p>", "<p>I bet that a 35 channel image merged into RGB would look like\u2026 an RGB image, because displays have only 3 colours and the eye has (most commonly) only 3 sensors.<br>\nMost applications I have seen on multi and hyperspectral imaging rely on some kind of data reduction (like PCA) to extract information. Really interesting subject.</p>", "<p>The conversion of an n-channel brightfield image to RGB is described</p>\n<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"4\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<p><a href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep12096/MediaObjects/41598_2015_BFsrep12096_MOESM1_ESM.pdf\">here</a></p>\n</blockquote>\n</aside>\n<p>see section S1.4</p>", "<p>For CellProfiler, the relevant function is <a href=\"https://github.com/CellProfiler/CellProfiler/blob/7d3a9d56cb67a6d55f182934fcc1fdd35ace5724/cellprofiler/modules/unmixcolors.py#L384-L413\">here</a>, which calls out to <a href=\"https://github.com/CellProfiler/CellProfiler/blob/7d3a9d56cb67a6d55f182934fcc1fdd35ace5724/cellprofiler/modules/unmixcolors.py#L439-L467\">these two other functions</a> - only those 60 lines are \u201cfunctional\u201d, everything else is display, image saving, or the estimation of custom stains.</p>"], "77638": ["<p>Hello community!</p>\n<p>I am looking at two dyes that localize to the same location (DNA). I am using one of those to track my region of interest per nuclei, and the other one I will be obtaining median intensity of the channel for each nuclei, tracked over time. I have great signal to noise ratios within the pictures below, so I am hoping I can get just the thin line of florescence that surrounds the nucleus and the nucleoli/if there\u2019s any additional fluorescence within the center (but that is not all cells, just some).</p>\n<p>I do not want to JUST use IdentifyPrimaryObjects, as that will identify the whole nucleus, I just want effectively a continuous wire where the DNA is lit up.</p>\n<p>Additionally, a complication is that photobleaching over the timespan causes a significant darkening of the image. See the images attached for reference.</p>\n<p>I have thought about creating a mask by taking the nuclei object and shrinking it by a pixel or two, and then keeping just the outside \u2018wire\u2019 that is formed, but that will exclude both nucleoli, and if there is any <em>significant</em> fluorescence on the inside of the nuclei (see the last .png for two nuclei that are opposite behaviors, but both I\u2019d like to capture as much of the signal as possible to declare my region of interest) it would completely disregard that. I need a more intelligent way to find the regions within each nucleus that has fluorescence that will not have significant dark space.</p>\n<p>The reason being is including the whole nuclei could skew the results of \u2018median intensity\u2019 for that specific object significantly (easily visualized when you compare the two nuclei in the last picture attached), despite the fluorescent portion of them being relative similar.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885.png\" data-download-href=\"/uploads/short-url/pkm5KbodsBjBLpGpkCvAzrpnmoB.png?dl=1\" title=\"1stTimeStep\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885_2_500x500.png\" alt=\"1stTimeStep\" data-base62-sha1=\"pkm5KbodsBjBLpGpkCvAzrpnmoB\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885.png 2x\" data-dominant-color=\"00002A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">1stTimeStep</span><span class=\"informations\">800\u00d7800 462 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339.png\" data-download-href=\"/uploads/short-url/11AUrHpA8cEgfNhK3bZ2AiGNgWl.png?dl=1\" title=\"LastTimeStep\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339_2_500x500.png\" alt=\"LastTimeStep\" data-base62-sha1=\"11AUrHpA8cEgfNhK3bZ2AiGNgWl\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339.png 2x\" data-dominant-color=\"00001B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">LastTimeStep</span><span class=\"informations\">800\u00d7800 437 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6.jpeg\" data-download-href=\"/uploads/short-url/l7N8yEaJ5q6R4DtVSLa44DIeOQm.jpeg?dl=1\" title=\"LastTimeStepLargeFluorescenceComparison\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6_2_500x500.jpeg\" alt=\"LastTimeStepLargeFluorescenceComparison\" data-base62-sha1=\"l7N8yEaJ5q6R4DtVSLa44DIeOQm\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6.jpeg 2x\" data-dominant-color=\"01021A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">LastTimeStepLargeFluorescenceComparison</span><span class=\"informations\">800\u00d7800 37 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks for your help community!   <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Nick</p>", "<p>As an example of what I am looking to do in CellProfiler, is in the pictures below, where I zoomed into one nucleus and traced the flourescent signature by hand.</p>\n<p>This post may have become buried by a bunch of new posts and questions, so Beth, as you have been extraordinarily helpful regarding a few similar problems of mine, do you know how I can go about doing this/if not who could walk me through which combinations of modules can make this trace? <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a61ead9776f8a6d56d87982aa9582d45a1053d8b.png\" data-download-href=\"/uploads/short-url/nHyZelMHA7ieaagqeFCEmB2iNKP.png?dl=1\" title=\"Screenshot 2023-02-28 at 11.26.17 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a61ead9776f8a6d56d87982aa9582d45a1053d8b.png\" alt=\"Screenshot 2023-02-28 at 11.26.17 AM\" data-base62-sha1=\"nHyZelMHA7ieaagqeFCEmB2iNKP\" width=\"616\" height=\"500\" data-dominant-color=\"000229\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-28 at 11.26.17 AM</span><span class=\"informations\">1066\u00d7864 8.85 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06d1195d2491d78df238cae1e30cd9872a142439.png\" data-download-href=\"/uploads/short-url/YiRalr3JtIER6jDGBmnenl9SxX.png?dl=1\" title=\"ROITrace\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06d1195d2491d78df238cae1e30cd9872a142439.png\" alt=\"ROITrace\" data-base62-sha1=\"YiRalr3JtIER6jDGBmnenl9SxX\" width=\"616\" height=\"500\" data-dominant-color=\"010228\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ROITrace</span><span class=\"informations\">1066\u00d7864 40 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you!</p>\n<p>Nick</p>", "<p><a class=\"mention\" href=\"/u/lmurphy\">@lmurphy</a> Do you know of anyone that could help guide me on which modules to use to solve this problem? I feel as if this topic is likely buried under new topics, so without a direct  \u2018@\u2019 request I worry that this will be buried.</p>\n<p>Thanks,</p>\n<p>Nick</p>", "<p>Hey!</p>\n<p>I don\u2019t have a quick answer for you, but how about the following:</p>\n<ul>\n<li>First segment the nuclei. I\u2019ve had great success with <a href=\"https://www.cellpose.org/\" rel=\"noopener nofollow ugc\">CellPose</a>, especially if you\u2019re willing to train your own model. A GPU is practically mandatory, YMMV.</li>\n<li>Once you have a mask for each nucleus, shrink the ROI by a % of the total area. The .shape <a href=\"https://shapely.readthedocs.io/en/stable/reference/shapely.buffer.html?highlight=buffer\" rel=\"noopener nofollow ugc\">Shapely</a> function works well for me.</li>\n<li>Using the nuceli mask, drop the MGV of anything outside the masks to 0</li>\n<li>At this point, you should have an image that includes only the internal part of the nuclei and everything else should be 0</li>\n<li>Measure the feature directly, as you described above</li>\n</ul>\n<p>What do you think?</p>\n<p>Leo</p>", "<p>Hey Leo,</p>\n<p>Thank you for your response, that is a very interesting solution. Unfortunately, I have already spent significant time training my pipeline in CellProfiler to work with a variety of cell lines. I am hoping to tweak it just a smidge (but keep processing the same in case there are inherent biases in different softwares) to allow for this new modification.</p>\n<p>I also may be not fully understanding your answer, but I don\u2019t explicitly want just the internal part of the nucleus, I want *only the DNA containing regions of the nucleus, which are typically near the nuclear membrane, but as with some of these cells I highlighted above, can have DNA floating somewhere within the middle of the nucleus. So I would literally just want a tiny thick line tracing where the DNA has been lit up with one channel, and then measure that same location in another channel.</p>\n<p>I\u2019m sorry if my wording is confusing; if it is, please let me know!</p>\n<p>Thanks,</p>\n<p>Nick</p>", "<p>Let me rephrase:</p>\n<ul>\n<li>Bright parts of the nucleus contain DNA</li>\n<li>DNA is distributed around the edges of a nucleus and as filaments within the nucleus</li>\n<li>You\u2019d like to exclude anything below a certain threshold, which might change over time due to photobleaching</li>\n</ul>\n<p>Right?</p>\n<p>If so, the two-step process I proposed above should work. By segmenting the nuclei, you can calibrate a threshold for each frame such that it captures the feature you\u2019re interested in. Using the mask you get from setting a threshold for each frame, you can measure what you need on the other image.</p>\n<p>Unfortunately, I\u2019m not familiar enough with CellProfiler to offer anything other than conceptual tips.</p>\n<p>Leo</p>", "<p>Oh I understand now, thanks Leo for the suggestion! That can be a great backup plan to use if I find that CP is not capable of doing this, thank you!! I\u2019d like to stick with the same software that i have used for about 100 other experiments, as any internal differences in segmenting could give different population statistics (if I can).</p>\n<p>I appreciate the clarification, as that may help me move forward with this new program in the future!</p>\n<p>Thanks,</p>\n<p>Nick</p>", "<p>That makes sense. Happy to (try to) help<br>\nGood luck!</p>\n<p>Leo</p>"], "77639": ["<p>This was posted before but I need to still post this separately to describe my own unique account of the problem. After confirming the PVCAM camera was connected and loading the appropriate config file upon startup, Micro-Manager just freezes on the \u201cLoading system configuration. Please wait\u2026\u201d screen. I\u2019m not allow me to click anywhere, let alone the \u201cHelp \u2192 Report a Problem\u201d option. The only way to end it is through ending the associated Java Binary task in Task Manager.</p>\n<p>It\u2019s strange because it was initially working just fine on the Live feed, until a couple minutes later it acts up. This first started on version 2.0, and the problem still persists in 1.4. My PC config:</p>\n<ul>\n<li>Dell OptiPlex 5060</li>\n<li>Windows 10 Enterprise (10.0.19045)</li>\n<li>16GB RAM</li>\n<li>Intel Core i3-8100</li>\n</ul>\n<p>I welcome any suggestions!</p>", "<p>Since this seems to be tied to the PVCAM camera that you are using, you may want to ask Photometrics for support.</p>", "<p>I\u2019m confused, because you mentioned in the <a href=\"https://forum.image.sc/t/issue-with-loading-system-configuration/58336/6\">other thread that the issue is with the config file</a>, but here you\u2019re saying it\u2019s the PVCAM camera. But in any case, attached is the config file that was, again, working not only initially, but for even the old Windows 7 machine we used before the PC broke down.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/ppBjsCyd1Jp4PbX2J9oFQq4C6E5.txt\">2023_03_08.cfg.txt</a> (493 Bytes)</p>\n<p>Any suggestions besides contacting Photometrics support? Seeking all troubleshooting options.</p>", "<p>Since the only equipment in your config file is the PVCAM camera, both statements seem correct;).  You can try installing various versions of PVCAM, but your best bet is to ask Photometrics support.</p>"], "77665": ["<p>Dear all,</p>\n<p>I am trying to learn MorphoDynamx according to the model in \" The root meristem is shaped by brassinosteroid control of cell geometry\". However, I can not find the \"Cell Disk/Cell Tissue \" in MorphDynamx. Is it located here? Or where can I find this function bottom? Thanks so much in advance.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/8663ffa62001a622be77299d842d9f51a224bc3a.png\" data-download-href=\"/uploads/short-url/jaSe51BbajPLAggkwLTi45roUVY.png?dl=1\" title=\"Screenshot from 2023-02-23 14-30-52\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/8663ffa62001a622be77299d842d9f51a224bc3a_2_477x500.png\" alt=\"Screenshot from 2023-02-23 14-30-52\" data-base62-sha1=\"jaSe51BbajPLAggkwLTi45roUVY\" width=\"477\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/8663ffa62001a622be77299d842d9f51a224bc3a_2_477x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/8663ffa62001a622be77299d842d9f51a224bc3a.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/8663ffa62001a622be77299d842d9f51a224bc3a.png 2x\" data-dominant-color=\"6F756F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-02-23 14-30-52</span><span class=\"informations\">634\u00d7664 33.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best regards.</p>\n<p>Lei Li.</p>", "<p>It should be under the \u201cModel\u201d tab. You have to start MDX by typing \u201cmake run\u201d (not clicking on the .mdxv or .mdxm) so that it loads the library that contains the model.</p>", "<p>Dear Richard,</p>\n<p>I have correctly run MDX now and got the modeling result. If I want to measure the cell wall stiffness now, how to do it for the next step? Thanks so much,<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8b6e5e57f78fd998e7056f3ee83cce4d58ed463f.png\" data-download-href=\"/uploads/short-url/jTsPzJyuwS0L2KxbXvaeukCTxcH.png?dl=1\" title=\"Screenshot from 2023-03-02 14-41-26\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b6e5e57f78fd998e7056f3ee83cce4d58ed463f_2_690x328.png\" alt=\"Screenshot from 2023-03-02 14-41-26\" data-base62-sha1=\"jTsPzJyuwS0L2KxbXvaeukCTxcH\" width=\"690\" height=\"328\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b6e5e57f78fd998e7056f3ee83cce4d58ed463f_2_690x328.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b6e5e57f78fd998e7056f3ee83cce4d58ed463f_2_1035x492.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8b6e5e57f78fd998e7056f3ee83cce4d58ed463f_2_1380x656.png 2x\" data-dominant-color=\"335243\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot from 2023-03-02 14-41-26</span><span class=\"informations\">1788\u00d7852 53.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThanks so much.</p>\n<p>Best regards.</p>\n<p>Lei Li.</p>", "<p>You don\u2019t directly measure stiffness (or extensibility), but rather you would fit those parameters to your growth data. In that model, growth depends on stretch x extensibility, so either might change. One could fit the entire structure, or information by layers if you have it. IN the model it is possible to specify parameter for the different layers, or even different wall layers, for example epidermis-cortex.</p>"], "53112": ["<p>Hello:<br>\nI am trying to figure out how to use ASHLAR for stitching some images. I intend to apply it to some images my lab colleges will soon obtain but, for now, I am learning with the TONSIL datset provided by <a href=\"https://doi.org/10.6084/m9.figshare.11184539\" rel=\"noopener nofollow ugc\">https://doi.org/10.6084/m9.figshare.11184539</a></p>\n<p>I installed the package in python but it has almost no documentation. Is there none or just that I haven\u00b4t found it? Can I use ASHLAR as a regular Python package importing it from a Jupiter Notebook? Is it recommended to work from Docker?</p>\n<p>Thank you very much in advance.</p>", "<p>Hi <a class=\"mention\" href=\"/u/tesa_lobo\">@tesa_lobo</a> and welcome to the forum!</p>\n<p>I don\u2019t think there\u2019s much documentation yet for this tool, judging from this other topic:</p>\n<aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"49864\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jmuhlich/40/20114_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/ashlar-how-to-pass-multiple-images-to-be-stitched/49864/2\">Ashlar: how to pass multiple images to be stitched?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Hi Stefano, sorry for the lack of documentation \u2013 we\u2019re working on it! What kind of microscope and software are you using? The preferred Ashlar input format is whatever your microscope produces natively, as we can directly read useful metadata such as stage positions for each tile and the pixel size rather than requiring you to specify that stuff manually. If your microscope is not <a href=\"https://docs.openmicroscopy.org/bio-formats/6.6.0/supported-formats.html\" rel=\"noopener nofollow ugc\">BioFormats compatible</a>, Ashlar does have an as-yet-undocumented \u201cescape hatch\u201d for reading a directory of TIFFS usin\u2026\n  </blockquote>\n</aside>\n\n<p>I\u2019m tagging <a class=\"mention\" href=\"/u/jmuhlich\">@jmuhlich</a>, the author and maintainer of <a class=\"hashtag\" href=\"/tag/ashlar\">#<span>ashlar</span></a>, as I\u2019m sure he\u2019ll be able to provide some pointers.</p>", "<p>Hi Jan! Thanks for your answer. Yes\u2026 it doesn\u00b4t seem to be any documentation available and it makest it hard to start using ASHLAR. Maybe <a class=\"mention\" href=\"/u/jmuhlich\">@jmuhlich</a> will see my comment. Who knows</p>", "<p>Conda is probably the most reliable way to install Ashlar - you can use pip but you may run into some dependency issues.  The conda setup can be found at the bottom of the ASHLAR github page:</p>\n<p><a href=\"https://github.com/labsyspharm/ashlar\" rel=\"noopener nofollow ugc\">https://github.com/labsyspharm/ashlar</a></p>\n<p>As for usage, you specify the images with spaces after the ashlar command:</p>\n<p>~&gt;ashlar image1 image2 imageN  --flags-go-here</p>\n<p>For example, say there are 3 input images (cycle1.img, cycle2.img, cycle3.img) with 3 channels each that  you want to stitch and align with ashlar, aligning on channel 0, with a max shift of 50 and filtering with sigma 1, and output each aligned and stitched image separately,  the command would look something like:</p>\n<pre><code class=\"lang-auto\">ashlar cycle1.img cycle2.img cycle3.img --align-channel 0 --maximum-shift 50 --filter-sigma 1 -f ashlar_output_cycle{cycle}_channel{channel}.ome.tiff \n</code></pre>\n<p>This would give you 9 output ome.tiffs:<br>\nashlar_output_cycle0_channel0.ome.tiff<br>\nashlar_output_cycle0_channel1.ome.tff<br>\n\u2026<br>\n\u2026<br>\nashlar_output_cycle2_channel1.ome.tiff<br>\nashlar_output_cycle2_channel2.ome.tiff</p>\n<p>If you add the --pyramid flag to that command, then you\u2019d specify a file name without the {cycle} and {channel} parameters, and it would output a single pyramidal (but not fully compliant) ome.tiff.</p>\n<p>If you want to adjust with BaSiC illumination profiles, you would use the --ffp and --dfp flags, specifying one ffp and dfp image per input image, separating each ffp/dfp image path by spaces after their respective flags.</p>\n<p>So say you wanted to add illumination correction to the above command, it would look something like:</p>\n<pre><code class=\"lang-auto\">ashlar cycle1.img cycle2.img cycle3.img -ffp cycle1_ffp.img cycle2_ffp.img cycle3_ffp.img -dfp cycle1_dfp.img cycle2_dfp.img cycle3_dfp.img --align-channel 0 --maximum-shift 50 --filter-sigma 1 -f ashlar_output_illumination_adjusted_cycle{cycle}_channel{channel}.ome.tiff \n</code></pre>", "<p>Thank you very much. It has been of great help. I have a couple more questions if it is not too much to ask for.</p>\n<p>-Does ASHLAR detect by itself the overlap between the tyles inside a cycle? It doesn\u00b4t seem to require me to specify it.</p>\n<ul>\n<li>\u2013maximum-shift SHIFT: does this refer to the maximum shift for ashlar to consider something the same object?</li>\n<li>What is the default order for stitching tyles? left-&gt;right, up\u2013&gt;down or  up\u2013&gt;down, left-&gt;right,?Would the flip arguments modify this?</li>\n</ul>\n<p>Thank you!</p>", "<p>Ashlar tries to find the best fit/overlap between tiles automatically, but is constrained by the --maximum-shift value - it won\u2019t shift tiles beyond the number of pixels specified in that flag.</p>\n<p>The --filter-sigma option applies a blur to the image when fitting which can sometimes help alignment.</p>\n<p>I\u2019m not 100% sure on the default order for tile stitching - but I <em>think</em> it uses coordinates in the image metadata to find the order for stitching.</p>", "<p>Does anyone give a help for getting started with ashlar.<br>\nI have done installation on miniconda. I found ashlar package on my computer.<br>\nSince I am very new to python program I dont know how to get started with ashlar code in python environment</p>\n<p>Thanks</p>", "<p>What are you trying to accomplish with ASHLAR? Are you trying to stitch one set of images, align images from different timepoints, or both?</p>\n<p>What kind of images are you starting with?</p>", "<p>Thanks for kind reply. I am trying the multiplexed images, I have three channels together with DAPI channel in each cycle. I run around 4 cycles. I installed the ashlar with anaconda and I wrote the code in terminal. The error comes up. I think should I need to upload my image in the ashlar local folder. Is it correct?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/1/71c0b361cbaf412664a90b95d5cfbb730711d750.png\" data-download-href=\"/uploads/short-url/geiOrcY0XVn5g7IPh80Z3q7NVao.png?dl=1\" title=\"Screenshot (743)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/1/71c0b361cbaf412664a90b95d5cfbb730711d750_2_690x388.png\" alt=\"Screenshot (743)\" data-base62-sha1=\"geiOrcY0XVn5g7IPh80Z3q7NVao\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/1/71c0b361cbaf412664a90b95d5cfbb730711d750_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/1/71c0b361cbaf412664a90b95d5cfbb730711d750_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/1/71c0b361cbaf412664a90b95d5cfbb730711d750_2_1380x776.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/1/71c0b361cbaf412664a90b95d5cfbb730711d750_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot (743)</span><span class=\"informations\">1920\u00d71080 110 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p><a class=\"mention\" href=\"/u/russhorn\">@russhorn</a> I aim to stitch and align three cycles including three channels of each.<br>\nI have tiff images from multiplexed immunohistochemistry assay.</p>\n<ol>\n<li>I installed the ashlar on anaconda environment.</li>\n<li>I have one question that is it necessary to put the images in the same path that anaconda was installed.<br>\nlatest strip on my Anaconda Prompt is (ashlar) PS C:\\Users\\cherry&gt; ashlar</li>\n</ol>", "<aside class=\"quote no-group\" data-username=\"haymar_oo\" data-post=\"10\" data-topic=\"53112\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/haymar_oo/40/53650_2.png\" class=\"avatar\"> haymar oo:</div>\n<blockquote>\n<p>r</p>\n</blockquote>\n</aside>\n<p>Hi Haymar,</p>\n<p>Sorry I missed the reply several months ago.<br>\nYour images do <strong>not</strong> need to be in the same directory as your Anaconda .  As long as your ashlar Anaconda environment is active, you can work with images anywhere on your computer.</p>", "<p>Hi <a class=\"mention\" href=\"/u/jmuhlich\">@jmuhlich</a>!<br>\nI am trying to perform image registration of three separate multiplex OMETIFF-files which are all divided into three channels. The images have been scanned using GeoMx DSP, rather than a conventional microscope, have you hear of anyone else trying to accomplish that?</p>\n<p>I have installed Ashlar in an anaconda environment, and run in through the terminal (using a mac), but I cannot get it to work. It can read the first input file, but then errors occur:<br>\nWARNING: Stage coordinates undefined; falling back to (0, 0).<br>\nException: Can\u2019t handle non-square pixels (0.3990423, 0.398596942).</p>\n<p>I know that Ashlar is only compatible with tiled, non-stitched images but I have not been able to confirm that the output data from the GeomX is in fact stitched/unstitched. Does the error messages above imply that this is the problem, or could it be something else that I could actually fix?</p>\n<p>Thank you for your help!</p>", "<p>Are these cyclic bleached/stripped images of the same slide, or images of serial sections on separate slides? Ashlar would not be helpful in any case for serial sections, but my final answer below would still apply.</p>\n<p>GeoMX output images have already been stitched, so Ashlar is unable to help here. The error you saw is Ashlar being conservative about how out-of-square the pixels are allowed to be, since the internal math currently assumes they are perfectly square. But even if that were addressed you would not get a useful output from Ashlar for these images. Given enough time and RAM it would produce an image, but this would just be the rigid co-registration of your three stitched images which is not accurate enough for single-cell analysis.</p>\n<p>To align your images you would need a non-rigid whole-slide registration tool, of which several have been published recently. Here are some that we have our eye on, but we haven\u2019t evaluated them rigorously for single-cell-level accuracy:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/NHPatterson/wsireg\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/NHPatterson/wsireg\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e93dadf0d3f1ec8d4845c6ab45678e56fa0d137.png 2x\" data-dominant-color=\"ECEEED\"></div>\n\n<h3><a href=\"https://github.com/NHPatterson/wsireg\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - NHPatterson/wsireg: multimodal whole slide image registration in a...</a></h3>\n\n  <p>multimodal whole slide image registration in a graph structure - GitHub - NHPatterson/wsireg: multimodal whole slide image registration in a graph structure</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/ChristianMarzahl/WsiRegistration\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/ChristianMarzahl/WsiRegistration\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c40bb571d666a27eafa2d32f765ac53abf402ad3.png 2x\" data-dominant-color=\"ECEDEE\"></div>\n\n<h3><a href=\"https://github.com/ChristianMarzahl/WsiRegistration\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - ChristianMarzahl/WsiRegistration: Robust quad-tree based...</a></h3>\n\n  <p>Robust quad-tree based registration on whole slide images - GitHub - ChristianMarzahl/WsiRegistration: Robust quad-tree based registration on whole slide images</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p><a href=\"https://valis.readthedocs.io/en/latest/\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://valis.readthedocs.io/en/latest/</a></p>"], "32640": ["<p>For some reason I can get napari to work with jupyter notebooks on two machines, but not on a third (the kernel dies).</p>\n<p>It can be reproduced with:</p>\n<pre><code class=\"lang-python\">gui qt5\nimport napari\nnapari.Viewer()\n</code></pre>\n<p>On a desktop and a laptop it works fine, but on a second laptop it fails. All machines are running Ubuntu 18.04 LTS, with napari 0.2.8 in a fresh conda environment and jupyter running in firefox. The only real difference is spec, but the laptop is still powerful enough.</p>\n<p>Does anyone have any ideas why it\u2019s only failing on one machine?</p>", "<p>Hmm - it could be a finding of Qt bindings problem / a <code>PySide2</code> vs <code>PyQt5</code> problem. If you do <code>%gui qt</code> vs <code>%gui qt5</code> does it make a difference? Sometimes even the fresh conda environment can have problems finding the right Qt stuff.</p>\n<p>I take it you\u2019re not seeing our <code>napari requires a Qt event loop to run.</code> error message, but just getting a kernel failure?</p>\n<p>Also can you launch napari from the command line with just <code>napari</code>?</p>\n<p>Also can you paste the output of <code>napari --info</code> into this thread.</p>", "<p>Hi Nicholas,</p>\n<p><code>napari --info</code> just raised qt errors without printing information, same for launching napari from the command line.</p>\n<p>I think I found the issue though. I\u2019d always used <code>conda install jupyter</code> which seems to ship with an old version of PyQt5. <code>pip install PyQt5 --upgrade</code> solves the problem. Using <code>conda install notebook</code> still doesn\u2019t work, but it at least gives a qt error.</p>\n<p>My problem is fixed, but for your information, this seems weird:</p>\n<p>On this machine, napari works fine:</p>\n<pre><code class=\"lang-auto\">napari: 0.2.8\nPlatform: Linux-4.18.2-041802-generic-x86_64-with-debian-buster-sid\nPython: 3.7.5 (default, Oct 25 2019, 15:51:11)  [GCC 7.3.0]\nQt: 5.9.6\nPyQt5: 5.9.2\nVisPy: 0.6.4\nNumPy: 1.18.0\nSciPy: 1.4.1\nscikit-image: 0.16.2\nDask: 2.9.1\n\nGL version:  4.6.0 NVIDIA 418.67\nMAX_TEXTURE_SIZE: 32768\n</code></pre>\n<p>On another machine, with everything nearly the same, napari fails, and won\u2019t work until PyQt5 is updated:</p>\n<pre><code class=\"lang-auto\">napari: 0.2.8\nPlatform: Linux-4.15.0-1065-oem-x86_64-with-debian-buster-sid\nPython: 3.7.5 (default, Oct 25 2019, 15:51:11)  [GCC 7.3.0]\nQt: 5.14.0\nPyQt5: 5.14.0\nVisPy: 0.6.4\nNumPy: 1.18.0\nSciPy: 1.4.1\nscikit-image: 0.16.2\nDask: 2.9.1\n\nGL version:  3.0 Mesa 18.2.8\nMAX_TEXTURE_SIZE: 16384\n</code></pre>\n<p>The only differences seem to be the linux version (although both are Ubuntu 18.04 LTS) and the GL version.</p>\n<p>Thanks for your help!</p>", "<p>Hmm, ok - glad it works now. We should look into this more. In theory you should be able to have used with the PySide2 that we depend on (but you may have to call that with <code>%gui qt</code>. We should improve some of this stuff though so this is less of an issue</p>", "<p><code>%gui qt</code> also didn\u2019t work. What\u2019s weird is that rather than list available qt bindings (which jupyter normally does if it can\u2019t find one), the kernel just died.</p>\n<p>Let me know if I can help with any testing, and thanks again for your help.</p>", "<p>Can you paste the output of <code>conda list</code> From the broken environment, and the full error you get?</p>", "<p>Hi Talley,</p>\n<p>I reproduced the problem with:</p>\n<pre><code class=\"lang-bash\">conda create --name naparitest python=3.6 jupyter nb_conda\nconda activate naparitest\npip install napari\n</code></pre>\n<p>On one machine, napari gives a warning, but works fine. <code>napari --info</code> prints:</p>\n<pre><code class=\"lang-bash\">(naparitest) adam@pingu:~$ napari --info\n/home/adam/miniconda3/envs/naparitest/lib/python3.6/site-packages/napari/__init__.py:27: UserWarning: \n    napari was tested with QT library `&gt;=5.12.3`.\n    The version installed is 5.9.6. Please report any issues with this\n    specific QT version at https://github.com/Napari/napari/issues.\n    \n  warn(message=warn_message)\nnapari: 0.2.8\nPlatform: Linux-4.15.0-48-generic-x86_64-with-debian-buster-sid\nPython: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)  [GCC 7.3.0]\nQt: 5.9.6\nPyQt5: 5.9.2\nVisPy: 0.6.4\nNumPy: 1.18.0\nSciPy: 1.4.1\nscikit-image: 0.16.2\nDask: 2.9.1\n\nGL version:  4.6.0 NVIDIA 418.56\nMAX_TEXTURE_SIZE: 32768\n</code></pre>\n<p>On the machine with the issue, <code>napari --info</code> gives:</p>\n<pre><code class=\"lang-bash\">(naparitest) adam@tigger:~/staging$ napari --info\n/home/adam/miniconda3/envs/naparitest/lib/python3.6/site-packages/napari/__init__.py:27: UserWarning: \n    napari was tested with QT library `&gt;=5.12.3`.\n    The version installed is 5.9.6. Please report any issues with this\n    specific QT version at https://github.com/Napari/napari/issues.\n    \n  warn(message=warn_message)\nSegmentation fault (core dumped)\n</code></pre>\n<p>And the same error with just <code>napari</code>. When using jupyter notebooks, there are no additional errors, the kernel just dies.</p>\n<p><code>conda list</code> in the broken environment gives:</p>\n<pre><code class=\"lang-bash\"># packages in environment at /home/adam/miniconda3/envs/naparitest:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \nalabaster                 0.7.12                   pypi_0    pypi\nasciitree                 0.3.3                    pypi_0    pypi\nattrs                     19.3.0                     py_0  \nbabel                     2.8.0                    pypi_0    pypi\nbackcall                  0.1.0                    py36_0  \nbleach                    3.1.0                      py_0  \nca-certificates           2019.11.27                    0  \ncertifi                   2019.11.28               py36_0  \nchardet                   3.0.4                    pypi_0    pypi\ncycler                    0.10.0                   pypi_0    pypi\ndask                      2.9.1                    pypi_0    pypi\ndbus                      1.13.12              h746ee38_0  \ndecorator                 4.4.1                      py_0  \ndefusedxml                0.6.0                      py_0  \ndocutils                  0.15.2                   pypi_0    pypi\nentrypoints               0.3                      py36_0  \nexpat                     2.2.6                he6710b0_0  \nfasteners                 0.15                     pypi_0    pypi\nfontconfig                2.13.0               h9420a91_0  \nfreetype                  2.9.1                h8a8886c_1  \nfreetype-py               2.1.0.post1              pypi_0    pypi\nfsspec                    0.6.2                    pypi_0    pypi\nglib                      2.63.1               h5a9c865_0  \ngmp                       6.1.2                h6c8ec71_1  \ngst-plugins-base          1.14.0               hbbd80ab_1  \ngstreamer                 1.14.0               hb453b48_1  \nicu                       58.2                 h9c2bf20_1  \nidna                      2.8                      pypi_0    pypi\nimageio                   2.6.1                    pypi_0    pypi\nimagesize                 1.2.0                    pypi_0    pypi\nimportlib_metadata        1.3.0                    py36_0  \nipykernel                 5.1.3            py36h39e3cac_0  \nipython                   7.10.2           py36h39e3cac_0  \nipython_genutils          0.2.0                    py36_0  \nipywidgets                7.5.1                      py_0  \njedi                      0.15.1                   py36_0  \njinja2                    2.10.3                     py_0  \njpeg                      9b                   h024ee3a_2  \njsonschema                3.2.0                    py36_0  \njupyter                   1.0.0                    py36_7  \njupyter_client            5.3.4                    py36_0  \njupyter_console           6.0.0                    py36_0  \njupyter_core              4.6.1                    py36_0  \nkiwisolver                1.1.0                    pypi_0    pypi\nlibedit                   3.1.20181209         hc058e9b_0  \nlibffi                    3.2.1                hd88cf55_4  \nlibgcc-ng                 9.1.0                hdf63c60_0  \nlibpng                    1.6.37               hbc83047_0  \nlibsodium                 1.0.16               h1bed415_0  \nlibstdcxx-ng              9.1.0                hdf63c60_0  \nlibuuid                   1.0.3                h1bed415_2  \nlibxcb                    1.13                 h1bed415_1  \nlibxml2                   2.9.9                hea5a465_1  \nmarkupsafe                1.1.1            py36h7b6447c_0  \nmatplotlib                3.1.2                    pypi_0    pypi\nmistune                   0.8.4            py36h7b6447c_0  \nmonotonic                 1.5                      pypi_0    pypi\nmore-itertools            8.0.2                      py_0  \nnapari                    0.2.8                    pypi_0    pypi\nnb_conda                  2.2.1                    py36_0  \nnb_conda_kernels          2.2.2                    py36_0  \nnbconvert                 5.6.1                    py36_0  \nnbformat                  4.4.0                    py36_0  \nncurses                   6.1                  he6710b0_1  \nnetworkx                  2.4                      pypi_0    pypi\nnotebook                  6.0.2                    py36_0  \nnumcodecs                 0.6.3                    pypi_0    pypi\nnumpy                     1.18.0                   pypi_0    pypi\nnumpydoc                  0.9.2                    pypi_0    pypi\nopenssl                   1.1.1d               h7b6447c_3  \npackaging                 19.2                     pypi_0    pypi\npandoc                    2.2.3.2                       0  \npandocfilters             1.4.2                    py36_1  \nparso                     0.5.2                      py_0  \npcre                      8.43                 he6710b0_0  \npexpect                   4.7.0                    py36_0  \npickleshare               0.7.5                    py36_0  \npillow                    7.0.0                    pypi_0    pypi\npip                       19.3.1                   py36_0  \nprometheus_client         0.7.1                      py_0  \nprompt_toolkit            2.0.10                     py_0  \nptyprocess                0.6.0                    py36_0  \npygments                  2.5.2                      py_0  \npyopengl                  3.1.4                    pypi_0    pypi\npyparsing                 2.4.6                    pypi_0    pypi\npyqt                      5.9.2            py36h05f1152_2  \npyrsistent                0.15.6           py36h7b6447c_0  \npyside2                   5.14.0                   pypi_0    pypi\npython                    3.6.9                h265db76_0  \npython-dateutil           2.8.1                      py_0  \npytz                      2019.3                   pypi_0    pypi\npywavelets                1.1.1                    pypi_0    pypi\npyzmq                     18.1.0           py36he6710b0_0  \nqt                        5.9.7                h5867ecd_1  \nqtconsole                 4.6.0                      py_0  \nqtpy                      1.9.0                    pypi_0    pypi\nreadline                  7.0                  h7b6447c_5  \nrequests                  2.22.0                   pypi_0    pypi\nscikit-image              0.16.2                   pypi_0    pypi\nscipy                     1.4.1                    pypi_0    pypi\nsend2trash                1.5.0                    py36_0  \nsetuptools                42.0.2                   py36_0  \nshiboken2                 5.14.0                   pypi_0    pypi\nsip                       4.19.8           py36hf484d3e_0  \nsix                       1.13.0                   py36_0  \nsnowballstemmer           2.0.0                    pypi_0    pypi\nsphinx                    2.3.1                    pypi_0    pypi\nsphinxcontrib-applehelp   1.0.1                    pypi_0    pypi\nsphinxcontrib-devhelp     1.0.1                    pypi_0    pypi\nsphinxcontrib-htmlhelp    1.0.2                    pypi_0    pypi\nsphinxcontrib-jsmath      1.0.1                    pypi_0    pypi\nsphinxcontrib-qthelp      1.0.2                    pypi_0    pypi\nsphinxcontrib-serializinghtml 1.1.3                    pypi_0    pypi\nsqlite                    3.30.1               h7b6447c_0  \nterminado                 0.8.3                    py36_0  \ntestpath                  0.4.4                      py_0  \ntk                        8.6.8                hbc83047_0  \ntoolz                     0.10.0                   pypi_0    pypi\ntornado                   6.0.3            py36h7b6447c_0  \ntraitlets                 4.3.3                    py36_0  \nurllib3                   1.25.7                   pypi_0    pypi\nvispy                     0.6.4                    pypi_0    pypi\nwcwidth                   0.1.7                    py36_0  \nwebencodings              0.5.1                    py36_1  \nwheel                     0.33.6                   py36_0  \nwidgetsnbextension        3.5.1                    py36_0  \nwrapt                     1.11.2                   pypi_0    pypi\nxz                        5.2.4                h14c3975_4  \nzarr                      2.3.2                    pypi_0    pypi\nzeromq                    4.3.1                he6710b0_3  \nzipp                      0.6.0                      py_0  \nzlib                      1.2.11               h7b6447c_3  \n</code></pre>\n<p>I can\u2019t seem to reproduce the issue with needing to update PyQt if installing jupyter notebooks with <code>conda install notebook</code>, so I guess the only issue is with napari not working in an environment with jupyter notebooks installed via <code>conda install jupyter</code>, but that seems to not be the official way anyway. Strange that I can only reproduce this on one machine of three.</p>", "<p>I\u2019m experiencing the same issue on my Ubuntu desktop. Here is my <code>napari --info</code> output:</p>\n<pre><code class=\"lang-auto\">napari: 0.2.12\nPlatform: Linux-4.15.0-91-generic-x86_64-with-debian-buster-sid\nPython: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]\nQt: 5.14.1\nPyQt5: 5.14.1\nVisPy: 0.6.4\nNumPy: 1.17.4\nSciPy: 1.3.2\nscikit-image: 0.15.0\nDask: 2.9.0\n\nGL version:  3.1 Mesa 19.2.8\nMAX_TEXTURE_SIZE: 8192\n</code></pre>", "<p>hey tae!  can you just restate the problem as you\u2019re seeing it please?  it basically works from the command line and within ipython, but not in jupyter notebooks?  can you tell me more about your environment (<code>conda list</code> and <code>pip freeze</code>) along with some basic steps i can follow to try to reproduce it (e.g. environment creation and how you\u2019re starting napari when it fails)</p>", "<p>Hey Talley! Thanks for trying to reproduce and resolve this issue. it does not work from the command line and within ipython, nor does in jupyter notebook. Within ipython, the error looks like:</p>\n<pre><code class=\"lang-auto\">Python 3.7.3 (default, Mar 27 2019, 22:11:17) \nType 'copyright', 'credits' or 'license' for more information\nIPython 7.13.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import napari                                                           \n\nIn [2]: from skimage import data                                                \n\nIn [3]: %gui qt                                                                 \n\nIn [4]: viewer = napari.view_image(data.astronaut(), rgb=True)                  \nStack dump:\nSegmentation fault (core dumped)\n</code></pre>\n<p>In jupyter notebook, kernel dies when running <code>napari.view_image()</code>. Here is the <code>conda list</code> (sorry it\u2019s base environment so kinda long\u2026)</p>\n<pre><code class=\"lang-auto\"># Name                    Version                   Build  Channel\n_anaconda_depends         2019.03                  py37_0  \n_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \n_libgcc_mutex             0.1                        main  \n_tflow_select             2.1.0                       gpu  \nabsl-py                   0.8.1                    py37_0  \nadjusttext                0.7.2              pyh39e3cac_0    phlya\nalabaster                 0.7.12                   py37_0  \nanaconda                  custom                   py37_1  \nanaconda-client           1.7.2                    py37_0  \nanaconda-navigator        1.9.2                    py37_0  \nanaconda-project          0.8.4                      py_0  \napbs                      1.5                  h14c3975_3    schrodinger\nappdirs                   1.4.3            py37h28b3542_0  \napptools                  4.4.0                    pypi_0    pypi\nargh                      0.26.2                   py37_0  \nasn1crypto                1.3.0                    py37_0  \nastor                     0.8.0                    py37_0  \nastroid                   2.3.3                    py37_0  \nastropy                   3.2.3            py37h7b6447c_0  \natomicwrites              1.3.0                    py37_1  \nattrs                     19.3.0                     py_0  \nautomat                   0.8.0                      py_0  \nautopep8                  1.4.4                      py_0  \nbabel                     2.8.0                      py_0  \nbackcall                  0.1.0                    py37_0  \nbackports                 1.0                        py_2  \nbackports.os              0.1.1                    py37_0  \nbackports.shutil_get_terminal_size 1.0.0                    py37_2  \nbcrypt                    3.1.7            py37h7b6447c_0  \nbeautifulsoup4            4.8.2                    py37_0  \nbitarray                  1.2.1            py37h7b6447c_0  \nbkcharts                  0.2                      py37_0  \nblas                      1.0                         mkl  \nblaze                     0.11.3                   py37_0  \nbleach                    3.1.0                    py37_0  \nblosc                     1.16.3               hd408876_0  \nbokeh                     1.4.0                    py37_0  \nboto                      2.49.0                   py37_0  \nboto3                     1.9.134                  pypi_0    pypi\nbotocore                  1.12.134                 pypi_0    pypi\nbottleneck                1.3.1            py37hdd07704_0  \nbzip2                     1.0.8                h7b6447c_0  \nc-ares                    1.15.0            h7b6447c_1001  \nca-certificates           2019.11.28           hecc5488_0    conda-forge\ncairo                     1.14.12              h8948797_3  \ncertifi                   2019.11.28       py37hc8dfbb8_1    conda-forge\ncffi                      1.14.0                   pypi_0    pypi\nchardet                   3.0.4                 py37_1003  \nclick                     7.0                      py37_0  \ncloudpickle               1.2.2                      py_0  \nclyent                    1.2.2                    py37_1  \ncolorama                  0.4.3                      py_0  \nconda                     4.8.3            py37hc8dfbb8_0    conda-forge\nconda-build               3.15.1                   py37_0  \nconda-env                 2.6.0                         1  \nconda-package-handling    1.6.0            py37h7b6447c_0  \nconfigobj                 5.0.6                    pypi_0    pypi\nconfigparser              4.0.2                    pypi_0    pypi\nconstantly                15.1.0           py37h28b3542_0  \ncontextlib2               0.6.0.post1                py_0  \ncryptography              2.8              py37h1ba5d50_0  \ncudatoolkit               10.0.130                      0  \ncudnn                     7.6.5                cuda10.0_0  \ncupti                     10.0.130                      0  \ncurl                      7.67.0               hbc83047_0  \ncycler                    0.10.0                   py37_0  \ncython                    0.29.6                   pypi_0    pypi\ncytoolz                   0.10.1           py37h7b6447c_0  \ndask                      2.9.0                      py_0  \ndask-core                 2.9.0                      py_0  \ndatashape                 0.5.4                    py37_1  \ndbus                      1.13.12              h746ee38_0  \ndecorator                 4.4.1                      py_0  \ndefusedxml                0.6.0                      py_0  \ndiff-match-patch          20181111                   py_0  \ndistributed               2.10.0                     py_0  \ndocutils                  0.16                     py37_0  \nentrypoints               0.3                      py37_0  \nenvisage                  4.7.2                    pypi_0    pypi\net_xmlfile                1.0.1                    py37_0  \nexpat                     2.2.6                he6710b0_0  \nfastcache                 1.1.0            py37h7b6447c_0  \nfilelock                  3.0.12                     py_0  \nflake8                    3.7.9                    py37_0  \nflask                     1.1.1                      py_0  \nflask-cors                3.0.8                      py_0  \nfontconfig                2.13.0               h9420a91_0  \nfreemol                   1.158                    py37_1    schrodinger\nfreetype                  2.9.1                h8a8886c_1  \nfreetype-py               2.1.0.post1              pypi_0    pypi\nfribidi                   1.0.5                h7b6447c_0  \nfsspec                    0.6.2                      py_0  \nfuncsigs                  1.0.2                    pypi_0    pypi\nfuture                    0.17.1                   pypi_0    pypi\ngast                      0.2.2                    py37_0  \nget_terminal_size         1.0.0                haa9412d_0  \ngevent                    1.4.0            py37h7b6447c_0  \nglew                      2.0.0                         0    schrodinger\nglib                      2.63.1               h5a9c865_0  \nglob2                     0.7                        py_0  \ngmp                       6.1.2                h6c8ec71_1  \ngmpy2                     2.0.8            py37h10f8cd9_2  \ngoogle-pasta              0.1.8                      py_0  \ngraphite2                 1.3.13               h23475e2_0  \ngreenlet                  0.4.15           py37h7b6447c_0  \ngrpcio                    1.16.1           py37hf8bcb03_1  \ngst-plugins-base          1.14.0               hbbd80ab_1  \ngstreamer                 1.14.0               hb453b48_1  \nh5py                      2.8.0            py37h989c5e5_3  \nharfbuzz                  1.8.8                hffaf4a1_0  \nhdf5                      1.10.2               hba1933b_1  \nheapdict                  1.0.1                      py_0  \nhtml5lib                  1.0.1                    py37_0  \nhyperlink                 19.0.0                     py_0  \nhypothesis                4.54.2                   py37_0  \nicu                       58.2                 h9c2bf20_1  \nidna                      2.8                      py37_0  \nimageio                   2.6.1                    py37_0  \nimagej                    0.3.1                    pypi_0    pypi\nimagesize                 1.2.0                      py_0  \nimportlib_metadata        1.4.0                    py37_0  \nimreg-dft                 2.0.0                    pypi_0    pypi\nincremental               17.5.0                   py37_0  \nintel-openmp              2019.4                      243  \nintervaltree              3.0.2                      py_0  \nipykernel                 5.1.4            py37h39e3cac_0  \nipython                   7.13.0                   pypi_0    pypi\nipython_genutils          0.2.0                    py37_0  \nipywidgets                7.5.1                      py_0  \nisort                     4.3.21                   py37_0  \nitsdangerous              1.1.0                    py37_0  \njavabridge                1.0.18                   pypi_0    pypi\njbig                      2.1                  hdba287a_0  \njdcal                     1.4.1                      py_0  \njedi                      0.15.2                   py37_0  \njeepney                   0.4.2                      py_0  \njinja2                    2.10.3                     py_0  \njmespath                  0.9.4                    pypi_0    pypi\njoblib                    0.14.1                     py_0  \njpeg                      9b                   h024ee3a_2  \njson5                     0.8.5                      py_0  \njsonschema                3.2.0                    py37_0  \njupyter                   1.0.0                    py37_7  \njupyter_client            5.3.4                    py37_0  \njupyter_console           5.2.0                    py37_1  \njupyter_core              4.6.1                    py37_0  \njupyterlab                1.0.6                    py37_0    conda-forge\njupyterlab_launcher       0.13.1                   py37_0  \njupyterlab_server         1.0.6                      py_0  \nkeras-applications        1.0.8                      py_0  \nkeras-preprocessing       1.1.0                      py_1  \nkeyring                   21.1.0                   py37_0  \nkiwisolver                1.1.0            py37he6710b0_0  \nkrb5                      1.16.4               h173b8e3_0  \nlazy-object-proxy         1.4.3            py37h7b6447c_0  \nlibarchive                3.3.3                h5d8350f_5  \nlibcurl                   7.67.0               h20c2e04_0  \nlibedit                   3.1.20181209         hc058e9b_0  \nlibffi                    3.2.1                hd88cf55_4  \nlibgcc-ng                 9.1.0                hdf63c60_0  \nlibgfortran-ng            7.3.0                hdf63c60_0  \nlibglu                    9.0.0                hf484d3e_1  \nlibhwloc                  2.0.3                h3c4fd83_1    conda-forge\nliblief                   0.9.0                h7725739_2  \nlibopenblas               0.3.6                h5a2b251_2  \nlibpng                    1.6.37               hbc83047_0  \nlibprotobuf               3.11.2               hd408876_0  \nlibsodium                 1.0.16               h1bed415_0  \nlibspatialindex           1.9.3                he6710b0_0  \nlibssh2                   1.8.2                h1ba5d50_0  \nlibstdcxx-ng              9.1.0                hdf63c60_0  \nlibtiff                   4.1.0                h2733197_0  \nlibtool                   2.4.6                h7b6447c_5  \nlibuuid                   1.0.3                h1bed415_2  \nlibxcb                    1.13                 h1bed415_1  \nlibxml2                   2.9.9                hea5a465_1  \nlibxslt                   1.1.33               h7d1a2b0_0  \nllvmlite                  0.30.0           py37hd408876_0  \nlocket                    0.2.0                    py37_1  \nlxml                      4.4.2            py37hefd8a0e_0  \nlz4-c                     1.8.1.2              h14c3975_0  \nlzo                       2.10                 h49e0be7_2  \nmahotas                   1.4.5            py37hf8a1672_0    conda-forge\nmako                      1.1.1                    pypi_0    pypi\nmarkdown                  3.1.1                    py37_0  \nmarkupsafe                1.1.1            py37h7b6447c_0  \nmatplotlib                3.1.1            py37h5429711_0  \nmayavi                    4.6.2                    pypi_0    pypi\nmccabe                    0.6.1                    py37_1  \nmengine                   1                    h14c3975_1    schrodinger\nmistune                   0.8.4            py37h7b6447c_0  \nmkl                       2019.4                      243  \nmkl-service               2.3.0            py37he904b0f_0  \nmkl_fft                   1.0.15           py37ha843d7b_0  \nmkl_random                1.1.0            py37hd6b4f25_0  \nmore-itertools            8.0.2                      py_0  \nmpc                       1.1.0                h10f8cd9_1  \nmpeg_encode               1                    h14c3975_1    schrodinger\nmpfr                      4.0.1                hdf1c602_3  \nmpmath                    1.1.0                    py37_0  \nmsgpack-python            0.6.1            py37hfd86e86_1  \nmtz2ccp4_px               1.0                  h9ac9557_3    schrodinger\nmultipledispatch          0.6.0                    py37_0  \nnapari                    0.2.12                   pypi_0    pypi\nnavigator-updater         0.2.1                    py37_0  \nnbconvert                 5.6.1                    py37_0  \nnbformat                  5.0.4                      py_0  \nncurses                   6.1                  he6710b0_1  \nnd2reader                 3.1.0                    pypi_0    pypi\nnetworkx                  2.4                        py_0  \nnltk                      3.4.5                    py37_0  \nnose                      1.3.7                    py37_2  \nnotebook                  6.0.3                    py37_0  \nnumba                     0.46.0           py37h962f231_0  \nnumexpr                   2.7.1            py37h423224d_0  \nnumpy                     1.18.0                   pypi_0    pypi\nnumpy-base                1.17.4           py37hde5b4d6_0  \nnumpydoc                  0.9.2                      py_0  \nocl-icd                   2.2.12            h516909a_1005    conda-forge\nocl-icd-system            1.0.0                         1    conda-forge\noclgrind                  18.3              h29592fa_1002    conda-forge\nodo                       0.5.1                    py37_0  \nolefile                   0.46                     py37_0  \nopenpyxl                  3.0.3                      py_0  \nopenssl                   1.1.1d               h516909a_0    conda-forge\nopt_einsum                3.1.0                      py_0  \npackaging                 20.1                       py_0  \npandas                    0.25.3           py37he6710b0_0  \npandoc                    2.2.3.2                       0  \npandocfilters             1.4.2                    py37_1  \npango                     1.42.4               h049681c_0  \nparso                     0.6.0                      py_0  \npartd                     1.1.0                      py_0  \npatchelf                  0.10                 he6710b0_0  \npath                      13.1.0                   py37_0  \npath.py                   12.4.0                        0  \npathlib2                  2.3.5                    py37_0  \npathtools                 0.1.2                      py_1  \npatsy                     0.5.1                    py37_0  \npcre                      8.43                 he6710b0_0  \npdb2pqr                   2.1.1                    py37_1    schrodinger\npep8                      1.7.1                    py37_0  \npexpect                   4.8.0                    py37_0  \npickleshare               0.7.5                    py37_0  \npillow                    7.0.0            py37hb39fc2d_0  \npims                      0.4.1                      py_1    conda-forge\npims-nd2                  1.0                      pypi_0    pypi\npip                       19.1.1                   pypi_0    pypi\npixman                    0.38.0               h7b6447c_0  \npkginfo                   1.5.0.1                  py37_0  \npluggy                    0.13.1                   py37_0  \nply                       3.11                     py37_0  \npmw                       2.0.1                    py37_2    schrodinger\npocl                      1.4                  h7d38c80_0    conda-forge\nprometheus_client         0.7.1                      py_0  \nprompt-toolkit            3.0.4                    pypi_0    pypi\nprotobuf                  3.11.2           py37he6710b0_0  \npsutil                    5.6.7            py37h7b6447c_0  \nptyprocess                0.6.0                    py37_0  \npy                        1.8.1                      py_0  \npy-lief                   0.9.0            py37h7725739_2  \npyasn1                    0.4.8                      py_0  \npyasn1-modules            0.2.7                      py_0  \npybind11                  2.4.3                    pypi_0    pypi\npycodestyle               2.5.0                    py37_0  \npycosat                   0.6.3            py37h7b6447c_0  \npycparser                 2.19                     py37_0  \npycrypto                  2.6.1            py37h14c3975_9  \npycurl                    7.43.0.4         py37h1ba5d50_0  \npydocstyle                4.0.1                      py_0  \npyface                    6.1.0                    pypi_0    pypi\npyflakes                  2.1.1                    py37_0  \npygments                  2.5.2                      py_0  \npyhamcrest                1.9.0                    py37_2  \npylint                    2.4.4                    py37_0  \npymol                     2.3.2            py37h75f9260_0    schrodinger\npympler                   0.7                        py_0  \npyodbc                    4.0.28           py37he6710b0_0  \npyopencl                  2019.1.2         py37h9de70de_0    conda-forge\npyopengl                  3.1.0                    pypi_0    pypi\npyopenssl                 19.1.0                   py37_0  \npyparsing                 2.4.6                      py_0  \npyqt                      5.9.2            py37h05f1152_2  \npyqt5                     5.14.1                   pypi_0    pypi\npyqt5-sip                 12.7.1                   pypi_0    pypi\npyrsistent                0.15.7           py37h7b6447c_0  \npyside2                   5.14.1                   pypi_0    pypi\npysocks                   1.7.1                    py37_0  \npytables                  3.4.4            py37ha205bf6_0  \npytest                    5.3.4                    py37_0  \npytest-arraydiff          0.3              py37h39e3cac_0  \npytest-astropy            0.7.0                      py_0  \npytest-astropy-header     0.1.1                      py_0  \npytest-doctestplus        0.5.0                      py_0  \npytest-openfiles          0.4.0                      py_0  \npytest-remotedata         0.3.2                    py37_0  \npython                    3.7.3                h0371630_0  \npython-bioformats         1.5.2                    pypi_0    pypi\npython-dateutil           2.8.1                      py_0  \npython-jsonrpc-server     0.3.2                      py_0  \npython-language-server    0.31.2                   py37_0  \npython-libarchive-c       2.8                     py37_13  \npython_abi                3.7                     1_cp37m    conda-forge\npytools                   2020.1                     py_0    conda-forge\npytz                      2019.3                     py_0  \npywavelets                1.1.1            py37h7b6447c_0  \npyxdg                     0.26                       py_0  \npyyaml                    5.2              py37h7b6447c_0  \npyzmq                     18.0.0           py37he6710b0_0    anaconda\nqdarkstyle                2.7                        py_0  \nqt                        5.9.7                h5867ecd_1  \nqtawesome                 0.6.1                      py_0  \nqtconsole                 4.6.0                      py_1  \nqtpy                      1.9.0                      py_0  \nread-roi                  1.5.2                      py_0    conda-forge\nreadline                  7.0                  h7b6447c_5  \nreikna                    0.7.4                    pypi_0    pypi\nrequests                  2.22.0                   py37_1  \nrigimol                   1.3                           2    schrodinger\nrope                      0.16.0                     py_0  \nrtree                     0.8.3                    py37_0  \nruamel_yaml               0.15.87          py37h7b6447c_0  \ns3transfer                0.2.0                    pypi_0    pypi\nscikit-image              0.15.0           py37he6710b0_0  \nscikit-learn              0.22.1           py37hd81dba3_0  \nscikit-tensor-py3         0.4.1                    pypi_0    pypi\nscipy                     1.3.2            py37h7c811a0_0  \nseaborn                   0.9.0              pyh91ea838_1  \nsecretstorage             3.1.2                    py37_0  \nsend2trash                1.5.0                    py37_0  \nservice_identity          18.1.0           py37h28b3542_0  \nsetuptools                40.8.0                   pypi_0    pypi\nshiboken2                 5.14.1                   pypi_0    pypi\nsimplegeneric             0.8.1                    py37_2  \nsingledispatch            3.4.0.3                  py37_0  \nsip                       4.19.8           py37hf484d3e_0  \nsix                       1.14.0                   py37_0  \nslicerator                1.0.0                      py_0    conda-forge\nsnappy                    1.1.7                hbae5bb6_3  \nsnowballstemmer           2.0.0                      py_0  \nsortedcollections         1.1.2                    py37_0  \nsortedcontainers          2.1.0                    py37_0  \nsoupsieve                 1.9.5                    py37_0  \nsphinx                    2.3.1                      py_0  \nsphinxcontrib             1.0                      py37_1  \nsphinxcontrib-applehelp   1.0.1                      py_0  \nsphinxcontrib-devhelp     1.0.1                      py_0  \nsphinxcontrib-htmlhelp    1.0.2                      py_0  \nsphinxcontrib-jsmath      1.0.1                      py_0  \nsphinxcontrib-qthelp      1.0.2                      py_0  \nsphinxcontrib-serializinghtml 1.1.3                      py_0  \nsphinxcontrib-websupport  1.1.2                      py_0  \nspyder                    3.3.6                    py37_0  \nspyder-kernels            0.5.2                    py37_0  \nsqlalchemy                1.3.13           py37h7b6447c_0  \nsqlite                    3.30.1               h7b6447c_0  \nstatsmodels               0.10.1           py37hdd07704_0  \nsympy                     1.5.1                    py37_0  \ntbb                       2019.8               hfd86e86_0  \ntblib                     1.6.0                      py_0  \ntensorboard               2.0.0              pyhb38c66f_1  \ntensorflow                2.0.0           gpu_py37h768510d_0  \ntensorflow-base           2.0.0           gpu_py37h0ec5d1f_0  \ntensorflow-estimator      2.0.0              pyh2649769_0  \ntensorflow-gpu            2.0.0                h0d30ee6_0  \ntermcolor                 1.1.0                    py37_1  \nterminado                 0.8.3                    py37_0  \ntestpath                  0.4.4                      py_0  \ntifffile                  0.15.1          py37h3010b51_1001    conda-forge\ntk                        8.6.8                hbc83047_0  \ntoolz                     0.10.0                     py_0  \ntornado                   6.0.3            py37h7b6447c_0  \ntqdm                      4.42.0                     py_0  \ntrackpy                   0.4.1                      py_1    conda-forge\ntraitlets                 4.3.3                    py37_0  \ntraits                    5.1.1                    pypi_0    pypi\ntraitsui                  6.1.0                    pypi_0    pypi\ntwisted                   19.10.0          py37h7b6447c_0  \nujson                     1.35             py37h14c3975_0  \nunicodecsv                0.14.1                   py37_0  \nunixodbc                  2.3.7                h14c3975_0  \nurllib3                   1.25.8                   py37_0  \nvispy                     0.6.4                    pypi_0    pypi\nvisvis                    1.11.2                   pypi_0    pypi\nvtk                       8.1.2                    pypi_0    pypi\nwatchdog                  0.9.0                    py37_1  \nwcwidth                   0.1.7                    py37_0  \nwebencodings              0.5.1                    py37_1  \nwerkzeug                  0.16.1                     py_0  \nwheel                     0.34.1                   py37_0  \nwidgetsnbextension        3.5.1                    py37_0  \nwrapt                     1.11.2           py37h7b6447c_0  \nwurlitzer                 2.0.0                    py37_0  \nxlrd                      1.2.0                    py37_0  \nxlsxwriter                1.2.7                      py_0  \nxlwt                      1.3.0                    py37_0  \nxmltodict                 0.12.0                   pypi_0    pypi\nxz                        5.2.4                h14c3975_4  \nyaml                      0.1.7                had09818_2  \nyapf                      0.28.0                     py_0  \nzeromq                    4.3.1                he6710b0_3  \nzict                      1.0.0                      py_0  \nzipp                      0.6.0                      py_0  \nzlib                      1.2.11               h7b6447c_3  \nzope                      1.0                      py37_1  \nzope.interface            4.7.1            py37h7b6447c_0  \nzstd                      1.3.7                h0b5b093_0  \n</code></pre>\n<p>And <code>pip freeze</code>:</p>\n<pre><code class=\"lang-auto\">absl-py==0.8.1\nadjustText==0.7.2\nalabaster==0.7.12\nanaconda-client==1.7.2\nanaconda-navigator==1.9.2\nanaconda-project==0.8.3\nappdirs==1.4.3\napptools==4.4.0\nargh==0.26.2\nasn1crypto==1.3.0\nastor==0.8.0\nastroid==2.3.3\nastropy==3.2.3\natomicwrites==1.3.0\nattrs==19.3.0\nAutomat==0.8.0\nautopep8==1.4.4\nBabel==2.8.0\nbackcall==0.1.0\nbackports.os==0.1.1\nbackports.shutil-get-terminal-size==1.0.0\nbcrypt==3.1.7\nbeautifulsoup4==4.8.2\nbitarray==1.2.1\nbkcharts==0.2\nblaze==0.11.3\nbleach==3.1.0\nbokeh==1.4.0\nboto==2.49.0\nboto3==1.9.134\nbotocore==1.12.134\nBottleneck==1.3.1\ncertifi==2019.11.28\ncffi==1.14.0\nchardet==3.0.4\nClick==7.0\ncloudpickle==1.2.2\nclyent==1.2.2\ncolorama==0.4.3\nconda==4.8.3\nconda-build==3.15.1\nconda-package-handling==1.6.0\nconfigobj==5.0.6\nconfigparser==4.0.2\nconstantly==15.1.0\ncontextlib2==0.6.0.post1\ncryptography==2.8\ncycler==0.10.0\nCython==0.29.14\ncytoolz==0.10.1\ndask==2.9.0\ndatashape==0.5.4\ndecorator==4.4.1\ndefusedxml==0.6.0\ndiff-match-patch==20181111\ndistributed==2.10.0\ndocutils==0.16\nentrypoints==0.3\nenvisage==4.7.2\net-xmlfile==1.0.1\nfastcache==1.1.0\nfilelock==3.0.12\nflake8==3.7.9\nFlask==1.1.1\nFlask-Cors==3.0.8\nfreemol==1.158\nfreetype-py==2.1.0.post1\nfsspec==0.6.2\nfuncsigs==1.0.2\nfuture==0.18.2\ngast==0.2.2\ngevent==1.4.0\nglob2==0.7\ngmpy2==2.0.8\ngoogle-pasta==0.1.8\ngreenlet==0.4.15\ngrpcio==1.16.1\nh5py==2.8.0\nHeapDict==1.0.1\nhtml5lib==1.0.1\nhyperlink==19.0.0\nhypothesis==4.54.2\nidna==2.8\nimageio==2.6.1\nimagej==0.3.1\nimagesize==1.2.0\nimportlib-metadata==1.4.0\nimreg-dft==2.0.0\nincremental==17.5.0\nintervaltree==3.0.2\nipykernel==5.1.4\nipython==7.13.0\nipython-genutils==0.2.0\nipywidgets==7.5.1\nisort==4.3.21\nitsdangerous==1.1.0\njavabridge==1.0.18\njdcal==1.4.1\njedi==0.15.2\njeepney==0.4.2\nJinja2==2.10.3\njmespath==0.9.4\njoblib==0.14.1\njson5==0.8.5\njsonschema==3.2.0\njupyter==1.0.0\njupyter-client==5.3.4\njupyter-console==5.2.0\njupyter-core==4.6.1\njupyterlab==1.0.6\njupyterlab-launcher==0.13.1\njupyterlab-server==1.0.6\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nkeyring==21.1.0\nkiwisolver==1.1.0\nlazy-object-proxy==1.4.3\nlibarchive-c==2.8\nlief==0.9.0\nllvmlite==0.30.0\nlocket==0.2.0\nlxml==4.4.2\nmahotas==1.4.5\nMako==1.1.1\nMarkdown==3.1.1\nMarkupSafe==1.1.1\nmatplotlib==3.1.1\nmayavi==4.6.2\nmccabe==0.6.1\nmistune==0.8.4\nmkl-fft==1.0.15\nmkl-random==1.1.0\nmkl-service==2.3.0\nmore-itertools==8.0.2\nmpmath==1.1.0\nmsgpack==0.6.1\nmultipledispatch==0.6.0\nnapari==0.2.12\nnavigator-updater==0.2.1\nnbconvert==5.6.1\nnbformat==5.0.4\nnd2reader==3.1.0\nnetworkx==2.4\nnltk==3.4.5\nnose==1.3.7\nnotebook==6.0.3\nnumba==0.46.0\nnumexpr==2.7.1\nnumpy==1.18.0\nnumpydoc==0.9.2\nodo==0.5.1\nolefile==0.46\nopenpyxl==3.0.3\nopt-einsum==3.1.0\npackaging==20.1\npandas==0.25.3\npandocfilters==1.4.2\nparso==0.6.0\npartd==1.1.0\npath==13.1.0\npathlib2==2.3.5\npathtools==0.1.2\npatsy==0.5.1\npdb2pqr-minimal==0.0.0\npep8==1.7.1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==7.0.0\nPIMS==0.4.1\npims-nd2==1.0\npkginfo==1.5.0.1\npluggy==0.13.1\nply==3.11\nprometheus-client==0.7.1\nprompt-toolkit==3.0.4\nprotobuf==3.11.2\npsutil==5.6.7\nptyprocess==0.6.0\npy==1.8.1\npyasn1==0.4.8\npyasn1-modules==0.2.7\npybind11==2.4.3\npycodestyle==2.5.0\npycosat==0.6.3\npycparser==2.19\npycrypto==2.6.1\npycurl==7.43.0.4\npydocstyle==4.0.1\npyface==6.1.0\npyflakes==2.1.1\nPygments==2.5.2\nPyHamcrest==1.9.0\npylint==2.4.4\npymol==2.3.2\nPympler==0.7\npyodbc==4.0.28\npyopencl==2019.1.2\nPyOpenGL==3.1.0\npyOpenSSL==19.1.0\npyparsing==2.4.6\nPyQt5==5.14.1\nPyQt5-sip==12.7.1\npyrsistent==0.15.7\nPySide2==5.14.1\nPySocks==1.7.1\npytest==5.3.4\npytest-arraydiff==0.3\npytest-astropy==0.7.0\npytest-astropy-header==0.1.1\npytest-doctestplus==0.5.0\npytest-openfiles==0.4.0\npytest-remotedata==0.3.2\npython-bioformats==1.5.2\npython-dateutil==2.8.1\npython-jsonrpc-server==0.3.2\npython-language-server==0.31.2\npytools==2020.1\npytz==2019.3\nPyWavelets==1.1.1\npyxdg==0.26\nPyYAML==5.2\npyzmq==18.0.0\nQDarkStyle==2.7\nQtAwesome==0.6.1\nqtconsole==4.6.0\nQtPy==1.9.0\nread-roi==1.5.2\nreikna==0.7.4\nrequests==2.22.0\nrope==0.16.0\nRtree==0.8.3\nruamel-yaml==0.15.87\ns3transfer==0.2.0\nscikit-image==0.15.0\nscikit-learn==0.22.1\nscikit-tensor-py3==0.4.1\nscipy==1.3.2\nseaborn==0.9.0\nSecretStorage==3.1.2\nSend2Trash==1.5.0\nservice-identity==18.1.0\nshiboken2==5.14.1\nsimplegeneric==0.8.1\nsingledispatch==3.4.0.3\nsix==1.14.0\nslicerator==1.0.0\nsnowballstemmer==2.0.0\nsortedcollections==1.1.2\nsortedcontainers==2.1.0\nsoupsieve==1.9.5\nSphinx==2.3.1\nsphinxcontrib-applehelp==1.0.1\nsphinxcontrib-devhelp==1.0.1\nsphinxcontrib-htmlhelp==1.0.2\nsphinxcontrib-jsmath==1.0.1\nsphinxcontrib-qthelp==1.0.2\nsphinxcontrib-serializinghtml==1.1.3\nsphinxcontrib-websupport==1.1.2\nspyder==3.3.6\nspyder-kernels==0.5.2\nSQLAlchemy==1.3.13\nstatsmodels==0.10.1\nsympy==1.5.1\ntables==3.4.4\ntblib==1.6.0\ntensorboard==2.0.0\ntensorflow==2.0.0\ntensorflow-estimator==2.0.0\ntermcolor==1.1.0\nterminado==0.8.3\ntestpath==0.4.4\ntifffile==0.15.1\ntoolz==0.10.0\ntornado==6.0.3\ntqdm==4.42.0\ntrackpy==0.4.1\ntraitlets==4.3.3\ntraits==5.1.1\ntraitsui==6.1.0\nTwisted==19.10.0\nujson==1.35\nunicodecsv==0.14.1\nurllib3==1.25.8\nvispy==0.6.4\nvisvis==1.11.2\nvtk==8.1.2\nwatchdog==0.9.0\nwcwidth==0.1.7\nwebencodings==0.5.1\nWerkzeug==0.16.1\nwidgetsnbextension==3.5.1\nwrapt==1.11.2\nwurlitzer==2.0.0\nxlrd==1.2.0\nXlsxWriter==1.2.7\nxlwt==1.3.0\nxmltodict==0.12.0\nyapf==0.28.0\nzict==1.0.0\nzipp==0.6.0\nzope.interface==4.7.1\n</code></pre>", "<p>thanks, and if you could test one more thing\u2026 just to make sure this has something to do with your python environment, can you make a clean one and test it?</p>\n<pre><code class=\"lang-python\">conda create -n testenv -y python=3.7.3\nconda activate testenv\npip install napari\nipython --gui=qt\n\n&gt;&gt;&gt; import napari\n&gt;&gt;&gt; from skimage import data\n&gt;&gt;&gt;viewer = napari.view_image(data.astronaut(), rgb=True)\n</code></pre>\n<p>my guess is that there\u2019s something conflicting with all of those packages in that environment, but we\u2019ll need to narrow it down somehow.  might have something to do with multiple qt installations (you have both pyqt5 from conda and pyside2 from pip)\u2026 I\u2019ll see what I can figure out.</p>", "<p>It works with the new virtual environment! Thanks <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Great, thanks for checking.  It\u2019s always tough to debug in the base environment\u2026 but we\u2019ll see what we can figure out</p>", "<p>well, unfortunately (for the purposes of debugging this), I just installed anaconda into a new location on my ubuntu desktop, and then, in the base environment, ran <code>pip install napari</code>.  It worked fine.  I then tried to install as many things from your environment that I could that seemed like they might potentially conflict with each other (for instance, you have <code>pyqt==5.9.2</code>, <code>pyqt5==5.14.1</code> <em>and</em> <code>pyside2==5.14.1</code> from both pip and conda)  but even after replicating that, it <em>still</em> works for me.</p>\n<p>There are some things I can\u2019t replicate because of conda dependency conflicts, but here\u2019s what I ended up with:</p>\n<pre><code class=\"lang-auto\">napari: 0.2.12\nPlatform: Linux-5.3.0-42-generic-x86_64-with-debian-buster-sid\nPython: 3.7.6 (default, Jan  8 2020, 19:59:22)  [GCC 7.3.0]\nQt: 5.14.1\nPyQt5: 5.14.1\nVisPy: 0.6.4\nNumPy: 1.18.1\nSciPy: 1.4.1\nscikit-image: 0.16.2\nDask: 2.11.0\n\nGL version:  4.6.0 NVIDIA 440.64\nMAX_TEXTURE_SIZE: 32768\n</code></pre>\n<p>couple notable differences in there: numpy, scipy, skimage, dask\u2026 but who knows.</p>\n<p>The truth of the matter here is that it\u2019s just <em>really</em> hard to debug these sorts of environmental problems when the base environment is used as a \u201ckitchen sink\u201d.  I know it\u2019s a bit of a cop-out, but it is strongly recommend to build distinct environments for various tasks.  you <em>can</em> have a second \u201csink\u201d environment where you try to install everything you want to use\u2026 and if eventually you install something that breaks something else, you can either remove it, or nuke the environment and start again.  (That\u2019s much harder to do with your base environment).</p>\n<p>Since I can\u2019t reproduce this on my linux computer, and it works for you in a clean environment, I\u2019m afraid this may just have to remain unsolved for now.  If you can eventually figure out a reproducible environment that gives you the segfault, I\u2019m happy to look closer at what might be causing it.</p>", "<p>Hi <a class=\"mention\" href=\"/u/talley\">@talley</a></p>\n<p>After the gorgeous presentation of napari by <a class=\"mention\" href=\"/u/sofroniewn\">@sofroniewn</a> yesterday, I decided to make napari work for my tasks. It works nicely if I start it on my home/office computer. However my main interest would be to use it on remote servers.</p>\n<p>I am experiencing similar issue as <span class=\"mention\">@taebongwith</span> trying to run napari at the remote server and connecting to it via ssh. I decided not to create a new forum thread because I have similar errors so I would appreciate your ideas.</p>\n<p><strong>So, my inputs:</strong><br>\nI have anaconda installed at the remote server. The server normally supports graphics through ssh -X.</p>\n<p>I created a new environment how <a class=\"mention\" href=\"/u/talley\">@talley</a> recommended, i.e with:<br>\n<code>conda create -n testenv -y python=3.7.3</code></p>\n<p>so it looks like:</p>\n<pre><code class=\"lang-auto\"># packages in environment at /mx-beta/anaconda3/envs/naparienv:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main\nalabaster                 0.7.12                    &lt;pip&gt;\nattrs                     19.3.0                    &lt;pip&gt;\nBabel                     2.8.0                     &lt;pip&gt;\nbackcall                  0.1.0                     &lt;pip&gt;\nbleach                    3.1.5                     &lt;pip&gt;\nca-certificates           2020.1.1                      0\ncachey                    0.2.1                     &lt;pip&gt;\ncertifi                   2020.4.5.1               py37_0\nchardet                   3.0.4                     &lt;pip&gt;\ncycler                    0.10.0                    &lt;pip&gt;\ndask                      2.17.2                    &lt;pip&gt;\ndecorator                 4.4.2                     &lt;pip&gt;\ndefusedxml                0.6.0                     &lt;pip&gt;\ndocutils                  0.16                      &lt;pip&gt;\nentrypoints               0.3                       &lt;pip&gt;\nfreetype-py               2.1.0.post1               &lt;pip&gt;\nHeapDict                  1.0.1                     &lt;pip&gt;\nidna                      2.9                       &lt;pip&gt;\nimageio                   2.8.0                     &lt;pip&gt;\nimagesize                 1.2.0                     &lt;pip&gt;\nimportlib-metadata        1.6.0                     &lt;pip&gt;\nipykernel                 5.3.0                     &lt;pip&gt;\nipython                   7.15.0                    &lt;pip&gt;\nipython-genutils          0.2.0                     &lt;pip&gt;\njedi                      0.17.0                    &lt;pip&gt;\nJinja2                    2.11.2                    &lt;pip&gt;\njsonschema                3.2.0                     &lt;pip&gt;\njupyter-client            6.1.3                     &lt;pip&gt;\njupyter-core              4.6.3                     &lt;pip&gt;\nkiwisolver                1.2.0                     &lt;pip&gt;\nlibedit                   3.1.20181209         hc058e9b_0\nlibffi                    3.2.1                hd88cf55_4\nlibgcc-ng                 9.1.0                hdf63c60_0\nlibstdcxx-ng              9.1.0                hdf63c60_0\nMarkupSafe                1.1.1                     &lt;pip&gt;\nmatplotlib                3.2.1                     &lt;pip&gt;\nmistune                   0.8.4                     &lt;pip&gt;\nnapari                    0.3.4                     &lt;pip&gt;\nnapari-plugin-engine      0.1.5                     &lt;pip&gt;\nnapari-svg                0.1.3                     &lt;pip&gt;\nnbconvert                 5.6.1                     &lt;pip&gt;\nnbformat                  5.0.6                     &lt;pip&gt;\nncurses                   6.2                  he6710b0_1\nnetworkx                  2.4                       &lt;pip&gt;\nnotebook                  6.0.3                     &lt;pip&gt;\nnumpy                     1.18.5                    &lt;pip&gt;\nnumpydoc                  1.0.0                     &lt;pip&gt;\nopenssl                   1.1.1g               h7b6447c_0\npackaging                 20.4                      &lt;pip&gt;\npandocfilters             1.4.2                     &lt;pip&gt;\nparso                     0.7.0                     &lt;pip&gt;\npexpect                   4.8.0                     &lt;pip&gt;\npickleshare               0.7.5                     &lt;pip&gt;\nPillow                    7.1.2                     &lt;pip&gt;\npip                       20.0.2                   py37_3\nprometheus-client         0.8.0                     &lt;pip&gt;\nprompt-toolkit            3.0.5                     &lt;pip&gt;\npsutil                    5.7.0                     &lt;pip&gt;\nptyprocess                0.6.0                     &lt;pip&gt;\nPygments                  2.6.1                     &lt;pip&gt;\nPyOpenGL                  3.1.5                     &lt;pip&gt;\npyparsing                 2.4.7                     &lt;pip&gt;\nPyQt5                     5.14.2                    &lt;pip&gt;\nPyQt5-sip                 12.8.0                    &lt;pip&gt;\npyrsistent                0.16.0                    &lt;pip&gt;\npython                    3.7.3                h0371630_0\npython-dateutil           2.8.1                     &lt;pip&gt;\npytz                      2020.1                    &lt;pip&gt;\nPyWavelets                1.1.1                     &lt;pip&gt;\nPyYAML                    5.3.1                     &lt;pip&gt;\npyzmq                     19.0.1                    &lt;pip&gt;\nqtconsole                 4.7.4                     &lt;pip&gt;\nQtPy                      1.9.0                     &lt;pip&gt;\nreadline                  7.0                  h7b6447c_5\nrequests                  2.23.0                    &lt;pip&gt;\nscikit-image              0.17.2                    &lt;pip&gt;\nscipy                     1.4.1                     &lt;pip&gt;\nSend2Trash                1.5.0                     &lt;pip&gt;\nsetuptools                47.1.1                   py37_0\nsix                       1.15.0                    &lt;pip&gt;\nsnowballstemmer           2.0.0                     &lt;pip&gt;\nSphinx                    3.0.4                     &lt;pip&gt;\nsphinxcontrib-applehelp   1.0.2                     &lt;pip&gt;\nsphinxcontrib-devhelp     1.0.2                     &lt;pip&gt;\nsphinxcontrib-htmlhelp    1.0.3                     &lt;pip&gt;\nsphinxcontrib-jsmath      1.0.1                     &lt;pip&gt;\nsphinxcontrib-qthelp      1.0.3                     &lt;pip&gt;\nsphinxcontrib-serializinghtml 1.1.4                     &lt;pip&gt;\nsqlite                    3.31.1               h62c20be_1\nterminado                 0.8.3                     &lt;pip&gt;\ntestpath                  0.4.4                     &lt;pip&gt;\ntifffile                  2020.6.3                  &lt;pip&gt;\ntk                        8.6.8                hbc83047_0\ntoolz                     0.10.0                    &lt;pip&gt;\ntornado                   6.0.4                     &lt;pip&gt;\ntraitlets                 4.3.3                     &lt;pip&gt;\nurllib3                   1.25.9                    &lt;pip&gt;\nvispy                     0.6.4                     &lt;pip&gt;\nwcwidth                   0.2.3                     &lt;pip&gt;\nwebencodings              0.5.1                     &lt;pip&gt;\nwheel                     0.34.2                   py37_0\nwrapt                     1.12.1                    &lt;pip&gt;\nxz                        5.2.5                h7b6447c_0\nzipp                      3.1.0                     &lt;pip&gt;\nzlib                      1.2.11               h7b6447c_3\n</code></pre>\n<p>Then,</p>\n<pre><code class=\"lang-auto\">ipython --gui=qt\n\n&gt;&gt;&gt; import napari\n&gt;&gt;&gt; from skimage import data\n</code></pre>\n<p>works fine.</p>\n<p><code>&gt;&gt;&gt;viewer = napari.view_image(data.astronaut(), rgb=True)</code></p>\n<p>raises an error:</p>\n<pre><code class=\"lang-auto\">RuntimeError: napari requires a Qt event loop to run. To create one, try one of the following:\n  - use the `napari.gui_qt()` context manager. See https://github.com/napari/napari/tree/master/examples for usage examples.\n  - In IPython or a local Jupyter instance, use the `%gui qt` magic command.\n  - Launch IPython with the option `--gui=qt`.\n  - (recommended) in your IPython configuration file, add or uncomment the line `c.TerminalIPythonApp.gui = 'qt'`. Then, restart IPython.\n</code></pre>\n<p><strong>Then</strong>, If I do:<br>\n<code>napari --info</code></p>\n<p>I get:</p>\n<pre><code class=\"lang-auto\">WARNING: Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found.\nWARNING:vispy:Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found.\nWARNING: This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.\n\nAvailable platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.\n\nWARNING:vispy:This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.\n\nAvailable platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.\n\nAborted (core dumped)\n</code></pre>\n<p><strong>I am wondering</strong> if the problem is due to napari or due to the fact that I try to ssh -X it and something breaks on the way?</p>\n<p><strong>Also, my final goal is to use napari through establishing remote notebook server</strong> as described here:<br>\n<a href=\"https://amber-md.github.io/pytraj/latest/tutorials/remote_jupyter_notebook\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://amber-md.github.io/pytraj/latest/tutorials/remote_jupyter_notebook</a><br>\nThis works fine for standard notebook-inline graphics.</p>\n<p>However, trying to set napari, the kernel just dies after<br>\n<code>%gui qt</code><br>\nDo you have any ideas?</p>", "<blockquote>\n<p>is the problem is due to napari or due to the fact that I try to ssh -X it and something breaks on the way?</p>\n</blockquote>\n<p>it\u2019s due to the ssh.  Because vispy (the rendering engine behind napari) uses OpenGL hardware acceleration, simple X forwarding isn\u2019t going to work.  (see <a href=\"https://github.com/napari/napari/issues/799\">#799</a>)</p>\n<p>I\u2019m definitely not an expert on this, so maybe there are others here who have experience forwarding frame buffers from openGL programs running remotely.  At least one solution, if you can set it up, is to use <a href=\"https://www.virtualgl.org/\">VirtualGL</a> and run napari remotely with <code>vglrun napari</code>.</p>\n<p>hah\u2026 in fact, I can see that <a href=\"https://github.com/napari/napari/issues/896#issuecomment-577249815\">I commented on #869</a> that I apparently got this to work, and promised a writeup\u2026 which I never got to!  I\u2019ll try to work my way through it again and write up a tutorial.  But if you get it running in the meantime, please let us know any tips!</p>", "<aside class=\"quote no-group\" data-username=\"mpolikarpov\" data-post=\"15\" data-topic=\"32640\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/7ea924/40.png\" class=\"avatar\"> mpolikarpov:</div>\n<blockquote>\n<p>the kernel just dies after<br>\n<code>%gui qt</code></p>\n</blockquote>\n</aside>\n<p>actually\u2026 rereading your post: if you can\u2019t even get a Qt app to start, you may have some missing drivers on your remote server. (libxcb?).  See <a href=\"https://askubuntu.com/questions/308128/failed-to-load-platform-plugin-xcb-while-launching-qt5-app-on-linux-without\">this thread</a> for some debugging tips there (mostly: use the <code>export QT_DEBUG_PLUGINS=1</code> environment varialbe before starting a qt app).</p>\n<p>You <em>will</em> still need the virtualGL stuff above for it to ultimately work, but you may need to solve this problem first before napari will even launch.</p>", "<p>I have started using napari recently and found it really useful. It was working well installed in a conda environment and running in PyCharm jupyter notebook. All of a sudden with no apparent changes to environment I am experiencing an issue where napari GUI runs fine from terminal but fails to initialize when run inside Jupyter or ipython. The GUI window opens blank and unresponsive.</p>\n<p>This problem persists in a clean environment:</p>\n<pre><code class=\"lang-auto\">conda create -n testenv -y python=3.7.3\nconda activate testenv\npip install 'napari[all]'\npip install scikit-image\nipython --gui=qt\n\n&gt;&gt;&gt; import napari\n&gt;&gt;&gt; from skimage import data\n&gt;&gt;&gt;viewer = napari.view_image(data.astronaut(), rgb=True)\n</code></pre>\n<p>There are no error messages displayed.</p>\n<p>Here is the output from conda list:</p>\n<pre><code class=\"lang-auto\"># Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \nalabaster                 0.7.12                   pypi_0    pypi\nappdirs                   1.4.4                    pypi_0    pypi\nattrs                     20.3.0                   pypi_0    pypi\nbabel                     2.9.0                    pypi_0    pypi\nbackcall                  0.2.0                    pypi_0    pypi\nca-certificates           2021.4.13            h06a4308_1  \ncachey                    0.2.1                    pypi_0    pypi\ncertifi                   2020.12.5        py37h06a4308_0  \ncloudpickle               1.6.0                    pypi_0    pypi\ncycler                    0.10.0                   pypi_0    pypi\ndask                      2021.4.0                 pypi_0    pypi\ndecorator                 4.4.2                    pypi_0    pypi\ndocstring-parser          0.7.3                    pypi_0    pypi\ndocutils                  0.16                     pypi_0    pypi\nfreetype-py               2.2.0                    pypi_0    pypi\nfsspec                    0.9.0                    pypi_0    pypi\nheapdict                  1.0.1                    pypi_0    pypi\nimageio                   2.9.0                    pypi_0    pypi\nimagesize                 1.2.0                    pypi_0    pypi\nipykernel                 5.5.3                    pypi_0    pypi\nipython                   7.22.0                   pypi_0    pypi\nipython-genutils          0.2.0                    pypi_0    pypi\njedi                      0.18.0                   pypi_0    pypi\njinja2                    2.11.3                   pypi_0    pypi\njsonschema                3.2.0                    pypi_0    pypi\njupyter-client            6.1.12                   pypi_0    pypi\njupyter-core              4.7.1                    pypi_0    pypi\nkiwisolver                1.3.1                    pypi_0    pypi\nlibedit                   3.1.20210216         h27cfd23_1  \nlibffi                    3.2.1             hf484d3e_1007  \nlibgcc-ng                 9.1.0                hdf63c60_0  \nlibstdcxx-ng              9.1.0                hdf63c60_0  \nlocket                    0.2.1                    pypi_0    pypi\nmagicgui                  0.2.9                    pypi_0    pypi\nmarkupsafe                1.1.1                    pypi_0    pypi\nmatplotlib                3.4.1                    pypi_0    pypi\nnapari                    0.4.7                    pypi_0    pypi\nnapari-console            0.0.3                    pypi_0    pypi\nnapari-plugin-engine      0.1.9                    pypi_0    pypi\nnapari-svg                0.1.4                    pypi_0    pypi\nncurses                   6.2                  he6710b0_1  \nnetworkx                  2.5.1                    pypi_0    pypi\nnumpy                     1.20.2                   pypi_0    pypi\nnumpydoc                  1.1.0                    pypi_0    pypi\nopenssl                   1.1.1k               h27cfd23_0  \npackaging                 20.9                     pypi_0    pypi\nparso                     0.8.2                    pypi_0    pypi\npartd                     1.2.0                    pypi_0    pypi\npexpect                   4.8.0                    pypi_0    pypi\npickleshare               0.7.5                    pypi_0    pypi\npillow                    8.2.0                    pypi_0    pypi\npip                       21.0.1           py37h06a4308_0  \nprompt-toolkit            3.0.18                   pypi_0    pypi\npsutil                    5.8.0                    pypi_0    pypi\nptyprocess                0.7.0                    pypi_0    pypi\npydantic                  1.8.1                    pypi_0    pypi\npygments                  2.8.1                    pypi_0    pypi\npyopengl                  3.1.5                    pypi_0    pypi\npyparsing                 2.4.7                    pypi_0    pypi\npyqt5                     5.15.4                   pypi_0    pypi\npyqt5-qt5                 5.15.2                   pypi_0    pypi\npyqt5-sip                 12.8.1                   pypi_0    pypi\npyrsistent                0.17.3                   pypi_0    pypi\npython                    3.7.3                h0371630_0  \npython-dateutil           2.8.1                    pypi_0    pypi\npytz                      2021.1                   pypi_0    pypi\npywavelets                1.1.1                    pypi_0    pypi\npyyaml                    5.4.1                    pypi_0    pypi\npyzmq                     22.0.3                   pypi_0    pypi\nqtconsole                 5.0.3                    pypi_0    pypi\nqtpy                      1.9.0                    pypi_0    pypi\nreadline                  7.0                  h7b6447c_5  \nscikit-image              0.18.1                   pypi_0    pypi\nscipy                     1.6.2                    pypi_0    pypi\nsetuptools                52.0.0           py37h06a4308_0  \nsnowballstemmer           2.1.0                    pypi_0    pypi\nsphinx                    3.5.4                    pypi_0    pypi\nsphinxcontrib-applehelp   1.0.2                    pypi_0    pypi\nsphinxcontrib-devhelp     1.0.2                    pypi_0    pypi\nsphinxcontrib-htmlhelp    1.0.3                    pypi_0    pypi\nsphinxcontrib-jsmath      1.0.1                    pypi_0    pypi\nsphinxcontrib-qthelp      1.0.3                    pypi_0    pypi\nsphinxcontrib-serializinghtml 1.1.4                    pypi_0    pypi\nsqlite                    3.33.0               h62c20be_0  \ntifffile                  2021.4.8                 pypi_0    pypi\ntk                        8.6.10               hbc83047_0  \ntoolz                     0.11.1                   pypi_0    pypi\ntornado                   6.1                      pypi_0    pypi\ntraitlets                 5.0.5                    pypi_0    pypi\nvispy                     0.6.6                    pypi_0    pypi\nwcwidth                   0.2.5                    pypi_0    pypi\nwheel                     0.36.2             pyhd3eb1b0_0  \nxz                        5.2.5                h7b6447c_0  \nzlib                      1.2.11               h7b6447c_3  \n</code></pre>\n<p>and from napari --info:</p>\n<pre><code class=\"lang-auto\">napari: 0.4.7\nPlatform: Linux-5.11.0-7612-generic-x86_64-with-debian-bullseye-sid\nSystem: Pop!_OS 20.04 LTS\nPython: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]\nQt: 5.15.2\nPyQt5: 5.15.4\nNumPy: 1.20.2\nSciPy: 1.6.2\nDask: 2021.04.0\nVisPy: 0.6.6\n\nOpenGL:\n  - GL version:  4.6 (Compatibility Profile) Mesa 21.0.0\n  - MAX_TEXTURE_SIZE: 16384\n\nScreens:\n  - screen 1: resolution 2560x1440, scale 1.0\n  - screen 2: resolution 1920x1080, scale 1.0\n\nPlugins:\n  - console: 0.0.3\n  - svg: 0.1.4\n</code></pre>\n<p>Any ideas what could be the problem?</p>", "<p>hi <a class=\"mention\" href=\"/u/jnkoberstein\">@jnkoberstein</a> and welcome.</p>\n<p>I tried a few things to reproduce this but wasn\u2019t able to just yet (but haven\u2019t had a chance to try on ubuntu yet).  Nothing about your environment seems immediately wrong\u2026 so i wonder whether there is some more global system-environment thing that might be taking precedence?  sorry I don\u2019t immediately have a suggestion</p>", "<p>Thanks Talley! Not sure how helpful this is, but I noticed I can still successfully open a simple qt GUI using the following inside a jupyter notebook:</p>\n<pre><code class=\"lang-auto\">%gui qt\n\nfrom PyQt5.QtWidgets import QApplication, QLabel\n\napp = QApplication([])\nlabel = QLabel('Hello World!')\nlabel.show()\napp.exec()\n</code></pre>\n<p>So it seems PyQt5 isn\u2019t completely dysfunctional but rather there might be an issue with the way napari is generating GUI.</p>"], "77740": ["<p>Hi everyone,</p>\n<p>I have an Olympus confocal as my daily driver and I get .OIR files from the Olympus software.<br>\nWith the napari-bioformats plug-in, I can drag and drop the file in napari.<br>\nHow can I open the file in jupyter notebook and read in the file as NumPy arrays?</p>\n<p>Thanks!</p>\n<p>Wanpeng</p>", "<p>Hi <a class=\"mention\" href=\"/u/wanpeng-wang\">@Wanpeng-Wang</a></p>\n<p><code>skimage.io.imread()</code> can be used to import some proprietary files with the <code>tifffile</code> plugin. I\u2019ve never used it for .OIR files, but you could try something like:</p>\n<pre><code class=\"lang-auto\">image = io.imread(FILE_NAME, plugin = 'tifffile')\n</code></pre>", "<p>napari plugins simply expose a reader function to napari. You just need to figure out what the function is and then access that directly in a notebook. In this case, the source code is here:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/tlambert03/napari-bioformats\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/tlambert03/napari-bioformats\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/8/a8b5fcc1468c9a477a35d7a3b1d6a5c58ce336f5_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/8/a8b5fcc1468c9a477a35d7a3b1d6a5c58ce336f5_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/8/a8b5fcc1468c9a477a35d7a3b1d6a5c58ce336f5_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/8/a8b5fcc1468c9a477a35d7a3b1d6a5c58ce336f5.png 2x\" data-dominant-color=\"EFF1F1\"></div>\n\n<h3><a href=\"https://github.com/tlambert03/napari-bioformats\" target=\"_blank\" rel=\"noopener\">GitHub - tlambert03/napari-bioformats</a></h3>\n\n  <p>Contribute to tlambert03/napari-bioformats development by creating an account on GitHub.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>You can see that it exposes a <code>get_reader</code> function here:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L37-L54\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L37-L54\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L37-L54\" target=\"_blank\" rel=\"noopener\">tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L37-L54</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"37\" style=\"counter-reset: li-counter 36 ;\">\n          <li>@napari_hook_implementation(trylast=True)</li>\n          <li>def napari_get_reader(path):</li>\n          <li>    \"\"\"A basic implementation of the napari_get_reader hook specification.</li>\n          <li>\n          </li>\n<li>    Parameters</li>\n          <li>    ----------</li>\n          <li>    path : str or list of str</li>\n          <li>        Path to file, or list of paths.</li>\n          <li>\n          </li>\n<li>    Returns</li>\n          <li>    -------</li>\n          <li>    function or None</li>\n          <li>        If the path is a recognized format, return a function that accepts the</li>\n          <li>        same path or list of paths, and returns a list of layer data tuples.</li>\n          <li>    \"\"\"</li>\n          <li>    if isinstance(path, (str, Path)) and str(path).endswith(SUPPORTED_FORMATS):</li>\n          <li>        return read_bioformats</li>\n          <li>    return None</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>and finally the actual reader function here:</p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L74-L167\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L74-L167\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L74-L167\" target=\"_blank\" rel=\"noopener\">tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L74-L167</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"74\" style=\"counter-reset: li-counter 73 ;\">\n          <li>def read_bioformats(path, split_channels=True):</li>\n          <li>    \"\"\"Take a path or list of paths and return a list of LayerData tuples.</li>\n          <li>\n          </li>\n<li>    Parameters</li>\n          <li>    ----------</li>\n          <li>    path : str or list of str</li>\n          <li>        Path to file, or list of paths.</li>\n          <li>\n          </li>\n<li>    Returns</li>\n          <li>    -------</li>\n          <li>    layer_data : list of tuples</li>\n          <li>        A list of LayerData tuples where each tuple in the list contains</li>\n          <li>        (data, metadata, layer_type), where data is a numpy array, metadata is</li>\n          <li>        a dict of keyword arguments for the corresponding viewer.add_* method</li>\n          <li>        in napari, and layer_type is a lower-case string naming the type of layer.</li>\n          <li>        Both \"meta\", and \"layer_type\" are optional. napari will default to</li>\n          <li>        layer_type==\"image\" if not provided</li>\n          <li>    \"\"\"</li>\n          <li>    import jpype</li>\n          <li>    from pims.bioformats import BioformatsReader</li>\n      </ol>\n    </code></pre>\n\n\n  This file has been truncated. <a href=\"https://github.com/tlambert03/napari-bioformats/blob/039c61f289ca05b6b57225ab0638cde0c4419ec3/napari_bioformats/_core.py#L74-L167\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>So you should be able to do <code>from napari_bioformats import read_bioformats</code>, and then <code>array, metadata = read_bioformats('path/to/image.OIR')</code>.</p>\n<p>Do also note that napari-bioformats is archived (no longer being updated) and it recommends using aicsimageio. So you can repeat this process looking at:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/AllenCellModeling/aicsimageio\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/AllenCellModeling/aicsimageio\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57cccc7b10b37751a175ad4c9215e30f9533df90_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57cccc7b10b37751a175ad4c9215e30f9533df90_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57cccc7b10b37751a175ad4c9215e30f9533df90_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57cccc7b10b37751a175ad4c9215e30f9533df90.png 2x\" data-dominant-color=\"EDEFF2\"></div>\n\n<h3><a href=\"https://github.com/AllenCellModeling/aicsimageio\" target=\"_blank\" rel=\"noopener\">GitHub - AllenCellModeling/aicsimageio: Image Reading, Metadata Conversion,...</a></h3>\n\n  <p>Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Python - GitHub - AllenCellModeling/aicsimageio: Image Reading, Metadata Conversion, and Image Writing for Microscopy ...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>and</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/AllenCellModeling/napari-aicsimageio\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/AllenCellModeling/napari-aicsimageio\" target=\"_blank\" rel=\"noopener\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fc0d19d2d944f741b67f460df70607f15d32b27_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fc0d19d2d944f741b67f460df70607f15d32b27_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8fc0d19d2d944f741b67f460df70607f15d32b27_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8fc0d19d2d944f741b67f460df70607f15d32b27.png 2x\" data-dominant-color=\"EDEFF1\"></div>\n\n<h3><a href=\"https://github.com/AllenCellModeling/napari-aicsimageio\" target=\"_blank\" rel=\"noopener\">GitHub - AllenCellModeling/napari-aicsimageio: Multiple file format reading...</a></h3>\n\n  <p>Multiple file format reading directly into napari using pure Python - GitHub - AllenCellModeling/napari-aicsimageio: Multiple file format reading directly into napari using pure Python</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hi Juan,<br>\nThank you for your response!<br>\nI can drag-and-drop the .oir files into napari and the AICSImageIO plug-in could open it. But When I tried to import data from the notebook, I get this error message. I confirmed that bioformats_jar is installed.</p>\n<pre><code class=\"lang-auto\">from aicsimageio import AICSImage\nimg = AICSImage(\"/Users/Desktop/composite_0001.oir\")\n</code></pre>\n<pre><code class=\"lang-auto\">---------------------------------------------------------------------------\nUnsupportedFileFormatError                Traceback (most recent call last)\nCell In[2], line 2\n      1 # pip install bioformats_jar\n----&gt; 2 img = AICSImage(\"/Users/Desktop/composite_0001.oir\")\n\nFile /opt/anaconda3/envs/napari-env/lib/python3.9/site-packages/aicsimageio/aics_image.py:264, in AICSImage.__init__(self, image, reader, reconstruct_mosaic, fs_kwargs, **kwargs)\n    254 def __init__(\n    255     self,\n    256     image: types.ImageLike,\n   (...)\n    260     **kwargs: Any,\n    261 ):\n    262     if reader is None:\n    263         # Determine reader class and create dask delayed array\n--&gt; 264         ReaderClass = self.determine_reader(image, fs_kwargs=fs_kwargs, **kwargs)\n    265     else:\n    266         # Init reader\n    267         ReaderClass = reader\n\nFile /opt/anaconda3/envs/napari-env/lib/python3.9/site-packages/aicsimageio/aics_image.py:218, in AICSImage.determine_reader(image, fs_kwargs, **kwargs)\n    216 if readers[0] in READER_TO_INSTALL:\n    217     installer = READER_TO_INSTALL[readers[0]]\n--&gt; 218     raise exceptions.UnsupportedFileFormatError(\n    219         \"AICSImage\",\n    220         path,\n    221         msg_extra=(\n    222             f\"File extension suggests format: '{format_ext}'. \"\n    223             f\"Install extra format dependency with: \"\n    224             f\"`pip install {installer}`. \"\n    225             f\"See all known format extensions and their \"\n    226             f\"extra install name with \"\n    227             f\"`aicsimageio.formats.FORMAT_IMPLEMENTATIONS`. \"\n    228             f\"If the extra dependency is already installed this \"\n    229             f\"error may have raised because the file is \"\n    230             f\"corrupt or similar issue. For potentially more \"\n    231             f\"information and to help debug, try loading the file \"\n    232             f\"directly with the desired file format reader \"\n    233             f\"instead of with the AICSImage object.\"\n    234         ),\n    235     )\n    236 else:\n    237     raise exceptions.UnsupportedFileFormatError(\n    238         \"AICSImage\",\n    239         path,\n    240     )\n\nUnsupportedFileFormatError: AICSImage does not support the image: '/Users/Desktop/composite_0001.oir'. File extension suggests format: 'oir'. Install extra format dependency with: `pip install bioformats_jar`. See all known format extensions and their extra install name with `aicsimageio.formats.FORMAT_IMPLEMENTATIONS`. If the extra dependency is already installed this error may have raised because the file is corrupt or similar issue. For potentially more information and to help debug, try loading the file directly with the desired file format reader instead of with the AICSImage object.\n</code></pre>\n<p>So I followed the instruction and found the bioformats reader. And that did the work.</p>\n<pre><code class=\"lang-auto\">img = aicsimageio.readers.bioformats_reader.BioformatsReader(\"/Users/Desktop/composite_0001.oir\")\n</code></pre>"], "77764": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/c/dca8d6f481839f42f2334551b0cbe1397c11fe85.jpeg\" data-download-href=\"/uploads/short-url/vu2MptfTyMN8OPPOPBpXZUznblb.jpeg?dl=1\" title=\"fiji_vs_napari\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dca8d6f481839f42f2334551b0cbe1397c11fe85_2_690x221.jpeg\" alt=\"fiji_vs_napari\" data-base62-sha1=\"vu2MptfTyMN8OPPOPBpXZUznblb\" width=\"690\" height=\"221\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dca8d6f481839f42f2334551b0cbe1397c11fe85_2_690x221.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dca8d6f481839f42f2334551b0cbe1397c11fe85_2_1035x331.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/c/dca8d6f481839f42f2334551b0cbe1397c11fe85_2_1380x442.jpeg 2x\" data-dominant-color=\"252526\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">fiji_vs_napari</span><span class=\"informations\">1920\u00d7617 61.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hi everyone, I\u2019ve received a lot of help from looking at this forum, but finally ran into a new issue <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nI\u2019m trying to migrate off of ImageJ/Fiji, since I work with stacks of size &gt;= 1024 x 1024 x 1024 x 3. I\u2019ve found that ImageJ volume viewer is very slow for this kind of data, and other issues like saving low resolution screenshots + hard to script animations is making me try to move off of ImageJ.</p>\n<p>Enter Napari, which seems to tick all the boxes. My problem is that when I load volumes with napari, I think they look a lot worse than the ones loaded with ImageJ. I am hoping there is some way to replicate the rendering/colormap settings from ImageJ, in napari.</p>\n<p>I\u2019ve tried loading Napari-imageJ with the plugin, but I don\u2019t think the volume-viewer is accessible from there. I\u2019ve also tried napariJ plugin, but that didnt seem to communicate with the volume viewer either. Otherwise I\u2019ve tried all the rendering settings in the main Nd viewer window.</p>\n<p>Is there some way forward on this?</p>\n<p>EDIT: I did manage to load an image through the napariJ plugin, which gave me access to additional colormaps, but nothing like ImageJ, also this method seems slower to manipulate than pure imageJ</p>", "<p>Yes, accessing imagej through napari is only going to be slower than just using imagej: your pipeline speed is capped by the slowest element in the pipeline.</p>\n<p>If you\u2019ve already tried the rendering modes, unfortunately, that is the extent of what napari offers at the moment.</p>\n<p>napari\u2019s rendering modes are directly inherited from <a href=\"https://github.com/vispy/vispy/blob/df6c6be9c5aa6a67abf7e3072780264886e2be77/vispy/visuals/volume.py\">VisPy\u2019s Volume visual</a>. Over the years we\u2019ve made a couple of improvements there, and we intend to keep making them when we have a specific target in mind. I think <a class=\"mention\" href=\"/u/kevinyamauchi\">@kevinyamauchi</a> (and <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a>?) looked at letting us configure the light source at one point, but I\u2019m not sure whether that came to anything in the end, and anyway that doesn\u2019t seem to be the magic sauce in making the data look \u201ctextured\u201d in the volume viewer. I don\u2019t know immediately how we would get such effects (but maybe <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a> has ideas).</p>", "<p>Makes sense. Most users\u2019 motivations are probably not aesthetic, but the structure I\u2019ve observed in my data is a little richer with the ImageJ viewer.</p>\n<p>As an aside, I\u2019ve found that the volume renderer in PyVista is butter smooth. I assume its downsampling in the background since 1024x1024x1024 is a pretty large volume.</p>", "<p>Hi <a class=\"mention\" href=\"/u/moseyic\">@moseyic</a> , what exactly are you trying to replicate and doesn\u2019t look right in napari?</p>", "<p>Hi <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a>, I posted a side by side comparison above. I find that with napari I don\u2019t get nearly the same level of detail that I get with imageJ volume renders. Maybe they\u2019re just different.<br>\nI asked a similar question to the PyVista discussion area, and someone speculated that ImageJ isn\u2019t doing a volume render in the same way, but instead extracting and plotting many isosurfaces.</p>", "<p>Ah, I see. Yeah, that looks like it could be the case, now that I look at it. And what is it about the isosurface mode in napari that looks worse compared to the imajej?</p>", "<p>Here\u2019s an example of the same data using ISO mode<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80.jpeg\" data-download-href=\"/uploads/short-url/jfhMLRQgJ8ionrM6Jbq5v4DY6By.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80_2_690x459.jpeg\" alt=\"image\" data-base62-sha1=\"jfhMLRQgJ8ionrM6Jbq5v4DY6By\" width=\"690\" height=\"459\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80_2_690x459.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80_2_1035x688.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86e3ae7e30b9f1429c92cd0aa17c26033a873c80.jpeg 2x\" data-dominant-color=\"353639\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1082\u00d7721 105 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Its better, but is there any way to actually use the RGB valued data? The data itself is 1024x1024x1024x3, but this just looks grayscale.</p>\n<p>P.S. Not sure what\u2019s going on, but when I rotate the data in the napari viewer, my display seizes up / freezes for a couple seconds before loading. Sometimes it even crashes my display. I don\u2019t think its a resource thing since my temps, and mem/cpu usage dont spike at all.</p>\n<p>Since I\u2019m already here: I tried this FPS counter on the data generated by <code>p.random.default_rng().random((345, 900, 1600)</code>, and wow I get like 5-8fps.<br>\ni7-13700k, nvidia 3070ti, 64GB ram.</p>\n<aside class=\"quote quote-modified\" data-post=\"6\" data-topic=\"68103\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jni/40/5991_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/poor-3d-viewer-performance-in-napari/68103/6\">Poor 3D viewer performance in Napari</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Yeah, so then I think it\u2019s a limitation with the Volume shader in VisPy, which doesn\u2019t really have many tricks for accelerating performance in large volumes. The anisotropic behaviour makes less sense in GL though because in all cases you\u2019re traversing every pixel \u2014 just in a different order and with different \u201cray\u201d size. Perhaps sampling a short ray is nonlinearly faster than sampling a long ray? Or maybe long rays are also cast along the \u201ccorners\u201d, so that you end up sampling more pixels? \nAny\u2026\n  </blockquote>\n</aside>\n", "<p><a class=\"mention\" href=\"/u/moseyic\">@moseyic</a> are you able to share your dataset? Even a small crop would be useful. It would be much easier for us to experiment if we had actual data to play around with.</p>\n<p>I\u2019m a little surprised that isosurface even works with RGB data but that might be something that we can improve with relatively little effort. We can also improve the performance \u2014 there\u2019s already a PR in the works to dynamically change the step size while interacting with a volume so that it\u2019s smooth:</p>\n<aside class=\"onebox githubpullrequest\" data-onebox-src=\"https://github.com/napari/napari/pull/4764\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/napari/napari/pull/4764\" target=\"_blank\" rel=\"noopener\">github.com/napari/napari</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n\n\n\n    <div class=\"github-icon-container\" title=\"Pull Request\">\n      <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 12 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z\"></path></svg>\n    </div>\n\n  <div class=\"github-info-container\">\n\n\n\n      <h4>\n        <a href=\"https://github.com/napari/napari/pull/4764\" target=\"_blank\" rel=\"noopener\">[WIP] adaptively set volume rendering ray step size</a>\n      </h4>\n\n    <div class=\"branches\">\n      <code>napari:main</code> \u2190 <code>kevinyamauchi:monitor-fps</code>\n    </div>\n\n      <div class=\"github-info\">\n        <div class=\"date\">\n          opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-06-29\" data-time=\"14:54:17\" data-timezone=\"UTC\">02:54PM - 29 Jun 22 UTC</span>\n        </div>\n\n        <div class=\"user\">\n          <a href=\"https://github.com/kevinyamauchi\" target=\"_blank\" rel=\"noopener\">\n            <img alt=\"kevinyamauchi\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f99fd7b72021bce8ec2fe8bc1bff252b8ba4485.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n            kevinyamauchi\n          </a>\n        </div>\n\n        <div class=\"lines\" title=\"7 commits changed 5 files with 166 additions and 2 deletions\">\n          <a href=\"https://github.com/napari/napari/pull/4764/files\" target=\"_blank\" rel=\"noopener\">\n            <span class=\"added\">+166</span>\n            <span class=\"removed\">-2</span>\n          </a>\n        </div>\n      </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Description\nInspired by a conversation that @jni had with @royerloic that was<span class=\"show-more-container\"><a href=\"https://github.com/napari/napari/pull/4764\" target=\"_blank\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\"> discussed during the Eurasia community meeting, This WIP PR introduces a `FrameRate` monitor that hooks into the canvas `monitor_fps()` callback. Using this, we can adaptively set rendering parameters when the framerate drops a specified value. Currently, this demo doubles the ray step size when the FPS &lt; 30 and halves the ray step size when FPS &gt; 60 (only for volume rendering).\n\nDue to the way that framerate is calculated in the `canvas.monitor_fps()` function, the `FramerateMonitor` invalidates old measurements (marks them as stale) and implements some simple debouncing to prevent transient movements from causing false positives.\n\nThere is a demoscript below that displays the current measured frame rate and ray step size as a text overlay.\n\n&lt;details&gt;&lt;summary&gt;example script&lt;/summary&gt;\n&lt;p&gt;\n\n```python\n\"\"\"\nViewer FPS label\n================\n\nDisplay a 3D volume and the fps label.\n\"\"\"\nimport time\n\nimport numpy as np\nimport napari\n\n\n\nviewer = napari.Viewer(ndisplay=3)\nimage_layer = viewer.add_image(np.random.random((700, 500, 500)), colormap='red', opacity=0.8)\nviewer.text_overlay.visible = True\n\nvisual = viewer.window._qt_window._qt_viewer.layer_to_visual[image_layer]\nnode = visual._layer_node.get_node(3)\nprint(node.relative_step_size)\n\n\ndef on_draw(event=None):\n    \"\"\"Display the current frame rate and step size as a text overlay\"\"\"\n\n    fps_monitor = viewer.window._qt_window._qt_viewer._fps_monitor\n    fps = viewer.window._qt_window._qt_viewer.canvas.fps\n    fps_valid = fps_monitor.valid\n\n    viewer.text_overlay.text = f\"{fps:1.1f} FPS, valid: {fps_valid}, step: {node.relative_step_size}\"\n\n\nviewer.window.qt_viewer.canvas.events.draw.connect(on_draw)\n\n\nif __name__ == '__main__':\n    napari.run()\n\n```\n\n&lt;/p&gt;\n&lt;/details&gt;\n\n## Type of change\n- [x] New feature (non-breaking change which adds functionality)\n\n\n# References\n\n\n\n# How has this been tested?\n\n- [ ] example: the test suite for my feature covers cases x, y, and z\n- [ ] example: all tests pass with my change\n- [ ] example: I check if my changes works with both PySide and PyQt backends\n      as there are small differences between the two Qt bindings.  \n\n## Final checklist:\n- [ ] My PR is the minimum possible work for the desired functionality\n- [ ] I have commented my code, particularly in hard-to-understand areas\n- [ ] I have made corresponding changes to the documentation\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] If I included new strings, I have used `trans.` to make them localizable.\n      For more information see our [translations guide](https://napari.org/developers/translations.html).</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>But all of this is easier if we have a target dataset + visualisation in mind.</p>\n<p>It would also be great if you could come to one or two of our <a href=\"https://napari.org/dev/community/meeting_schedule.html\">napari community meetings</a> and demonstrate the issue.</p>", "<p>Of course, here is a Drive link to a couple datasets in .tiff format<br>\n<a href=\"https://drive.google.com/drive/folders/1BZ1vtbyh3sfQHPk3Po53-BXzouaihLZt?usp=sharing\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://drive.google.com/drive/folders/1BZ1vtbyh3sfQHPk3Po53-BXzouaihLZt?usp=sharing</a></p>\n<p>The file <code>21316414z-8_scale-4.0_cdim-3_net-4_wmean--2_wstd-0.85_0.tif</code> is the one I\u2019ve been showing here.</p>\n<p>I would be happy to drop in on March 15: 8:30AM to talk about this <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Yeah, isosurface rendering is really expensive in napari, it\u2019s not well optimized. The main issue is that the shader is re-sampling the texture a <em>bunch</em> of times in order to calculate lighting, and it\u2019s not being smart about caching it.</p>", "<p>I don\u2019t think the ImageJ viewer is showing isosurfaces. That looks more like a sophisticated (2D+) transfer function to me. I\u2019d be excited to take up some work on this for napari and/or vispy <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">.</p>\n<p>This is a relevant review of transfer functions: <a href=\"https://www.diva-portal.org/smash/get/diva2:954156/FULLTEXT01.pdf\" rel=\"noopener nofollow ugc\">https://www.diva-portal.org/smash/get/diva2:954156/FULLTEXT01.pdf</a></p>", "<aside class=\"quote no-group\" data-username=\"moseyic\" data-post=\"3\" data-topic=\"77764\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/8491ac/40.png\" class=\"avatar\"> moseyic:</div>\n<blockquote>\n<p>As an aside, I\u2019ve found that the volume renderer in PyVista is butter smooth. I assume its downsampling in the background since 1024x1024x1024 is a pretty large volume.</p>\n</blockquote>\n</aside>\n<p>This surprises me, but I\u2019m not very familiar with PyVista. Do you have some sample code I can use? Memory usage exploded to &gt;25 GB when I tried to do this with a 1024 cube of random values.</p>", "<p>I can edit this with some code later. But the memory usage increase is quite manageable with 64GB of system memory. Turning on shading does slow it way down though.</p>"], "53189": ["<p>Hello ,<br>\nI have a problem when I want to export my data under Cytomap. It does not work \u2026 I can select the folder and follow the procedure described here ([<a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352/4\" class=\"inline-onebox\">There and back again, QuPath&lt;==&gt;CytoMAP cluster analysis - #4 by Mike_Nelson</a>]). Nothing is loading and I don\u2019t understand why \u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=9\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\"></p>\n<p>Thanks for your help</p>\n<p><img src=\"https://emoji.discourse-cdn.com/twitter/innocent.png?v=9\" title=\":innocent:\" class=\"emoji\" alt=\":innocent:\"> Sorry for my English, I\u2019m Frenchy <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=9\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\"></p>", "<p>Hi!</p>\n<p>Could you clarify which step or steps are not working? I do not think I understand the question. It would also help to understand what types of data you are trying to work with, and what the CSV file looks like (a screenshot would help).</p>\n<p>Are you unable to load any data into CytoMAP? Can you create a basic CSV file with only a few lines and see if that works? Which version of CytoMAP are you using?</p>\n<p>Cheers,<br>\nMike</p>", "<p>Hi <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nSo, I manage to import my data from qupath to Cytolab.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/98a799e502143a476df8837b85e1646c9bde30e5.png\" data-download-href=\"/uploads/short-url/lMrESpNHxQcUb94DQNfMhLpUPNb.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98a799e502143a476df8837b85e1646c9bde30e5_2_690x388.png\" alt=\"image\" data-base62-sha1=\"lMrESpNHxQcUb94DQNfMhLpUPNb\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98a799e502143a476df8837b85e1646c9bde30e5_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98a799e502143a476df8837b85e1646c9bde30e5_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98a799e502143a476df8837b85e1646c9bde30e5_2_1380x776.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98a799e502143a476df8837b85e1646c9bde30e5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71080 375 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I make my gates etc\u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/445100da19409a952e9fb3fdcf57cc0b71d9ce0f.jpeg\" data-download-href=\"/uploads/short-url/9Km12EJ2Sqa3sADSI8tqfXo4nZJ.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/445100da19409a952e9fb3fdcf57cc0b71d9ce0f_2_690x387.jpeg\" alt=\"image\" data-base62-sha1=\"9Km12EJ2Sqa3sADSI8tqfXo4nZJ\" width=\"690\" height=\"387\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/445100da19409a952e9fb3fdcf57cc0b71d9ce0f_2_690x387.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/445100da19409a952e9fb3fdcf57cc0b71d9ce0f_2_1035x580.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/445100da19409a952e9fb3fdcf57cc0b71d9ce0f.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/445100da19409a952e9fb3fdcf57cc0b71d9ce0f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1174\u00d7660 233 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And, when I want to export my data (to import them then into qupath), nothing happens \u2026 No file is created in the selected folder. And I have a windows bug sound <img src=\"https://emoji.discourse-cdn.com/twitter/frowning.png?v=9\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\"></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/d/4d4220e1d5e36d831b50e7c885dc009e90b6621c.png\" data-download-href=\"/uploads/short-url/b1srzGlDPKIFmOn7WIeRSeBV11i.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/d/4d4220e1d5e36d831b50e7c885dc009e90b6621c_2_690x429.png\" alt=\"image\" data-base62-sha1=\"b1srzGlDPKIFmOn7WIeRSeBV11i\" width=\"690\" height=\"429\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/d/4d4220e1d5e36d831b50e7c885dc009e90b6621c_2_690x429.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/d/4d4220e1d5e36d831b50e7c885dc009e90b6621c_2_1035x643.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/d/4d4220e1d5e36d831b50e7c885dc009e90b6621c.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/d/4d4220e1d5e36d831b50e7c885dc009e90b6621c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1092\u00d7679 172 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/2712e857d3f8259a2c18cd1708a7ed5825435fb9.png\" data-download-href=\"/uploads/short-url/5zF86iVX3vTnavCAAF0ACoEN1Gh.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/2712e857d3f8259a2c18cd1708a7ed5825435fb9_2_690x388.png\" alt=\"image\" data-base62-sha1=\"5zF86iVX3vTnavCAAF0ACoEN1Gh\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/2712e857d3f8259a2c18cd1708a7ed5825435fb9_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/2712e857d3f8259a2c18cd1708a7ed5825435fb9_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/2712e857d3f8259a2c18cd1708a7ed5825435fb9_2_1380x776.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/2712e857d3f8259a2c18cd1708a7ed5825435fb9_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71080 309 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>I am not 100% sure, but are you sure you have selected a folder? In the image, it looks like it might be looking for the folder \u201cIHC\u201d inside of the IHC folder.</p>\n<p>But there is no IHC folder there, just KI67 and Nouveau dossier folders.</p>\n<p>Might it be looking for a folder that does not exist?</p>", "<p>I tried to create a new folder, and still the same<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/b/bb2715cd38319b6b6ea919a0598524d260c74d40.jpeg\" data-download-href=\"/uploads/short-url/qHD1E3jfJYFcmu5jp5rXMDfqLbq.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bb2715cd38319b6b6ea919a0598524d260c74d40_2_690x388.jpeg\" alt=\"image\" data-base62-sha1=\"qHD1E3jfJYFcmu5jp5rXMDfqLbq\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bb2715cd38319b6b6ea919a0598524d260c74d40_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bb2715cd38319b6b6ea919a0598524d260c74d40_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bb2715cd38319b6b6ea919a0598524d260c74d40_2_1380x776.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/b/bb2715cd38319b6b6ea919a0598524d260c74d40_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71080 386 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hmm, I do not think that is something I have encountered. Could you try the export on a different computer?</p>", "<p>Oki, I will try today at work <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">  I will tell you <img src=\"https://emoji.discourse-cdn.com/twitter/crossed_fingers/2.png?v=9\" title=\":crossed_fingers:t2:\" class=\"emoji\" alt=\":crossed_fingers:t2:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/crossed_fingers/2.png?v=9\" title=\":crossed_fingers:t2:\" class=\"emoji\" alt=\":crossed_fingers:t2:\"></p>", "<p>Hi <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>I tried on another computer, it works <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"></p>\n<p>Now I have another problem .(I know\u2026).<br>\nI cannot import into qupath \u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c54b96a26857016339c18b54f57aa6f106932cf4.png\" data-download-href=\"/uploads/short-url/s9m0z1FeaqJKEQNtkd5Lb9xzRVW.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/5/c54b96a26857016339c18b54f57aa6f106932cf4.png\" alt=\"image\" data-base62-sha1=\"s9m0z1FeaqJKEQNtkd5Lb9xzRVW\" width=\"690\" height=\"486\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/5/c54b96a26857016339c18b54f57aa6f106932cf4_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1257\u00d7886 53.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18f489a6ba75670dde99ce7aefc42643b9bc4c55.png\" data-download-href=\"/uploads/short-url/3yLnutRHkgBcmTbx2GSYypewwYd.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18f489a6ba75670dde99ce7aefc42643b9bc4c55.png\" alt=\"image\" data-base62-sha1=\"3yLnutRHkgBcmTbx2GSYypewwYd\" width=\"499\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/8/18f489a6ba75670dde99ce7aefc42643b9bc4c55_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">801\u00d7802 48.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Back to the first point, it might be that you would need to run CytoMAP \u201cAs administrator\u201d or save to a different file location, some file systems protect certain areas from non-admin programs writing new files to them (security issues).</p>\n<p>As for the new issue, it looks like the CSV file is not matching up to any of the cells. I assume the cells exist in the file you are importing to? You won\u2019t be able to import into a blank QuPath image - all you are doing is editing the Measurement list of the cell objects that already exist.</p>", "<p>Hi  <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>sorry for my late response, family weekend <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>I found the solution <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nproblem 1: I reinstall my computer <img src=\"https://emoji.discourse-cdn.com/twitter/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"></p>\n<p>problem 2: I am working from an image scan in .ndpi (hamamatsu scanner), and the import was not working. So I made an annotation of my interest zone, then I exported this annotation to have a TIFF image.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/b/eba2210d66a200a0ec9fd41a0c51ed32fa433dae.png\" data-download-href=\"/uploads/short-url/xCvz43WyVATTh9OXeUbfoXgiNCC.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/b/eba2210d66a200a0ec9fd41a0c51ed32fa433dae.png\" alt=\"image\" data-base62-sha1=\"xCvz43WyVATTh9OXeUbfoXgiNCC\" width=\"509\" height=\"500\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/b/eba2210d66a200a0ec9fd41a0c51ed32fa433dae_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">534\u00d7524 23.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I opened cytomap, etc. and import the data into quPATH and it works <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/8/387c04d313f7f82955e310922d268d5b0ef990fd.png\" data-download-href=\"/uploads/short-url/83GrKb1riZlsrQa9EGPumQ4FP9b.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/8/387c04d313f7f82955e310922d268d5b0ef990fd.png\" alt=\"image\" data-base62-sha1=\"83GrKb1riZlsrQa9EGPumQ4FP9b\" width=\"690\" height=\"440\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/8/387c04d313f7f82955e310922d268d5b0ef990fd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1058\u00d7675 34.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>thank you very much for your help <img src=\"https://emoji.discourse-cdn.com/twitter/grinning.png?v=9\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/grinning.png?v=9\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\"> <img src=\"https://emoji.discourse-cdn.com/twitter/grinning.png?v=9\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\"></p>", "<p>There have been some other mentioned of NDPI files that have not worked well with QuPath, depending on the system and how recent the scanner is. Sometimes the NDPI file works when you force \u201cBioformats\u201d as the import server.</p>\n<p>If you could provide more details about any error message, a sample file (on google drive or similar), or check whether the NDPI file fails on both Openslide and Bioformats, that may help <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> in the future. Also, the operating system you are running QuPath on may be a factor as well.</p>", "<p>in fact my ndpi file works perfectly under qupath, I can make my annotations etc. I can export the measurements to open them in cytomap (all the beginning of my post), but when I want to re-import from cytomap to qupath it does not work (error message with X and Y coordinates not found).</p>\n<p>And by doing the exact same procedure with a tiff extracted from this scan, it works.</p>", "<p>Interesting, I know Openslide had some issues with the coordinate system which may impact the XY coordinates. Could you try creating a new project, opening the NDPI file with BioFormats, and testing a small area to see if the import works?</p>", "<p>It works <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>when we open the NDPI file with BioFormats, I can import my data from cytomap <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>Thank you so much <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Hello poeple, I am reactivating this topic since I got the same issue.</p>\n<p>I used to export full data csv with regions and import back to QuPath easily. After weeks, when I try to redo the same things, I was unable to export full data anymore. After select the folder, nothing happened, not even the error display in Matlab command window. I have tried both loading workplace from previous work, and loading a new table of cells then export. Still didn\u2019t work.</p>\n<p>I have checked the response to the similar issue and tried:</p>\n<ol>\n<li>Reinstall Matlab and CytoMAP</li>\n<li>Run Matlab as administrator</li>\n<li>Export to the folder in other disk</li>\n</ol>\n<p>Thanks for your help!</p>", "<p>The previous user solved it by using a different computer. It sounds like there may be security settings enabled which prevent you from writing to the specified directory, or which prevent CytoMAP from creating any new files. Aside from that, I do not know enough to make a recommendation.</p>", "<p>Thank you Mike, I will try to get another computer and see then.</p>"], "55243": ["<p>Hi! <a class=\"mention\" href=\"/u/sofroniewn\">@sofroniewn</a> or <a class=\"mention\" href=\"/u/talley\">@talley</a>  or <a class=\"mention\" href=\"/u/kevinyamauchi\">@kevinyamauchi</a> Is there a way in napari to remove and keep selected buttons on shapes layer. For example, in the below screenshot I would like to keep select, delete shapes, rectangle button and remove the rest so the user does not interact with them accidentally causing issues. I was looking into this function - <a href=\"https://github.com/napari/napari/blob/a8e618a742ef915ea3638a61592981fb17b2bd7f/napari/_qt/layer_controls/qt_shapes_controls.py#L467\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">napari/qt_shapes_controls.py at a8e618a742ef915ea3638a61592981fb17b2bd7f \u00b7 napari/napari \u00b7 GitHub</a> but can\u2019t seem to change anything despite giving a list and setting the layer to editable, maybe I am missing something on how this function is expected to be used.</p>\n<pre><code class=\"lang-auto\">    # Disabling other buttons than rectangle, select, delete\n    qtctrl = QtShapesControls(shapes_layer)\n    widget_list = [\n                'ellipse_button',\n                'line_button',\n                'path_button',\n                'polygon_button',\n                'vertex_remove_button',\n                'vertex_insert_button',\n                'move_back_button',\n                'move_front_button']\n    for wdg in widget_list:\n        print(wdg)\n        widget = getattr(qtctrl, wdg)\n        widget.setEnabled(False)\n        op = QGraphicsOpacityEffect(qtctrl)\n        op.setOpacity(0)\n        widget.setGraphicsEffect(op)\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c36f1e8a07a0fed0639bd5bd9b4fa6d423df67e.jpeg\" data-download-href=\"/uploads/short-url/hIQVn5mpObZCGo1rcKt4E0MIP8y.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c36f1e8a07a0fed0639bd5bd9b4fa6d423df67e_2_690x487.jpeg\" alt=\"image\" data-base62-sha1=\"hIQVn5mpObZCGo1rcKt4E0MIP8y\" width=\"690\" height=\"487\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c36f1e8a07a0fed0639bd5bd9b4fa6d423df67e_2_690x487.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c36f1e8a07a0fed0639bd5bd9b4fa6d423df67e_2_1035x730.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c36f1e8a07a0fed0639bd5bd9b4fa6d423df67e.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c36f1e8a07a0fed0639bd5bd9b4fa6d423df67e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1130\u00d7799 85.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"Pranathi.vemuri\" data-post=\"1\" data-topic=\"55243\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/pranathi.vemuri/40/41532_2.png\" class=\"avatar\"> Pranathi.vemuri:</div>\n<blockquote>\n<p>maybe I am missing something on how this function is expected to be used.</p>\n</blockquote>\n</aside>\n<p>like most of the stuff in the <code>_qt</code> folder, that stuff wasn\u2019t really intended for \u201cpublic\u201d usage to modify the viewer.  But it\u2019s still possible to do what you want.</p>\n<p>But if you\u2019re instantiating your <em>own</em> <code>QtShapesControls</code> (like in your example), then that\u2019s probably your issue.  You need to get the one that\u2019s actually being used in the viewer:</p>\n<pre><code class=\"lang-auto\">import napari\n\nviewer = napari.Viewer()\nshapes = viewer.add_shapes()\n\ndef lock_controls(layer, widgets=(), locked=True):\n    qctrl = viewer.window.qt_viewer.controls.widgets[layer]\n    for wdg in widget_list:\n        getattr(qctrl, wdg).setEnabled(not locked)\n        # or setVisible() if you want to just hide them completely\n\nwidget_list = [\n    'ellipse_button',\n    'line_button',\n    'path_button',\n    'polygon_button',\n    'vertex_remove_button',\n    'vertex_insert_button',\n    'move_back_button',\n    'move_front_button'\n]\n\n# lock them\nlock_controls(shapes, widget_list)\n\n# unlock them\nlock_controls(shapes, widget_list, False)\n\n</code></pre>", "<p>Oops, that makes sense on getting the existing shapes layer controls instead of instantiating one myself, thanks so much for the reply!</p>", "<p>I think though in the longer term we\u2019ll hopefully implement the functionality requested in <a href=\"https://github.com/napari/napari/issues/1570#issuecomment-679368795\" class=\"inline-onebox\">\"Lock\" layer to prevent deletion \u00b7 Issue #1570 \u00b7 napari/napari \u00b7 GitHub</a> which will allow you to \u201clock\u201d various aspects of the layer. This will prevent users from then editing things you don\u2019t want them to edit, and disable the relevant buttons, without you having to go into the widget yourself!</p>\n<p>If you wanted to start working on a PR for those features for napari <a class=\"mention\" href=\"/u/pranathi.vemuri\">@Pranathi.vemuri</a> we could definitely help you out!</p>", "<p>Thank you Talley for your very useful answer.</p>\n<p>I\u2019m new on Napari and not a good developer on PyQt. I was able to set invisible the QSlider of \u201copacity\u201d and \u201cedge width\u201d but I can\u2019t set invisible their respective labels.<br>\nDo you have any tips to solve my issue ?</p>\n<p>Thank you in advance.</p>", "<p>since this really wasn\u2019t designed to be easily editable, I hesitate to paste exact code to achieve this, since it may well change and be invalid in the future.  Instead, here are the general steps you need to do when trying to modify a Qt widget:</p>\n<ol>\n<li>find the <em>handle</em> to the widget.  That is, find in the code where the widget is created.  This is sometimes the tricky part and takes a little digging.  currently, that label is created in <a href=\"https://github.com/napari/napari/blob/e4f745f43413488440fff2f94328f21066a45dce/napari/_qt/layer_controls/qt_shapes_controls.py#L295\">this line here</a>.  (Note, in this case, the label is just created and added to the layout, so you\u2019re going to have to look at the docs of <a href=\"https://doc.qt.io/qt-5/qgridlayout.html\"><code>QGridLayout</code></a> to figure out how to access children of the layout that don\u2019t have a direct pointer.</li>\n<li>Once you get a handle to the widget, just use set <code>setVisible(False)</code> or <code>setEnabled(False)</code>\n</li>\n</ol>\n<p>I know that\u2019s not a direct answer, but as I said, this isn\u2019t a directly supported thing, so it might be best here if you do the digging through Qt to learn how to access that widget, since it may change again on you in the future.</p>", "<p>Without pasting the code, I succeeded to do something as I want removing some widgets in the gridLayout :</p>\n<pre><code class=\"lang-auto\">grid = viewer.window.qt_viewer.controls.widgets[layer].grid_layout\n   for i in range(1, grid.count()):\n       grid.itemAt(i).widget().deleteLater()\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e3ce6c3182b64dc3a8c9efcb1287e4c2915c2b41.jpeg\" data-download-href=\"/uploads/short-url/wvgDWrXQ61ERqqn09s1bXCTcf61.jpeg?dl=1\" title=\"nap\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3ce6c3182b64dc3a8c9efcb1287e4c2915c2b41_2_690x349.jpeg\" alt=\"nap\" data-base62-sha1=\"wvgDWrXQ61ERqqn09s1bXCTcf61\" width=\"690\" height=\"349\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3ce6c3182b64dc3a8c9efcb1287e4c2915c2b41_2_690x349.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3ce6c3182b64dc3a8c9efcb1287e4c2915c2b41_2_1035x523.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3ce6c3182b64dc3a8c9efcb1287e4c2915c2b41_2_1380x698.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3ce6c3182b64dc3a8c9efcb1287e4c2915c2b41_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">nap</span><span class=\"informations\">1919\u00d7973 63.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>But I can\u2019t remove inner margins in the gridLayout because it is impossible to delete rows of gridLayout. Therefore, I can\u2019t make beautiful GUI in this way.</p>\n<p>I\u2019m going to modify the Napari\u2019s code to do beautiful grid as I want.</p>", "<aside class=\"quote no-group\" data-username=\"tcotte\" data-post=\"7\" data-topic=\"55243\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/tcotte/40/52313_2.png\" class=\"avatar\"> tcotte:</div>\n<blockquote>\n<p>But I can\u2019t remove inner margins in the gridLayout because it is impossible to delete rows of gridLayout. Therefore, I can\u2019t make beautiful GUI in this way.</p>\n</blockquote>\n</aside>\n<p><a href=\"https://doc.qt.io/qt-5/qlayout.html#removeWidget\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://doc.qt.io/qt-5/qlayout.html#removeWidget</a></p>\n<p>(once you\u2019ve removed all the widgets in a row, it will take no space)</p>", "<p>It is what I also think.<br>\nSo, the size of the parent\u2019s gridLayout is fixed somewhere\u2026 Do you know where this file lie ?</p>\n<p>I have changed the code of Napari, to create this :<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c75609fd27cbbfa754a33f3792e83a3fd3ff1fb9.png\" data-download-href=\"/uploads/short-url/srplYbIYSLjQQAQkFsAeyHz1m0p.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c75609fd27cbbfa754a33f3792e83a3fd3ff1fb9_2_578x500.png\" alt=\"image\" data-base62-sha1=\"srplYbIYSLjQQAQkFsAeyHz1m0p\" width=\"578\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c75609fd27cbbfa754a33f3792e83a3fd3ff1fb9_2_578x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c75609fd27cbbfa754a33f3792e83a3fd3ff1fb9.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c75609fd27cbbfa754a33f3792e83a3fd3ff1fb9.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c75609fd27cbbfa754a33f3792e83a3fd3ff1fb9_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">766\u00d7662 101 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I\u2019m almost satisfied. Thank you for all !</p>", "<p>Hi! <a class=\"mention\" href=\"/u/talley\">@talley</a>, <a class=\"mention\" href=\"/u/sofroniewn\">@sofroniewn</a> or <a class=\"mention\" href=\"/u/kevinyamauchi\">@kevinyamauchi</a></p>\n<p>I am writing a plugin in napari and this thread really helped me to customise displayed controls. Thank you!!! However, when I am using a reference to the QT viewer:</p>\n<pre><code class=\"lang-auto\">qctrl = viewer.window.qt_viewer.controls.widgets[layer]\n</code></pre>\n<p>in the current version of napari it results in a following warning:</p>\n<p>\u201cPublic access to Window.qt_viewer is deprecated and will be removed in v0.5.0. It is considered an \u201cimplementation detail\u201d (\u2026)\u201d</p>\n<p>Can you please indicate the updated approach to disabling layer buttons in napari that won\u2019t become obsolete when the version will change??</p>", "<p>We haven\u2019t worked that out yet <a class=\"mention\" href=\"/u/jjfrackowiak\">@jjfrackowiak</a>. You should keep using this for now. My guess is that at some point we\u2019ll have a way to:</p>\n<ul>\n<li>remove all layer controls from the viewer</li>\n<li>create arbitrary buttons (including whatever controls you want) in various widgets on the viewer</li>\n</ul>\n<p>and those two combined will give you similar functionality to this. But, that doesn\u2019t exist yet so this is still the best answer, even despite the warning.</p>"], "69590": ["<p>Hi , so i have used ilastik Autocontext feature to train on 6 labels and 1 background label, (total of 7 trained classes).<br>\nThe output from ilastik is the hdf5 file that is a probability mask with 7 stacks corresponding to each training class.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/6ua1CuxpVSgHW9gXHTRpKpdX7FZ.tif\">p1_probabilities_1.tif</a> (19.2 MB)</p>\n<p>Using FIJI i can save the h5 as a multi stack TIFF, but my goal is to use CellProfiler  to  combine separate masks using  probability of (class 1 vs. background,  class 2 vs. background, \u2026 , class 7 vs background)  and layer the segmentation together using each 7 probability map.    For example if i can identify in cellProfiler the primary objects,  i can use the Class 1 vs. background mask as a filtering step (\u201cMaskObjects\u201d).    and then repeat this for each class.     so how can i unstack the probability maps returned from ilastik?    thank you again</p>", "<p>Hi,<br>\nI am having a similar problem and was wondering if you might have found a solution to this question. I have currently generated the h5 file probability map from Ilastik, but I have not been able to convert it to a multistack tiff nor generate a single cell segmentation map from the .h5 file in Cellprofiler.<br>\nBest!</p>"], "67549": ["<p>Dear all,<br>\nI am trying to add to a set of images stored in a folder the filename, but removing the extension (.jpg), which means the last 4 digits. I use MIP extension (\"multiple image processor).</p>\n<p>Below, I report the code I used:</p>\n<pre><code class=\"lang-auto\">imgName = getTitle();\nrename(substring(imgName,0,lengthOf(imgName)-4));\nsetForegroundColor(255, 0, 0);\nsetFont(\"SansSerif\", 18, \" antialiased\");\nsetColor(\"red\");\ndrawString(imgName, 78, 97);\n</code></pre>\n<p>Basically it works with the exception that it looks like it ignores the second line. I used it to remove the last 4 digits of the namefile obtained with<code> getTitle()</code>. I think I am making some kind of error of syntax.</p>\n<p>Does somebody know what can be?</p>", "<p>So as to be more clear, I add the whole macro code with the example images on the attachments:</p>\n<pre><code class=\"lang-auto\">dir1 = getDirectory(\"Choose Source Directory \");\nformat = getFormat();\ndir2 = getDirectory(\"Choose Destination Directory \");\nlist = getFileList(dir1);\nsetBatchMode(true);\nfor (i=0; i&lt;list.length; i++) {\nshowProgress(i+1, list.length);\nopen(dir1+list[i]);\n\n\n\n\n\n\n// INSERT MACRO HERE\nimgName = getTitle();\nrename(substring(imgName,0,lengthOf(imgName)-4));\nsetForegroundColor(255, 0, 0);\nsetFont(\"SansSerif\", 18, \" antialiased\");\nsetColor(\"red\");\ndrawString(imgName, 78, 97);\n\n\n\n\n\n\n\n\nif (format==\"8-bit TIFF\" || format==\"GIF\")\nconvertTo8Bit();\nsaveAs(format, dir2+list[i]);\nclose();\n}\nfunction getFormat() {\nformats = newArray(\"TIFF\", \"8-bit TIFF\", \"JPEG\", \"GIF\", \"PNG\",\n\"PGM\", \"BMP\", \"FITS\", \"Text Image\", \"ZIP\", \"Raw\");\nDialog.create(\"Batch Convert\");\nDialog.addChoice(\"Convert to: \", formats, \"TIFF\");\nDialog.show();\nreturn Dialog.getChoice();\n}\nfunction convertTo8Bit() {\nif (bitDepth==24)\nrun(\"8-bit Color\", \"number=256\");\nelse\nrun(\"8-bit\");\n}\n</code></pre>\n<p><a class=\"attachment\" href=\"/uploads/short-url/eWDkAlb7g45kuesLViMARIJ7r3H.zip\">testt.zip</a> (437.6 KB)</p>\n<p>I am lookint to remove that <code>.JPG</code> text in output folder.</p>", "<p>Hi Giacomo,</p>\n<p>You are renaming the window, but have not modified the variable imgName, so Fiji uses this unchanged variable.</p>\n<p>Change your code to this:</p>\n<pre><code class=\"lang-auto\">imgName = getTitle();\nimgName=substring(imgName,0,lengthOf(imgName)-4);\n</code></pre>\n<p>Sincerely,</p>\n<p>Matthieu</p>", "<p>It works and it is fine, thank you a lot for this!<br>\nIn my trials I made something similar but I got an error. I wrote the following code line:</p>\n<p><code>imgName=rename(substring(imgName,0,lengthOf(imgName)-4));</code></p>\n<p>Getting the following::</p>\n<pre><code class=\"lang-auto\">Memory\t*\t35MB of 6003MB (&lt;1%)\nnImages()\t*\t1\ngetTitle()\t*\t\"Cattura.JPG\"\ndir1\t*\t\"C:\\Users\\gdelbianco.DEV\\Desktop\\testt\\input\\\"\nformat\t*\t\"PNG\"\ndir2\t*\t\"C:\\Users\\gdelbianco.DEV\\Desktop\\testt\\Output\\\"\nlist\t*\tarray[2]\ni\t*\t0\nimgName\t*\t\"Cattura.JPG\"\n\n---\t\t---\nError:\t\tNumber or numeric function expected in line 18:\n\t\n\t\timgName = &lt;rename&gt; ( substring ( imgName , 0 , lengthOf ( imgName ) - 4 ) ) ;\n</code></pre>\n<p>Why <code>rename()</code> is creating this?</p>", "<p>Sorry I don\u2019t know this -I\u2019m just a user.  Maybe someone with a formal knowledge in coding will be able to explain this to you.  I would hazard that the defining a variable (imgName=) expects a result that the command \u201crename\u201d cannot possibly give, hence the error.</p>\n<p>Matthieu</p>", "<p>Hi Giacomo,</p>\n<p>Sorry, a little bit late, but there are some built-in functions that could help you with this task without spending too much effort:<br>\nFile.getNameWithoutExtension(path) : Returns the name (e.g., \u201cblobs\u201d) from <em>path</em> without the extension. Requires 1.52r.<br>\nor<br>\nFile.nameWithoutExtension** :  The name of the last file opened with the extension removed</p>\n<p>For more info , you can have a look on: <a href=\"https://imagej.nih.gov/ij/developer/macro/functions.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Built-in Macro Functions</a></p>\n<p>Best wishes,<br>\nGhazi</p>", "<p>Thank you Ghazi,<br>\nI made the macro on my own and I was successful. I didn\u2019t know about these already built-in macros.<br>\nAnyhow, your link is very interesting because I can browse in for any need and use some of them for any future need!</p>\n<p>Thank you!</p>\n<p>Giacomo</p>"], "77796": ["<p>The updated version of our paper ( <a href=\"https://forum.image.sc/t/image-analysis-for-identification-of-fat-cells-in-heterogeneous-hematoxylin-and-eosin-tissue-sections/61392\">Previous post</a> ) on fat analysis tool has recently published. Please feel free to use and comment on our tool.</p>\n<p>Tool snapshot:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b47cae96c13490f9653f39128837d7e8a5702f73.jpeg\" data-download-href=\"/uploads/short-url/pKF4CN7xJqoGUoxChHHiflbEAX9.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b47cae96c13490f9653f39128837d7e8a5702f73_2_690x388.jpeg\" alt=\"image\" data-base62-sha1=\"pKF4CN7xJqoGUoxChHHiflbEAX9\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b47cae96c13490f9653f39128837d7e8a5702f73_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b47cae96c13490f9653f39128837d7e8a5702f73_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b47cae96c13490f9653f39128837d7e8a5702f73_2_1380x776.jpeg 2x\" data-dominant-color=\"DAD7DB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71080 102 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Paper:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.sciencedirect.com/science/article/pii/S2667160323000133\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/000a88b211fc700d3929bb1e37ab763b322cfe10.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://www.sciencedirect.com/science/article/pii/S2667160323000133\" target=\"_blank\" rel=\"noopener nofollow ugc\">sciencedirect.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:500/157;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/a/4a8b08d8ee14c295e37fe378d07636aeffe81b8d.jpeg\" class=\"thumbnail\" width=\"500\" height=\"157\"></div>\n\n<h3><a href=\"https://www.sciencedirect.com/science/article/pii/S2667160323000133\" target=\"_blank\" rel=\"noopener nofollow ugc\">Automated image analysis method to detect and quantify fat cell infiltration...</a></h3>\n\n  <p>Fatty infiltration in pancreas leading to steatosis is a major risk factor in pancreas transplantation. Hematoxylin and eosin (H and E) is one of the \u2026</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Code and data:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/anniedhempe/Fatquant\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/anniedhempe/Fatquant\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ebe23a4d21a5477eb12608dbb56f775227cff6f_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ebe23a4d21a5477eb12608dbb56f775227cff6f_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ebe23a4d21a5477eb12608dbb56f775227cff6f_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ebe23a4d21a5477eb12608dbb56f775227cff6f.png 2x\" data-dominant-color=\"EBEBED\"></div>\n\n<h3><a href=\"https://github.com/anniedhempe/Fatquant\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - anniedhempe/Fatquant: An automated image analysis tool for...</a></h3>\n\n  <p>An automated image analysis tool for quantification of fat cells - GitHub - anniedhempe/Fatquant: An automated image analysis tool for quantification of fat cells</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/nehalkalita/Fatquant-GUI\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/nehalkalita/Fatquant-GUI\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58ed68d3b206740ca72e6732873d7e4ed9340abd_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58ed68d3b206740ca72e6732873d7e4ed9340abd_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/58ed68d3b206740ca72e6732873d7e4ed9340abd_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/58ed68d3b206740ca72e6732873d7e4ed9340abd.png 2x\" data-dominant-color=\"F5F5F6\"></div>\n\n<h3><a href=\"https://github.com/nehalkalita/Fatquant-GUI\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - nehalkalita/Fatquant-GUI: GUI application for the Fatquant tool</a></h3>\n\n  <p>GUI application for the Fatquant tool. Contribute to nehalkalita/Fatquant-GUI development by creating an account on GitHub.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Video tutorial:</p><div class=\"onebox lazyYT lazyYT-container\" data-youtube-id=\"ZsvcVp1jbbI\" data-youtube-title=\"Fat analysis tool for H&amp;E stained microscopic images\" data-parameters=\"feature=oembed&amp;wmode=opaque\">\n  <a href=\"https://www.youtube.com/watch?v=ZsvcVp1jbbI\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img class=\"ytp-thumbnail-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/8/3843fae54df5f35bb3663a0ca3ff94151cc1a523.jpeg\" title=\"Fat analysis tool for H&amp;E stained microscopic images\" width=\"480\" height=\"360\">\n  </a>\n</div>\n"], "12261": ["<p>I cannot seem to run the macro version of PureDenoise, when launching the plugin through a macro (run(\"PureDenoise \", \"parameters=\u20181 1\u2019 estimation=\u2018Auto Individual\u2019 \")<img src=\"//forum.image.sc/images/emoji/twitter/wink.png?v=5\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> this is the error that I\u2019m getting:</p>\n<p>[Tue Jul 31 14:57:03 CEST 2018] [ERROR] <span class=\"chcklst-box fa fa-square-o\"></span> null<br>\njava.lang.RuntimeException: Could not even fall back  to javac in the PATH<br>\nat org.scijava.minimaven.JavaCompiler.call(JavaCompiler.java:129)<br>\nat org.scijava.minimaven.MavenProject.build(MavenProject.java:538)<br>\nat org.scijava.minimaven.MavenProject.build(MavenProject.java:463)<br>\nat org.scijava.minimaven.MavenProject.build(MavenProject.java:446)<br>\nat org.scijava.plugins.scripting.java.JavaEngine.compile(JavaEngine.java:209)<br>\nat org.scijava.plugins.scripting.java.JavaEngine.eval(JavaEngine.java:136)<br>\nat org.scijava.script.ScriptModule.run(ScriptModule.java:160)<br>\nat org.scijava.module.ModuleRunner.run(ModuleRunner.java:168)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:127)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:66)<br>\nat org.scijava.thread.DefaultThreadService$3.call(DefaultThreadService.java:238)<br>\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br>\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br>\nat java.lang.Thread.run(Thread.java:745)<br>\nCaused by: java.lang.RuntimeException: java.io.IOException: Cannot run program \u201cjavac\u201d (in directory \u201c.\u201d): CreateProcess error=2, The system cannot find the file specified<br>\nat org.scijava.util.ProcessUtils.exec(ProcessUtils.java:138)<br>\nat org.scijava.util.ProcessUtils.exec(ProcessUtils.java:72)<br>\nat org.scijava.minimaven.JavaCompiler.execute(JavaCompiler.java:169)<br>\nat org.scijava.minimaven.JavaCompiler.call(JavaCompiler.java:126)<br>\n\u2026 14 more<br>\nCaused by: java.io.IOException: Cannot run program \u201cjavac\u201d (in directory \u201c.\u201d): CreateProcess error=2, The system cannot find the file specified<br>\nat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)<br>\nat java.lang.Runtime.exec(Runtime.java:620)<br>\nat org.scijava.util.ProcessUtils.exec(ProcessUtils.java:99)<br>\n\u2026 17 more<br>\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified<br>\nat java.lang.ProcessImpl.create(Native Method)<br>\nat java.lang.ProcessImpl.(ProcessImpl.java:386)<br>\nat java.lang.ProcessImpl.start(ProcessImpl.java:137)<br>\nat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)<br>\n\u2026 19 more<br>\n[Tue Jul 31 14:57:19 CEST 2018] [ERROR] <span class=\"chcklst-box fa fa-square-o\"></span> null<br>\njava.lang.RuntimeException: Could not even fall back  to javac in the PATH<br>\nat org.scijava.minimaven.JavaCompiler.call(JavaCompiler.java:129)<br>\nat org.scijava.minimaven.MavenProject.build(MavenProject.java:538)<br>\nat org.scijava.minimaven.MavenProject.build(MavenProject.java:463)<br>\nat org.scijava.minimaven.MavenProject.build(MavenProject.java:446)<br>\nat org.scijava.plugins.scripting.java.JavaEngine.compile(JavaEngine.java:209)<br>\nat org.scijava.plugins.scripting.java.JavaEngine.eval(JavaEngine.java:136)<br>\nat org.scijava.script.ScriptModule.run(ScriptModule.java:160)<br>\nat org.scijava.module.ModuleRunner.run(ModuleRunner.java:168)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:127)<br>\nat org.scijava.module.ModuleRunner.call(ModuleRunner.java:66)<br>\nat org.scijava.thread.DefaultThreadService$3.call(DefaultThreadService.java:238)<br>\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br>\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br>\nat java.lang.Thread.run(Thread.java:745)<br>\nCaused by: java.lang.RuntimeException: java.io.IOException: Cannot run program \u201cjavac\u201d (in directory \u201c.\u201d): CreateProcess error=2, The system cannot find the file specified<br>\nat org.scijava.util.ProcessUtils.exec(ProcessUtils.java:138)<br>\nat org.scijava.util.ProcessUtils.exec(ProcessUtils.java:72)<br>\nat org.scijava.minimaven.JavaCompiler.execute(JavaCompiler.java:169)<br>\nat org.scijava.minimaven.JavaCompiler.call(JavaCompiler.java:126)<br>\n\u2026 14 more<br>\nCaused by: java.io.IOException: Cannot run program \u201cjavac\u201d (in directory \u201c.\u201d): CreateProcess error=2, The system cannot find the file specified<br>\nat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)<br>\nat java.lang.Runtime.exec(Runtime.java:620)<br>\nat org.scijava.util.ProcessUtils.exec(ProcessUtils.java:99)<br>\n\u2026 17 more<br>\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified<br>\nat java.lang.ProcessImpl.create(Native Method)<br>\nat java.lang.ProcessImpl.(ProcessImpl.java:386)<br>\nat java.lang.ProcessImpl.start(ProcessImpl.java:137)<br>\nat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)<br>\n\u2026 19 more</p>\n<p>The same plugin works in plain ImageJ, I tried disabling the jre bundled with FIJI by renaming the java folder and using the system\u2019s JDK8 (downloaded today) but it fails as well.<br>\nAfter a google search I came up with another user experiencing similar issues that were caused by the removal of a java component in FIJI due to legal reasons.<br>\nI would like to keep using this plugin as it is because the results are good with the standard GUI version and it would be very useful to script the denoising in the context of a macro, as it stands this would force me to use the plain imagej lacking several useful features.</p>", "<p>I forgot to add the url for the plugin:<br>\n<a href=\"http://bigwww.epfl.ch/algorithms/denoise/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://bigwww.epfl.ch/algorithms/denoise/</a></p>\n<p><a href=\"http://bigwww.epfl.ch/algorithms/denoise/PureDenoise.zip\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://bigwww.epfl.ch/algorithms/denoise/PureDenoise.zip</a></p>\n<p>Apologies</p>", "<p>Hi DS4242,</p>\n<p>have you ever managed to solve the problem? I got a similiar error using my FIJI installation of ImageJ. With a fresh ImageJ  the macro version is working \u2018somewhat\u2019. Unfortunatly the results of the macro version are VERY different (ununsable) to the ones of the.jar plugin.</p>\n<p>Is somebody around who can shed some light on what might be gong on?!</p>\n<p>Here\u2019s a comparison of the two results from the different versions on the denoiser.</p>\n<p><a href=\"/uploads/short-url/f7oyYK835LFKqYMt2KQn9lq0feA.jpeg\">denoise_plugin_vs_macro|690x360</a></p>\n<p>Best regards, Lars</p>", "<p>I never solved this nasty issue, installed the JDK in my ubuntu system too (Version 11). As a workaround I just run the batch denoising in plain ImageJ. With other plugins distributed as source code .java files, I managed to use the run and compile in ImageJ and then transfer the compiled .class file in FIJI, but this has never worked for me either.</p>\n<p>This is a case where licensing concerns about distributing the java compiled prevailed over maintaining expected functionality.</p>", "<p>I modified the proposed macro on the BiG website into (add the three dots after \u201cPure Denoise\u201d):</p>\n<pre><code class=\"lang-auto\">run(\"URL...\", \"url=http://bigwww.epfl.ch/algorithms/denoise/dataset/Noisy-Test-Data.tif\");\nrename(\"Input PureDenoise Macro\");\n\n// Slow (multi-channnel(10)/redundant(4)/individual parameters estimation)\nselectWindow(\"Input PureDenoise Macro\");\nrun(\"PureDenoise ...\", \"parameters='10 4' estimation='Auto Individual' \");\nrename(\"Auto-Individual nf=10 cs=3\");\n</code></pre>\n<p>And it worked in FiJi.<br>\nB</p>"], "55271": ["<p>Hi!</p>\n<p>I am working with z-stacks with 44 image layers and two fluorescence channels each (.nd2 file format). I would like to analyse biofilm properties (e.g. coverage of each fluorophore) per each z-stack layer using BiofilmQ, but so far I haven\u2019t figured out how to do this. Each of my stacks represents only one location on the biofilm, and do not contain a temporal dimension. I have tried to plot the data with \u2018Frame\u2019 on the x axis, but that only produces one data point on the plot as the software seems to interpret the z-stack as one image. I have read through all the tutorials and watched the BiofilmQ workshop videos on Youtube, but I haven\u2019t come across any examples regarding this type of analysis. I also searched for similar queries here on image.sc forum, but didn\u2019t find anything that matched.</p>\n<p>Is there an option in BiofilmQ to produce these kinds of (depth-wise) plots, or would it be possible to apply a custom script to overcome the issue?</p>\n<p>Thank you in advance, and thank you for providing such a great software for biofilm analysis! <img src=\"https://emoji.discourse-cdn.com/twitter/blush.png?v=9\" title=\":blush:\" class=\"emoji\" alt=\":blush:\"></p>\n<p>Best wishes,<br>\nMirla</p>", "<p>Hi Mirla,</p>\n<p>welcome to the image.sc forum!</p>\n<p>To fully understand your problem:</p>\n<ul>\n<li>Does BiofimQ detect the 3D stack correctly (i.e. you can use the <code>Show z-stack</code> button in the <code>Image preview</code> panel)?</li>\n<li>Do you use the <code>Cubes</code> dissection for <code>Object declumping</code> during the segmentation?</li>\n<li>Isn\u2019t <code>CentroidCoordinate_z</code> the property you are looking for the x-axis?</li>\n</ul>\n<p>As far as I understand you have an unknown number of z-stacks with 44 z-planes each. You want to read the stacks separately into BiofilmQ, define a fluorescent threshold value, and quantify the segmented \u201carea\u201d for each plane in the z-stack?</p>\n<p>Best wishes,</p>\n<p>Eric</p>", "<p>Hi Eric,</p>\n<p>I have the same problem as Mirla and since she didn\u2019t answer, I don\u2019t know how to fix it.</p>\n<p>To answer your questions:</p>\n<ul>\n<li>yes I can use the Show z-stack and navigate through the different images in the preview panel.</li>\n<li>I use the cubes during segmentation on both channels before merging the second channel on the firs one.</li>\n<li>My aim is to do something similar as the 4D-XYZC-scatter plot presented on your website (in Data visualization), with one graph per frame (<a href=\"https://drescherlab.org/data/biofilmQ/docs/usage/visualization.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Data Visualization \u2014 BiofilmQ @ Drescher lab</a>). However, when I enter the same properties along the same axis, I end up with only one graph for the frame \u201c0\u201d, as if the whole stack was considered as being a single frame.</li>\n</ul>\n<p>My images are .czi files before importation, each of them being a z-stack of 30 z-planes for both channel.</p>\n<p>I hope this makes the problem clearer. Thank you in advance for your answer!</p>\n<p>Yazid</p>"], "77799": ["<p>The issue:<br>\nThe Fiji plugin \u201cVolume Viewer\u201d is awesome and I\u2019ve used it off and on for at least 5 years. BUT, in the past few weeks, it started opening the image window in a format that is too tall for the monitor. If I adjust the window size to something short (but extending somewhere off screen at the bottom), then move the window up, it automatically makes it too tall again.</p>\n<p>Thus, I can no longer access key functions at the bottom of the window.<br>\nI note that I tried to adjust ImageJ\\Edit\\Options\\Appearance\\GUI scale as a workaround, but this did not solve the problem (it just made fonts even smaller, window stayed too tall\u2026).</p>\n<p>I\u2019ve confirmed the software is up to date.<br>\nThe present version of  Fiji is as follows:<br>\n(Fiji is Just) ImageJ 2.9.0/1.54b; Java 1.8.0_322 [64 b-bit].<br>\nThis is on a windows 10 enterprise edition 64 bit pc, i.e. pretty standard.</p>\n<p>Please help. We miss being able to use this very powerful plugin.</p>", "<aside class=\"quote no-group\" data-username=\"Bryan_Huey\" data-post=\"1\" data-topic=\"77799\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bryan_huey/40/68375_2.png\" class=\"avatar\"> nanoftw:</div>\n<blockquote>\n<p>it started opening the image window in a format that is too tall for the monitor.</p>\n</blockquote>\n</aside>\n<p>Are these new types/sizes of images? Have you tried adjusting (increasing) the monitor resolution in Windows?</p>", "<p>Thanks for commenting early. There\u2019s nothing new about the images vs. prior files that worked fine (all tif stacks, few hundred pixels on a side, 20-200 layers).<br>\nDidn\u2019t play with monitor resolution, it\u2019s already maxed out. I did try on two different monitors with different resolutions, but no luck (3440x1440, and 1920x1080).</p>", "<p><a href=\"https://imagej.nih.gov/ij/plugins/volume-viewer.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://imagej.nih.gov/ij/plugins/volume-viewer.html</a><br>\nNot sure if it is updated anymore, maybe <a class=\"mention\" href=\"/u/wayne\">@Wayne</a> or <a class=\"mention\" href=\"/u/barthel\">@barthel</a> knows more about what might be happening?</p>\n<p>I see very odd behavior as well on my end, though I can end up using it, it stretches off one monitor onto several others before I can re-snap it back to the primary monitor. It flickers quite a bit before mostly settling down.</p>", "<p>You could also open an issue on the Volume viewer repository: <a href=\"https://github.com/fiji/Volume_Viewer\" class=\"inline-onebox\">GitHub - fiji/Volume_Viewer</a>.</p>\n<p>Meanwhile:</p>\n<ol>\n<li>Volume Viewer accepts macro parameters at startup (<a href=\"https://imagej.nih.gov/ij/plugins/volume-viewer.html\">documentation on plugin page</a>). You could specify the frame dimensions by starting it through a macro, like so:</li>\n</ol>\n<pre><code class=\"lang-javascript\">run(\"Volume Viewer\", \"width=500 height=500\");\n</code></pre>\n<ol start=\"2\">\n<li>If the viewer is already open, you could resize its frame by using something like this Groovy script:</li>\n</ol>\n<pre><code class=\"lang-groovy\">import java.awt.Frame\nimport org.scijava.ui.awt.AWTWindows\n\nfor (frame in Frame.getFrames()) {\n\tif (frame.getTitle().startsWith(\"Volume Viewer\")) {\n\t\tAWTWindows.ensureSizeReasonable(frame)\n\t\tAWTWindows.centerWindow(frame)\n\t\tbreak\n\t}\n}\n</code></pre>\n<p>Option 1 is likely safest.</p>", "<p>These are a great help. Sadly neither solves the problem. The window just keeps extending below the bottom of the monitor. I don\u2019t run into this problem with any other programs on my machine. Seems like I may have to revert to an older version of fiji/java.</p>", "<p>for me: some flickering/resetting, takes awhile, but never fixes itself.<br>\nbummer\u2026</p>", "<p>No help, and this should probably be a bug report, but volume viewer hasn\u2019t been working for me for a few months now. The window opens, I briefly see the slices/volume, then the whole window just turns white (with one red or green dot visible) and becomes unresponsive. (This is with Fiji / Imagej 1.53t).</p>", "<p>Unfortunately I have never used this plugin, but here are a couple of ideas:</p>\n<ol>\n<li>If all the issues you are having are related to the GUI (resizing of window, flickering, etc.): Volume viewer\u2019s GUI uses Swing. You could check if other (unrelated) Swing components are also affected by the same issues. If they are, then there could be more ways to tackle this. Some other plugins that seem to use Swing that have also not been updated in a while include Color Inspector 3D (there are likely others)</li>\n<li>You could start Fiji from the terminal (<a href=\"https://imagej.net/learn/launcher#usage\">details</a>) under debug mode: Maybe the logs will capture which exceptions being triggered.</li>\n</ol>\n<p>Either-way, yes. Opening a formal Github issue is always useful: More eyes will see it, and worse case scenario it will help make the case for plugins that are irreversible outdated or not maintained. Ultimately all this helps the community to make decisions.</p>", "<p>Hi, I had the exact same problem and I raised an issue on Github.  Greatly appreciate their effort, the issue was solved promptly.  You should test if it works for you, too.  I basically replaced the VolumeViewer  with the updated version that they just released on latest distribution of ImageJ.  Good luck!</p>"], "4082": ["<p>Hey everyone! How do I convert a .czi file to single channel .tiff files in Fiji ImageJ?</p>", "<p>Hey <a class=\"mention\" href=\"/u/kate_m\">@Kate_M</a>,</p>\n<p>Please have a look at this wonderful feature in ImageJ / Fiji (see \u201cconvert\u201d):<br>\n<a href=\"https://imagej.nih.gov/ij/docs/guide/146-29.html#toc-Subsection-29.12\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://imagej.nih.gov/ij/docs/guide/146-29.html#toc-Subsection-29.12</a></p>\n<p>You have an option to read files using Bio-Formats.</p>\n<p>Best,</p>", "<p>Hi all,</p>\n<p>We had made a small script to extract LIF files in a given folder. I\u2019ve modified it and it should work with CZI files, feel free to give it a shot. If you experience any problems, please post an example CZI file so that I may debug the issue.</p>\n<aside class=\"onebox githubgist\">\n  <header class=\"source\">\n      <a href=\"https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc\" target=\"_blank\" rel=\"nofollow noopener\">gist.github.com</a>\n  </header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc\" target=\"_blank\" rel=\"nofollow noopener\">https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc</a></h4>\n<h5>export_images_folder.ijm</h5>\n<pre><code class=\"\">/*\n * Complex Format EXPORT MACRO\n * By Olivier Burri @ EPFL - SV - PTECH - BIOP\n * Given a folder, extracts all series inside all multi-file files with given extension in new folders \n * Last edit: 13.02.2017\n */\n\n\n ////////////////////// SET PARAMETERS //////////////////////\n ////////////////////////////////////////////////////////////</code></pre>\nThis file has been truncated. <a href=\"https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc\" target=\"_blank\" rel=\"nofollow noopener\">show original</a>\n\n<p>\n</p>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Best</p>\n<p>Oli</p>", "<p>HI<br>\nI could not open the CZI images</p>", "<p>Hi Shaista,</p>\n<p>I know my answer comes late, but is it coming from a recent version of your Zeiss software?<br>\nDid you check that everything is up-to-date with ImageJ + plugin?<br>\nYou have the ability to report an error to Bio-Formats guys when you encounter some trouble opening your images:<br>\n<a href=\"https://docs.openmicroscopy.org/bio-formats/5.7.1/about/bug-reporting.html\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://docs.openmicroscopy.org/bio-formats/5.7.1/about/bug-reporting.html</a></p>\n<p>Here is the compatibility table:<br>\n<a href=\"https://docs.openmicroscopy.org/bio-formats/5.7.1/supported-formats.html#term-ratings-legend-and-definitions\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://docs.openmicroscopy.org/bio-formats/5.7.1/supported-formats.html#term-ratings-legend-and-definitions</a></p>\n<p>Best,</p>", "<p><a class=\"mention\" href=\"/u/oburri\">@oburri</a> The code works like a charm! But it gives me 8 bit tiff files, I am wondering how to tweak it to obtain a monochrome single channel 16 bit tiff for each tile</p>", "<p>That is a bit surprising, what is the original format of your data?</p>\n<p>Best</p>\n<p>Oli</p>", "<p>Sorry I thought I was dealing with 16-bit original data, It\u2019s not surprising then\u2026any recomendation about how to export 8-bit RGB tiffs then?</p>", "<p><a class=\"mention\" href=\"/u/oburri\">@oburri</a> Also, this is not related but some of the images I take are not strictly a rectangular tiling (to save microscopy time I image around the borders of the tissue and some corners are missing). is there way to either add completely black images to fill in the gaps or fix the image labelling so that stitching process has the correct left to right order?</p>", "<p>Hi Oli, the script works brilliantly but is saving each image as an individual file. Could you suggest an amendment that would save all converted czi files to one destination?</p>\n<p>Best,<br>\nOwen</p>", "<p>Hi <a class=\"mention\" href=\"/u/owen_connolly\">@Owen_Connolly</a>,</p>\n<p>I\u2019m afraid I don\u2019t understand. You have several czi files? How do you want to save them?</p>\n<p><strong>EDIT: I think I understand,</strong><br>\nPerhaps check line 20,<br>\n<code>is_save_individual_planes = true;</code></p>\n<p>If you set that to <code>false</code>, it might do what you want?</p>\n<p>Best</p>", "<p>Hi Oli, is there a way to export all channels by themselves in their colors (for me all single files show the same color) and also a merge? Thanks</p>", "<p>Hello,</p>\n<p>The color of the images is on line 55<br>\n<code>color_mode=Default</code><br>\nThis means that bioformats tries its best to find the colors as they are in the file. If they show up as the same colors, then it was not able to do it and this script doesn\u2019t try to customize the colors\u2026</p>\n<p>We do have an ActionBar called \u201cChannel Tools\u201d that does this for you after the export is done.</p>\n<p>There are instructions here on using and installing it.<br>\n<a href=\"https://c4science.ch/w/bioimaging_and_optics_platform_biop/image-processing/imagej_tools/ijab-biop_channel_tools/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://c4science.ch/w/bioimaging_and_optics_platform_biop/image-processing/imagej_tools/ijab-biop_channel_tools/</a></p>", "<p>Hi Oli. Thanks a lot for this, it is very helpful. I am trying to get the macro to convert all files in a given folder to TIFF (from CZI) but at the moment it is creating a folder for each converted file. Is there a way to get it to convert all files into TIFF and to store all these individual TIFFs together in one folder?</p>\n<p>Best</p>\n<p>Iannish</p>", "<p>Hi <a class=\"mention\" href=\"/u/isurgeon\">@isurgeon</a></p>\n<p>Why not exporting the CZI from within ZEN as TIFF, BigTIFF or OME-TIFF?</p>\n<p>This can be done from the UI, the Batch Tool or via Python Scripting.</p>", "<p>Hi <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a>  The images have already been taken a while back. If I understand correctly, you suggest opening the images again in ZEN and then export as TIFF? Yes, I guess I could do that. I was just wondering if it was possible to do it without ZEN using the macro that Oli wrote above (the reason is that I have ZEN at work but only FIJI at home).</p>\n<p>Thank you for your suggestion though, I may do this if there is no alternative.</p>", "<p>Hi <a class=\"mention\" href=\"/u/isurgeon\">@isurgeon</a></p>\n<p>You can change <a href=\"https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc#file-export_images_folder-ijm-L69\">line 69</a> to not use the folder name in the save path</p>\n<pre><code class=\"lang-diff\">- run(\"Image Sequence... \", \"format=TIFF name=[\"+fileName+\"] digits=\"+pad+\" save=[\"+dir+File.separator+dirName+File.separator+\"]\");\n+ run(\"Image Sequence... \", \"format=TIFF name=[\"+fileName+\"] digits=\"+pad+\" save=[\"+dir+File.separator+\"]\");\n</code></pre>\n<p>or <a href=\"https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc#file-export_images_folder-ijm-L72\">line 72</a> if you are saving the entire Tiff stack and not individual planes</p>\n<pre><code class=\"lang-diff\">- saveAs(\"tiff\", dir+File.separator+dirName+File.separator+fileName+\"_\"+(i+1)+\".tif\");\n+ saveAs(\"tiff\", dir+File.separator+fileName+\"_\"+(i+1)+\".tif\");\n</code></pre>\n<p>The script will still create the directories. You can stop that by commenting <a href=\"https://gist.github.com/lacan/16e12482b52f539795e49cb2122060cc#file-export_images_folder-ijm-L59\">line 59</a></p>\n<pre><code class=\"lang-diff\">- saveAs(\"tiff\", dir+File.separator+dirName+File.separator+fileName+\"_\"+(i+1)+\".tif\");\n+ // saveAs(\"tiff\", dir+File.separator+dirName+File.separator+fileName+\"_\"+(i+1)+\".tif\");</code></pre>", "<p>Hi <a class=\"mention\" href=\"/u/isurgeon\">@isurgeon</a></p>\n<p>of course you can load a CZI in Fiji using BioFormats and then export the file there as OME-TIFF. It just is one additional step.</p>", "<p>Thank you, that works beautifully!</p>", "<p>Hi <a class=\"mention\" href=\"/u/oburri\">@oburri</a></p>\n<p>Thank you so much for this. It works very well. I\u2019ve noticed that in the final file name it still contains .czi extension. Is there a way to get the title or file name without having the old extension name included?</p>\n<p>Thanks <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "77816": ["<p>Hello all,</p>\n<p>Has anyone found a convenient method to search for the labels on which a trained model makes the largest mistakes?</p>\n<p>I\u2019m interested both as a way of guiding my selection of refinement frames and to check the quality of my labeling (especially mislabeling or missing keypoints).</p>", "<p>You can run evaluation for each bodypart separately. Or modify this function <a href=\"https://github.com/DeepLabCut/DeepLabCut/blob/d71e7d50433c150f37c0ef6d1cbb7d977987540d/deeplabcut/pose_estimation_tensorflow/core/evaluate.py#L21\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DeepLabCut/evaluate.py at d71e7d50433c150f37c0ef6d1cbb7d977987540d \u00b7 DeepLabCut/DeepLabCut \u00b7 GitHub</a> and evaluation function so it outputs per bodypart rmse.</p>\n<p>Though probably it\u2019s easier to run a loop across bodyparts and do evaluation for each separately</p>", "<p>This is working great! I added the single line <code>RMSE.to_csv(resultfilename[:-3]+\"_Dists.csv\")</code> after the RMSE is calculated within the <code>evaluate_network</code> function, and quickly found a frame that had been seriously mislabeled previously due to what looks like an accidental ctrl-C <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "28666": ["<p>Hello all,<br>\nI just installed Cellprofiler in order to automatically count yeasts (stained and unstained with methylene blue) on an haemocytometer.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/oDMg4b3mh4r9HPh70uHByJWzWih.tiff\">yc2.tiff</a> (423.4 KB)</p>\n<p>I tried to adapt the script \u201cexample percentage positive\u201d but i do not discriminate the images with phase contrast, only by staining.<br>\nI am suspecting that the grid may be a problem too?</p>\n<p>Moreover, would intensity differences between alive and stained dead cells be enough to discriminate them?<br>\nThank you for your help!</p>\n<p>Have a good day</p>\n<p>ACM</p>", "<p>Hi. Did you ever get anywhere with this?</p>"], "77821": ["<p>TL;DW: It works, but human modifications improve the end result, especially given that it is not trained on any data past 2021.</p><div class=\"onebox lazyYT lazyYT-container\" data-youtube-id=\"xbVjioDmzF8\" data-youtube-title=\"Using ChatGPT (and pyvips) to convert proprietary images into open source formats\" data-parameters=\"feature=oembed&amp;wmode=opaque\">\n  <a href=\"https://www.youtube.com/watch?v=xbVjioDmzF8\" target=\"_blank\" rel=\"noopener\">\n    <img class=\"ytp-thumbnail-image\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1c5dcc251ecddc72f000f0b1a3dfd2fc0191bc3.jpeg\" title=\"Using ChatGPT (and pyvips) to convert proprietary images into open source formats\" width=\"480\" height=\"360\">\n  </a>\n</div>\n<p>\nGiven the recent trends of using ChatGPT to write programs, I figured I\u2019d give it a shot and see how it fares when tasked to convert proprietary image formats into open source ones. There have been a few forum posts such as <a href=\"https://forum.image.sc/t/how-to-distinguish-between-tumor-and-normal-cell-types-and-find-biomarkers-presence-on-whole-slide-image-in-mrxs-format/76093/14\">this</a> and <a href=\"https://forum.image.sc/t/support-for-mrxs-files-best-walkarounds/53368\">this</a>, asking for ways to convert .mrxs files into something open source, like OME tiffs. If prompted with file input and output formats, and optionally a library (like pyvips) to use, one can build a converter with relative ease.</p>\n<p>The disadvantages are largely linked to the prompt provided. Without specifying the need to trim empty whitespace, or that the path variable needs to be updated with a directory containing external .dlls, those won\u2019t be taken into consideration when the code is being generated. Furthermore, since ChatGPT has only been trained on data captured in 2021, updates to packages and their functions won\u2019t be reflected in it\u2019s knowledge base, potentially explaining why initial attempts included functions with arguments that they do not support. Human experts, such as the developer of a package, are still required.</p>\n<p>ChatGPT code:</p>\n<pre><code class=\"lang-auto\">#%% declare constants\nimport os\nvipshome = r'E:\\Img_storage\\Tutorials\\pyvips testing\\vips-dev-8.14\\bin'\nos.environ['PATH'] = vipshome + ';' + os.environ['PATH']\ninput_image=r'E:\\Img_storage\\Tutorials\\pyvips testing\\image\\CMU-3\\CMU-3.mrxs'\noutput_image=r'E:\\Img_storage\\Tutorials\\pyvips testing\\image\\CMU-3 OMEtiff\\CHATGPT_CMU-3.ome.tiff'\n#pyvips must be loaded after the environment has vips-dev-8.14\\bin in it's path\nimport pyvips\n#%% chat gpt portion\n# def mrxs_to_ome_tiff(input_file, output_file):\n#     # load the input image\n#     image = pyvips.Image.new_from_file(input_file, access='sequential')\n    \n#     # save the image as ome-tiff\n#     image.tiffsave(output_file, compression='jpeg', pyramid=True, bigtiff=True, tile=True, tile_width=256, tile_height=256)\n\n\ndef mrxs_to_ome_tiff(input_file, output_file):\n    # load the input image\n    image = pyvips.Image.new_from_file(input_file, access='sequential')\n    \n    # get the metadata from the input image\n    metadata = image.get_fields()\n    \n    # save the image as ome-tiff with the metadata\n    image.tiffsave(output_file, compression='jpeg', pyramid=True, bigtiff=True, tile=True, tile_width=256, tile_height=256, sample_format='uint16', Xres=metadata.xres, Yres=metadata.yres, Xoffset=metadata.xoffset, Yoffset=metadata.yoffset)\n\n\nmrxs_to_ome_tiff(input_image, output_image)\n</code></pre>\n<p>Modified human-written code, adapted from <a href=\"https://forum.image.sc/t/writing-qupath-bio-formats-compatible-pyramidal-image-with-libvips/51223/14\">this</a> post:</p>\n<pre><code class=\"lang-auto\">#%% load libraries and set constants\nimport os\nvipshome = r'E:\\Img_storage\\Tutorials\\pyvips testing\\vips-dev-8.14\\bin'\nos.environ['PATH'] = vipshome + ';' + os.environ['PATH']\ninput_image=r'E:\\Img_storage\\Tutorials\\pyvips testing\\image\\CMU-3\\CMU-3.mrxs'\noutput_image=r'E:\\Img_storage\\Tutorials\\pyvips testing\\image\\CMU-3 OMEtiff\\CMU-3.ome.tiff'\n#pyvips must be loaded after the environment has vips-dev-8.14\\bin in it's path\nimport pyvips\n\n#%% load mrxs file\n# the \"rgb=True\" will make openslide drop the alpha from the image, saving a\n# useful chunk of memory\nim = pyvips.Image.new_from_file(input_image, rgb=False,autocrop=True)\n\n#%% process image and extract metadata\nimage_height = im.height\n\n# split to separate image planes and stack vertically ready for OME \n#im = pyvips.Image.arrayjoin(im.bandsplit(), across=1)\n\n# set minimal OME metadata\n# before we can modify an image (set metadata in this case), we must take a \n# private copy\nim = im.copy()\nim.set_type(pyvips.GValue.gint_type, \"page-height\", image_height)\nim.set_type(pyvips.GValue.gstr_type, \"image-description\",\nf\"\"\"&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;OME xmlns=\"http://www.openmicroscopy.org/Schemas/OME/2016-06\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://www.openmicroscopy.org/Schemas/OME/2016-06 http://www.openmicroscopy.org/Schemas/OME/2016-06/ome.xsd\"&gt;\n    &lt;Image ID=\"Image:0\"&gt;\n        &lt;!-- Minimum required fields about image dimensions --&gt;\n        &lt;Pixels DimensionOrder=\"XYCZT\"\n                ID=\"Pixels:0\"\n                SizeC=\"3\"\n                SizeT=\"1\"\n                SizeX=\"{im.width}\"\n                SizeY=\"{image_height}\"\n                SizeZ=\"1\"\n                Type=\"uint8\"&gt;\n        &lt;/Pixels&gt;\n    &lt;/Image&gt;\n&lt;/OME&gt;\"\"\")\n#%% Write image\nim.tiffsave(output_image, compression=\"jpeg\", tile=True,\n            tile_width=512, tile_height=512,\n            pyramid=True, subifd=True)\n</code></pre>\n<p>Extremely grateful for the amazing work of <a class=\"mention\" href=\"/u/jcupitt\">@jcupitt</a> for developing and actively maintaining the <a href=\"https://pypi.org/project/pyvips/\">pyvips</a> package, of which ChatGPT would not have been able to make the libvips-based conversion without. The above scripts used the \u201cNon-Conda install\u201d method using binaries from <a href=\"https://www.libvips.org/install.html\">libvips</a></p>", "<p>What was the prompt you gave to ChatGPT to create its code?</p>\n<p>I\u2019ve tried this with other specialized code and found that you have to be very specific, and even then, it usually makes mistakes that need careful scrutiny to catch.</p>", "<p>The prompt was \u201cWrite a program in python that will convert a .mrxs image file into the .ome.tiff, using the pyvips package\u201d.</p>\n<p>If I didn\u2019t specify a library, and instead prompted with \u201ccreate a script to convert .mrxs to .ome.tiff\u201d, the output would be code using the bioformats library:</p>\n<pre><code class=\"lang-auto\">import bioformats as bf\nimport javabridge as jv\nimport os\n\n# Initialize Java Virtual Machine\njv.start_vm(class_path=bf.JARS)\n\n# Specify input and output directories\ninput_dir = \"/path/to/input/dir\"\noutput_dir = \"/path/to/output/dir\"\n\n# Loop through all .mrxs files in the input directory\nfor filename in os.listdir(input_dir):\n    if filename.endswith(\".mrxs\"):\n        # Construct input and output file paths\n        input_path = os.path.join(input_dir, filename)\n        output_filename = os.path.splitext(filename)[0] + \".ome.tiff\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        # Use Bio-Formats to convert the file\n        bf.convert(input_path, output_path, format=\"OME-TIFF\")\n\n# Shutdown Java Virtual Machine\njv.kill_vm()\n</code></pre>\n<p>The summary of the above untested script is as follows:</p>\n<pre><code class=\"lang-auto\">To convert .mrxs to .ome.tiff, we will need to use Bio-Formats, a Java library that supports a wide range of microscopy file formats. Here's a Python script that uses the bioformats package to perform the conversion:\n\npython\n\nimport bioformats as bf\nimport javabridge as jv\nimport os\n\n# Initialize Java Virtual Machine\njv.start_vm(class_path=bf.JARS)\n\n# Specify input and output directories\ninput_dir = \"/path/to/input/dir\"\noutput_dir = \"/path/to/output/dir\"\n\n# Loop through all .mrxs files in the input directory\nfor filename in os.listdir(input_dir):\n    if filename.endswith(\".mrxs\"):\n        # Construct input and output file paths\n        input_path = os.path.join(input_dir, filename)\n        output_filename = os.path.splitext(filename)[0] + \".ome.tiff\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        # Use Bio-Formats to convert the file\n        bf.convert(input_path, output_path, format=\"OME-TIFF\")\n\n# Shutdown Java Virtual Machine\njv.kill_vm()\n</code></pre>\n<p>Here\u2019s a breakdown of what the script does:</p>\n<blockquote>\n<pre><code>Imports the necessary packages (bioformats and javabridge).\nStarts the Java Virtual Machine (JVM) using javabridge.start_vm(). This is necessary because Bio-Formats is a Java library and requires a running JVM.\nSpecifies the input and output directories. Modify these paths to match your setup.\nLoops through all .mrxs files in the input directory.\nConstructs the input and output file paths based on the current file being processed.\nUses the bioformats.convert() function to convert the input file to OME-TIFF format and save it to the output file path.\nAfter all files have been processed, shuts down the JVM using javabridge.kill_vm().\n</code></pre>\n<p>Note that Bio-Formats supports many other input and output formats besides MRXS and OME-TIFF, so you can modify the script to convert between other file formats as needed. You can find more information about Bio-Formats and its capabilities in the official documentation: <a href=\"https://docs.openmicroscopy.org/bio-formats/latest/users/index.html\" class=\"inline-onebox\">User Information \u2014 Bio-Formats 6.12.0 documentation</a></p>\n</blockquote>\n<p>I imagine why the code shown in the video (and potentially this one too) was relatively successful with minimum alterations was due to the fact that there have been likely dozens of posts on a variety of forums, asking for ways to convert .mrxs files, coupled with publicly available documentation on libraries that can accomplish this. And given that it was trained with web-scraped data from 2021, its possible that it learned enough to write basic conversion tools for converting between image formats. In fact, I wonder if the above script also works, and if not, the extent of modifications required to get it to do so.</p>\n<p>I guess the unanswered question is \u201chow specialized is too specialized for ChatGPT?\u201d</p>", "<aside class=\"quote no-group\" data-username=\"Mark_Zaidi\" data-post=\"3\" data-topic=\"77821\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mark_zaidi/40/37001_2.png\" class=\"avatar\"> Mark Zaidi:</div>\n<blockquote>\n<p>I guess the unanswered question is \u201chow specialized is too specialized for ChatGPT?\u201d</p>\n</blockquote>\n</aside>\n<p>In my opinion, that may be the wrong question to ask of ChatGPT. At its heart, ChatGPT is a language model, which as far as my limited understanding goes, it just puts together strings of phrases or words that have high probability of being together depending on the prompt.</p>\n<p>In that sense, ChatGPT isn\u2019t really specialised for writing code or scripts, just good at suggesting and stringing words/language that are likely to go together, based on what has been scraped and it has trained on, which may not be accurate or correct.</p>\n<p>Since code is kind of its own language, it is thus not so surprising that ChatGPT can write convincing codes/scripts and sometimes gets it right!</p>", "<p>Oh wow, I never thought of it like that! I suppose grammatical syntax in sentences serves the same purpose as programmatic syntax, just with a different \u201cvocabulary\u201d</p>\n<blockquote>\n<p>it just puts together strings of phrases or words that have high probability of being together depending on the prompt</p>\n</blockquote>\n<p>My understanding is that it uses a supervised approach rather than probability of phrase patterns, such that a prompt from training is randomly taken and passed to a human \u201ctrainer\u201d, who provides an answer, which is then used to train the dataset:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c83e39b19ce5931eef8f8ac4815ec474e9a8321.png\" data-download-href=\"/uploads/short-url/dcquTDeaTorIa3SYD3thhheNAeR.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c83e39b19ce5931eef8f8ac4815ec474e9a8321_2_690x390.png\" alt=\"image\" data-base62-sha1=\"dcquTDeaTorIa3SYD3thhheNAeR\" width=\"690\" height=\"390\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/c/5c83e39b19ce5931eef8f8ac4815ec474e9a8321_2_690x390.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c83e39b19ce5931eef8f8ac4815ec474e9a8321.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/c/5c83e39b19ce5931eef8f8ac4815ec474e9a8321.png 2x\" data-dominant-color=\"F1F2F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">964\u00d7545 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n(from <a href=\"https://openai.com/blog/chatgpt\" class=\"inline-onebox\">Introducing ChatGPT</a>)</p>\n<p>What I want to know more about is who these \u201clabelers\u201d are. They could be human volunteers, and if that\u2019s the case, then they may have more programming experience than the typical person, given that one could assume if you\u2019re volunteering for a project like this, then you may have some familiarity or experience with artificial intelligence. As a result, perhaps they can better correctly identify and correct erroneous or nonsensical code generated by ChatGPT, and in doing so, essentially train ChatGPT to write code that can work (assuming it\u2019s relatively basic and using well known libraries).</p>\n<p>But at that point, I\u2019m just speculating as to why ChatGPT can (on occasion) write functional code. Would be interesting to see a variant of ChatGPT emerge, where all the prompts are coding questions taken from StackOverflow or similar forums, and the labelers are individuals with programming backgrounds.</p>", "<p>Here\u2019s an article about how OpenAI is hiring programmers:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.semafor.com/article/01/27/2023/openai-has-hired-an-army-of-contractors-to-make-basic-coding-obsolete\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bdf2c62b584ef07b45888283cc8e02190b73e83.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.semafor.com/article/01/27/2023/openai-has-hired-an-army-of-contractors-to-make-basic-coding-obsolete\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"05:19PM - 27 January 2023\">semafor.com \u2013 27 Jan 23</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/361;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8bd128e3b796318d6a7a66d3452eec3976f1d592_2_690x361.png\" class=\"thumbnail\" width=\"690\" height=\"361\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8bd128e3b796318d6a7a66d3452eec3976f1d592_2_690x361.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/b/8bd128e3b796318d6a7a66d3452eec3976f1d592_2_1035x541.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/b/8bd128e3b796318d6a7a66d3452eec3976f1d592.png 2x\" data-dominant-color=\"6E6C8E\"></div>\n\n<h3><a href=\"https://www.semafor.com/article/01/27/2023/openai-has-hired-an-army-of-contractors-to-make-basic-coding-obsolete\" target=\"_blank\" rel=\"noopener nofollow ugc\">OpenAI has hired an army of contractors to make basic coding obsolete | Semafor</a></h3>\n\n  <p>The company behind ChatGPT now employs around 1,000 people around the world to label data and help OpenAI\u2019s models learn software engineering tasks.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<blockquote>\n<p>About 60% of the contractors were hired to do what\u2019s called \u201cdata labeling\u201d \u2014 creating massive sets of images, audio clips, and other information that can then be used to train artificial intelligence tools or <a href=\"https://restofworld.org/2021/self-driving-cars-outsourcing/\">autonomous vehicles</a>.</p>\n</blockquote>\n<p>Interesting, seems like there\u2019s definitely a possibility that the data labelers may have some programming background. But then again, other articles allege that labeling was outsourced to Kenyan workers for &lt;$2/hr (<a href=\"https://time.com/6247678/openai-chatgpt-kenya-workers/\" class=\"inline-onebox\">OpenAI Used Kenyan Workers on Less Than $2 Per Hour: Exclusive | Time</a>)</p>", "<p>That would be the labelling of offensive content. Outsourcing that to the cheapest available contractors has been standard for several years now.  Wired did an article years ago about how this is done for Facebook, and the toll it takes on the people who do those jobs.</p>\n<p>The new twist is hiring coders to improve the programming advice.  They\u2019re still trying to find coders in the least expensive parts of the world they can.</p>", "<p>I\u2019ll go ahead and link this here since the article opens with chatGPT <a href=\"https://forum.image.sc/t/ai-in-pathology/78368\" class=\"inline-onebox\">AI in Pathology</a></p>"]}